{
    "language": "python",
    "commit_url": "https://github.com/comfyanonymous/ComfyUI/commit/d7430a1651a300e8230867ce3e6d86cc0101facc",
    "commit_message": "Add a way to load the diffusion model in fp8 with UNETLoader node.",
    "commit_snapshots": {
        "comfy/sd.py": [
            [
                "import torch\n",
                "from enum import Enum\n",
                "import logging\n",
                "\n",
                "from comfy import model_management\n",
                "from .ldm.models.autoencoder import AutoencoderKL, AutoencodingEngine\n",
                "from .ldm.cascade.stage_a import StageA\n",
                "from .ldm.cascade.stage_c_coder import StageC_coder\n",
                "from .ldm.audio.autoencoder import AudioOobleckVAE\n",
                "import yaml\n",
                "\n",
                "import comfy.utils\n",
                "\n",
                "from . import clip_vision\n",
                "from . import gligen\n",
                "from . import diffusers_convert\n",
                "from . import model_detection\n",
                "\n",
                "from . import sd1_clip\n",
                "from . import sdxl_clip\n",
                "import comfy.text_encoders.sd2_clip\n",
                "import comfy.text_encoders.sd3_clip\n",
                "import comfy.text_encoders.sa_t5\n",
                "import comfy.text_encoders.aura_t5\n",
                "import comfy.text_encoders.hydit\n",
                "import comfy.text_encoders.flux\n",
                "\n",
                "import comfy.model_patcher\n",
                "import comfy.lora\n",
                "import comfy.t2i_adapter.adapter\n",
                "import comfy.supported_models_base\n",
                "import comfy.taesd.taesd\n",
                "\n",
                "def load_lora_for_models(model, clip, lora, strength_model, strength_clip):\n",
                "    key_map = {}\n",
                "    if model is not None:\n",
                "        key_map = comfy.lora.model_lora_keys_unet(model.model, key_map)\n",
                "    if clip is not None:\n",
                "        key_map = comfy.lora.model_lora_keys_clip(clip.cond_stage_model, key_map)\n",
                "\n",
                "    loaded = comfy.lora.load_lora(lora, key_map)\n",
                "    if model is not None:\n",
                "        new_modelpatcher = model.clone()\n",
                "        k = new_modelpatcher.add_patches(loaded, strength_model)\n",
                "    else:\n",
                "        k = ()\n",
                "        new_modelpatcher = None\n",
                "\n",
                "    if clip is not None:\n",
                "        new_clip = clip.clone()\n",
                "        k1 = new_clip.add_patches(loaded, strength_clip)\n",
                "    else:\n",
                "        k1 = ()\n",
                "        new_clip = None\n",
                "    k = set(k)\n",
                "    k1 = set(k1)\n",
                "    for x in loaded:\n",
                "        if (x not in k) and (x not in k1):\n",
                "            logging.warning(\"NOT LOADED {}\".format(x))\n",
                "\n",
                "    return (new_modelpatcher, new_clip)\n",
                "\n",
                "\n",
                "class CLIP:\n",
                "    def __init__(self, target=None, embedding_directory=None, no_init=False, tokenizer_data={}):\n",
                "        if no_init:\n",
                "            return\n",
                "        params = target.params.copy()\n",
                "        clip = target.clip\n",
                "        tokenizer = target.tokenizer\n",
                "\n",
                "        load_device = model_management.text_encoder_device()\n",
                "        offload_device = model_management.text_encoder_offload_device()\n",
                "        params['device'] = offload_device\n",
                "        dtype = model_management.text_encoder_dtype(load_device)\n",
                "        params['dtype'] = dtype\n",
                "\n",
                "        self.cond_stage_model = clip(**(params))\n",
                "\n",
                "        for dt in self.cond_stage_model.dtypes:\n",
                "            if not model_management.supports_cast(load_device, dt):\n",
                "                load_device = offload_device\n",
                "\n",
                "        self.tokenizer = tokenizer(embedding_directory=embedding_directory, tokenizer_data=tokenizer_data)\n",
                "        self.patcher = comfy.model_patcher.ModelPatcher(self.cond_stage_model, load_device=load_device, offload_device=offload_device)\n",
                "        self.layer_idx = None\n",
                "        logging.debug(\"CLIP model load device: {}, offload device: {}\".format(load_device, offload_device))\n",
                "\n",
                "    def clone(self):\n",
                "        n = CLIP(no_init=True)\n",
                "        n.patcher = self.patcher.clone()\n",
                "        n.cond_stage_model = self.cond_stage_model\n",
                "        n.tokenizer = self.tokenizer\n",
                "        n.layer_idx = self.layer_idx\n",
                "        return n\n",
                "\n",
                "    def add_patches(self, patches, strength_patch=1.0, strength_model=1.0):\n",
                "        return self.patcher.add_patches(patches, strength_patch, strength_model)\n",
                "\n",
                "    def clip_layer(self, layer_idx):\n",
                "        self.layer_idx = layer_idx\n",
                "\n",
                "    def tokenize(self, text, return_word_ids=False):\n",
                "        return self.tokenizer.tokenize_with_weights(text, return_word_ids)\n",
                "\n",
                "    def encode_from_tokens(self, tokens, return_pooled=False, return_dict=False):\n",
                "        self.cond_stage_model.reset_clip_options()\n",
                "\n",
                "        if self.layer_idx is not None:\n",
                "            self.cond_stage_model.set_clip_options({\"layer\": self.layer_idx})\n",
                "\n",
                "        if return_pooled == \"unprojected\":\n",
                "            self.cond_stage_model.set_clip_options({\"projected_pooled\": False})\n",
                "\n",
                "        self.load_model()\n",
                "        o = self.cond_stage_model.encode_token_weights(tokens)\n",
                "        cond, pooled = o[:2]\n",
                "        if return_dict:\n",
                "            out = {\"cond\": cond, \"pooled_output\": pooled}\n",
                "            if len(o) > 2:\n",
                "                for k in o[2]:\n",
                "                    out[k] = o[2][k]\n",
                "            return out\n",
                "\n",
                "        if return_pooled:\n",
                "            return cond, pooled\n",
                "        return cond\n",
                "\n",
                "    def encode(self, text):\n",
                "        tokens = self.tokenize(text)\n",
                "        return self.encode_from_tokens(tokens)\n",
                "\n",
                "    def load_sd(self, sd, full_model=False):\n",
                "        if full_model:\n",
                "            return self.cond_stage_model.load_state_dict(sd, strict=False)\n",
                "        else:\n",
                "            return self.cond_stage_model.load_sd(sd)\n",
                "\n",
                "    def get_sd(self):\n",
                "        sd_clip = self.cond_stage_model.state_dict()\n",
                "        sd_tokenizer = self.tokenizer.state_dict()\n",
                "        for k in sd_tokenizer:\n",
                "            sd_clip[k] = sd_tokenizer[k]\n",
                "        return sd_clip\n",
                "\n",
                "    def load_model(self):\n",
                "        model_management.load_model_gpu(self.patcher)\n",
                "        return self.patcher\n",
                "\n",
                "    def get_key_patches(self):\n",
                "        return self.patcher.get_key_patches()\n",
                "\n",
                "class VAE:\n",
                "    def __init__(self, sd=None, device=None, config=None, dtype=None):\n",
                "        if 'decoder.up_blocks.0.resnets.0.norm1.weight' in sd.keys(): #diffusers format\n",
                "            sd = diffusers_convert.convert_vae_state_dict(sd)\n",
                "\n",
                "        self.memory_used_encode = lambda shape, dtype: (1767 * shape[2] * shape[3]) * model_management.dtype_size(dtype) #These are for AutoencoderKL and need tweaking (should be lower)\n",
                "        self.memory_used_decode = lambda shape, dtype: (2178 * shape[2] * shape[3] * 64) * model_management.dtype_size(dtype)\n",
                "        self.downscale_ratio = 8\n",
                "        self.upscale_ratio = 8\n",
                "        self.latent_channels = 4\n",
                "        self.output_channels = 3\n",
                "        self.process_input = lambda image: image * 2.0 - 1.0\n",
                "        self.process_output = lambda image: torch.clamp((image + 1.0) / 2.0, min=0.0, max=1.0)\n",
                "        self.working_dtypes = [torch.bfloat16, torch.float32]\n",
                "\n",
                "        if config is None:\n",
                "            if \"decoder.mid.block_1.mix_factor\" in sd:\n",
                "                encoder_config = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n",
                "                decoder_config = encoder_config.copy()\n",
                "                decoder_config[\"video_kernel_size\"] = [3, 1, 1]\n",
                "                decoder_config[\"alpha\"] = 0.0\n",
                "                self.first_stage_model = AutoencodingEngine(regularizer_config={'target': \"comfy.ldm.models.autoencoder.DiagonalGaussianRegularizer\"},\n",
                "                                                            encoder_config={'target': \"comfy.ldm.modules.diffusionmodules.model.Encoder\", 'params': encoder_config},\n",
                "                                                            decoder_config={'target': \"comfy.ldm.modules.temporal_ae.VideoDecoder\", 'params': decoder_config})\n",
                "            elif \"taesd_decoder.1.weight\" in sd:\n",
                "                self.latent_channels = sd[\"taesd_decoder.1.weight\"].shape[1]\n",
                "                self.first_stage_model = comfy.taesd.taesd.TAESD(latent_channels=self.latent_channels)\n",
                "            elif \"vquantizer.codebook.weight\" in sd: #VQGan: stage a of stable cascade\n",
                "                self.first_stage_model = StageA()\n",
                "                self.downscale_ratio = 4\n",
                "                self.upscale_ratio = 4\n",
                "                #TODO\n",
                "                #self.memory_used_encode\n",
                "                #self.memory_used_decode\n",
                "                self.process_input = lambda image: image\n",
                "                self.process_output = lambda image: image\n",
                "            elif \"backbone.1.0.block.0.1.num_batches_tracked\" in sd: #effnet: encoder for stage c latent of stable cascade\n",
                "                self.first_stage_model = StageC_coder()\n",
                "                self.downscale_ratio = 32\n",
                "                self.latent_channels = 16\n",
                "                new_sd = {}\n",
                "                for k in sd:\n",
                "                    new_sd[\"encoder.{}\".format(k)] = sd[k]\n",
                "                sd = new_sd\n",
                "            elif \"blocks.11.num_batches_tracked\" in sd: #previewer: decoder for stage c latent of stable cascade\n",
                "                self.first_stage_model = StageC_coder()\n",
                "                self.latent_channels = 16\n",
                "                new_sd = {}\n",
                "                for k in sd:\n",
                "                    new_sd[\"previewer.{}\".format(k)] = sd[k]\n",
                "                sd = new_sd\n",
                "            elif \"encoder.backbone.1.0.block.0.1.num_batches_tracked\" in sd: #combined effnet and previewer for stable cascade\n",
                "                self.first_stage_model = StageC_coder()\n",
                "                self.downscale_ratio = 32\n",
                "                self.latent_channels = 16\n",
                "            elif \"decoder.conv_in.weight\" in sd:\n",
                "                #default SD1.x/SD2.x VAE parameters\n",
                "                ddconfig = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n",
                "\n",
                "                if 'encoder.down.2.downsample.conv.weight' not in sd and 'decoder.up.3.upsample.conv.weight' not in sd: #Stable diffusion x4 upscaler VAE\n",
                "                    ddconfig['ch_mult'] = [1, 2, 4]\n",
                "                    self.downscale_ratio = 4\n",
                "                    self.upscale_ratio = 4\n",
                "\n",
                "                self.latent_channels = ddconfig['z_channels'] = sd[\"decoder.conv_in.weight\"].shape[1]\n",
                "                if 'quant_conv.weight' in sd:\n",
                "                    self.first_stage_model = AutoencoderKL(ddconfig=ddconfig, embed_dim=4)\n",
                "                else:\n",
                "                    self.first_stage_model = AutoencodingEngine(regularizer_config={'target': \"comfy.ldm.models.autoencoder.DiagonalGaussianRegularizer\"},\n",
                "                                                                encoder_config={'target': \"comfy.ldm.modules.diffusionmodules.model.Encoder\", 'params': ddconfig},\n",
                "                                                                decoder_config={'target': \"comfy.ldm.modules.diffusionmodules.model.Decoder\", 'params': ddconfig})\n",
                "            elif \"decoder.layers.1.layers.0.beta\" in sd:\n",
                "                self.first_stage_model = AudioOobleckVAE()\n",
                "                self.memory_used_encode = lambda shape, dtype: (1000 * shape[2]) * model_management.dtype_size(dtype)\n",
                "                self.memory_used_decode = lambda shape, dtype: (1000 * shape[2] * 2048) * model_management.dtype_size(dtype)\n",
                "                self.latent_channels = 64\n",
                "                self.output_channels = 2\n",
                "                self.upscale_ratio = 2048\n",
                "                self.downscale_ratio =  2048\n",
                "                self.process_output = lambda audio: audio\n",
                "                self.process_input = lambda audio: audio\n",
                "                self.working_dtypes = [torch.float16, torch.bfloat16, torch.float32]\n",
                "            else:\n",
                "                logging.warning(\"WARNING: No VAE weights detected, VAE not initalized.\")\n",
                "                self.first_stage_model = None\n",
                "                return\n",
                "        else:\n",
                "            self.first_stage_model = AutoencoderKL(**(config['params']))\n",
                "        self.first_stage_model = self.first_stage_model.eval()\n",
                "\n",
                "        m, u = self.first_stage_model.load_state_dict(sd, strict=False)\n",
                "        if len(m) > 0:\n",
                "            logging.warning(\"Missing VAE keys {}\".format(m))\n",
                "\n",
                "        if len(u) > 0:\n",
                "            logging.debug(\"Leftover VAE keys {}\".format(u))\n",
                "\n",
                "        if device is None:\n",
                "            device = model_management.vae_device()\n",
                "        self.device = device\n",
                "        offload_device = model_management.vae_offload_device()\n",
                "        if dtype is None:\n",
                "            dtype = model_management.vae_dtype(self.device, self.working_dtypes)\n",
                "        self.vae_dtype = dtype\n",
                "        self.first_stage_model.to(self.vae_dtype)\n",
                "        self.output_device = model_management.intermediate_device()\n",
                "\n",
                "        self.patcher = comfy.model_patcher.ModelPatcher(self.first_stage_model, load_device=self.device, offload_device=offload_device)\n",
                "        logging.debug(\"VAE load device: {}, offload device: {}, dtype: {}\".format(self.device, offload_device, self.vae_dtype))\n",
                "\n",
                "    def vae_encode_crop_pixels(self, pixels):\n",
                "        dims = pixels.shape[1:-1]\n",
                "        for d in range(len(dims)):\n",
                "            x = (dims[d] // self.downscale_ratio) * self.downscale_ratio\n",
                "            x_offset = (dims[d] % self.downscale_ratio) // 2\n",
                "            if x != dims[d]:\n",
                "                pixels = pixels.narrow(d + 1, x_offset, x)\n",
                "        return pixels\n",
                "\n",
                "    def decode_tiled_(self, samples, tile_x=64, tile_y=64, overlap = 16):\n",
                "        steps = samples.shape[0] * comfy.utils.get_tiled_scale_steps(samples.shape[3], samples.shape[2], tile_x, tile_y, overlap)\n",
                "        steps += samples.shape[0] * comfy.utils.get_tiled_scale_steps(samples.shape[3], samples.shape[2], tile_x // 2, tile_y * 2, overlap)\n",
                "        steps += samples.shape[0] * comfy.utils.get_tiled_scale_steps(samples.shape[3], samples.shape[2], tile_x * 2, tile_y // 2, overlap)\n",
                "        pbar = comfy.utils.ProgressBar(steps)\n",
                "\n",
                "        decode_fn = lambda a: self.first_stage_model.decode(a.to(self.vae_dtype).to(self.device)).float()\n",
                "        output = self.process_output(\n",
                "            (comfy.utils.tiled_scale(samples, decode_fn, tile_x // 2, tile_y * 2, overlap, upscale_amount = self.upscale_ratio, output_device=self.output_device, pbar = pbar) +\n",
                "            comfy.utils.tiled_scale(samples, decode_fn, tile_x * 2, tile_y // 2, overlap, upscale_amount = self.upscale_ratio, output_device=self.output_device, pbar = pbar) +\n",
                "             comfy.utils.tiled_scale(samples, decode_fn, tile_x, tile_y, overlap, upscale_amount = self.upscale_ratio, output_device=self.output_device, pbar = pbar))\n",
                "            / 3.0)\n",
                "        return output\n",
                "\n",
                "    def decode_tiled_1d(self, samples, tile_x=128, overlap=32):\n",
                "        decode_fn = lambda a: self.first_stage_model.decode(a.to(self.vae_dtype).to(self.device)).float()\n",
                "        return comfy.utils.tiled_scale_multidim(samples, decode_fn, tile=(tile_x,), overlap=overlap, upscale_amount=self.upscale_ratio, out_channels=self.output_channels, output_device=self.output_device)\n",
                "\n",
                "    def encode_tiled_(self, pixel_samples, tile_x=512, tile_y=512, overlap = 64):\n",
                "        steps = pixel_samples.shape[0] * comfy.utils.get_tiled_scale_steps(pixel_samples.shape[3], pixel_samples.shape[2], tile_x, tile_y, overlap)\n",
                "        steps += pixel_samples.shape[0] * comfy.utils.get_tiled_scale_steps(pixel_samples.shape[3], pixel_samples.shape[2], tile_x // 2, tile_y * 2, overlap)\n",
                "        steps += pixel_samples.shape[0] * comfy.utils.get_tiled_scale_steps(pixel_samples.shape[3], pixel_samples.shape[2], tile_x * 2, tile_y // 2, overlap)\n",
                "        pbar = comfy.utils.ProgressBar(steps)\n",
                "\n",
                "        encode_fn = lambda a: self.first_stage_model.encode((self.process_input(a)).to(self.vae_dtype).to(self.device)).float()\n",
                "        samples = comfy.utils.tiled_scale(pixel_samples, encode_fn, tile_x, tile_y, overlap, upscale_amount = (1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device, pbar=pbar)\n",
                "        samples += comfy.utils.tiled_scale(pixel_samples, encode_fn, tile_x * 2, tile_y // 2, overlap, upscale_amount = (1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device, pbar=pbar)\n",
                "        samples += comfy.utils.tiled_scale(pixel_samples, encode_fn, tile_x // 2, tile_y * 2, overlap, upscale_amount = (1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device, pbar=pbar)\n",
                "        samples /= 3.0\n",
                "        return samples\n",
                "\n",
                "    def encode_tiled_1d(self, samples, tile_x=128 * 2048, overlap=32 * 2048):\n",
                "        encode_fn = lambda a: self.first_stage_model.encode((self.process_input(a)).to(self.vae_dtype).to(self.device)).float()\n",
                "        return comfy.utils.tiled_scale_multidim(samples, encode_fn, tile=(tile_x,), overlap=overlap, upscale_amount=(1/self.downscale_ratio), out_channels=self.latent_channels, output_device=self.output_device)\n",
                "\n",
                "    def decode(self, samples_in):\n",
                "        try:\n",
                "            memory_used = self.memory_used_decode(samples_in.shape, self.vae_dtype)\n",
                "            model_management.load_models_gpu([self.patcher], memory_required=memory_used)\n",
                "            free_memory = model_management.get_free_memory(self.device)\n",
                "            batch_number = int(free_memory / memory_used)\n",
                "            batch_number = max(1, batch_number)\n",
                "\n",
                "            pixel_samples = torch.empty((samples_in.shape[0], self.output_channels) + tuple(map(lambda a: a * self.upscale_ratio, samples_in.shape[2:])), device=self.output_device)\n",
                "            for x in range(0, samples_in.shape[0], batch_number):\n",
                "                samples = samples_in[x:x+batch_number].to(self.vae_dtype).to(self.device)\n",
                "                pixel_samples[x:x+batch_number] = self.process_output(self.first_stage_model.decode(samples).to(self.output_device).float())\n",
                "        except model_management.OOM_EXCEPTION as e:\n",
                "            logging.warning(\"Warning: Ran out of memory when regular VAE decoding, retrying with tiled VAE decoding.\")\n",
                "            if len(samples_in.shape) == 3:\n",
                "                pixel_samples = self.decode_tiled_1d(samples_in)\n",
                "            else:\n",
                "                pixel_samples = self.decode_tiled_(samples_in)\n",
                "\n",
                "        pixel_samples = pixel_samples.to(self.output_device).movedim(1,-1)\n",
                "        return pixel_samples\n",
                "\n",
                "    def decode_tiled(self, samples, tile_x=64, tile_y=64, overlap = 16):\n",
                "        model_management.load_model_gpu(self.patcher)\n",
                "        output = self.decode_tiled_(samples, tile_x, tile_y, overlap)\n",
                "        return output.movedim(1,-1)\n",
                "\n",
                "    def encode(self, pixel_samples):\n",
                "        pixel_samples = self.vae_encode_crop_pixels(pixel_samples)\n",
                "        pixel_samples = pixel_samples.movedim(-1,1)\n",
                "        try:\n",
                "            memory_used = self.memory_used_encode(pixel_samples.shape, self.vae_dtype)\n",
                "            model_management.load_models_gpu([self.patcher], memory_required=memory_used)\n",
                "            free_memory = model_management.get_free_memory(self.device)\n",
                "            batch_number = int(free_memory / memory_used)\n",
                "            batch_number = max(1, batch_number)\n",
                "            samples = torch.empty((pixel_samples.shape[0], self.latent_channels) + tuple(map(lambda a: a // self.downscale_ratio, pixel_samples.shape[2:])), device=self.output_device)\n",
                "            for x in range(0, pixel_samples.shape[0], batch_number):\n",
                "                pixels_in = self.process_input(pixel_samples[x:x+batch_number]).to(self.vae_dtype).to(self.device)\n",
                "                samples[x:x+batch_number] = self.first_stage_model.encode(pixels_in).to(self.output_device).float()\n",
                "\n",
                "        except model_management.OOM_EXCEPTION as e:\n",
                "            logging.warning(\"Warning: Ran out of memory when regular VAE encoding, retrying with tiled VAE encoding.\")\n",
                "            if len(pixel_samples.shape) == 3:\n",
                "                samples = self.encode_tiled_1d(pixel_samples)\n",
                "            else:\n",
                "                samples = self.encode_tiled_(pixel_samples)\n",
                "\n",
                "        return samples\n",
                "\n",
                "    def encode_tiled(self, pixel_samples, tile_x=512, tile_y=512, overlap = 64):\n",
                "        pixel_samples = self.vae_encode_crop_pixels(pixel_samples)\n",
                "        model_management.load_model_gpu(self.patcher)\n",
                "        pixel_samples = pixel_samples.movedim(-1,1)\n",
                "        samples = self.encode_tiled_(pixel_samples, tile_x=tile_x, tile_y=tile_y, overlap=overlap)\n",
                "        return samples\n",
                "\n",
                "    def get_sd(self):\n",
                "        return self.first_stage_model.state_dict()\n",
                "\n",
                "class StyleModel:\n",
                "    def __init__(self, model, device=\"cpu\"):\n",
                "        self.model = model\n",
                "\n",
                "    def get_cond(self, input):\n",
                "        return self.model(input.last_hidden_state)\n",
                "\n",
                "\n",
                "def load_style_model(ckpt_path):\n",
                "    model_data = comfy.utils.load_torch_file(ckpt_path, safe_load=True)\n",
                "    keys = model_data.keys()\n",
                "    if \"style_embedding\" in keys:\n",
                "        model = comfy.t2i_adapter.adapter.StyleAdapter(width=1024, context_dim=768, num_head=8, n_layes=3, num_token=8)\n",
                "    else:\n",
                "        raise Exception(\"invalid style model {}\".format(ckpt_path))\n",
                "    model.load_state_dict(model_data)\n",
                "    return StyleModel(model)\n",
                "\n",
                "class CLIPType(Enum):\n",
                "    STABLE_DIFFUSION = 1\n",
                "    STABLE_CASCADE = 2\n",
                "    SD3 = 3\n",
                "    STABLE_AUDIO = 4\n",
                "    HUNYUAN_DIT = 5\n",
                "    FLUX = 6\n",
                "\n",
                "def load_clip(ckpt_paths, embedding_directory=None, clip_type=CLIPType.STABLE_DIFFUSION):\n",
                "    clip_data = []\n",
                "    for p in ckpt_paths:\n",
                "        clip_data.append(comfy.utils.load_torch_file(p, safe_load=True))\n",
                "\n",
                "    class EmptyClass:\n",
                "        pass\n",
                "\n",
                "    for i in range(len(clip_data)):\n",
                "        if \"transformer.resblocks.0.ln_1.weight\" in clip_data[i]:\n",
                "            clip_data[i] = comfy.utils.clip_text_transformers_convert(clip_data[i], \"\", \"\")\n",
                "        else:\n",
                "            if \"text_projection\" in clip_data[i]:\n",
                "                clip_data[i][\"text_projection.weight\"] = clip_data[i][\"text_projection\"].transpose(0, 1) #old models saved with the CLIPSave node\n",
                "\n",
                "    clip_target = EmptyClass()\n",
                "    clip_target.params = {}\n",
                "    if len(clip_data) == 1:\n",
                "        if \"text_model.encoder.layers.30.mlp.fc1.weight\" in clip_data[0]:\n",
                "            if clip_type == CLIPType.STABLE_CASCADE:\n",
                "                clip_target.clip = sdxl_clip.StableCascadeClipModel\n",
                "                clip_target.tokenizer = sdxl_clip.StableCascadeTokenizer\n",
                "            else:\n",
                "                clip_target.clip = sdxl_clip.SDXLRefinerClipModel\n",
                "                clip_target.tokenizer = sdxl_clip.SDXLTokenizer\n",
                "        elif \"text_model.encoder.layers.22.mlp.fc1.weight\" in clip_data[0]:\n",
                "            clip_target.clip = comfy.text_encoders.sd2_clip.SD2ClipModel\n",
                "            clip_target.tokenizer = comfy.text_encoders.sd2_clip.SD2Tokenizer\n",
                "        elif \"encoder.block.23.layer.1.DenseReluDense.wi_1.weight\" in clip_data[0]:\n",
                "            weight = clip_data[0][\"encoder.block.23.layer.1.DenseReluDense.wi_1.weight\"]\n",
                "            dtype_t5 = weight.dtype\n",
                "            if weight.shape[-1] == 4096:\n",
                "                clip_target.clip = comfy.text_encoders.sd3_clip.sd3_clip(clip_l=False, clip_g=False, t5=True, dtype_t5=dtype_t5)\n",
                "                clip_target.tokenizer = comfy.text_encoders.sd3_clip.SD3Tokenizer\n",
                "            elif weight.shape[-1] == 2048:\n",
                "                clip_target.clip = comfy.text_encoders.aura_t5.AuraT5Model\n",
                "                clip_target.tokenizer = comfy.text_encoders.aura_t5.AuraT5Tokenizer\n",
                "        elif \"encoder.block.0.layer.0.SelfAttention.k.weight\" in clip_data[0]:\n",
                "            clip_target.clip = comfy.text_encoders.sa_t5.SAT5Model\n",
                "            clip_target.tokenizer = comfy.text_encoders.sa_t5.SAT5Tokenizer\n",
                "        else:\n",
                "            clip_target.clip = sd1_clip.SD1ClipModel\n",
                "            clip_target.tokenizer = sd1_clip.SD1Tokenizer\n",
                "    elif len(clip_data) == 2:\n",
                "        if clip_type == CLIPType.SD3:\n",
                "            clip_target.clip = comfy.text_encoders.sd3_clip.sd3_clip(clip_l=True, clip_g=True, t5=False)\n",
                "            clip_target.tokenizer = comfy.text_encoders.sd3_clip.SD3Tokenizer\n",
                "        elif clip_type == CLIPType.HUNYUAN_DIT:\n",
                "            clip_target.clip = comfy.text_encoders.hydit.HyditModel\n",
                "            clip_target.tokenizer = comfy.text_encoders.hydit.HyditTokenizer\n",
                "        elif clip_type == CLIPType.FLUX:\n",
                "            weight_name = \"encoder.block.23.layer.1.DenseReluDense.wi_1.weight\"\n",
                "            weight = clip_data[0].get(weight_name, clip_data[1].get(weight_name, None))\n",
                "            dtype_t5 = None\n",
                "            if weight is not None:\n",
                "                dtype_t5 = weight.dtype\n",
                "\n",
                "            clip_target.clip = comfy.text_encoders.flux.flux_clip(dtype_t5=dtype_t5)\n",
                "            clip_target.tokenizer = comfy.text_encoders.flux.FluxTokenizer\n",
                "        else:\n",
                "            clip_target.clip = sdxl_clip.SDXLClipModel\n",
                "            clip_target.tokenizer = sdxl_clip.SDXLTokenizer\n",
                "    elif len(clip_data) == 3:\n",
                "        clip_target.clip = comfy.text_encoders.sd3_clip.SD3ClipModel\n",
                "        clip_target.tokenizer = comfy.text_encoders.sd3_clip.SD3Tokenizer\n",
                "\n",
                "    clip = CLIP(clip_target, embedding_directory=embedding_directory)\n",
                "    for c in clip_data:\n",
                "        m, u = clip.load_sd(c)\n",
                "        if len(m) > 0:\n",
                "            logging.warning(\"clip missing: {}\".format(m))\n",
                "\n",
                "        if len(u) > 0:\n",
                "            logging.debug(\"clip unexpected: {}\".format(u))\n",
                "    return clip\n",
                "\n",
                "def load_gligen(ckpt_path):\n",
                "    data = comfy.utils.load_torch_file(ckpt_path, safe_load=True)\n",
                "    model = gligen.load_gligen(data)\n",
                "    if model_management.should_use_fp16():\n",
                "        model = model.half()\n",
                "    return comfy.model_patcher.ModelPatcher(model, load_device=model_management.get_torch_device(), offload_device=model_management.unet_offload_device())\n",
                "\n",
                "def load_checkpoint(config_path=None, ckpt_path=None, output_vae=True, output_clip=True, embedding_directory=None, state_dict=None, config=None):\n",
                "    logging.warning(\"Warning: The load checkpoint with config function is deprecated and will eventually be removed, please use the other one.\")\n",
                "    model, clip, vae, _ = load_checkpoint_guess_config(ckpt_path, output_vae=output_vae, output_clip=output_clip, output_clipvision=False, embedding_directory=embedding_directory, output_model=True)\n",
                "    #TODO: this function is a mess and should be removed eventually\n",
                "    if config is None:\n",
                "        with open(config_path, 'r') as stream:\n",
                "            config = yaml.safe_load(stream)\n",
                "    model_config_params = config['model']['params']\n",
                "    clip_config = model_config_params['cond_stage_config']\n",
                "    scale_factor = model_config_params['scale_factor']\n",
                "\n",
                "    if \"parameterization\" in model_config_params:\n",
                "        if model_config_params[\"parameterization\"] == \"v\":\n",
                "            m = model.clone()\n",
                "            class ModelSamplingAdvanced(comfy.model_sampling.ModelSamplingDiscrete, comfy.model_sampling.V_PREDICTION):\n",
                "                pass\n",
                "            m.add_object_patch(\"model_sampling\", ModelSamplingAdvanced(model.model.model_config))\n",
                "            model = m\n",
                "\n",
                "    layer_idx = clip_config.get(\"params\", {}).get(\"layer_idx\", None)\n",
                "    if layer_idx is not None:\n",
                "        clip.clip_layer(layer_idx)\n",
                "\n",
                "    return (model, clip, vae)\n",
                "\n",
                "def load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, output_clipvision=False, embedding_directory=None, output_model=True):\n",
                "    sd = comfy.utils.load_torch_file(ckpt_path)\n",
                "    sd_keys = sd.keys()\n",
                "    clip = None\n",
                "    clipvision = None\n",
                "    vae = None\n",
                "    model = None\n",
                "    model_patcher = None\n",
                "    clip_target = None\n",
                "\n",
                "    diffusion_model_prefix = model_detection.unet_prefix_from_state_dict(sd)\n",
                "    parameters = comfy.utils.calculate_parameters(sd, diffusion_model_prefix)\n",
                "    load_device = model_management.get_torch_device()\n",
                "\n",
                "    model_config = model_detection.model_config_from_unet(sd, diffusion_model_prefix)\n",
                "    if model_config is None:\n",
                "        raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(ckpt_path))\n",
                "\n",
                "    unet_dtype = model_management.unet_dtype(model_params=parameters, supported_dtypes=model_config.supported_inference_dtypes)\n",
                "    manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device, model_config.supported_inference_dtypes)\n",
                "    model_config.set_inference_dtype(unet_dtype, manual_cast_dtype)\n",
                "\n",
                "    if model_config.clip_vision_prefix is not None:\n",
                "        if output_clipvision:\n",
                "            clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n",
                "\n",
                "    if output_model:\n",
                "        inital_load_device = model_management.unet_inital_load_device(parameters, unet_dtype)\n",
                "        offload_device = model_management.unet_offload_device()\n",
                "        model = model_config.get_model(sd, diffusion_model_prefix, device=inital_load_device)\n",
                "        model.load_model_weights(sd, diffusion_model_prefix)\n",
                "\n",
                "    if output_vae:\n",
                "        vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n",
                "        vae_sd = model_config.process_vae_state_dict(vae_sd)\n",
                "        vae = VAE(sd=vae_sd)\n",
                "\n",
                "    if output_clip:\n",
                "        clip_target = model_config.clip_target(state_dict=sd)\n",
                "        if clip_target is not None:\n",
                "            clip_sd = model_config.process_clip_state_dict(sd)\n",
                "            if len(clip_sd) > 0:\n",
                "                clip = CLIP(clip_target, embedding_directory=embedding_directory, tokenizer_data=clip_sd)\n",
                "                m, u = clip.load_sd(clip_sd, full_model=True)\n",
                "                if len(m) > 0:\n",
                "                    m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n",
                "                    if len(m_filter) > 0:\n",
                "                        logging.warning(\"clip missing: {}\".format(m))\n",
                "                    else:\n",
                "                        logging.debug(\"clip missing: {}\".format(m))\n",
                "\n",
                "                if len(u) > 0:\n",
                "                    logging.debug(\"clip unexpected {}:\".format(u))\n",
                "            else:\n",
                "                logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n",
                "\n",
                "    left_over = sd.keys()\n",
                "    if len(left_over) > 0:\n",
                "        logging.debug(\"left over keys: {}\".format(left_over))\n",
                "\n",
                "    if output_model:\n",
                "        model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=model_management.unet_offload_device(), current_device=inital_load_device)\n",
                "        if inital_load_device != torch.device(\"cpu\"):\n",
                "            logging.info(\"loaded straight to GPU\")\n",
                "            model_management.load_model_gpu(model_patcher)\n",
                "\n",
                "    return (model_patcher, clip, vae, clipvision)\n",
                "\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "def load_unet_state_dict(sd): #load unet in diffusers or regular format\n"
                ],
                "after": [
                    "def load_unet_state_dict(sd, dtype=None): #load unet in diffusers or regular format\n"
                ],
                "parent_version_range": {
                    "start": 569,
                    "end": 570
                },
                "child_version_range": {
                    "start": 569,
                    "end": 570
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "load_unet_state_dict",
                        "signature": "def load_unet_state_dict(sd):",
                        "at_line": 569
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: comfy/sd.py\nCode:\n566 566        return (model_patcher, clip, vae, clipvision)\n567 567    \n568 568    \n569      - def load_unet_state_dict(sd): #load unet in diffusers or regular format\n    569  + def load_unet_state_dict(sd, dtype=None): #load unet in diffusers or regular format\n570 570    \n571 571        #Allow loading unets from checkpoint files\n572 572        diffusion_model_prefix = model_detection.unet_prefix_from_state_dict(sd)\n         ...\n",
                "file_path": "comfy/sd.py",
                "identifiers_before": [
                    "load_unet_state_dict",
                    "sd"
                ],
                "identifiers_after": [
                    "dtype",
                    "load_unet_state_dict",
                    "sd"
                ],
                "prefix": [
                    "    return (model_patcher, clip, vae, clipvision)\n",
                    "\n",
                    "\n"
                ],
                "suffix": [
                    "\n",
                    "    #Allow loading unets from checkpoint files\n",
                    "    diffusion_model_prefix = model_detection.unet_prefix_from_state_dict(sd)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "load_unet_state_dict",
                            "position": {
                                "start": {
                                    "line": 569,
                                    "column": 4
                                },
                                "end": {
                                    "line": 569,
                                    "column": 24
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 569,
                                    "column": 29
                                },
                                "end": {
                                    "line": 569,
                                    "column": 34
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 569,
                                    "column": 29
                                },
                                "end": {
                                    "line": 569,
                                    "column": 34
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 569,
                                    "column": 29
                                },
                                "end": {
                                    "line": 569,
                                    "column": 34
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "load_unet_state_dict",
                            "position": {
                                "start": {
                                    "line": 569,
                                    "column": 4
                                },
                                "end": {
                                    "line": 569,
                                    "column": 24
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    3
                ]
            },
            [
                "\n",
                "    #Allow loading unets from checkpoint files\n",
                "    diffusion_model_prefix = model_detection.unet_prefix_from_state_dict(sd)\n",
                "    temp_sd = comfy.utils.state_dict_prefix_replace(sd, {diffusion_model_prefix: \"\"}, filter_keys=True)\n",
                "    if len(temp_sd) > 0:\n",
                "        sd = temp_sd\n",
                "\n",
                "    parameters = comfy.utils.calculate_parameters(sd)\n"
            ],
            {
                "type": "delete",
                "before": [
                    "    unet_dtype = model_management.unet_dtype(model_params=parameters)\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 578,
                    "end": 579
                },
                "child_version_range": {
                    "start": 578,
                    "end": 578
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "load_unet_state_dict",
                        "signature": "def load_unet_state_dict(sd):",
                        "at_line": 569
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: comfy/sd.py\nCode:\n           def load_unet_state_dict(sd):\n               ...\n575 575            sd = temp_sd\n576 576    \n577 577        parameters = comfy.utils.calculate_parameters(sd)\n578      -     unet_dtype = model_management.unet_dtype(model_params=parameters)\n579 578        load_device = model_management.get_torch_device()\n580 579        model_config = model_detection.model_config_from_unet(sd, \"\")\n581 580    \n         ...\n",
                "file_path": "comfy/sd.py",
                "identifiers_before": [
                    "model_management",
                    "model_params",
                    "parameters",
                    "unet_dtype"
                ],
                "identifiers_after": [],
                "prefix": [
                    "        sd = temp_sd\n",
                    "\n",
                    "    parameters = comfy.utils.calculate_parameters(sd)\n"
                ],
                "suffix": [
                    "    load_device = model_management.get_torch_device()\n",
                    "    model_config = model_detection.model_config_from_unet(sd, \"\")\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    load_device = model_management.get_torch_device()\n",
                "    model_config = model_detection.model_config_from_unet(sd, \"\")\n",
                "\n",
                "    if model_config is not None:\n",
                "        new_sd = sd\n",
                "    else:\n",
                "        new_sd = model_detection.convert_diffusers_mmdit(sd, \"\")\n",
                "        if new_sd is not None: #diffusers mmdit\n",
                "            model_config = model_detection.model_config_from_unet(new_sd, \"\")\n",
                "            if model_config is None:\n",
                "                return None\n",
                "        else: #diffusers unet\n",
                "            model_config = model_detection.model_config_from_diffusers_unet(sd)\n",
                "            if model_config is None:\n",
                "                return None\n",
                "\n",
                "            diffusers_keys = comfy.utils.unet_to_diffusers(model_config.unet_config)\n",
                "\n",
                "            new_sd = {}\n",
                "            for k in diffusers_keys:\n",
                "                if k in sd:\n",
                "                    new_sd[diffusers_keys[k]] = sd.pop(k)\n",
                "                else:\n",
                "                    logging.warning(\"{} {}\".format(diffusers_keys[k], k))\n",
                "\n",
                "    offload_device = model_management.unet_offload_device()\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    unet_dtype = model_management.unet_dtype(model_params=parameters, supported_dtypes=model_config.supported_inference_dtypes)\n"
                ],
                "after": [
                    "    if dtype is None:\n",
                    "        unet_dtype = model_management.unet_dtype(model_params=parameters, supported_dtypes=model_config.supported_inference_dtypes)\n",
                    "    else:\n",
                    "        unet_dtype = dtype\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 605,
                    "end": 606
                },
                "child_version_range": {
                    "start": 604,
                    "end": 609
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "load_unet_state_dict",
                        "signature": "def load_unet_state_dict(sd):",
                        "at_line": 569
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: comfy/sd.py\nCode:\n           def load_unet_state_dict(sd):\n               ...\n602 601                        logging.warning(\"{} {}\".format(diffusers_keys[k], k))\n603 602    \n604 603        offload_device = model_management.unet_offload_device()\n605      -     unet_dtype = model_management.unet_dtype(model_params=parameters, supported_dtypes=model_config.supported_inference_dtypes)\n    604  +     if dtype is None:\n    605  +         unet_dtype = model_management.unet_dtype(model_params=parameters, supported_dtypes=model_config.supported_inference_dtypes)\n    606  +     else:\n    607  +         unet_dtype = dtype\n    608  + \n606 609        manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device, model_config.supported_inference_dtypes)\n607 610        model_config.set_inference_dtype(unet_dtype, manual_cast_dtype)\n608 611        model = model_config.get_model(new_sd, \"\")\n         ...\n",
                "file_path": "comfy/sd.py",
                "identifiers_before": [
                    "model_config",
                    "model_management",
                    "model_params",
                    "parameters",
                    "supported_dtypes",
                    "supported_inference_dtypes",
                    "unet_dtype"
                ],
                "identifiers_after": [
                    "dtype",
                    "model_config",
                    "model_management",
                    "model_params",
                    "parameters",
                    "supported_dtypes",
                    "supported_inference_dtypes",
                    "unet_dtype"
                ],
                "prefix": [
                    "                    logging.warning(\"{} {}\".format(diffusers_keys[k], k))\n",
                    "\n",
                    "    offload_device = model_management.unet_offload_device()\n"
                ],
                "suffix": [
                    "    manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device, model_config.supported_inference_dtypes)\n",
                    "    model_config.set_inference_dtype(unet_dtype, manual_cast_dtype)\n",
                    "    model = model_config.get_model(new_sd, \"\")\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 604,
                                    "column": 7
                                },
                                "end": {
                                    "line": 604,
                                    "column": 12
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 607,
                                    "column": 21
                                },
                                "end": {
                                    "line": 607,
                                    "column": 26
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device, model_config.supported_inference_dtypes)\n",
                "    model_config.set_inference_dtype(unet_dtype, manual_cast_dtype)\n",
                "    model = model_config.get_model(new_sd, \"\")\n",
                "    model = model.to(offload_device)\n",
                "    model.load_model_weights(new_sd, \"\")\n",
                "    left_over = sd.keys()\n",
                "    if len(left_over) > 0:\n",
                "        logging.info(\"left over keys in unet: {}\".format(left_over))\n",
                "    return comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=offload_device)\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "def load_unet(unet_path):\n"
                ],
                "after": [
                    "def load_unet(unet_path, dtype=None):\n"
                ],
                "parent_version_range": {
                    "start": 616,
                    "end": 617
                },
                "child_version_range": {
                    "start": 619,
                    "end": 620
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "load_unet",
                        "signature": "def load_unet(unet_path):",
                        "at_line": 616
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: comfy/sd.py\nCode:\n613 616            logging.info(\"left over keys in unet: {}\".format(left_over))\n614 617        return comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=offload_device)\n615 618    \n616      - def load_unet(unet_path):\n    619  + def load_unet(unet_path, dtype=None):\n617 620        sd = comfy.utils.load_torch_file(unet_path)\n         ...\n",
                "file_path": "comfy/sd.py",
                "identifiers_before": [
                    "load_unet",
                    "unet_path"
                ],
                "identifiers_after": [
                    "dtype",
                    "load_unet",
                    "unet_path"
                ],
                "prefix": [
                    "        logging.info(\"left over keys in unet: {}\".format(left_over))\n",
                    "    return comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=offload_device)\n",
                    "\n"
                ],
                "suffix": [
                    "    sd = comfy.utils.load_torch_file(unet_path)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "load_unet",
                            "position": {
                                "start": {
                                    "line": 616,
                                    "column": 4
                                },
                                "end": {
                                    "line": 616,
                                    "column": 13
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "load_unet",
                            "position": {
                                "start": {
                                    "line": 619,
                                    "column": 4
                                },
                                "end": {
                                    "line": 619,
                                    "column": 13
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 619,
                                    "column": 25
                                },
                                "end": {
                                    "line": 619,
                                    "column": 30
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 619,
                                    "column": 25
                                },
                                "end": {
                                    "line": 619,
                                    "column": 30
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    0
                ]
            },
            [
                "    sd = comfy.utils.load_torch_file(unet_path)\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    model = load_unet_state_dict(sd)\n"
                ],
                "after": [
                    "    model = load_unet_state_dict(sd, dtype=dtype)\n"
                ],
                "parent_version_range": {
                    "start": 618,
                    "end": 619
                },
                "child_version_range": {
                    "start": 621,
                    "end": 622
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "load_unet",
                        "signature": "def load_unet(unet_path):",
                        "at_line": 616
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: comfy/sd.py\nCode:\n           def load_unet(unet_path):\n               ...\n617 620        sd = comfy.utils.load_torch_file(unet_path)\n618      -     model = load_unet_state_dict(sd)\n    621  +     model = load_unet_state_dict(sd, dtype=dtype)\n619 622        if model is None:\n620 623            logging.error(\"ERROR UNSUPPORTED UNET {}\".format(unet_path))\n621 624            raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(unet_path))\n         ...\n",
                "file_path": "comfy/sd.py",
                "identifiers_before": [
                    "load_unet_state_dict",
                    "model",
                    "sd"
                ],
                "identifiers_after": [
                    "dtype",
                    "load_unet_state_dict",
                    "model",
                    "sd"
                ],
                "prefix": [
                    "    sd = comfy.utils.load_torch_file(unet_path)\n"
                ],
                "suffix": [
                    "    if model is None:\n",
                    "        logging.error(\"ERROR UNSUPPORTED UNET {}\".format(unet_path))\n",
                    "        raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(unet_path))\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "load_unet_state_dict",
                            "position": {
                                "start": {
                                    "line": 618,
                                    "column": 12
                                },
                                "end": {
                                    "line": 618,
                                    "column": 32
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 621,
                                    "column": 37
                                },
                                "end": {
                                    "line": 621,
                                    "column": 42
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 621,
                                    "column": 43
                                },
                                "end": {
                                    "line": 621,
                                    "column": 48
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "load_unet_state_dict",
                            "position": {
                                "start": {
                                    "line": 621,
                                    "column": 12
                                },
                                "end": {
                                    "line": 621,
                                    "column": 32
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/comfy/sd.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    if model is None:\n",
                "        logging.error(\"ERROR UNSUPPORTED UNET {}\".format(unet_path))\n",
                "        raise RuntimeError(\"ERROR: Could not detect model type of: {}\".format(unet_path))\n",
                "    return model\n",
                "\n",
                "def save_checkpoint(output_path, model, clip=None, vae=None, clip_vision=None, metadata=None, extra_keys={}):\n",
                "    clip_sd = None\n",
                "    load_models = [model]\n",
                "    if clip is not None:\n",
                "        load_models.append(clip.load_model())\n",
                "        clip_sd = clip.get_sd()\n",
                "\n",
                "    model_management.load_models_gpu(load_models, force_patch_weights=True)\n",
                "    clip_vision_sd = clip_vision.get_sd() if clip_vision is not None else None\n",
                "    sd = model.model.state_dict_for_saving(clip_sd, vae.get_sd(), clip_vision_sd)\n",
                "    for k in extra_keys:\n",
                "        sd[k] = extra_keys[k]\n",
                "\n",
                "    for k in sd:\n",
                "        t = sd[k]\n",
                "        if not t.is_contiguous():\n",
                "            sd[k] = t.contiguous()\n",
                "\n",
                "    comfy.utils.save_torch_file(sd, output_path, metadata=metadata)"
            ]
        ],
        "nodes.py": [
            [
                "import torch\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import json\n",
                "import hashlib\n",
                "import traceback\n",
                "import math\n",
                "import time\n",
                "import random\n",
                "import logging\n",
                "\n",
                "from PIL import Image, ImageOps, ImageSequence, ImageFile\n",
                "from PIL.PngImagePlugin import PngInfo\n",
                "\n",
                "import numpy as np\n",
                "import safetensors.torch\n",
                "\n",
                "sys.path.insert(0, os.path.join(os.path.dirname(os.path.realpath(__file__)), \"comfy\"))\n",
                "\n",
                "import comfy.diffusers_load\n",
                "import comfy.samplers\n",
                "import comfy.sample\n",
                "import comfy.sd\n",
                "import comfy.utils\n",
                "import comfy.controlnet\n",
                "\n",
                "import comfy.clip_vision\n",
                "\n",
                "import comfy.model_management\n",
                "from comfy.cli_args import args\n",
                "\n",
                "import importlib\n",
                "\n",
                "import folder_paths\n",
                "import latent_preview\n",
                "import node_helpers\n",
                "\n",
                "def before_node_execution():\n",
                "    comfy.model_management.throw_exception_if_processing_interrupted()\n",
                "\n",
                "def interrupt_processing(value=True):\n",
                "    comfy.model_management.interrupt_current_processing(value)\n",
                "\n",
                "MAX_RESOLUTION=16384\n",
                "\n",
                "class CLIPTextEncode:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"text\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}), \"clip\": (\"CLIP\", )}}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"encode\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def encode(self, clip, text):\n",
                "        tokens = clip.tokenize(text)\n",
                "        output = clip.encode_from_tokens(tokens, return_pooled=True, return_dict=True)\n",
                "        cond = output.pop(\"cond\")\n",
                "        return ([[cond, output]], )\n",
                "\n",
                "class ConditioningCombine:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning_1\": (\"CONDITIONING\", ), \"conditioning_2\": (\"CONDITIONING\", )}}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"combine\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def combine(self, conditioning_1, conditioning_2):\n",
                "        return (conditioning_1 + conditioning_2, )\n",
                "\n",
                "class ConditioningAverage :\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning_to\": (\"CONDITIONING\", ), \"conditioning_from\": (\"CONDITIONING\", ),\n",
                "                              \"conditioning_to_strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01})\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"addWeighted\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def addWeighted(self, conditioning_to, conditioning_from, conditioning_to_strength):\n",
                "        out = []\n",
                "\n",
                "        if len(conditioning_from) > 1:\n",
                "            logging.warning(\"Warning: ConditioningAverage conditioning_from contains more than 1 cond, only the first one will actually be applied to conditioning_to.\")\n",
                "\n",
                "        cond_from = conditioning_from[0][0]\n",
                "        pooled_output_from = conditioning_from[0][1].get(\"pooled_output\", None)\n",
                "\n",
                "        for i in range(len(conditioning_to)):\n",
                "            t1 = conditioning_to[i][0]\n",
                "            pooled_output_to = conditioning_to[i][1].get(\"pooled_output\", pooled_output_from)\n",
                "            t0 = cond_from[:,:t1.shape[1]]\n",
                "            if t0.shape[1] < t1.shape[1]:\n",
                "                t0 = torch.cat([t0] + [torch.zeros((1, (t1.shape[1] - t0.shape[1]), t1.shape[2]))], dim=1)\n",
                "\n",
                "            tw = torch.mul(t1, conditioning_to_strength) + torch.mul(t0, (1.0 - conditioning_to_strength))\n",
                "            t_to = conditioning_to[i][1].copy()\n",
                "            if pooled_output_from is not None and pooled_output_to is not None:\n",
                "                t_to[\"pooled_output\"] = torch.mul(pooled_output_to, conditioning_to_strength) + torch.mul(pooled_output_from, (1.0 - conditioning_to_strength))\n",
                "            elif pooled_output_from is not None:\n",
                "                t_to[\"pooled_output\"] = pooled_output_from\n",
                "\n",
                "            n = [tw, t_to]\n",
                "            out.append(n)\n",
                "        return (out, )\n",
                "\n",
                "class ConditioningConcat:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\n",
                "            \"conditioning_to\": (\"CONDITIONING\",),\n",
                "            \"conditioning_from\": (\"CONDITIONING\",),\n",
                "            }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"concat\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def concat(self, conditioning_to, conditioning_from):\n",
                "        out = []\n",
                "\n",
                "        if len(conditioning_from) > 1:\n",
                "            logging.warning(\"Warning: ConditioningConcat conditioning_from contains more than 1 cond, only the first one will actually be applied to conditioning_to.\")\n",
                "\n",
                "        cond_from = conditioning_from[0][0]\n",
                "\n",
                "        for i in range(len(conditioning_to)):\n",
                "            t1 = conditioning_to[i][0]\n",
                "            tw = torch.cat((t1, cond_from),1)\n",
                "            n = [tw, conditioning_to[i][1].copy()]\n",
                "            out.append(n)\n",
                "\n",
                "        return (out, )\n",
                "\n",
                "class ConditioningSetArea:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n",
                "                              \"width\": (\"INT\", {\"default\": 64, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"height\": (\"INT\", {\"default\": 64, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"append\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def append(self, conditioning, width, height, x, y, strength):\n",
                "        c = node_helpers.conditioning_set_values(conditioning, {\"area\": (height // 8, width // 8, y // 8, x // 8),\n",
                "                                                                \"strength\": strength,\n",
                "                                                                \"set_area_to_bounds\": False})\n",
                "        return (c, )\n",
                "\n",
                "class ConditioningSetAreaPercentage:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n",
                "                              \"width\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n",
                "                              \"height\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n",
                "                              \"x\": (\"FLOAT\", {\"default\": 0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n",
                "                              \"y\": (\"FLOAT\", {\"default\": 0, \"min\": 0, \"max\": 1.0, \"step\": 0.01}),\n",
                "                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"append\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def append(self, conditioning, width, height, x, y, strength):\n",
                "        c = node_helpers.conditioning_set_values(conditioning, {\"area\": (\"percentage\", height, width, y, x),\n",
                "                                                                \"strength\": strength,\n",
                "                                                                \"set_area_to_bounds\": False})\n",
                "        return (c, )\n",
                "\n",
                "class ConditioningSetAreaStrength:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n",
                "                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"append\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def append(self, conditioning, strength):\n",
                "        c = node_helpers.conditioning_set_values(conditioning, {\"strength\": strength})\n",
                "        return (c, )\n",
                "\n",
                "\n",
                "class ConditioningSetMask:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n",
                "                              \"mask\": (\"MASK\", ),\n",
                "                              \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n",
                "                              \"set_cond_area\": ([\"default\", \"mask bounds\"],),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"append\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def append(self, conditioning, mask, set_cond_area, strength):\n",
                "        set_area_to_bounds = False\n",
                "        if set_cond_area != \"default\":\n",
                "            set_area_to_bounds = True\n",
                "        if len(mask.shape) < 3:\n",
                "            mask = mask.unsqueeze(0)\n",
                "\n",
                "        c = node_helpers.conditioning_set_values(conditioning, {\"mask\": mask,\n",
                "                                                                \"set_area_to_bounds\": set_area_to_bounds,\n",
                "                                                                \"mask_strength\": strength})\n",
                "        return (c, )\n",
                "\n",
                "class ConditioningZeroOut:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning\": (\"CONDITIONING\", )}}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"zero_out\"\n",
                "\n",
                "    CATEGORY = \"advanced/conditioning\"\n",
                "\n",
                "    def zero_out(self, conditioning):\n",
                "        c = []\n",
                "        for t in conditioning:\n",
                "            d = t[1].copy()\n",
                "            pooled_output = d.get(\"pooled_output\", None)\n",
                "            if pooled_output is not None:\n",
                "                d[\"pooled_output\"] = torch.zeros_like(pooled_output)\n",
                "            n = [torch.zeros_like(t[0]), d]\n",
                "            c.append(n)\n",
                "        return (c, )\n",
                "\n",
                "class ConditioningSetTimestepRange:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n",
                "                             \"start\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n",
                "                             \"end\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001})\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"set_range\"\n",
                "\n",
                "    CATEGORY = \"advanced/conditioning\"\n",
                "\n",
                "    def set_range(self, conditioning, start, end):\n",
                "        c = node_helpers.conditioning_set_values(conditioning, {\"start_percent\": start,\n",
                "                                                                \"end_percent\": end})\n",
                "        return (c, )\n",
                "\n",
                "class VAEDecode:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", )}}\n",
                "    RETURN_TYPES = (\"IMAGE\",)\n",
                "    FUNCTION = \"decode\"\n",
                "\n",
                "    CATEGORY = \"latent\"\n",
                "\n",
                "    def decode(self, vae, samples):\n",
                "        return (vae.decode(samples[\"samples\"]), )\n",
                "\n",
                "class VAEDecodeTiled:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", ),\n",
                "                             \"tile_size\": (\"INT\", {\"default\": 512, \"min\": 320, \"max\": 4096, \"step\": 64})\n",
                "                            }}\n",
                "    RETURN_TYPES = (\"IMAGE\",)\n",
                "    FUNCTION = \"decode\"\n",
                "\n",
                "    CATEGORY = \"_for_testing\"\n",
                "\n",
                "    def decode(self, vae, samples, tile_size):\n",
                "        return (vae.decode_tiled(samples[\"samples\"], tile_x=tile_size // 8, tile_y=tile_size // 8, ), )\n",
                "\n",
                "class VAEEncode:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", )}}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"encode\"\n",
                "\n",
                "    CATEGORY = \"latent\"\n",
                "\n",
                "    def encode(self, vae, pixels):\n",
                "        t = vae.encode(pixels[:,:,:,:3])\n",
                "        return ({\"samples\":t}, )\n",
                "\n",
                "class VAEEncodeTiled:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", ),\n",
                "                             \"tile_size\": (\"INT\", {\"default\": 512, \"min\": 320, \"max\": 4096, \"step\": 64})\n",
                "                            }}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"encode\"\n",
                "\n",
                "    CATEGORY = \"_for_testing\"\n",
                "\n",
                "    def encode(self, vae, pixels, tile_size):\n",
                "        t = vae.encode_tiled(pixels[:,:,:,:3], tile_x=tile_size, tile_y=tile_size, )\n",
                "        return ({\"samples\":t}, )\n",
                "\n",
                "class VAEEncodeForInpaint:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"pixels\": (\"IMAGE\", ), \"vae\": (\"VAE\", ), \"mask\": (\"MASK\", ), \"grow_mask_by\": (\"INT\", {\"default\": 6, \"min\": 0, \"max\": 64, \"step\": 1}),}}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"encode\"\n",
                "\n",
                "    CATEGORY = \"latent/inpaint\"\n",
                "\n",
                "    def encode(self, vae, pixels, mask, grow_mask_by=6):\n",
                "        x = (pixels.shape[1] // vae.downscale_ratio) * vae.downscale_ratio\n",
                "        y = (pixels.shape[2] // vae.downscale_ratio) * vae.downscale_ratio\n",
                "        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(pixels.shape[1], pixels.shape[2]), mode=\"bilinear\")\n",
                "\n",
                "        pixels = pixels.clone()\n",
                "        if pixels.shape[1] != x or pixels.shape[2] != y:\n",
                "            x_offset = (pixels.shape[1] % vae.downscale_ratio) // 2\n",
                "            y_offset = (pixels.shape[2] % vae.downscale_ratio) // 2\n",
                "            pixels = pixels[:,x_offset:x + x_offset, y_offset:y + y_offset,:]\n",
                "            mask = mask[:,:,x_offset:x + x_offset, y_offset:y + y_offset]\n",
                "\n",
                "        #grow mask by a few pixels to keep things seamless in latent space\n",
                "        if grow_mask_by == 0:\n",
                "            mask_erosion = mask\n",
                "        else:\n",
                "            kernel_tensor = torch.ones((1, 1, grow_mask_by, grow_mask_by))\n",
                "            padding = math.ceil((grow_mask_by - 1) / 2)\n",
                "\n",
                "            mask_erosion = torch.clamp(torch.nn.functional.conv2d(mask.round(), kernel_tensor, padding=padding), 0, 1)\n",
                "\n",
                "        m = (1.0 - mask.round()).squeeze(1)\n",
                "        for i in range(3):\n",
                "            pixels[:,:,:,i] -= 0.5\n",
                "            pixels[:,:,:,i] *= m\n",
                "            pixels[:,:,:,i] += 0.5\n",
                "        t = vae.encode(pixels)\n",
                "\n",
                "        return ({\"samples\":t, \"noise_mask\": (mask_erosion[:,:,:x,:y].round())}, )\n",
                "\n",
                "\n",
                "class InpaintModelConditioning:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n",
                "                             \"negative\": (\"CONDITIONING\", ),\n",
                "                             \"vae\": (\"VAE\", ),\n",
                "                             \"pixels\": (\"IMAGE\", ),\n",
                "                             \"mask\": (\"MASK\", ),\n",
                "                             }}\n",
                "\n",
                "    RETURN_TYPES = (\"CONDITIONING\",\"CONDITIONING\",\"LATENT\")\n",
                "    RETURN_NAMES = (\"positive\", \"negative\", \"latent\")\n",
                "    FUNCTION = \"encode\"\n",
                "\n",
                "    CATEGORY = \"conditioning/inpaint\"\n",
                "\n",
                "    def encode(self, positive, negative, pixels, vae, mask):\n",
                "        x = (pixels.shape[1] // 8) * 8\n",
                "        y = (pixels.shape[2] // 8) * 8\n",
                "        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(pixels.shape[1], pixels.shape[2]), mode=\"bilinear\")\n",
                "\n",
                "        orig_pixels = pixels\n",
                "        pixels = orig_pixels.clone()\n",
                "        if pixels.shape[1] != x or pixels.shape[2] != y:\n",
                "            x_offset = (pixels.shape[1] % 8) // 2\n",
                "            y_offset = (pixels.shape[2] % 8) // 2\n",
                "            pixels = pixels[:,x_offset:x + x_offset, y_offset:y + y_offset,:]\n",
                "            mask = mask[:,:,x_offset:x + x_offset, y_offset:y + y_offset]\n",
                "\n",
                "        m = (1.0 - mask.round()).squeeze(1)\n",
                "        for i in range(3):\n",
                "            pixels[:,:,:,i] -= 0.5\n",
                "            pixels[:,:,:,i] *= m\n",
                "            pixels[:,:,:,i] += 0.5\n",
                "        concat_latent = vae.encode(pixels)\n",
                "        orig_latent = vae.encode(orig_pixels)\n",
                "\n",
                "        out_latent = {}\n",
                "\n",
                "        out_latent[\"samples\"] = orig_latent\n",
                "        out_latent[\"noise_mask\"] = mask\n",
                "\n",
                "        out = []\n",
                "        for conditioning in [positive, negative]:\n",
                "            c = node_helpers.conditioning_set_values(conditioning, {\"concat_latent_image\": concat_latent,\n",
                "                                                                    \"concat_mask\": mask})\n",
                "            out.append(c)\n",
                "        return (out[0], out[1], out_latent)\n",
                "\n",
                "\n",
                "class SaveLatent:\n",
                "    def __init__(self):\n",
                "        self.output_dir = folder_paths.get_output_directory()\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\", ),\n",
                "                              \"filename_prefix\": (\"STRING\", {\"default\": \"latents/ComfyUI\"})},\n",
                "                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n",
                "                }\n",
                "    RETURN_TYPES = ()\n",
                "    FUNCTION = \"save\"\n",
                "\n",
                "    OUTPUT_NODE = True\n",
                "\n",
                "    CATEGORY = \"_for_testing\"\n",
                "\n",
                "    def save(self, samples, filename_prefix=\"ComfyUI\", prompt=None, extra_pnginfo=None):\n",
                "        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n",
                "\n",
                "        # support save metadata for latent sharing\n",
                "        prompt_info = \"\"\n",
                "        if prompt is not None:\n",
                "            prompt_info = json.dumps(prompt)\n",
                "\n",
                "        metadata = None\n",
                "        if not args.disable_metadata:\n",
                "            metadata = {\"prompt\": prompt_info}\n",
                "            if extra_pnginfo is not None:\n",
                "                for x in extra_pnginfo:\n",
                "                    metadata[x] = json.dumps(extra_pnginfo[x])\n",
                "\n",
                "        file = f\"{filename}_{counter:05}_.latent\"\n",
                "\n",
                "        results = list()\n",
                "        results.append({\n",
                "            \"filename\": file,\n",
                "            \"subfolder\": subfolder,\n",
                "            \"type\": \"output\"\n",
                "        })\n",
                "\n",
                "        file = os.path.join(full_output_folder, file)\n",
                "\n",
                "        output = {}\n",
                "        output[\"latent_tensor\"] = samples[\"samples\"]\n",
                "        output[\"latent_format_version_0\"] = torch.tensor([])\n",
                "\n",
                "        comfy.utils.save_torch_file(output, file, metadata=metadata)\n",
                "        return { \"ui\": { \"latents\": results } }\n",
                "\n",
                "\n",
                "class LoadLatent:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        input_dir = folder_paths.get_input_directory()\n",
                "        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f)) and f.endswith(\".latent\")]\n",
                "        return {\"required\": {\"latent\": [sorted(files), ]}, }\n",
                "\n",
                "    CATEGORY = \"_for_testing\"\n",
                "\n",
                "    RETURN_TYPES = (\"LATENT\", )\n",
                "    FUNCTION = \"load\"\n",
                "\n",
                "    def load(self, latent):\n",
                "        latent_path = folder_paths.get_annotated_filepath(latent)\n",
                "        latent = safetensors.torch.load_file(latent_path, device=\"cpu\")\n",
                "        multiplier = 1.0\n",
                "        if \"latent_format_version_0\" not in latent:\n",
                "            multiplier = 1.0 / 0.18215\n",
                "        samples = {\"samples\": latent[\"latent_tensor\"].float() * multiplier}\n",
                "        return (samples, )\n",
                "\n",
                "    @classmethod\n",
                "    def IS_CHANGED(s, latent):\n",
                "        image_path = folder_paths.get_annotated_filepath(latent)\n",
                "        m = hashlib.sha256()\n",
                "        with open(image_path, 'rb') as f:\n",
                "            m.update(f.read())\n",
                "        return m.digest().hex()\n",
                "\n",
                "    @classmethod\n",
                "    def VALIDATE_INPUTS(s, latent):\n",
                "        if not folder_paths.exists_annotated_filepath(latent):\n",
                "            return \"Invalid latent file: {}\".format(latent)\n",
                "        return True\n",
                "\n",
                "\n",
                "class CheckpointLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"config_name\": (folder_paths.get_filename_list(\"configs\"), ),\n",
                "                              \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), )}}\n",
                "    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n",
                "    FUNCTION = \"load_checkpoint\"\n",
                "\n",
                "    CATEGORY = \"advanced/loaders\"\n",
                "\n",
                "    def load_checkpoint(self, config_name, ckpt_name):\n",
                "        config_path = folder_paths.get_full_path(\"configs\", config_name)\n",
                "        ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n",
                "        return comfy.sd.load_checkpoint(config_path, ckpt_path, output_vae=True, output_clip=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n",
                "\n",
                "class CheckpointLoaderSimple:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), ),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n",
                "    FUNCTION = \"load_checkpoint\"\n",
                "\n",
                "    CATEGORY = \"loaders\"\n",
                "\n",
                "    def load_checkpoint(self, ckpt_name):\n",
                "        ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n",
                "        out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n",
                "        return out[:3]\n",
                "\n",
                "class DiffusersLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(cls):\n",
                "        paths = []\n",
                "        for search_path in folder_paths.get_folder_paths(\"diffusers\"):\n",
                "            if os.path.exists(search_path):\n",
                "                for root, subdir, files in os.walk(search_path, followlinks=True):\n",
                "                    if \"model_index.json\" in files:\n",
                "                        paths.append(os.path.relpath(root, start=search_path))\n",
                "\n",
                "        return {\"required\": {\"model_path\": (paths,), }}\n",
                "    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\")\n",
                "    FUNCTION = \"load_checkpoint\"\n",
                "\n",
                "    CATEGORY = \"advanced/loaders/deprecated\"\n",
                "\n",
                "    def load_checkpoint(self, model_path, output_vae=True, output_clip=True):\n",
                "        for search_path in folder_paths.get_folder_paths(\"diffusers\"):\n",
                "            if os.path.exists(search_path):\n",
                "                path = os.path.join(search_path, model_path)\n",
                "                if os.path.exists(path):\n",
                "                    model_path = path\n",
                "                    break\n",
                "\n",
                "        return comfy.diffusers_load.load_diffusers(model_path, output_vae=output_vae, output_clip=output_clip, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n",
                "\n",
                "\n",
                "class unCLIPCheckpointLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), ),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"MODEL\", \"CLIP\", \"VAE\", \"CLIP_VISION\")\n",
                "    FUNCTION = \"load_checkpoint\"\n",
                "\n",
                "    CATEGORY = \"loaders\"\n",
                "\n",
                "    def load_checkpoint(self, ckpt_name, output_vae=True, output_clip=True):\n",
                "        ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n",
                "        out = comfy.sd.load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, output_clipvision=True, embedding_directory=folder_paths.get_folder_paths(\"embeddings\"))\n",
                "        return out\n",
                "\n",
                "class CLIPSetLastLayer:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"clip\": (\"CLIP\", ),\n",
                "                              \"stop_at_clip_layer\": (\"INT\", {\"default\": -1, \"min\": -24, \"max\": -1, \"step\": 1}),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"CLIP\",)\n",
                "    FUNCTION = \"set_last_layer\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def set_last_layer(self, clip, stop_at_clip_layer):\n",
                "        clip = clip.clone()\n",
                "        clip.clip_layer(stop_at_clip_layer)\n",
                "        return (clip,)\n",
                "\n",
                "class LoraLoader:\n",
                "    def __init__(self):\n",
                "        self.loaded_lora = None\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"model\": (\"MODEL\",),\n",
                "                              \"clip\": (\"CLIP\", ),\n",
                "                              \"lora_name\": (folder_paths.get_filename_list(\"loras\"), ),\n",
                "                              \"strength_model\": (\"FLOAT\", {\"default\": 1.0, \"min\": -100.0, \"max\": 100.0, \"step\": 0.01}),\n",
                "                              \"strength_clip\": (\"FLOAT\", {\"default\": 1.0, \"min\": -100.0, \"max\": 100.0, \"step\": 0.01}),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"MODEL\", \"CLIP\")\n",
                "    FUNCTION = \"load_lora\"\n",
                "\n",
                "    CATEGORY = \"loaders\"\n",
                "\n",
                "    def load_lora(self, model, clip, lora_name, strength_model, strength_clip):\n",
                "        if strength_model == 0 and strength_clip == 0:\n",
                "            return (model, clip)\n",
                "\n",
                "        lora_path = folder_paths.get_full_path(\"loras\", lora_name)\n",
                "        lora = None\n",
                "        if self.loaded_lora is not None:\n",
                "            if self.loaded_lora[0] == lora_path:\n",
                "                lora = self.loaded_lora[1]\n",
                "            else:\n",
                "                temp = self.loaded_lora\n",
                "                self.loaded_lora = None\n",
                "                del temp\n",
                "\n",
                "        if lora is None:\n",
                "            lora = comfy.utils.load_torch_file(lora_path, safe_load=True)\n",
                "            self.loaded_lora = (lora_path, lora)\n",
                "\n",
                "        model_lora, clip_lora = comfy.sd.load_lora_for_models(model, clip, lora, strength_model, strength_clip)\n",
                "        return (model_lora, clip_lora)\n",
                "\n",
                "class LoraLoaderModelOnly(LoraLoader):\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"model\": (\"MODEL\",),\n",
                "                              \"lora_name\": (folder_paths.get_filename_list(\"loras\"), ),\n",
                "                              \"strength_model\": (\"FLOAT\", {\"default\": 1.0, \"min\": -100.0, \"max\": 100.0, \"step\": 0.01}),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"MODEL\",)\n",
                "    FUNCTION = \"load_lora_model_only\"\n",
                "\n",
                "    def load_lora_model_only(self, model, lora_name, strength_model):\n",
                "        return (self.load_lora(model, None, lora_name, strength_model, 0)[0],)\n",
                "\n",
                "class VAELoader:\n",
                "    @staticmethod\n",
                "    def vae_list():\n",
                "        vaes = folder_paths.get_filename_list(\"vae\")\n",
                "        approx_vaes = folder_paths.get_filename_list(\"vae_approx\")\n",
                "        sdxl_taesd_enc = False\n",
                "        sdxl_taesd_dec = False\n",
                "        sd1_taesd_enc = False\n",
                "        sd1_taesd_dec = False\n",
                "        sd3_taesd_enc = False\n",
                "        sd3_taesd_dec = False\n",
                "\n",
                "        for v in approx_vaes:\n",
                "            if v.startswith(\"taesd_decoder.\"):\n",
                "                sd1_taesd_dec = True\n",
                "            elif v.startswith(\"taesd_encoder.\"):\n",
                "                sd1_taesd_enc = True\n",
                "            elif v.startswith(\"taesdxl_decoder.\"):\n",
                "                sdxl_taesd_dec = True\n",
                "            elif v.startswith(\"taesdxl_encoder.\"):\n",
                "                sdxl_taesd_enc = True\n",
                "            elif v.startswith(\"taesd3_decoder.\"):\n",
                "                sd3_taesd_dec = True\n",
                "            elif v.startswith(\"taesd3_encoder.\"):\n",
                "                sd3_taesd_enc = True\n",
                "        if sd1_taesd_dec and sd1_taesd_enc:\n",
                "            vaes.append(\"taesd\")\n",
                "        if sdxl_taesd_dec and sdxl_taesd_enc:\n",
                "            vaes.append(\"taesdxl\")\n",
                "        if sd3_taesd_dec and sd3_taesd_enc:\n",
                "            vaes.append(\"taesd3\")\n",
                "        return vaes\n",
                "\n",
                "    @staticmethod\n",
                "    def load_taesd(name):\n",
                "        sd = {}\n",
                "        approx_vaes = folder_paths.get_filename_list(\"vae_approx\")\n",
                "\n",
                "        encoder = next(filter(lambda a: a.startswith(\"{}_encoder.\".format(name)), approx_vaes))\n",
                "        decoder = next(filter(lambda a: a.startswith(\"{}_decoder.\".format(name)), approx_vaes))\n",
                "\n",
                "        enc = comfy.utils.load_torch_file(folder_paths.get_full_path(\"vae_approx\", encoder))\n",
                "        for k in enc:\n",
                "            sd[\"taesd_encoder.{}\".format(k)] = enc[k]\n",
                "\n",
                "        dec = comfy.utils.load_torch_file(folder_paths.get_full_path(\"vae_approx\", decoder))\n",
                "        for k in dec:\n",
                "            sd[\"taesd_decoder.{}\".format(k)] = dec[k]\n",
                "\n",
                "        if name == \"taesd\":\n",
                "            sd[\"vae_scale\"] = torch.tensor(0.18215)\n",
                "            sd[\"vae_shift\"] = torch.tensor(0.0)\n",
                "        elif name == \"taesdxl\":\n",
                "            sd[\"vae_scale\"] = torch.tensor(0.13025)\n",
                "            sd[\"vae_shift\"] = torch.tensor(0.0)\n",
                "        elif name == \"taesd3\":\n",
                "            sd[\"vae_scale\"] = torch.tensor(1.5305)\n",
                "            sd[\"vae_shift\"] = torch.tensor(0.0609)\n",
                "        return sd\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"vae_name\": (s.vae_list(), )}}\n",
                "    RETURN_TYPES = (\"VAE\",)\n",
                "    FUNCTION = \"load_vae\"\n",
                "\n",
                "    CATEGORY = \"loaders\"\n",
                "\n",
                "    #TODO: scale factor?\n",
                "    def load_vae(self, vae_name):\n",
                "        if vae_name in [\"taesd\", \"taesdxl\", \"taesd3\"]:\n",
                "            sd = self.load_taesd(vae_name)\n",
                "        else:\n",
                "            vae_path = folder_paths.get_full_path(\"vae\", vae_name)\n",
                "            sd = comfy.utils.load_torch_file(vae_path)\n",
                "        vae = comfy.sd.VAE(sd=sd)\n",
                "        return (vae,)\n",
                "\n",
                "class ControlNetLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"control_net_name\": (folder_paths.get_filename_list(\"controlnet\"), )}}\n",
                "\n",
                "    RETURN_TYPES = (\"CONTROL_NET\",)\n",
                "    FUNCTION = \"load_controlnet\"\n",
                "\n",
                "    CATEGORY = \"loaders\"\n",
                "\n",
                "    def load_controlnet(self, control_net_name):\n",
                "        controlnet_path = folder_paths.get_full_path(\"controlnet\", control_net_name)\n",
                "        controlnet = comfy.controlnet.load_controlnet(controlnet_path)\n",
                "        return (controlnet,)\n",
                "\n",
                "class DiffControlNetLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"model\": (\"MODEL\",),\n",
                "                              \"control_net_name\": (folder_paths.get_filename_list(\"controlnet\"), )}}\n",
                "\n",
                "    RETURN_TYPES = (\"CONTROL_NET\",)\n",
                "    FUNCTION = \"load_controlnet\"\n",
                "\n",
                "    CATEGORY = \"loaders\"\n",
                "\n",
                "    def load_controlnet(self, model, control_net_name):\n",
                "        controlnet_path = folder_paths.get_full_path(\"controlnet\", control_net_name)\n",
                "        controlnet = comfy.controlnet.load_controlnet(controlnet_path, model)\n",
                "        return (controlnet,)\n",
                "\n",
                "\n",
                "class ControlNetApply:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n",
                "                             \"control_net\": (\"CONTROL_NET\", ),\n",
                "                             \"image\": (\"IMAGE\", ),\n",
                "                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01})\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"apply_controlnet\"\n",
                "\n",
                "    CATEGORY = \"conditioning/controlnet\"\n",
                "\n",
                "    def apply_controlnet(self, conditioning, control_net, image, strength):\n",
                "        if strength == 0:\n",
                "            return (conditioning, )\n",
                "\n",
                "        c = []\n",
                "        control_hint = image.movedim(-1,1)\n",
                "        for t in conditioning:\n",
                "            n = [t[0], t[1].copy()]\n",
                "            c_net = control_net.copy().set_cond_hint(control_hint, strength)\n",
                "            if 'control' in t[1]:\n",
                "                c_net.set_previous_controlnet(t[1]['control'])\n",
                "            n[1]['control'] = c_net\n",
                "            n[1]['control_apply_to_uncond'] = True\n",
                "            c.append(n)\n",
                "        return (c, )\n",
                "\n",
                "\n",
                "class ControlNetApplyAdvanced:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n",
                "                             \"negative\": (\"CONDITIONING\", ),\n",
                "                             \"control_net\": (\"CONTROL_NET\", ),\n",
                "                             \"image\": (\"IMAGE\", ),\n",
                "                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n",
                "                             \"start_percent\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001}),\n",
                "                             \"end_percent\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.001})\n",
                "                             }}\n",
                "\n",
                "    RETURN_TYPES = (\"CONDITIONING\",\"CONDITIONING\")\n",
                "    RETURN_NAMES = (\"positive\", \"negative\")\n",
                "    FUNCTION = \"apply_controlnet\"\n",
                "\n",
                "    CATEGORY = \"conditioning/controlnet\"\n",
                "\n",
                "    def apply_controlnet(self, positive, negative, control_net, image, strength, start_percent, end_percent, vae=None):\n",
                "        if strength == 0:\n",
                "            return (positive, negative)\n",
                "\n",
                "        control_hint = image.movedim(-1,1)\n",
                "        cnets = {}\n",
                "\n",
                "        out = []\n",
                "        for conditioning in [positive, negative]:\n",
                "            c = []\n",
                "            for t in conditioning:\n",
                "                d = t[1].copy()\n",
                "\n",
                "                prev_cnet = d.get('control', None)\n",
                "                if prev_cnet in cnets:\n",
                "                    c_net = cnets[prev_cnet]\n",
                "                else:\n",
                "                    c_net = control_net.copy().set_cond_hint(control_hint, strength, (start_percent, end_percent), vae)\n",
                "                    c_net.set_previous_controlnet(prev_cnet)\n",
                "                    cnets[prev_cnet] = c_net\n",
                "\n",
                "                d['control'] = c_net\n",
                "                d['control_apply_to_uncond'] = False\n",
                "                n = [t[0], d]\n",
                "                c.append(n)\n",
                "            out.append(c)\n",
                "        return (out[0], out[1])\n",
                "\n",
                "\n",
                "class UNETLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"unet_name\": (folder_paths.get_filename_list(\"unet\"), ),\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "                              \"weight_dtype\": ([\"default\", \"fp8_e4m3fn\", \"fp8_e5m2\"],)\n"
                ],
                "parent_version_range": {
                    "start": 820,
                    "end": 820
                },
                "child_version_range": {
                    "start": 820,
                    "end": 821
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "UNETLoader",
                        "signature": "class UNETLoader:",
                        "at_line": 816
                    },
                    {
                        "type": "function",
                        "name": "INPUT_TYPES",
                        "signature": "def INPUT_TYPES(s):",
                        "at_line": 818
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: nodes.py\nCode:\n           class UNETLoader:\n               ...\n817 817        @classmethod\n818 818        def INPUT_TYPES(s):\n819 819            return {\"required\": { \"unet_name\": (folder_paths.get_filename_list(\"unet\"), ),\n    820  +                               \"weight_dtype\": ([\"default\", \"fp8_e4m3fn\", \"fp8_e5m2\"],)\n820 821                                 }}\n821 822        RETURN_TYPES = (\"MODEL\",)\n822 823        FUNCTION = \"load_unet\"\n         ...\n",
                "file_path": "nodes.py",
                "identifiers_before": [],
                "identifiers_after": [],
                "prefix": [
                    "    @classmethod\n",
                    "    def INPUT_TYPES(s):\n",
                    "        return {\"required\": { \"unet_name\": (folder_paths.get_filename_list(\"unet\"), ),\n"
                ],
                "suffix": [
                    "                             }}\n",
                    "    RETURN_TYPES = (\"MODEL\",)\n",
                    "    FUNCTION = \"load_unet\"\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "                             }}\n",
                "    RETURN_TYPES = (\"MODEL\",)\n",
                "    FUNCTION = \"load_unet\"\n",
                "\n",
                "    CATEGORY = \"advanced/loaders\"\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    def load_unet(self, unet_name):\n"
                ],
                "after": [
                    "    def load_unet(self, unet_name, weight_dtype):\n",
                    "        weight_dtype = {\"default\":None, \"fp8_e4m3fn\":torch.float8_e4m3fn, \"fp8_e5m2\":torch.float8_e4m3fn}[weight_dtype]\n"
                ],
                "parent_version_range": {
                    "start": 826,
                    "end": 827
                },
                "child_version_range": {
                    "start": 827,
                    "end": 829
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "UNETLoader",
                        "signature": "class UNETLoader:",
                        "at_line": 816
                    },
                    {
                        "type": "function",
                        "name": "load_unet",
                        "signature": "def load_unet(self, unet_name):",
                        "at_line": 826
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: nodes.py\nCode:\n           class UNETLoader:\n               ...\n823 824    \n824 825        CATEGORY = \"advanced/loaders\"\n825 826    \n826      -     def load_unet(self, unet_name):\n    827  +     def load_unet(self, unet_name, weight_dtype):\n    828  +         weight_dtype = {\"default\":None, \"fp8_e4m3fn\":torch.float8_e4m3fn, \"fp8_e5m2\":torch.float8_e4m3fn}[weight_dtype]\n827 829            unet_path = folder_paths.get_full_path(\"unet\", unet_name)\n         ...\n",
                "file_path": "nodes.py",
                "identifiers_before": [
                    "load_unet",
                    "self",
                    "unet_name"
                ],
                "identifiers_after": [
                    "float8_e4m3fn",
                    "load_unet",
                    "self",
                    "torch",
                    "unet_name",
                    "weight_dtype"
                ],
                "prefix": [
                    "\n",
                    "    CATEGORY = \"advanced/loaders\"\n",
                    "\n"
                ],
                "suffix": [
                    "        unet_path = folder_paths.get_full_path(\"unet\", unet_name)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        unet_path = folder_paths.get_full_path(\"unet\", unet_name)\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        model = comfy.sd.load_unet(unet_path)\n"
                ],
                "after": [
                    "        model = comfy.sd.load_unet(unet_path, dtype=weight_dtype)\n"
                ],
                "parent_version_range": {
                    "start": 828,
                    "end": 829
                },
                "child_version_range": {
                    "start": 830,
                    "end": 831
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "UNETLoader",
                        "signature": "class UNETLoader:",
                        "at_line": 816
                    },
                    {
                        "type": "function",
                        "name": "load_unet",
                        "signature": "def load_unet(self, unet_name):",
                        "at_line": 826
                    }
                ],
                "idx": 7,
                "hunk_diff": "File: nodes.py\nCode:\n           class UNETLoader:\n               ...\n               def load_unet(self, unet_name):\n                   ...\n827 829            unet_path = folder_paths.get_full_path(\"unet\", unet_name)\n828      -         model = comfy.sd.load_unet(unet_path)\n    830  +         model = comfy.sd.load_unet(unet_path, dtype=weight_dtype)\n829 831            return (model,)\n830 832    \n831 833    class CLIPLoader:\n         ...\n",
                "file_path": "nodes.py",
                "identifiers_before": [
                    "comfy",
                    "load_unet",
                    "model",
                    "sd",
                    "unet_path"
                ],
                "identifiers_after": [
                    "comfy",
                    "dtype",
                    "load_unet",
                    "model",
                    "sd",
                    "unet_path",
                    "weight_dtype"
                ],
                "prefix": [
                    "        unet_path = folder_paths.get_full_path(\"unet\", unet_name)\n"
                ],
                "suffix": [
                    "        return (model,)\n",
                    "\n",
                    "class CLIPLoader:\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "load_unet",
                            "position": {
                                "start": {
                                    "line": 828,
                                    "column": 25
                                },
                                "end": {
                                    "line": 828,
                                    "column": 34
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/nodes.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "load_unet",
                            "position": {
                                "start": {
                                    "line": 830,
                                    "column": 25
                                },
                                "end": {
                                    "line": 830,
                                    "column": 34
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/nodes.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "dtype",
                            "position": {
                                "start": {
                                    "line": 830,
                                    "column": 46
                                },
                                "end": {
                                    "line": 830,
                                    "column": 51
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/nodes.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        return (model,)\n",
                "\n",
                "class CLIPLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"clip_name\": (folder_paths.get_filename_list(\"clip\"), ),\n",
                "                              \"type\": ([\"stable_diffusion\", \"stable_cascade\", \"sd3\", \"stable_audio\"], ),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CLIP\",)\n",
                "    FUNCTION = \"load_clip\"\n",
                "\n",
                "    CATEGORY = \"advanced/loaders\"\n",
                "\n",
                "    def load_clip(self, clip_name, type=\"stable_diffusion\"):\n",
                "        if type == \"stable_cascade\":\n",
                "            clip_type = comfy.sd.CLIPType.STABLE_CASCADE\n",
                "        elif type == \"sd3\":\n",
                "            clip_type = comfy.sd.CLIPType.SD3\n",
                "        elif type == \"stable_audio\":\n",
                "            clip_type = comfy.sd.CLIPType.STABLE_AUDIO\n",
                "        else:\n",
                "            clip_type = comfy.sd.CLIPType.STABLE_DIFFUSION\n",
                "\n",
                "        clip_path = folder_paths.get_full_path(\"clip\", clip_name)\n",
                "        clip = comfy.sd.load_clip(ckpt_paths=[clip_path], embedding_directory=folder_paths.get_folder_paths(\"embeddings\"), clip_type=clip_type)\n",
                "        return (clip,)\n",
                "\n",
                "class DualCLIPLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"clip_name1\": (folder_paths.get_filename_list(\"clip\"), ),\n",
                "                              \"clip_name2\": (folder_paths.get_filename_list(\"clip\"), ),\n",
                "                              \"type\": ([\"sdxl\", \"sd3\", \"flux\"], ),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CLIP\",)\n",
                "    FUNCTION = \"load_clip\"\n",
                "\n",
                "    CATEGORY = \"advanced/loaders\"\n",
                "\n",
                "    def load_clip(self, clip_name1, clip_name2, type):\n",
                "        clip_path1 = folder_paths.get_full_path(\"clip\", clip_name1)\n",
                "        clip_path2 = folder_paths.get_full_path(\"clip\", clip_name2)\n",
                "        if type == \"sdxl\":\n",
                "            clip_type = comfy.sd.CLIPType.STABLE_DIFFUSION\n",
                "        elif type == \"sd3\":\n",
                "            clip_type = comfy.sd.CLIPType.SD3\n",
                "        elif type == \"flux\":\n",
                "            clip_type = comfy.sd.CLIPType.FLUX\n",
                "\n",
                "        clip = comfy.sd.load_clip(ckpt_paths=[clip_path1, clip_path2], embedding_directory=folder_paths.get_folder_paths(\"embeddings\"), clip_type=clip_type)\n",
                "        return (clip,)\n",
                "\n",
                "class CLIPVisionLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"clip_name\": (folder_paths.get_filename_list(\"clip_vision\"), ),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CLIP_VISION\",)\n",
                "    FUNCTION = \"load_clip\"\n",
                "\n",
                "    CATEGORY = \"loaders\"\n",
                "\n",
                "    def load_clip(self, clip_name):\n",
                "        clip_path = folder_paths.get_full_path(\"clip_vision\", clip_name)\n",
                "        clip_vision = comfy.clip_vision.load(clip_path)\n",
                "        return (clip_vision,)\n",
                "\n",
                "class CLIPVisionEncode:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"clip_vision\": (\"CLIP_VISION\",),\n",
                "                              \"image\": (\"IMAGE\",)\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CLIP_VISION_OUTPUT\",)\n",
                "    FUNCTION = \"encode\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def encode(self, clip_vision, image):\n",
                "        output = clip_vision.encode_image(image)\n",
                "        return (output,)\n",
                "\n",
                "class StyleModelLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"style_model_name\": (folder_paths.get_filename_list(\"style_models\"), )}}\n",
                "\n",
                "    RETURN_TYPES = (\"STYLE_MODEL\",)\n",
                "    FUNCTION = \"load_style_model\"\n",
                "\n",
                "    CATEGORY = \"loaders\"\n",
                "\n",
                "    def load_style_model(self, style_model_name):\n",
                "        style_model_path = folder_paths.get_full_path(\"style_models\", style_model_name)\n",
                "        style_model = comfy.sd.load_style_model(style_model_path)\n",
                "        return (style_model,)\n",
                "\n",
                "\n",
                "class StyleModelApply:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n",
                "                             \"style_model\": (\"STYLE_MODEL\", ),\n",
                "                             \"clip_vision_output\": (\"CLIP_VISION_OUTPUT\", ),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"apply_stylemodel\"\n",
                "\n",
                "    CATEGORY = \"conditioning/style_model\"\n",
                "\n",
                "    def apply_stylemodel(self, clip_vision_output, style_model, conditioning):\n",
                "        cond = style_model.get_cond(clip_vision_output).flatten(start_dim=0, end_dim=1).unsqueeze(dim=0)\n",
                "        c = []\n",
                "        for t in conditioning:\n",
                "            n = [torch.cat((t[0], cond), dim=1), t[1].copy()]\n",
                "            c.append(n)\n",
                "        return (c, )\n",
                "\n",
                "class unCLIPConditioning:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning\": (\"CONDITIONING\", ),\n",
                "                             \"clip_vision_output\": (\"CLIP_VISION_OUTPUT\", ),\n",
                "                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": -10.0, \"max\": 10.0, \"step\": 0.01}),\n",
                "                             \"noise_augmentation\": (\"FLOAT\", {\"default\": 0.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"apply_adm\"\n",
                "\n",
                "    CATEGORY = \"conditioning\"\n",
                "\n",
                "    def apply_adm(self, conditioning, clip_vision_output, strength, noise_augmentation):\n",
                "        if strength == 0:\n",
                "            return (conditioning, )\n",
                "\n",
                "        c = []\n",
                "        for t in conditioning:\n",
                "            o = t[1].copy()\n",
                "            x = {\"clip_vision_output\": clip_vision_output, \"strength\": strength, \"noise_augmentation\": noise_augmentation}\n",
                "            if \"unclip_conditioning\" in o:\n",
                "                o[\"unclip_conditioning\"] = o[\"unclip_conditioning\"][:] + [x]\n",
                "            else:\n",
                "                o[\"unclip_conditioning\"] = [x]\n",
                "            n = [t[0], o]\n",
                "            c.append(n)\n",
                "        return (c, )\n",
                "\n",
                "class GLIGENLoader:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"gligen_name\": (folder_paths.get_filename_list(\"gligen\"), )}}\n",
                "\n",
                "    RETURN_TYPES = (\"GLIGEN\",)\n",
                "    FUNCTION = \"load_gligen\"\n",
                "\n",
                "    CATEGORY = \"loaders\"\n",
                "\n",
                "    def load_gligen(self, gligen_name):\n",
                "        gligen_path = folder_paths.get_full_path(\"gligen\", gligen_name)\n",
                "        gligen = comfy.sd.load_gligen(gligen_path)\n",
                "        return (gligen,)\n",
                "\n",
                "class GLIGENTextBoxApply:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\"conditioning_to\": (\"CONDITIONING\", ),\n",
                "                              \"clip\": (\"CLIP\", ),\n",
                "                              \"gligen_textbox_model\": (\"GLIGEN\", ),\n",
                "                              \"text\": (\"STRING\", {\"multiline\": True, \"dynamicPrompts\": True}),\n",
                "                              \"width\": (\"INT\", {\"default\": 64, \"min\": 8, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"height\": (\"INT\", {\"default\": 64, \"min\": 8, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                             }}\n",
                "    RETURN_TYPES = (\"CONDITIONING\",)\n",
                "    FUNCTION = \"append\"\n",
                "\n",
                "    CATEGORY = \"conditioning/gligen\"\n",
                "\n",
                "    def append(self, conditioning_to, clip, gligen_textbox_model, text, width, height, x, y):\n",
                "        c = []\n",
                "        cond, cond_pooled = clip.encode_from_tokens(clip.tokenize(text), return_pooled=\"unprojected\")\n",
                "        for t in conditioning_to:\n",
                "            n = [t[0], t[1].copy()]\n",
                "            position_params = [(cond_pooled, height // 8, width // 8, y // 8, x // 8)]\n",
                "            prev = []\n",
                "            if \"gligen\" in n[1]:\n",
                "                prev = n[1]['gligen'][2]\n",
                "\n",
                "            n[1]['gligen'] = (\"position\", gligen_textbox_model, prev + position_params)\n",
                "            c.append(n)\n",
                "        return (c, )\n",
                "\n",
                "class EmptyLatentImage:\n",
                "    def __init__(self):\n",
                "        self.device = comfy.model_management.intermediate_device()\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"width\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"height\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096})}}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"generate\"\n",
                "\n",
                "    CATEGORY = \"latent\"\n",
                "\n",
                "    def generate(self, width, height, batch_size=1):\n",
                "        latent = torch.zeros([batch_size, 4, height // 8, width // 8], device=self.device)\n",
                "        return ({\"samples\":latent}, )\n",
                "\n",
                "\n",
                "class LatentFromBatch:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\",),\n",
                "                              \"batch_index\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 63}),\n",
                "                              \"length\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"frombatch\"\n",
                "\n",
                "    CATEGORY = \"latent/batch\"\n",
                "\n",
                "    def frombatch(self, samples, batch_index, length):\n",
                "        s = samples.copy()\n",
                "        s_in = samples[\"samples\"]\n",
                "        batch_index = min(s_in.shape[0] - 1, batch_index)\n",
                "        length = min(s_in.shape[0] - batch_index, length)\n",
                "        s[\"samples\"] = s_in[batch_index:batch_index + length].clone()\n",
                "        if \"noise_mask\" in samples:\n",
                "            masks = samples[\"noise_mask\"]\n",
                "            if masks.shape[0] == 1:\n",
                "                s[\"noise_mask\"] = masks.clone()\n",
                "            else:\n",
                "                if masks.shape[0] < s_in.shape[0]:\n",
                "                    masks = masks.repeat(math.ceil(s_in.shape[0] / masks.shape[0]), 1, 1, 1)[:s_in.shape[0]]\n",
                "                s[\"noise_mask\"] = masks[batch_index:batch_index + length].clone()\n",
                "        if \"batch_index\" not in s:\n",
                "            s[\"batch_index\"] = [x for x in range(batch_index, batch_index+length)]\n",
                "        else:\n",
                "            s[\"batch_index\"] = samples[\"batch_index\"][batch_index:batch_index + length]\n",
                "        return (s,)\n",
                "    \n",
                "class RepeatLatentBatch:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\",),\n",
                "                              \"amount\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64}),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"repeat\"\n",
                "\n",
                "    CATEGORY = \"latent/batch\"\n",
                "\n",
                "    def repeat(self, samples, amount):\n",
                "        s = samples.copy()\n",
                "        s_in = samples[\"samples\"]\n",
                "        \n",
                "        s[\"samples\"] = s_in.repeat((amount, 1,1,1))\n",
                "        if \"noise_mask\" in samples and samples[\"noise_mask\"].shape[0] > 1:\n",
                "            masks = samples[\"noise_mask\"]\n",
                "            if masks.shape[0] < s_in.shape[0]:\n",
                "                masks = masks.repeat(math.ceil(s_in.shape[0] / masks.shape[0]), 1, 1, 1)[:s_in.shape[0]]\n",
                "            s[\"noise_mask\"] = samples[\"noise_mask\"].repeat((amount, 1,1,1))\n",
                "        if \"batch_index\" in s:\n",
                "            offset = max(s[\"batch_index\"]) - min(s[\"batch_index\"]) + 1\n",
                "            s[\"batch_index\"] = s[\"batch_index\"] + [x + (i * offset) for i in range(1, amount) for x in s[\"batch_index\"]]\n",
                "        return (s,)\n",
                "\n",
                "class LatentUpscale:\n",
                "    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"bislerp\"]\n",
                "    crop_methods = [\"disabled\", \"center\"]\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\",), \"upscale_method\": (s.upscale_methods,),\n",
                "                              \"width\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"height\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"crop\": (s.crop_methods,)}}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"upscale\"\n",
                "\n",
                "    CATEGORY = \"latent\"\n",
                "\n",
                "    def upscale(self, samples, upscale_method, width, height, crop):\n",
                "        if width == 0 and height == 0:\n",
                "            s = samples\n",
                "        else:\n",
                "            s = samples.copy()\n",
                "\n",
                "            if width == 0:\n",
                "                height = max(64, height)\n",
                "                width = max(64, round(samples[\"samples\"].shape[3] * height / samples[\"samples\"].shape[2]))\n",
                "            elif height == 0:\n",
                "                width = max(64, width)\n",
                "                height = max(64, round(samples[\"samples\"].shape[2] * width / samples[\"samples\"].shape[3]))\n",
                "            else:\n",
                "                width = max(64, width)\n",
                "                height = max(64, height)\n",
                "\n",
                "            s[\"samples\"] = comfy.utils.common_upscale(samples[\"samples\"], width // 8, height // 8, upscale_method, crop)\n",
                "        return (s,)\n",
                "\n",
                "class LatentUpscaleBy:\n",
                "    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"bislerp\"]\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\",), \"upscale_method\": (s.upscale_methods,),\n",
                "                              \"scale_by\": (\"FLOAT\", {\"default\": 1.5, \"min\": 0.01, \"max\": 8.0, \"step\": 0.01}),}}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"upscale\"\n",
                "\n",
                "    CATEGORY = \"latent\"\n",
                "\n",
                "    def upscale(self, samples, upscale_method, scale_by):\n",
                "        s = samples.copy()\n",
                "        width = round(samples[\"samples\"].shape[3] * scale_by)\n",
                "        height = round(samples[\"samples\"].shape[2] * scale_by)\n",
                "        s[\"samples\"] = comfy.utils.common_upscale(samples[\"samples\"], width, height, upscale_method, \"disabled\")\n",
                "        return (s,)\n",
                "\n",
                "class LatentRotate:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\",),\n",
                "                              \"rotation\": ([\"none\", \"90 degrees\", \"180 degrees\", \"270 degrees\"],),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"rotate\"\n",
                "\n",
                "    CATEGORY = \"latent/transform\"\n",
                "\n",
                "    def rotate(self, samples, rotation):\n",
                "        s = samples.copy()\n",
                "        rotate_by = 0\n",
                "        if rotation.startswith(\"90\"):\n",
                "            rotate_by = 1\n",
                "        elif rotation.startswith(\"180\"):\n",
                "            rotate_by = 2\n",
                "        elif rotation.startswith(\"270\"):\n",
                "            rotate_by = 3\n",
                "\n",
                "        s[\"samples\"] = torch.rot90(samples[\"samples\"], k=rotate_by, dims=[3, 2])\n",
                "        return (s,)\n",
                "\n",
                "class LatentFlip:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\",),\n",
                "                              \"flip_method\": ([\"x-axis: vertically\", \"y-axis: horizontally\"],),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"flip\"\n",
                "\n",
                "    CATEGORY = \"latent/transform\"\n",
                "\n",
                "    def flip(self, samples, flip_method):\n",
                "        s = samples.copy()\n",
                "        if flip_method.startswith(\"x\"):\n",
                "            s[\"samples\"] = torch.flip(samples[\"samples\"], dims=[2])\n",
                "        elif flip_method.startswith(\"y\"):\n",
                "            s[\"samples\"] = torch.flip(samples[\"samples\"], dims=[3])\n",
                "\n",
                "        return (s,)\n",
                "\n",
                "class LatentComposite:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples_to\": (\"LATENT\",),\n",
                "                              \"samples_from\": (\"LATENT\",),\n",
                "                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"feather\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"composite\"\n",
                "\n",
                "    CATEGORY = \"latent\"\n",
                "\n",
                "    def composite(self, samples_to, samples_from, x, y, composite_method=\"normal\", feather=0):\n",
                "        x =  x // 8\n",
                "        y = y // 8\n",
                "        feather = feather // 8\n",
                "        samples_out = samples_to.copy()\n",
                "        s = samples_to[\"samples\"].clone()\n",
                "        samples_to = samples_to[\"samples\"]\n",
                "        samples_from = samples_from[\"samples\"]\n",
                "        if feather == 0:\n",
                "            s[:,:,y:y+samples_from.shape[2],x:x+samples_from.shape[3]] = samples_from[:,:,:samples_to.shape[2] - y, :samples_to.shape[3] - x]\n",
                "        else:\n",
                "            samples_from = samples_from[:,:,:samples_to.shape[2] - y, :samples_to.shape[3] - x]\n",
                "            mask = torch.ones_like(samples_from)\n",
                "            for t in range(feather):\n",
                "                if y != 0:\n",
                "                    mask[:,:,t:1+t,:] *= ((1.0/feather) * (t + 1))\n",
                "\n",
                "                if y + samples_from.shape[2] < samples_to.shape[2]:\n",
                "                    mask[:,:,mask.shape[2] -1 -t: mask.shape[2]-t,:] *= ((1.0/feather) * (t + 1))\n",
                "                if x != 0:\n",
                "                    mask[:,:,:,t:1+t] *= ((1.0/feather) * (t + 1))\n",
                "                if x + samples_from.shape[3] < samples_to.shape[3]:\n",
                "                    mask[:,:,:,mask.shape[3]- 1 - t: mask.shape[3]- t] *= ((1.0/feather) * (t + 1))\n",
                "            rev_mask = torch.ones_like(mask) - mask\n",
                "            s[:,:,y:y+samples_from.shape[2],x:x+samples_from.shape[3]] = samples_from[:,:,:samples_to.shape[2] - y, :samples_to.shape[3] - x] * mask + s[:,:,y:y+samples_from.shape[2],x:x+samples_from.shape[3]] * rev_mask\n",
                "        samples_out[\"samples\"] = s\n",
                "        return (samples_out,)\n",
                "\n",
                "class LatentBlend:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": {\n",
                "            \"samples1\": (\"LATENT\",),\n",
                "            \"samples2\": (\"LATENT\",),\n",
                "            \"blend_factor\": (\"FLOAT\", {\n",
                "                \"default\": 0.5,\n",
                "                \"min\": 0,\n",
                "                \"max\": 1,\n",
                "                \"step\": 0.01\n",
                "            }),\n",
                "        }}\n",
                "\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"blend\"\n",
                "\n",
                "    CATEGORY = \"_for_testing\"\n",
                "\n",
                "    def blend(self, samples1, samples2, blend_factor:float, blend_mode: str=\"normal\"):\n",
                "\n",
                "        samples_out = samples1.copy()\n",
                "        samples1 = samples1[\"samples\"]\n",
                "        samples2 = samples2[\"samples\"]\n",
                "\n",
                "        if samples1.shape != samples2.shape:\n",
                "            samples2.permute(0, 3, 1, 2)\n",
                "            samples2 = comfy.utils.common_upscale(samples2, samples1.shape[3], samples1.shape[2], 'bicubic', crop='center')\n",
                "            samples2.permute(0, 2, 3, 1)\n",
                "\n",
                "        samples_blended = self.blend_mode(samples1, samples2, blend_mode)\n",
                "        samples_blended = samples1 * blend_factor + samples_blended * (1 - blend_factor)\n",
                "        samples_out[\"samples\"] = samples_blended\n",
                "        return (samples_out,)\n",
                "\n",
                "    def blend_mode(self, img1, img2, mode):\n",
                "        if mode == \"normal\":\n",
                "            return img2\n",
                "        else:\n",
                "            raise ValueError(f\"Unsupported blend mode: {mode}\")\n",
                "\n",
                "class LatentCrop:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\",),\n",
                "                              \"width\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"height\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"x\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              \"y\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"crop\"\n",
                "\n",
                "    CATEGORY = \"latent/transform\"\n",
                "\n",
                "    def crop(self, samples, width, height, x, y):\n",
                "        s = samples.copy()\n",
                "        samples = samples['samples']\n",
                "        x =  x // 8\n",
                "        y = y // 8\n",
                "\n",
                "        #enfonce minimum size of 64\n",
                "        if x > (samples.shape[3] - 8):\n",
                "            x = samples.shape[3] - 8\n",
                "        if y > (samples.shape[2] - 8):\n",
                "            y = samples.shape[2] - 8\n",
                "\n",
                "        new_height = height // 8\n",
                "        new_width = width // 8\n",
                "        to_x = new_width + x\n",
                "        to_y = new_height + y\n",
                "        s['samples'] = samples[:,:,y:to_y, x:to_x]\n",
                "        return (s,)\n",
                "\n",
                "class SetLatentNoiseMask:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"samples\": (\"LATENT\",),\n",
                "                              \"mask\": (\"MASK\",),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"set_mask\"\n",
                "\n",
                "    CATEGORY = \"latent/inpaint\"\n",
                "\n",
                "    def set_mask(self, samples, mask):\n",
                "        s = samples.copy()\n",
                "        s[\"noise_mask\"] = mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1]))\n",
                "        return (s,)\n",
                "\n",
                "def common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n",
                "    latent_image = latent[\"samples\"]\n",
                "    latent_image = comfy.sample.fix_empty_latent_channels(model, latent_image)\n",
                "\n",
                "    if disable_noise:\n",
                "        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n",
                "    else:\n",
                "        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n",
                "        noise = comfy.sample.prepare_noise(latent_image, seed, batch_inds)\n",
                "\n",
                "    noise_mask = None\n",
                "    if \"noise_mask\" in latent:\n",
                "        noise_mask = latent[\"noise_mask\"]\n",
                "\n",
                "    callback = latent_preview.prepare_callback(model, steps)\n",
                "    disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED\n",
                "    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n",
                "                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,\n",
                "                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n",
                "    out = latent.copy()\n",
                "    out[\"samples\"] = samples\n",
                "    return (out, )\n",
                "\n",
                "class KSampler:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\":\n",
                "                    {\"model\": (\"MODEL\",),\n",
                "                    \"seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n",
                "                    \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n",
                "                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n",
                "                    \"sampler_name\": (comfy.samplers.KSampler.SAMPLERS, ),\n",
                "                    \"scheduler\": (comfy.samplers.KSampler.SCHEDULERS, ),\n",
                "                    \"positive\": (\"CONDITIONING\", ),\n",
                "                    \"negative\": (\"CONDITIONING\", ),\n",
                "                    \"latent_image\": (\"LATENT\", ),\n",
                "                    \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n",
                "                     }\n",
                "                }\n",
                "\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"sample\"\n",
                "\n",
                "    CATEGORY = \"sampling\"\n",
                "\n",
                "    def sample(self, model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0):\n",
                "        return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n",
                "\n",
                "class KSamplerAdvanced:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\":\n",
                "                    {\"model\": (\"MODEL\",),\n",
                "                    \"add_noise\": ([\"enable\", \"disable\"], ),\n",
                "                    \"noise_seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff}),\n",
                "                    \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n",
                "                    \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01}),\n",
                "                    \"sampler_name\": (comfy.samplers.KSampler.SAMPLERS, ),\n",
                "                    \"scheduler\": (comfy.samplers.KSampler.SCHEDULERS, ),\n",
                "                    \"positive\": (\"CONDITIONING\", ),\n",
                "                    \"negative\": (\"CONDITIONING\", ),\n",
                "                    \"latent_image\": (\"LATENT\", ),\n",
                "                    \"start_at_step\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 10000}),\n",
                "                    \"end_at_step\": (\"INT\", {\"default\": 10000, \"min\": 0, \"max\": 10000}),\n",
                "                    \"return_with_leftover_noise\": ([\"disable\", \"enable\"], ),\n",
                "                     }\n",
                "                }\n",
                "\n",
                "    RETURN_TYPES = (\"LATENT\",)\n",
                "    FUNCTION = \"sample\"\n",
                "\n",
                "    CATEGORY = \"sampling\"\n",
                "\n",
                "    def sample(self, model, add_noise, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, start_at_step, end_at_step, return_with_leftover_noise, denoise=1.0):\n",
                "        force_full_denoise = True\n",
                "        if return_with_leftover_noise == \"enable\":\n",
                "            force_full_denoise = False\n",
                "        disable_noise = False\n",
                "        if add_noise == \"disable\":\n",
                "            disable_noise = True\n",
                "        return common_ksampler(model, noise_seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise, disable_noise=disable_noise, start_step=start_at_step, last_step=end_at_step, force_full_denoise=force_full_denoise)\n",
                "\n",
                "class SaveImage:\n",
                "    def __init__(self):\n",
                "        self.output_dir = folder_paths.get_output_directory()\n",
                "        self.type = \"output\"\n",
                "        self.prefix_append = \"\"\n",
                "        self.compress_level = 4\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": \n",
                "                    {\"images\": (\"IMAGE\", ),\n",
                "                     \"filename_prefix\": (\"STRING\", {\"default\": \"ComfyUI\"})},\n",
                "                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n",
                "                }\n",
                "\n",
                "    RETURN_TYPES = ()\n",
                "    FUNCTION = \"save_images\"\n",
                "\n",
                "    OUTPUT_NODE = True\n",
                "\n",
                "    CATEGORY = \"image\"\n",
                "\n",
                "    def save_images(self, images, filename_prefix=\"ComfyUI\", prompt=None, extra_pnginfo=None):\n",
                "        filename_prefix += self.prefix_append\n",
                "        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n",
                "        results = list()\n",
                "        for (batch_number, image) in enumerate(images):\n",
                "            i = 255. * image.cpu().numpy()\n",
                "            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n",
                "            metadata = None\n",
                "            if not args.disable_metadata:\n",
                "                metadata = PngInfo()\n",
                "                if prompt is not None:\n",
                "                    metadata.add_text(\"prompt\", json.dumps(prompt))\n",
                "                if extra_pnginfo is not None:\n",
                "                    for x in extra_pnginfo:\n",
                "                        metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n",
                "\n",
                "            filename_with_batch_num = filename.replace(\"%batch_num%\", str(batch_number))\n",
                "            file = f\"{filename_with_batch_num}_{counter:05}_.png\"\n",
                "            img.save(os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=self.compress_level)\n",
                "            results.append({\n",
                "                \"filename\": file,\n",
                "                \"subfolder\": subfolder,\n",
                "                \"type\": self.type\n",
                "            })\n",
                "            counter += 1\n",
                "\n",
                "        return { \"ui\": { \"images\": results } }\n",
                "\n",
                "class PreviewImage(SaveImage):\n",
                "    def __init__(self):\n",
                "        self.output_dir = folder_paths.get_temp_directory()\n",
                "        self.type = \"temp\"\n",
                "        self.prefix_append = \"_temp_\" + ''.join(random.choice(\"abcdefghijklmnopqrstupvxyz\") for x in range(5))\n",
                "        self.compress_level = 1\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\":\n",
                "                    {\"images\": (\"IMAGE\", ), },\n",
                "                \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n",
                "                }\n",
                "\n",
                "class LoadImage:\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        input_dir = folder_paths.get_input_directory()\n",
                "        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n",
                "        return {\"required\":\n",
                "                    {\"image\": (sorted(files), {\"image_upload\": True})},\n",
                "                }\n",
                "\n",
                "    CATEGORY = \"image\"\n",
                "\n",
                "    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n",
                "    FUNCTION = \"load_image\"\n",
                "    def load_image(self, image):\n",
                "        image_path = folder_paths.get_annotated_filepath(image)\n",
                "        \n",
                "        img = node_helpers.pillow(Image.open, image_path)\n",
                "        \n",
                "        output_images = []\n",
                "        output_masks = []\n",
                "        w, h = None, None\n",
                "\n",
                "        excluded_formats = ['MPO']\n",
                "        \n",
                "        for i in ImageSequence.Iterator(img):\n",
                "            i = node_helpers.pillow(ImageOps.exif_transpose, i)\n",
                "\n",
                "            if i.mode == 'I':\n",
                "                i = i.point(lambda i: i * (1 / 255))\n",
                "            image = i.convert(\"RGB\")\n",
                "\n",
                "            if len(output_images) == 0:\n",
                "                w = image.size[0]\n",
                "                h = image.size[1]\n",
                "            \n",
                "            if image.size[0] != w or image.size[1] != h:\n",
                "                continue\n",
                "            \n",
                "            image = np.array(image).astype(np.float32) / 255.0\n",
                "            image = torch.from_numpy(image)[None,]\n",
                "            if 'A' in i.getbands():\n",
                "                mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0\n",
                "                mask = 1. - torch.from_numpy(mask)\n",
                "            else:\n",
                "                mask = torch.zeros((64,64), dtype=torch.float32, device=\"cpu\")\n",
                "            output_images.append(image)\n",
                "            output_masks.append(mask.unsqueeze(0))\n",
                "\n",
                "        if len(output_images) > 1 and img.format not in excluded_formats:\n",
                "            output_image = torch.cat(output_images, dim=0)\n",
                "            output_mask = torch.cat(output_masks, dim=0)\n",
                "        else:\n",
                "            output_image = output_images[0]\n",
                "            output_mask = output_masks[0]\n",
                "\n",
                "        return (output_image, output_mask)\n",
                "\n",
                "    @classmethod\n",
                "    def IS_CHANGED(s, image):\n",
                "        image_path = folder_paths.get_annotated_filepath(image)\n",
                "        m = hashlib.sha256()\n",
                "        with open(image_path, 'rb') as f:\n",
                "            m.update(f.read())\n",
                "        return m.digest().hex()\n",
                "\n",
                "    @classmethod\n",
                "    def VALIDATE_INPUTS(s, image):\n",
                "        if not folder_paths.exists_annotated_filepath(image):\n",
                "            return \"Invalid image file: {}\".format(image)\n",
                "\n",
                "        return True\n",
                "\n",
                "class LoadImageMask:\n",
                "    _color_channels = [\"alpha\", \"red\", \"green\", \"blue\"]\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        input_dir = folder_paths.get_input_directory()\n",
                "        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n",
                "        return {\"required\":\n",
                "                    {\"image\": (sorted(files), {\"image_upload\": True}),\n",
                "                     \"channel\": (s._color_channels, ), }\n",
                "                }\n",
                "\n",
                "    CATEGORY = \"mask\"\n",
                "\n",
                "    RETURN_TYPES = (\"MASK\",)\n",
                "    FUNCTION = \"load_image\"\n",
                "    def load_image(self, image, channel):\n",
                "        image_path = folder_paths.get_annotated_filepath(image)\n",
                "        i = node_helpers.pillow(Image.open, image_path)\n",
                "        i = node_helpers.pillow(ImageOps.exif_transpose, i)\n",
                "        if i.getbands() != (\"R\", \"G\", \"B\", \"A\"):\n",
                "            if i.mode == 'I':\n",
                "                i = i.point(lambda i: i * (1 / 255))\n",
                "            i = i.convert(\"RGBA\")\n",
                "        mask = None\n",
                "        c = channel[0].upper()\n",
                "        if c in i.getbands():\n",
                "            mask = np.array(i.getchannel(c)).astype(np.float32) / 255.0\n",
                "            mask = torch.from_numpy(mask)\n",
                "            if c == 'A':\n",
                "                mask = 1. - mask\n",
                "        else:\n",
                "            mask = torch.zeros((64,64), dtype=torch.float32, device=\"cpu\")\n",
                "        return (mask.unsqueeze(0),)\n",
                "\n",
                "    @classmethod\n",
                "    def IS_CHANGED(s, image, channel):\n",
                "        image_path = folder_paths.get_annotated_filepath(image)\n",
                "        m = hashlib.sha256()\n",
                "        with open(image_path, 'rb') as f:\n",
                "            m.update(f.read())\n",
                "        return m.digest().hex()\n",
                "\n",
                "    @classmethod\n",
                "    def VALIDATE_INPUTS(s, image):\n",
                "        if not folder_paths.exists_annotated_filepath(image):\n",
                "            return \"Invalid image file: {}\".format(image)\n",
                "\n",
                "        return True\n",
                "\n",
                "class ImageScale:\n",
                "    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n",
                "    crop_methods = [\"disabled\", \"center\"]\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n",
                "                              \"width\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n",
                "                              \"height\": (\"INT\", {\"default\": 512, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n",
                "                              \"crop\": (s.crop_methods,)}}\n",
                "    RETURN_TYPES = (\"IMAGE\",)\n",
                "    FUNCTION = \"upscale\"\n",
                "\n",
                "    CATEGORY = \"image/upscaling\"\n",
                "\n",
                "    def upscale(self, image, upscale_method, width, height, crop):\n",
                "        if width == 0 and height == 0:\n",
                "            s = image\n",
                "        else:\n",
                "            samples = image.movedim(-1,1)\n",
                "\n",
                "            if width == 0:\n",
                "                width = max(1, round(samples.shape[3] * height / samples.shape[2]))\n",
                "            elif height == 0:\n",
                "                height = max(1, round(samples.shape[2] * width / samples.shape[3]))\n",
                "\n",
                "            s = comfy.utils.common_upscale(samples, width, height, upscale_method, crop)\n",
                "            s = s.movedim(1,-1)\n",
                "        return (s,)\n",
                "\n",
                "class ImageScaleBy:\n",
                "    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"image\": (\"IMAGE\",), \"upscale_method\": (s.upscale_methods,),\n",
                "                              \"scale_by\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.01, \"max\": 8.0, \"step\": 0.01}),}}\n",
                "    RETURN_TYPES = (\"IMAGE\",)\n",
                "    FUNCTION = \"upscale\"\n",
                "\n",
                "    CATEGORY = \"image/upscaling\"\n",
                "\n",
                "    def upscale(self, image, upscale_method, scale_by):\n",
                "        samples = image.movedim(-1,1)\n",
                "        width = round(samples.shape[3] * scale_by)\n",
                "        height = round(samples.shape[2] * scale_by)\n",
                "        s = comfy.utils.common_upscale(samples, width, height, upscale_method, \"disabled\")\n",
                "        s = s.movedim(1,-1)\n",
                "        return (s,)\n",
                "\n",
                "class ImageInvert:\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"image\": (\"IMAGE\",)}}\n",
                "\n",
                "    RETURN_TYPES = (\"IMAGE\",)\n",
                "    FUNCTION = \"invert\"\n",
                "\n",
                "    CATEGORY = \"image\"\n",
                "\n",
                "    def invert(self, image):\n",
                "        s = 1.0 - image\n",
                "        return (s,)\n",
                "\n",
                "class ImageBatch:\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"image1\": (\"IMAGE\",), \"image2\": (\"IMAGE\",)}}\n",
                "\n",
                "    RETURN_TYPES = (\"IMAGE\",)\n",
                "    FUNCTION = \"batch\"\n",
                "\n",
                "    CATEGORY = \"image\"\n",
                "\n",
                "    def batch(self, image1, image2):\n",
                "        if image1.shape[1:] != image2.shape[1:]:\n",
                "            image2 = comfy.utils.common_upscale(image2.movedim(-1,1), image1.shape[2], image1.shape[1], \"bilinear\", \"center\").movedim(1,-1)\n",
                "        s = torch.cat((image1, image2), dim=0)\n",
                "        return (s,)\n",
                "\n",
                "class EmptyImage:\n",
                "    def __init__(self, device=\"cpu\"):\n",
                "        self.device = device\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\"required\": { \"width\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n",
                "                              \"height\": (\"INT\", {\"default\": 512, \"min\": 1, \"max\": MAX_RESOLUTION, \"step\": 1}),\n",
                "                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n",
                "                              \"color\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xFFFFFF, \"step\": 1, \"display\": \"color\"}),\n",
                "                              }}\n",
                "    RETURN_TYPES = (\"IMAGE\",)\n",
                "    FUNCTION = \"generate\"\n",
                "\n",
                "    CATEGORY = \"image\"\n",
                "\n",
                "    def generate(self, width, height, batch_size=1, color=0):\n",
                "        r = torch.full([batch_size, height, width, 1], ((color >> 16) & 0xFF) / 0xFF)\n",
                "        g = torch.full([batch_size, height, width, 1], ((color >> 8) & 0xFF) / 0xFF)\n",
                "        b = torch.full([batch_size, height, width, 1], ((color) & 0xFF) / 0xFF)\n",
                "        return (torch.cat((r, g, b), dim=-1), )\n",
                "\n",
                "class ImagePadForOutpaint:\n",
                "\n",
                "    @classmethod\n",
                "    def INPUT_TYPES(s):\n",
                "        return {\n",
                "            \"required\": {\n",
                "                \"image\": (\"IMAGE\",),\n",
                "                \"left\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                \"top\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                \"right\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                \"bottom\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 8}),\n",
                "                \"feathering\": (\"INT\", {\"default\": 40, \"min\": 0, \"max\": MAX_RESOLUTION, \"step\": 1}),\n",
                "            }\n",
                "        }\n",
                "\n",
                "    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n",
                "    FUNCTION = \"expand_image\"\n",
                "\n",
                "    CATEGORY = \"image\"\n",
                "\n",
                "    def expand_image(self, image, left, top, right, bottom, feathering):\n",
                "        d1, d2, d3, d4 = image.size()\n",
                "\n",
                "        new_image = torch.ones(\n",
                "            (d1, d2 + top + bottom, d3 + left + right, d4),\n",
                "            dtype=torch.float32,\n",
                "        ) * 0.5\n",
                "\n",
                "        new_image[:, top:top + d2, left:left + d3, :] = image\n",
                "\n",
                "        mask = torch.ones(\n",
                "            (d2 + top + bottom, d3 + left + right),\n",
                "            dtype=torch.float32,\n",
                "        )\n",
                "\n",
                "        t = torch.zeros(\n",
                "            (d2, d3),\n",
                "            dtype=torch.float32\n",
                "        )\n",
                "\n",
                "        if feathering > 0 and feathering * 2 < d2 and feathering * 2 < d3:\n",
                "\n",
                "            for i in range(d2):\n",
                "                for j in range(d3):\n",
                "                    dt = i if top != 0 else d2\n",
                "                    db = d2 - i if bottom != 0 else d2\n",
                "\n",
                "                    dl = j if left != 0 else d3\n",
                "                    dr = d3 - j if right != 0 else d3\n",
                "\n",
                "                    d = min(dt, db, dl, dr)\n",
                "\n",
                "                    if d >= feathering:\n",
                "                        continue\n",
                "\n",
                "                    v = (feathering - d) / feathering\n",
                "\n",
                "                    t[i, j] = v * v\n",
                "\n",
                "        mask[top:top + d2, left:left + d3] = t\n",
                "\n",
                "        return (new_image, mask)\n",
                "\n",
                "\n",
                "NODE_CLASS_MAPPINGS = {\n",
                "    \"KSampler\": KSampler,\n",
                "    \"CheckpointLoaderSimple\": CheckpointLoaderSimple,\n",
                "    \"CLIPTextEncode\": CLIPTextEncode,\n",
                "    \"CLIPSetLastLayer\": CLIPSetLastLayer,\n",
                "    \"VAEDecode\": VAEDecode,\n",
                "    \"VAEEncode\": VAEEncode,\n",
                "    \"VAEEncodeForInpaint\": VAEEncodeForInpaint,\n",
                "    \"VAELoader\": VAELoader,\n",
                "    \"EmptyLatentImage\": EmptyLatentImage,\n",
                "    \"LatentUpscale\": LatentUpscale,\n",
                "    \"LatentUpscaleBy\": LatentUpscaleBy,\n",
                "    \"LatentFromBatch\": LatentFromBatch,\n",
                "    \"RepeatLatentBatch\": RepeatLatentBatch,\n",
                "    \"SaveImage\": SaveImage,\n",
                "    \"PreviewImage\": PreviewImage,\n",
                "    \"LoadImage\": LoadImage,\n",
                "    \"LoadImageMask\": LoadImageMask,\n",
                "    \"ImageScale\": ImageScale,\n",
                "    \"ImageScaleBy\": ImageScaleBy,\n",
                "    \"ImageInvert\": ImageInvert,\n",
                "    \"ImageBatch\": ImageBatch,\n",
                "    \"ImagePadForOutpaint\": ImagePadForOutpaint,\n",
                "    \"EmptyImage\": EmptyImage,\n",
                "    \"ConditioningAverage\": ConditioningAverage ,\n",
                "    \"ConditioningCombine\": ConditioningCombine,\n",
                "    \"ConditioningConcat\": ConditioningConcat,\n",
                "    \"ConditioningSetArea\": ConditioningSetArea,\n",
                "    \"ConditioningSetAreaPercentage\": ConditioningSetAreaPercentage,\n",
                "    \"ConditioningSetAreaStrength\": ConditioningSetAreaStrength,\n",
                "    \"ConditioningSetMask\": ConditioningSetMask,\n",
                "    \"KSamplerAdvanced\": KSamplerAdvanced,\n",
                "    \"SetLatentNoiseMask\": SetLatentNoiseMask,\n",
                "    \"LatentComposite\": LatentComposite,\n",
                "    \"LatentBlend\": LatentBlend,\n",
                "    \"LatentRotate\": LatentRotate,\n",
                "    \"LatentFlip\": LatentFlip,\n",
                "    \"LatentCrop\": LatentCrop,\n",
                "    \"LoraLoader\": LoraLoader,\n",
                "    \"CLIPLoader\": CLIPLoader,\n",
                "    \"UNETLoader\": UNETLoader,\n",
                "    \"DualCLIPLoader\": DualCLIPLoader,\n",
                "    \"CLIPVisionEncode\": CLIPVisionEncode,\n",
                "    \"StyleModelApply\": StyleModelApply,\n",
                "    \"unCLIPConditioning\": unCLIPConditioning,\n",
                "    \"ControlNetApply\": ControlNetApply,\n",
                "    \"ControlNetApplyAdvanced\": ControlNetApplyAdvanced,\n",
                "    \"ControlNetLoader\": ControlNetLoader,\n",
                "    \"DiffControlNetLoader\": DiffControlNetLoader,\n",
                "    \"StyleModelLoader\": StyleModelLoader,\n",
                "    \"CLIPVisionLoader\": CLIPVisionLoader,\n",
                "    \"VAEDecodeTiled\": VAEDecodeTiled,\n",
                "    \"VAEEncodeTiled\": VAEEncodeTiled,\n",
                "    \"unCLIPCheckpointLoader\": unCLIPCheckpointLoader,\n",
                "    \"GLIGENLoader\": GLIGENLoader,\n",
                "    \"GLIGENTextBoxApply\": GLIGENTextBoxApply,\n",
                "    \"InpaintModelConditioning\": InpaintModelConditioning,\n",
                "\n",
                "    \"CheckpointLoader\": CheckpointLoader,\n",
                "    \"DiffusersLoader\": DiffusersLoader,\n",
                "\n",
                "    \"LoadLatent\": LoadLatent,\n",
                "    \"SaveLatent\": SaveLatent,\n",
                "\n",
                "    \"ConditioningZeroOut\": ConditioningZeroOut,\n",
                "    \"ConditioningSetTimestepRange\": ConditioningSetTimestepRange,\n",
                "    \"LoraLoaderModelOnly\": LoraLoaderModelOnly,\n",
                "}\n",
                "\n",
                "NODE_DISPLAY_NAME_MAPPINGS = {\n",
                "    # Sampling\n",
                "    \"KSampler\": \"KSampler\",\n",
                "    \"KSamplerAdvanced\": \"KSampler (Advanced)\",\n",
                "    # Loaders\n",
                "    \"CheckpointLoader\": \"Load Checkpoint With Config (DEPRECATED)\",\n",
                "    \"CheckpointLoaderSimple\": \"Load Checkpoint\",\n",
                "    \"VAELoader\": \"Load VAE\",\n",
                "    \"LoraLoader\": \"Load LoRA\",\n",
                "    \"CLIPLoader\": \"Load CLIP\",\n",
                "    \"ControlNetLoader\": \"Load ControlNet Model\",\n",
                "    \"DiffControlNetLoader\": \"Load ControlNet Model (diff)\",\n",
                "    \"StyleModelLoader\": \"Load Style Model\",\n",
                "    \"CLIPVisionLoader\": \"Load CLIP Vision\",\n",
                "    \"UpscaleModelLoader\": \"Load Upscale Model\",\n",
                "    # Conditioning\n",
                "    \"CLIPVisionEncode\": \"CLIP Vision Encode\",\n",
                "    \"StyleModelApply\": \"Apply Style Model\",\n",
                "    \"CLIPTextEncode\": \"CLIP Text Encode (Prompt)\",\n",
                "    \"CLIPSetLastLayer\": \"CLIP Set Last Layer\",\n",
                "    \"ConditioningCombine\": \"Conditioning (Combine)\",\n",
                "    \"ConditioningAverage \": \"Conditioning (Average)\",\n",
                "    \"ConditioningConcat\": \"Conditioning (Concat)\",\n",
                "    \"ConditioningSetArea\": \"Conditioning (Set Area)\",\n",
                "    \"ConditioningSetAreaPercentage\": \"Conditioning (Set Area with Percentage)\",\n",
                "    \"ConditioningSetMask\": \"Conditioning (Set Mask)\",\n",
                "    \"ControlNetApply\": \"Apply ControlNet\",\n",
                "    \"ControlNetApplyAdvanced\": \"Apply ControlNet (Advanced)\",\n",
                "    # Latent\n",
                "    \"VAEEncodeForInpaint\": \"VAE Encode (for Inpainting)\",\n",
                "    \"SetLatentNoiseMask\": \"Set Latent Noise Mask\",\n",
                "    \"VAEDecode\": \"VAE Decode\",\n",
                "    \"VAEEncode\": \"VAE Encode\",\n",
                "    \"LatentRotate\": \"Rotate Latent\",\n",
                "    \"LatentFlip\": \"Flip Latent\",\n",
                "    \"LatentCrop\": \"Crop Latent\",\n",
                "    \"EmptyLatentImage\": \"Empty Latent Image\",\n",
                "    \"LatentUpscale\": \"Upscale Latent\",\n",
                "    \"LatentUpscaleBy\": \"Upscale Latent By\",\n",
                "    \"LatentComposite\": \"Latent Composite\",\n",
                "    \"LatentBlend\": \"Latent Blend\",\n",
                "    \"LatentFromBatch\" : \"Latent From Batch\",\n",
                "    \"RepeatLatentBatch\": \"Repeat Latent Batch\",\n",
                "    # Image\n",
                "    \"SaveImage\": \"Save Image\",\n",
                "    \"PreviewImage\": \"Preview Image\",\n",
                "    \"LoadImage\": \"Load Image\",\n",
                "    \"LoadImageMask\": \"Load Image (as Mask)\",\n",
                "    \"ImageScale\": \"Upscale Image\",\n",
                "    \"ImageScaleBy\": \"Upscale Image By\",\n",
                "    \"ImageUpscaleWithModel\": \"Upscale Image (using Model)\",\n",
                "    \"ImageInvert\": \"Invert Image\",\n",
                "    \"ImagePadForOutpaint\": \"Pad Image for Outpainting\",\n",
                "    \"ImageBatch\": \"Batch Images\",\n",
                "    # _for_testing\n",
                "    \"VAEDecodeTiled\": \"VAE Decode (Tiled)\",\n",
                "    \"VAEEncodeTiled\": \"VAE Encode (Tiled)\",\n",
                "}\n",
                "\n",
                "EXTENSION_WEB_DIRS = {}\n",
                "\n",
                "\n",
                "def get_module_name(module_path: str) -> str:\n",
                "    \"\"\"\n",
                "    Returns the module name based on the given module path.\n",
                "    Examples:\n",
                "        get_module_name(\"C:/Users/username/ComfyUI/custom_nodes/my_custom_node.py\") -> \"my_custom_node\"\n",
                "        get_module_name(\"C:/Users/username/ComfyUI/custom_nodes/my_custom_node\") -> \"my_custom_node\"\n",
                "        get_module_name(\"C:/Users/username/ComfyUI/custom_nodes/my_custom_node/\") -> \"my_custom_node\"\n",
                "        get_module_name(\"C:/Users/username/ComfyUI/custom_nodes/my_custom_node/__init__.py\") -> \"my_custom_node\"\n",
                "        get_module_name(\"C:/Users/username/ComfyUI/custom_nodes/my_custom_node/__init__\") -> \"my_custom_node\"\n",
                "        get_module_name(\"C:/Users/username/ComfyUI/custom_nodes/my_custom_node/__init__/\") -> \"my_custom_node\"\n",
                "        get_module_name(\"C:/Users/username/ComfyUI/custom_nodes/my_custom_node.disabled\") -> \"custom_nodes\n",
                "    Args:\n",
                "        module_path (str): The path of the module.\n",
                "    Returns:\n",
                "        str: The module name.\n",
                "    \"\"\"\n",
                "    base_path = os.path.basename(module_path)\n",
                "    if os.path.isfile(module_path):\n",
                "        base_path = os.path.splitext(base_path)[0]\n",
                "    return base_path\n",
                "\n",
                "\n",
                "def load_custom_node(module_path: str, ignore=set(), module_parent=\"custom_nodes\") -> bool:\n",
                "    module_name = os.path.basename(module_path)\n",
                "    if os.path.isfile(module_path):\n",
                "        sp = os.path.splitext(module_path)\n",
                "        module_name = sp[0]\n",
                "    try:\n",
                "        logging.debug(\"Trying to load custom node {}\".format(module_path))\n",
                "        if os.path.isfile(module_path):\n",
                "            module_spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
                "            module_dir = os.path.split(module_path)[0]\n",
                "        else:\n",
                "            module_spec = importlib.util.spec_from_file_location(module_name, os.path.join(module_path, \"__init__.py\"))\n",
                "            module_dir = module_path\n",
                "\n",
                "        module = importlib.util.module_from_spec(module_spec)\n",
                "        sys.modules[module_name] = module\n",
                "        module_spec.loader.exec_module(module)\n",
                "\n",
                "        if hasattr(module, \"WEB_DIRECTORY\") and getattr(module, \"WEB_DIRECTORY\") is not None:\n",
                "            web_dir = os.path.abspath(os.path.join(module_dir, getattr(module, \"WEB_DIRECTORY\")))\n",
                "            if os.path.isdir(web_dir):\n",
                "                EXTENSION_WEB_DIRS[module_name] = web_dir\n",
                "\n",
                "        if hasattr(module, \"NODE_CLASS_MAPPINGS\") and getattr(module, \"NODE_CLASS_MAPPINGS\") is not None:\n",
                "            for name, node_cls in module.NODE_CLASS_MAPPINGS.items():\n",
                "                if name not in ignore:\n",
                "                    NODE_CLASS_MAPPINGS[name] = node_cls\n",
                "                    node_cls.RELATIVE_PYTHON_MODULE = \"{}.{}\".format(module_parent, get_module_name(module_path))\n",
                "            if hasattr(module, \"NODE_DISPLAY_NAME_MAPPINGS\") and getattr(module, \"NODE_DISPLAY_NAME_MAPPINGS\") is not None:\n",
                "                NODE_DISPLAY_NAME_MAPPINGS.update(module.NODE_DISPLAY_NAME_MAPPINGS)\n",
                "            return True\n",
                "        else:\n",
                "            logging.warning(f\"Skip {module_path} module for custom nodes due to the lack of NODE_CLASS_MAPPINGS.\")\n",
                "            return False\n",
                "    except Exception as e:\n",
                "        logging.warning(traceback.format_exc())\n",
                "        logging.warning(f\"Cannot import {module_path} module for custom nodes: {e}\")\n",
                "        return False\n",
                "\n",
                "def init_external_custom_nodes():\n",
                "    \"\"\"\n",
                "    Initializes the external custom nodes.\n",
                "\n",
                "    This function loads custom nodes from the specified folder paths and imports them into the application.\n",
                "    It measures the import times for each custom node and logs the results.\n",
                "\n",
                "    Returns:\n",
                "        None\n",
                "    \"\"\"\n",
                "    base_node_names = set(NODE_CLASS_MAPPINGS.keys())\n",
                "    node_paths = folder_paths.get_folder_paths(\"custom_nodes\")\n",
                "    node_import_times = []\n",
                "    for custom_node_path in node_paths:\n",
                "        possible_modules = os.listdir(os.path.realpath(custom_node_path))\n",
                "        if \"__pycache__\" in possible_modules:\n",
                "            possible_modules.remove(\"__pycache__\")\n",
                "\n",
                "        for possible_module in possible_modules:\n",
                "            module_path = os.path.join(custom_node_path, possible_module)\n",
                "            if os.path.isfile(module_path) and os.path.splitext(module_path)[1] != \".py\": continue\n",
                "            if module_path.endswith(\".disabled\"): continue\n",
                "            time_before = time.perf_counter()\n",
                "            success = load_custom_node(module_path, base_node_names, module_parent=\"custom_nodes\")\n",
                "            node_import_times.append((time.perf_counter() - time_before, module_path, success))\n",
                "\n",
                "    if len(node_import_times) > 0:\n",
                "        logging.info(\"\\nImport times for custom nodes:\")\n",
                "        for n in sorted(node_import_times):\n",
                "            if n[2]:\n",
                "                import_message = \"\"\n",
                "            else:\n",
                "                import_message = \" (IMPORT FAILED)\"\n",
                "            logging.info(\"{:6.1f} seconds{}: {}\".format(n[0], import_message, n[1]))\n",
                "        logging.info(\"\")\n",
                "\n",
                "def init_builtin_extra_nodes():\n",
                "    \"\"\"\n",
                "    Initializes the built-in extra nodes in ComfyUI.\n",
                "\n",
                "    This function loads the extra node files located in the \"comfy_extras\" directory and imports them into ComfyUI.\n",
                "    If any of the extra node files fail to import, a warning message is logged.\n",
                "\n",
                "    Returns:\n",
                "        None\n",
                "    \"\"\"\n",
                "    extras_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"comfy_extras\")\n",
                "    extras_files = [\n",
                "        \"nodes_latent.py\",\n",
                "        \"nodes_hypernetwork.py\",\n",
                "        \"nodes_upscale_model.py\",\n",
                "        \"nodes_post_processing.py\",\n",
                "        \"nodes_mask.py\",\n",
                "        \"nodes_compositing.py\",\n",
                "        \"nodes_rebatch.py\",\n",
                "        \"nodes_model_merging.py\",\n",
                "        \"nodes_tomesd.py\",\n",
                "        \"nodes_clip_sdxl.py\",\n",
                "        \"nodes_canny.py\",\n",
                "        \"nodes_freelunch.py\",\n",
                "        \"nodes_custom_sampler.py\",\n",
                "        \"nodes_hypertile.py\",\n",
                "        \"nodes_model_advanced.py\",\n",
                "        \"nodes_model_downscale.py\",\n",
                "        \"nodes_images.py\",\n",
                "        \"nodes_video_model.py\",\n",
                "        \"nodes_sag.py\",\n",
                "        \"nodes_perpneg.py\",\n",
                "        \"nodes_stable3d.py\",\n",
                "        \"nodes_sdupscale.py\",\n",
                "        \"nodes_photomaker.py\",\n",
                "        \"nodes_cond.py\",\n",
                "        \"nodes_morphology.py\",\n",
                "        \"nodes_stable_cascade.py\",\n",
                "        \"nodes_differential_diffusion.py\",\n",
                "        \"nodes_ip2p.py\",\n",
                "        \"nodes_model_merging_model_specific.py\",\n",
                "        \"nodes_pag.py\",\n",
                "        \"nodes_align_your_steps.py\",\n",
                "        \"nodes_attention_multiply.py\",\n",
                "        \"nodes_advanced_samplers.py\",\n",
                "        \"nodes_webcam.py\",\n",
                "        \"nodes_audio.py\",\n",
                "        \"nodes_sd3.py\",\n",
                "        \"nodes_gits.py\",\n",
                "        \"nodes_controlnet.py\",\n",
                "        \"nodes_hunyuan.py\",\n",
                "    ]\n",
                "\n",
                "    import_failed = []\n",
                "    for node_file in extras_files:\n",
                "        if not load_custom_node(os.path.join(extras_dir, node_file), module_parent=\"comfy_extras\"):\n",
                "            import_failed.append(node_file)\n",
                "\n",
                "    return import_failed\n",
                "\n",
                "\n",
                "def init_extra_nodes(init_custom_nodes=True):\n",
                "    import_failed = init_builtin_extra_nodes()\n",
                "\n",
                "    if init_custom_nodes:\n",
                "        init_external_custom_nodes()\n",
                "    else:\n",
                "        logging.info(\"Skipping loading of custom nodes\")\n",
                "\n",
                "    if len(import_failed) > 0:\n",
                "        logging.warning(\"WARNING: some comfy_extras/ nodes did not import correctly. This may be because they are missing some dependencies.\\n\")\n",
                "        for node in import_failed:\n",
                "            logging.warning(\"IMPORT FAILED: {}\".format(node))\n",
                "        logging.warning(\"\\nThis issue might be caused by new missing dependencies added the last time you updated ComfyUI.\")\n",
                "        if args.windows_standalone_build:\n",
                "            logging.warning(\"Please run the update script: update/update_comfyui.bat\")\n",
                "        else:\n",
                "            logging.warning(\"Please do a: pip install -r requirements.txt\")\n",
                "        logging.warning(\"\")"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                0,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                1,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                2,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "implement and use"
        },
        {
            "edit_hunk_pair": [
                3,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                3,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                4,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "implement and use"
        },
        {
            "edit_hunk_pair": [
                5,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "api sync"
        },
        {
            "edit_hunk_pair": [
                6,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        }
    ]
}