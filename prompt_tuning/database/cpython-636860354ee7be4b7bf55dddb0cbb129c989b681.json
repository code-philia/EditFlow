{
    "language": "python",
    "commit_url": "https://github.com/python/cpython/commit/636860354ee7be4b7bf55dddb0cbb129c989b681",
    "commit_message": "regrtest: add a summary of the summary, \"Result: xxx\"\n\nIt's sometimes hard to check quickly if tests succeeded, failed or something\nbad happened. I added a final \"Result: xxx\" line which summarizes all outputs\ninto a single line, written at the end (it should always be the last line of\nthe output).",
    "commit_snapshots": {
        "Lib/test/libregrtest/main.py": [
            [
                "import datetime\n",
                "import faulthandler\n",
                "import math\n",
                "import os\n",
                "import platform\n",
                "import random\n",
                "import re\n",
                "import sys\n",
                "import sysconfig\n",
                "import tempfile\n",
                "import textwrap\n",
                "import time\n",
                "from test.libregrtest.cmdline import _parse_args\n",
                "from test.libregrtest.runtest import (\n",
                "    findtests, runtest,\n",
                "    STDTESTS, NOTTESTS, PASSED, FAILED, ENV_CHANGED, SKIPPED, RESOURCE_DENIED,\n",
                "    INTERRUPTED, CHILD_ERROR,\n",
                "    PROGRESS_MIN_TIME, format_test_result)\n",
                "from test.libregrtest.setup import setup_tests\n",
                "from test import support\n",
                "try:\n",
                "    import gc\n",
                "except ImportError:\n",
                "    gc = None\n",
                "\n",
                "\n",
                "# When tests are run from the Python build directory, it is best practice\n",
                "# to keep the test files in a subfolder.  This eases the cleanup of leftover\n",
                "# files using the \"make distclean\" command.\n",
                "if sysconfig.is_python_build():\n",
                "    TEMPDIR = os.path.join(sysconfig.get_config_var('srcdir'), 'build')\n",
                "else:\n",
                "    TEMPDIR = tempfile.gettempdir()\n",
                "TEMPDIR = os.path.abspath(TEMPDIR)\n",
                "\n",
                "\n",
                "def format_duration(seconds):\n",
                "    if seconds < 1.0:\n",
                "        return '%.0f ms' % (seconds * 1e3)\n",
                "    if seconds < 60.0:\n",
                "        return '%.0f sec' % seconds\n",
                "\n",
                "    minutes, seconds = divmod(seconds, 60.0)\n",
                "    return '%.0f min %.0f sec' % (minutes, seconds)\n",
                "\n",
                "\n",
                "class Regrtest:\n",
                "    \"\"\"Execute a test suite.\n",
                "\n",
                "    This also parses command-line options and modifies its behavior\n",
                "    accordingly.\n",
                "\n",
                "    tests -- a list of strings containing test names (optional)\n",
                "    testdir -- the directory in which to look for tests (optional)\n",
                "\n",
                "    Users other than the Python test suite will certainly want to\n",
                "    specify testdir; if it's omitted, the directory containing the\n",
                "    Python test suite is searched for.\n",
                "\n",
                "    If the tests argument is omitted, the tests listed on the\n",
                "    command-line will be used.  If that's empty, too, then all *.py\n",
                "    files beginning with test_ will be used.\n",
                "\n",
                "    The other default arguments (verbose, quiet, exclude,\n",
                "    single, randomize, findleaks, use_resources, trace, coverdir,\n",
                "    print_slow, and random_seed) allow programmers calling main()\n",
                "    directly to set the values that would normally be set by flags\n",
                "    on the command line.\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        # Namespace of command line options\n",
                "        self.ns = None\n",
                "\n",
                "        # tests\n",
                "        self.tests = []\n",
                "        self.selected = []\n",
                "\n",
                "        # test results\n",
                "        self.good = []\n",
                "        self.bad = []\n",
                "        self.skipped = []\n",
                "        self.resource_denieds = []\n",
                "        self.environment_changed = []\n",
                "        self.interrupted = False\n",
                "\n",
                "        # used by --slow\n",
                "        self.test_times = []\n",
                "\n",
                "        # used by --coverage, trace.Trace instance\n",
                "        self.tracer = None\n",
                "\n",
                "        # used by --findleaks, store for gc.garbage\n",
                "        self.found_garbage = []\n",
                "\n",
                "        # used to display the progress bar \"[ 3/100]\"\n",
                "        self.start_time = time.monotonic()\n",
                "        self.test_count = ''\n",
                "        self.test_count_width = 1\n",
                "\n",
                "        # used by --single\n",
                "        self.next_single_test = None\n",
                "        self.next_single_filename = None\n",
                "\n",
                "    def accumulate_result(self, test, result):\n",
                "        ok, test_time = result\n",
                "        if ok not in (CHILD_ERROR, INTERRUPTED):\n",
                "            self.test_times.append((test_time, test))\n",
                "        if ok == PASSED:\n",
                "            self.good.append(test)\n",
                "        elif ok == FAILED:\n",
                "            self.bad.append(test)\n",
                "        elif ok == ENV_CHANGED:\n",
                "            self.environment_changed.append(test)\n",
                "        elif ok == SKIPPED:\n",
                "            self.skipped.append(test)\n",
                "        elif ok == RESOURCE_DENIED:\n",
                "            self.skipped.append(test)\n",
                "            self.resource_denieds.append(test)\n",
                "\n",
                "    def display_progress(self, test_index, test):\n",
                "        if self.ns.quiet:\n",
                "            return\n",
                "        if self.bad and not self.ns.pgo:\n",
                "            fmt = \"{time} [{test_index:{count_width}}{test_count}/{nbad}] {test_name}\"\n",
                "        else:\n",
                "            fmt = \"{time} [{test_index:{count_width}}{test_count}] {test_name}\"\n",
                "        test_time = time.monotonic() - self.start_time\n",
                "        test_time = datetime.timedelta(seconds=int(test_time))\n",
                "        line = fmt.format(count_width=self.test_count_width,\n",
                "                          test_index=test_index,\n",
                "                          test_count=self.test_count,\n",
                "                          nbad=len(self.bad),\n",
                "                          test_name=test,\n",
                "                          time=test_time)\n",
                "        print(line, flush=True)\n",
                "\n",
                "    def parse_args(self, kwargs):\n",
                "        ns = _parse_args(sys.argv[1:], **kwargs)\n",
                "\n",
                "        if ns.timeout and not hasattr(faulthandler, 'dump_traceback_later'):\n",
                "            print(\"Warning: The timeout option requires \"\n",
                "                  \"faulthandler.dump_traceback_later\", file=sys.stderr)\n",
                "            ns.timeout = None\n",
                "\n",
                "        if ns.threshold is not None and gc is None:\n",
                "            print('No GC available, ignore --threshold.', file=sys.stderr)\n",
                "            ns.threshold = None\n",
                "\n",
                "        if ns.findleaks:\n",
                "            if gc is not None:\n",
                "                # Uncomment the line below to report garbage that is not\n",
                "                # freeable by reference counting alone.  By default only\n",
                "                # garbage that is not collectable by the GC is reported.\n",
                "                pass\n",
                "                #gc.set_debug(gc.DEBUG_SAVEALL)\n",
                "            else:\n",
                "                print('No GC available, disabling --findleaks',\n",
                "                      file=sys.stderr)\n",
                "                ns.findleaks = False\n",
                "\n",
                "        # Strip .py extensions.\n",
                "        removepy(ns.args)\n",
                "\n",
                "        return ns\n",
                "\n",
                "    def find_tests(self, tests):\n",
                "        self.tests = tests\n",
                "\n",
                "        if self.ns.single:\n",
                "            self.next_single_filename = os.path.join(TEMPDIR, 'pynexttest')\n",
                "            try:\n",
                "                with open(self.next_single_filename, 'r') as fp:\n",
                "                    next_test = fp.read().strip()\n",
                "                    self.tests = [next_test]\n",
                "            except OSError:\n",
                "                pass\n",
                "\n",
                "        if self.ns.fromfile:\n",
                "            self.tests = []\n",
                "            # regex to match 'test_builtin' in line:\n",
                "            # '0:00:00 [  4/400] test_builtin -- test_dict took 1 sec'\n",
                "            regex = (r'^(?:[0-9]+:[0-9]+:[0-9]+ *)?'\n",
                "                     r'(?:\\[[0-9/ ]+\\] *)?'\n",
                "                     r'(test_[a-zA-Z0-9_]+)')\n",
                "            regex = re.compile(regex)\n",
                "            with open(os.path.join(support.SAVEDCWD, self.ns.fromfile)) as fp:\n",
                "                for line in fp:\n",
                "                    line = line.strip()\n",
                "                    if line.startswith('#'):\n",
                "                        continue\n",
                "                    match = regex.match(line)\n",
                "                    if match is None:\n",
                "                        continue\n",
                "                    self.tests.append(match.group(1))\n",
                "\n",
                "        removepy(self.tests)\n",
                "\n",
                "        stdtests = STDTESTS[:]\n",
                "        nottests = NOTTESTS.copy()\n",
                "        if self.ns.exclude:\n",
                "            for arg in self.ns.args:\n",
                "                if arg in stdtests:\n",
                "                    stdtests.remove(arg)\n",
                "                nottests.add(arg)\n",
                "            self.ns.args = []\n",
                "\n",
                "        # if testdir is set, then we are not running the python tests suite, so\n",
                "        # don't add default tests to be executed or skipped (pass empty values)\n",
                "        if self.ns.testdir:\n",
                "            alltests = findtests(self.ns.testdir, list(), set())\n",
                "        else:\n",
                "            alltests = findtests(self.ns.testdir, stdtests, nottests)\n",
                "\n",
                "        if not self.ns.fromfile:\n",
                "            self.selected = self.tests or self.ns.args or alltests\n",
                "        else:\n",
                "            self.selected = self.tests\n",
                "        if self.ns.single:\n",
                "            self.selected = self.selected[:1]\n",
                "            try:\n",
                "                pos = alltests.index(self.selected[0])\n",
                "                self.next_single_test = alltests[pos + 1]\n",
                "            except IndexError:\n",
                "                pass\n",
                "\n",
                "        # Remove all the selected tests that precede start if it's set.\n",
                "        if self.ns.start:\n",
                "            try:\n",
                "                del self.selected[:self.selected.index(self.ns.start)]\n",
                "            except ValueError:\n",
                "                print(\"Couldn't find starting test (%s), using all tests\"\n",
                "                      % self.ns.start, file=sys.stderr)\n",
                "\n",
                "        if self.ns.randomize:\n",
                "            if self.ns.random_seed is None:\n",
                "                self.ns.random_seed = random.randrange(10000000)\n",
                "            random.seed(self.ns.random_seed)\n",
                "            random.shuffle(self.selected)\n",
                "\n",
                "    def list_tests(self):\n",
                "        for name in self.selected:\n",
                "            print(name)\n",
                "\n",
                "    def rerun_failed_tests(self):\n",
                "        self.ns.verbose = True\n",
                "        self.ns.failfast = False\n",
                "        self.ns.verbose3 = False\n",
                "        self.ns.match_tests = None\n",
                "\n",
                "        print(\"Re-running failed tests in verbose mode\")\n",
                "        for test in self.bad[:]:\n",
                "            print(\"Re-running test %r in verbose mode\" % test, flush=True)\n",
                "            try:\n",
                "                self.ns.verbose = True\n",
                "                ok = runtest(self.ns, test)\n",
                "            except KeyboardInterrupt:\n",
                "                self.interrupted = True\n",
                "                # print a newline separate from the ^C\n",
                "                print()\n",
                "                break\n",
                "            else:\n",
                "                if ok[0] in {PASSED, ENV_CHANGED, SKIPPED, RESOURCE_DENIED}:\n",
                "                    self.bad.remove(test)\n",
                "        else:\n",
                "            if self.bad:\n",
                "                print(count(len(self.bad), 'test'), \"failed again:\")\n",
                "                printlist(self.bad)\n",
                "\n",
                "    def display_result(self):\n",
                "        if self.interrupted:\n",
                "            # print a newline after ^C\n",
                "            print()\n",
                "            print(\"Test suite interrupted by signal SIGINT.\")\n",
                "            executed = set(self.good) | set(self.bad) | set(self.skipped)\n",
                "            omitted = set(self.selected) - executed\n",
                "            print(count(len(omitted), \"test\"), \"omitted:\")\n",
                "            printlist(omitted)\n",
                "\n",
                "        # If running the test suite for PGO then no one cares about\n",
                "        # results.\n",
                "        if self.ns.pgo:\n",
                "            return\n",
                "\n",
                "        if self.good and not self.ns.quiet:\n",
                "            if (not self.bad\n",
                "                and not self.skipped\n",
                "                and not self.interrupted\n",
                "                and len(self.good) > 1):\n",
                "                print(\"All\", end=' ')\n",
                "            print(count(len(self.good), \"test\"), \"OK.\")\n",
                "\n",
                "        if self.ns.print_slow:\n",
                "            self.test_times.sort(reverse=True)\n",
                "            print()\n",
                "            print(\"10 slowest tests:\")\n",
                "            for time, test in self.test_times[:10]:\n",
                "                print(\"- %s: %s\" % (test, format_duration(time)))\n",
                "\n",
                "        if self.bad:\n",
                "            print()\n",
                "            print(count(len(self.bad), \"test\"), \"failed:\")\n",
                "            printlist(self.bad)\n",
                "\n",
                "        if self.environment_changed:\n",
                "            print()\n",
                "            print(\"{} altered the execution environment:\".format(\n",
                "                     count(len(self.environment_changed), \"test\")))\n",
                "            printlist(self.environment_changed)\n",
                "\n",
                "        if self.skipped and not self.ns.quiet:\n",
                "            print()\n",
                "            print(count(len(self.skipped), \"test\"), \"skipped:\")\n",
                "            printlist(self.skipped)\n",
                "\n",
                "    def run_tests_sequential(self):\n",
                "        if self.ns.trace:\n",
                "            import trace\n",
                "            self.tracer = trace.Trace(trace=False, count=True)\n",
                "\n",
                "        save_modules = sys.modules.keys()\n",
                "\n",
                "        print(\"Run tests sequentially\")\n",
                "\n",
                "        previous_test = None\n",
                "        for test_index, test in enumerate(self.tests, 1):\n",
                "            start_time = time.monotonic()\n",
                "\n",
                "            text = test\n",
                "            if previous_test:\n",
                "                text = '%s -- %s' % (text, previous_test)\n",
                "            self.display_progress(test_index, text)\n",
                "\n",
                "            if self.tracer:\n",
                "                # If we're tracing code coverage, then we don't exit with status\n",
                "                # if on a false return value from main.\n",
                "                cmd = ('result = runtest(self.ns, test); '\n",
                "                       'self.accumulate_result(test, result)')\n",
                "                ns = dict(locals())\n",
                "                self.tracer.runctx(cmd, globals=globals(), locals=ns)\n",
                "                result = ns['result']\n",
                "            else:\n",
                "                try:\n",
                "                    result = runtest(self.ns, test)\n",
                "                except KeyboardInterrupt:\n",
                "                    self.interrupted = True\n",
                "                    self.accumulate_result(test, (INTERRUPTED, None))\n",
                "                    break\n",
                "                else:\n",
                "                    self.accumulate_result(test, result)\n",
                "\n",
                "            previous_test = format_test_result(test, result[0])\n",
                "            test_time = time.monotonic() - start_time\n",
                "            if test_time >= PROGRESS_MIN_TIME:\n",
                "                previous_test = \"%s in %s\" % (previous_test, format_duration(test_time))\n",
                "            elif result[0] == PASSED:\n",
                "                # be quiet: say nothing if the test passed shortly\n",
                "                previous_test = None\n",
                "\n",
                "            if self.ns.findleaks:\n",
                "                gc.collect()\n",
                "                if gc.garbage:\n",
                "                    print(\"Warning: test created\", len(gc.garbage), end=' ')\n",
                "                    print(\"uncollectable object(s).\")\n",
                "                    # move the uncollectable objects somewhere so we don't see\n",
                "                    # them again\n",
                "                    self.found_garbage.extend(gc.garbage)\n",
                "                    del gc.garbage[:]\n",
                "\n",
                "            # Unload the newly imported modules (best effort finalization)\n",
                "            for module in sys.modules.keys():\n",
                "                if module not in save_modules and module.startswith(\"test.\"):\n",
                "                    support.unload(module)\n",
                "\n",
                "        if previous_test:\n",
                "            print(previous_test)\n",
                "\n",
                "    def _test_forever(self, tests):\n",
                "        while True:\n",
                "            for test in tests:\n",
                "                yield test\n",
                "                if self.bad:\n",
                "                    return\n",
                "\n",
                "    def run_tests(self):\n",
                "        # For a partial run, we do not need to clutter the output.\n",
                "        if (self.ns.verbose\n",
                "            or self.ns.header\n",
                "            or not (self.ns.pgo or self.ns.quiet or self.ns.single\n",
                "                    or self.tests or self.ns.args)):\n",
                "            # Print basic platform information\n",
                "            print(\"==\", platform.python_implementation(), *sys.version.split())\n",
                "            print(\"==  \", platform.platform(aliased=True),\n",
                "                          \"%s-endian\" % sys.byteorder)\n",
                "            print(\"==  \", \"hash algorithm:\", sys.hash_info.algorithm,\n",
                "                  \"64bit\" if sys.maxsize > 2**32 else \"32bit\")\n",
                "            print(\"==  \", os.getcwd())\n",
                "            print(\"Testing with flags:\", sys.flags)\n",
                "\n",
                "        if self.ns.randomize:\n",
                "            print(\"Using random seed\", self.ns.random_seed)\n",
                "\n",
                "        if self.ns.forever:\n",
                "            self.tests = self._test_forever(list(self.selected))\n",
                "            self.test_count = ''\n",
                "            self.test_count_width = 3\n",
                "        else:\n",
                "            self.tests = iter(self.selected)\n",
                "            self.test_count = '/{}'.format(len(self.selected))\n",
                "            self.test_count_width = len(self.test_count) - 1\n",
                "\n",
                "        if self.ns.use_mp:\n",
                "            from test.libregrtest.runtest_mp import run_tests_multiprocess\n",
                "            run_tests_multiprocess(self)\n",
                "        else:\n",
                "            self.run_tests_sequential()\n",
                "\n",
                "    def finalize(self):\n",
                "        if self.next_single_filename:\n",
                "            if self.next_single_test:\n",
                "                with open(self.next_single_filename, 'w') as fp:\n",
                "                    fp.write(self.next_single_test + '\\n')\n",
                "            else:\n",
                "                os.unlink(self.next_single_filename)\n",
                "\n",
                "        if self.tracer:\n",
                "            r = self.tracer.results()\n",
                "            r.write_results(show_missing=True, summary=True,\n",
                "                            coverdir=self.ns.coverdir)\n",
                "\n",
                "        print()\n",
                "        duration = time.monotonic() - self.start_time\n",
                "        print(\"Total duration: %s\" % format_duration(duration))\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        if self.bad:\n",
                    "            result = \"FAILURE\"\n",
                    "        elif self.interrupted:\n",
                    "            result = \"INTERRUPTED\"\n",
                    "        else:\n",
                    "            result = \"SUCCESS\"\n",
                    "        print(\"Result: %s\" % result)\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 433,
                    "end": 433
                },
                "child_version_range": {
                    "start": 433,
                    "end": 441
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "Regrtest",
                        "signature": "class Regrtest:",
                        "at_line": 46
                    },
                    {
                        "type": "function",
                        "name": "finalize",
                        "signature": "def finalize(self):",
                        "at_line": 416
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: Lib/test/libregrtest/main.py\nCode:\n           class Regrtest:\n               ...\n               def finalize(self):\n                   ...\n430 430            duration = time.monotonic() - self.start_time\n431 431            print(\"Total duration: %s\" % format_duration(duration))\n432 432    \n    433  +         if self.bad:\n    434  +             result = \"FAILURE\"\n    435  +         elif self.interrupted:\n    436  +             result = \"INTERRUPTED\"\n    437  +         else:\n    438  +             result = \"SUCCESS\"\n    439  +         print(\"Result: %s\" % result)\n    440  + \n433 441            if self.ns.runleaks:\n434 442                os.system(\"leaks %d\" % os.getpid())\n435 443    \n         ...\n",
                "file_path": "Lib/test/libregrtest/main.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "bad",
                    "interrupted",
                    "print",
                    "result",
                    "self"
                ],
                "prefix": [
                    "        duration = time.monotonic() - self.start_time\n",
                    "        print(\"Total duration: %s\" % format_duration(duration))\n",
                    "\n"
                ],
                "suffix": [
                    "        if self.ns.runleaks:\n",
                    "            os.system(\"leaks %d\" % os.getpid())\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        if self.ns.runleaks:\n",
                "            os.system(\"leaks %d\" % os.getpid())\n",
                "\n",
                "    def main(self, tests=None, **kwargs):\n",
                "        global TEMPDIR\n",
                "\n",
                "        if sysconfig.is_python_build():\n",
                "            try:\n",
                "                os.mkdir(TEMPDIR)\n",
                "            except FileExistsError:\n",
                "                pass\n",
                "\n",
                "        # Define a writable temp dir that will be used as cwd while running\n",
                "        # the tests. The name of the dir includes the pid to allow parallel\n",
                "        # testing (see the -j option).\n",
                "        test_cwd = 'test_python_{}'.format(os.getpid())\n",
                "        test_cwd = os.path.join(TEMPDIR, test_cwd)\n",
                "\n",
                "        # Run the tests in a context manager that temporarily changes the CWD to a\n",
                "        # temporary and writable directory.  If it's not possible to create or\n",
                "        # change the CWD, the original CWD will be used.  The original CWD is\n",
                "        # available from support.SAVEDCWD.\n",
                "        with support.temp_cwd(test_cwd, quiet=True):\n",
                "            self._main(tests, kwargs)\n",
                "\n",
                "    def _main(self, tests, kwargs):\n",
                "        self.ns = self.parse_args(kwargs)\n",
                "\n",
                "        if self.ns.slaveargs is not None:\n",
                "            from test.libregrtest.runtest_mp import run_tests_slave\n",
                "            run_tests_slave(self.ns.slaveargs)\n",
                "\n",
                "        if self.ns.wait:\n",
                "            input(\"Press any key to continue...\")\n",
                "\n",
                "        setup_tests(self.ns)\n",
                "\n",
                "        self.find_tests(tests)\n",
                "\n",
                "        if self.ns.list_tests:\n",
                "            self.list_tests()\n",
                "            sys.exit(0)\n",
                "\n",
                "        self.run_tests()\n",
                "        self.display_result()\n",
                "\n",
                "        if self.ns.verbose2 and self.bad:\n",
                "            self.rerun_failed_tests()\n",
                "\n",
                "        self.finalize()\n",
                "        sys.exit(len(self.bad) > 0 or self.interrupted)\n",
                "\n",
                "\n",
                "def removepy(names):\n",
                "    if not names:\n",
                "        return\n",
                "    for idx, name in enumerate(names):\n",
                "        basename, ext = os.path.splitext(name)\n",
                "        if ext == '.py':\n",
                "            names[idx] = basename\n",
                "\n",
                "\n",
                "def count(n, word):\n",
                "    if n == 1:\n",
                "        return \"%d %s\" % (n, word)\n",
                "    else:\n",
                "        return \"%d %ss\" % (n, word)\n",
                "\n",
                "\n",
                "def printlist(x, width=70, indent=4):\n",
                "    \"\"\"Print the elements of iterable x to stdout.\n",
                "\n",
                "    Optional arg width (default 70) is the maximum line length.\n",
                "    Optional arg indent (default 4) is the number of blanks with which to\n",
                "    begin each line.\n",
                "    \"\"\"\n",
                "\n",
                "    blanks = ' ' * indent\n",
                "    # Print the sorted list: 'x' may be a '--random' list or a set()\n",
                "    print(textwrap.fill(' '.join(str(elt) for elt in sorted(x)), width,\n",
                "                        initial_indent=blanks, subsequent_indent=blanks))\n",
                "\n",
                "\n",
                "def main(tests=None, **kwargs):\n",
                "    \"\"\"Run the Python suite.\"\"\"\n",
                "    Regrtest().main(tests=tests, **kwargs)"
            ]
        ],
        "Lib/test/test_regrtest.py": [
            [
                "\"\"\"\n",
                "Tests of regrtest.py.\n",
                "\n",
                "Note: test_regrtest cannot be run twice in parallel.\n",
                "\"\"\"\n",
                "\n",
                "import contextlib\n",
                "import faulthandler\n",
                "import io\n",
                "import os.path\n",
                "import platform\n",
                "import re\n",
                "import subprocess\n",
                "import sys\n",
                "import sysconfig\n",
                "import tempfile\n",
                "import textwrap\n",
                "import unittest\n",
                "from test import libregrtest\n",
                "from test import support\n",
                "\n",
                "\n",
                "Py_DEBUG = hasattr(sys, 'getobjects')\n",
                "ROOT_DIR = os.path.join(os.path.dirname(__file__), '..', '..')\n",
                "ROOT_DIR = os.path.abspath(os.path.normpath(ROOT_DIR))\n",
                "\n",
                "TEST_INTERRUPTED = textwrap.dedent(\"\"\"\n",
                "    from signal import SIGINT\n",
                "    try:\n",
                "        from _testcapi import raise_signal\n",
                "        raise_signal(SIGINT)\n",
                "    except ImportError:\n",
                "        import os\n",
                "        os.kill(os.getpid(), SIGINT)\n",
                "    \"\"\")\n",
                "\n",
                "\n",
                "class ParseArgsTestCase(unittest.TestCase):\n",
                "    \"\"\"\n",
                "    Test regrtest's argument parsing, function _parse_args().\n",
                "    \"\"\"\n",
                "\n",
                "    def checkError(self, args, msg):\n",
                "        with support.captured_stderr() as err, self.assertRaises(SystemExit):\n",
                "            libregrtest._parse_args(args)\n",
                "        self.assertIn(msg, err.getvalue())\n",
                "\n",
                "    def test_help(self):\n",
                "        for opt in '-h', '--help':\n",
                "            with self.subTest(opt=opt):\n",
                "                with support.captured_stdout() as out, \\\n",
                "                     self.assertRaises(SystemExit):\n",
                "                    libregrtest._parse_args([opt])\n",
                "                self.assertIn('Run Python regression tests.', out.getvalue())\n",
                "\n",
                "    @unittest.skipUnless(hasattr(faulthandler, 'dump_traceback_later'),\n",
                "                         \"faulthandler.dump_traceback_later() required\")\n",
                "    def test_timeout(self):\n",
                "        ns = libregrtest._parse_args(['--timeout', '4.2'])\n",
                "        self.assertEqual(ns.timeout, 4.2)\n",
                "        self.checkError(['--timeout'], 'expected one argument')\n",
                "        self.checkError(['--timeout', 'foo'], 'invalid float value')\n",
                "\n",
                "    def test_wait(self):\n",
                "        ns = libregrtest._parse_args(['--wait'])\n",
                "        self.assertTrue(ns.wait)\n",
                "\n",
                "    def test_slaveargs(self):\n",
                "        ns = libregrtest._parse_args(['--slaveargs', '[[], {}]'])\n",
                "        self.assertEqual(ns.slaveargs, '[[], {}]')\n",
                "        self.checkError(['--slaveargs'], 'expected one argument')\n",
                "\n",
                "    def test_start(self):\n",
                "        for opt in '-S', '--start':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, 'foo'])\n",
                "                self.assertEqual(ns.start, 'foo')\n",
                "                self.checkError([opt], 'expected one argument')\n",
                "\n",
                "    def test_verbose(self):\n",
                "        ns = libregrtest._parse_args(['-v'])\n",
                "        self.assertEqual(ns.verbose, 1)\n",
                "        ns = libregrtest._parse_args(['-vvv'])\n",
                "        self.assertEqual(ns.verbose, 3)\n",
                "        ns = libregrtest._parse_args(['--verbose'])\n",
                "        self.assertEqual(ns.verbose, 1)\n",
                "        ns = libregrtest._parse_args(['--verbose'] * 3)\n",
                "        self.assertEqual(ns.verbose, 3)\n",
                "        ns = libregrtest._parse_args([])\n",
                "        self.assertEqual(ns.verbose, 0)\n",
                "\n",
                "    def test_verbose2(self):\n",
                "        for opt in '-w', '--verbose2':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.verbose2)\n",
                "\n",
                "    def test_verbose3(self):\n",
                "        for opt in '-W', '--verbose3':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.verbose3)\n",
                "\n",
                "    def test_quiet(self):\n",
                "        for opt in '-q', '--quiet':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.quiet)\n",
                "                self.assertEqual(ns.verbose, 0)\n",
                "\n",
                "    def test_slow(self):\n",
                "        for opt in '-o', '--slowest':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.print_slow)\n",
                "\n",
                "    def test_header(self):\n",
                "        ns = libregrtest._parse_args(['--header'])\n",
                "        self.assertTrue(ns.header)\n",
                "\n",
                "    def test_randomize(self):\n",
                "        for opt in '-r', '--randomize':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.randomize)\n",
                "\n",
                "    def test_randseed(self):\n",
                "        ns = libregrtest._parse_args(['--randseed', '12345'])\n",
                "        self.assertEqual(ns.random_seed, 12345)\n",
                "        self.assertTrue(ns.randomize)\n",
                "        self.checkError(['--randseed'], 'expected one argument')\n",
                "        self.checkError(['--randseed', 'foo'], 'invalid int value')\n",
                "\n",
                "    def test_fromfile(self):\n",
                "        for opt in '-f', '--fromfile':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, 'foo'])\n",
                "                self.assertEqual(ns.fromfile, 'foo')\n",
                "                self.checkError([opt], 'expected one argument')\n",
                "                self.checkError([opt, 'foo', '-s'], \"don't go together\")\n",
                "\n",
                "    def test_exclude(self):\n",
                "        for opt in '-x', '--exclude':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.exclude)\n",
                "\n",
                "    def test_single(self):\n",
                "        for opt in '-s', '--single':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.single)\n",
                "                self.checkError([opt, '-f', 'foo'], \"don't go together\")\n",
                "\n",
                "    def test_match(self):\n",
                "        for opt in '-m', '--match':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, 'pattern'])\n",
                "                self.assertEqual(ns.match_tests, 'pattern')\n",
                "                self.checkError([opt], 'expected one argument')\n",
                "\n",
                "    def test_failfast(self):\n",
                "        for opt in '-G', '--failfast':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, '-v'])\n",
                "                self.assertTrue(ns.failfast)\n",
                "                ns = libregrtest._parse_args([opt, '-W'])\n",
                "                self.assertTrue(ns.failfast)\n",
                "                self.checkError([opt], '-G/--failfast needs either -v or -W')\n",
                "\n",
                "    def test_use(self):\n",
                "        for opt in '-u', '--use':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, 'gui,network'])\n",
                "                self.assertEqual(ns.use_resources, ['gui', 'network'])\n",
                "                ns = libregrtest._parse_args([opt, 'gui,none,network'])\n",
                "                self.assertEqual(ns.use_resources, ['network'])\n",
                "                expected = list(libregrtest.RESOURCE_NAMES)\n",
                "                expected.remove('gui')\n",
                "                ns = libregrtest._parse_args([opt, 'all,-gui'])\n",
                "                self.assertEqual(ns.use_resources, expected)\n",
                "                self.checkError([opt], 'expected one argument')\n",
                "                self.checkError([opt, 'foo'], 'invalid resource')\n",
                "\n",
                "    def test_memlimit(self):\n",
                "        for opt in '-M', '--memlimit':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, '4G'])\n",
                "                self.assertEqual(ns.memlimit, '4G')\n",
                "                self.checkError([opt], 'expected one argument')\n",
                "\n",
                "    def test_testdir(self):\n",
                "        ns = libregrtest._parse_args(['--testdir', 'foo'])\n",
                "        self.assertEqual(ns.testdir, os.path.join(support.SAVEDCWD, 'foo'))\n",
                "        self.checkError(['--testdir'], 'expected one argument')\n",
                "\n",
                "    def test_runleaks(self):\n",
                "        for opt in '-L', '--runleaks':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.runleaks)\n",
                "\n",
                "    def test_huntrleaks(self):\n",
                "        for opt in '-R', '--huntrleaks':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, ':'])\n",
                "                self.assertEqual(ns.huntrleaks, (5, 4, 'reflog.txt'))\n",
                "                ns = libregrtest._parse_args([opt, '6:'])\n",
                "                self.assertEqual(ns.huntrleaks, (6, 4, 'reflog.txt'))\n",
                "                ns = libregrtest._parse_args([opt, ':3'])\n",
                "                self.assertEqual(ns.huntrleaks, (5, 3, 'reflog.txt'))\n",
                "                ns = libregrtest._parse_args([opt, '6:3:leaks.log'])\n",
                "                self.assertEqual(ns.huntrleaks, (6, 3, 'leaks.log'))\n",
                "                self.checkError([opt], 'expected one argument')\n",
                "                self.checkError([opt, '6'],\n",
                "                                'needs 2 or 3 colon-separated arguments')\n",
                "                self.checkError([opt, 'foo:'], 'invalid huntrleaks value')\n",
                "                self.checkError([opt, '6:foo'], 'invalid huntrleaks value')\n",
                "\n",
                "    def test_multiprocess(self):\n",
                "        for opt in '-j', '--multiprocess':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, '2'])\n",
                "                self.assertEqual(ns.use_mp, 2)\n",
                "                self.checkError([opt], 'expected one argument')\n",
                "                self.checkError([opt, 'foo'], 'invalid int value')\n",
                "                self.checkError([opt, '2', '-T'], \"don't go together\")\n",
                "                self.checkError([opt, '2', '-l'], \"don't go together\")\n",
                "\n",
                "    def test_coverage(self):\n",
                "        for opt in '-T', '--coverage':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.trace)\n",
                "\n",
                "    def test_coverdir(self):\n",
                "        for opt in '-D', '--coverdir':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, 'foo'])\n",
                "                self.assertEqual(ns.coverdir,\n",
                "                                 os.path.join(support.SAVEDCWD, 'foo'))\n",
                "                self.checkError([opt], 'expected one argument')\n",
                "\n",
                "    def test_nocoverdir(self):\n",
                "        for opt in '-N', '--nocoverdir':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertIsNone(ns.coverdir)\n",
                "\n",
                "    def test_threshold(self):\n",
                "        for opt in '-t', '--threshold':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt, '1000'])\n",
                "                self.assertEqual(ns.threshold, 1000)\n",
                "                self.checkError([opt], 'expected one argument')\n",
                "                self.checkError([opt, 'foo'], 'invalid int value')\n",
                "\n",
                "    def test_nowindows(self):\n",
                "        for opt in '-n', '--nowindows':\n",
                "            with self.subTest(opt=opt):\n",
                "                with contextlib.redirect_stderr(io.StringIO()) as stderr:\n",
                "                    ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.nowindows)\n",
                "                err = stderr.getvalue()\n",
                "                self.assertIn('the --nowindows (-n) option is deprecated', err)\n",
                "\n",
                "    def test_forever(self):\n",
                "        for opt in '-F', '--forever':\n",
                "            with self.subTest(opt=opt):\n",
                "                ns = libregrtest._parse_args([opt])\n",
                "                self.assertTrue(ns.forever)\n",
                "\n",
                "\n",
                "    def test_unrecognized_argument(self):\n",
                "        self.checkError(['--xxx'], 'usage:')\n",
                "\n",
                "    def test_long_option__partial(self):\n",
                "        ns = libregrtest._parse_args(['--qui'])\n",
                "        self.assertTrue(ns.quiet)\n",
                "        self.assertEqual(ns.verbose, 0)\n",
                "\n",
                "    def test_two_options(self):\n",
                "        ns = libregrtest._parse_args(['--quiet', '--exclude'])\n",
                "        self.assertTrue(ns.quiet)\n",
                "        self.assertEqual(ns.verbose, 0)\n",
                "        self.assertTrue(ns.exclude)\n",
                "\n",
                "    def test_option_with_empty_string_value(self):\n",
                "        ns = libregrtest._parse_args(['--start', ''])\n",
                "        self.assertEqual(ns.start, '')\n",
                "\n",
                "    def test_arg(self):\n",
                "        ns = libregrtest._parse_args(['foo'])\n",
                "        self.assertEqual(ns.args, ['foo'])\n",
                "\n",
                "    def test_option_and_arg(self):\n",
                "        ns = libregrtest._parse_args(['--quiet', 'foo'])\n",
                "        self.assertTrue(ns.quiet)\n",
                "        self.assertEqual(ns.verbose, 0)\n",
                "        self.assertEqual(ns.args, ['foo'])\n",
                "\n",
                "\n",
                "class BaseTestCase(unittest.TestCase):\n",
                "    TEST_UNIQUE_ID = 1\n",
                "    TESTNAME_PREFIX = 'test_regrtest_'\n",
                "    TESTNAME_REGEX = r'test_[a-zA-Z0-9_]+'\n",
                "\n",
                "    def setUp(self):\n",
                "        self.testdir = os.path.realpath(os.path.dirname(__file__))\n",
                "\n",
                "        self.tmptestdir = tempfile.mkdtemp()\n",
                "        self.addCleanup(support.rmtree, self.tmptestdir)\n",
                "\n",
                "    def create_test(self, name=None, code=''):\n",
                "        if not name:\n",
                "            name = 'noop%s' % BaseTestCase.TEST_UNIQUE_ID\n",
                "            BaseTestCase.TEST_UNIQUE_ID += 1\n",
                "\n",
                "        # test_regrtest cannot be run twice in parallel because\n",
                "        # of setUp() and create_test()\n",
                "        name = self.TESTNAME_PREFIX + name\n",
                "        path = os.path.join(self.tmptestdir, name + '.py')\n",
                "\n",
                "        self.addCleanup(support.unlink, path)\n",
                "        # Use 'x' mode to ensure that we do not override existing tests\n",
                "        try:\n",
                "            with open(path, 'x', encoding='utf-8') as fp:\n",
                "                fp.write(code)\n",
                "        except PermissionError as exc:\n",
                "            if not sysconfig.is_python_build():\n",
                "                self.skipTest(\"cannot write %s: %s\" % (path, exc))\n",
                "            raise\n",
                "        return name\n",
                "\n",
                "    def regex_search(self, regex, output):\n",
                "        match = re.search(regex, output, re.MULTILINE)\n",
                "        if not match:\n",
                "            self.fail(\"%r not found in %r\" % (regex, output))\n",
                "        return match\n",
                "\n",
                "    def check_line(self, output, regex):\n",
                "        regex = re.compile(r'^' + regex, re.MULTILINE)\n",
                "        self.assertRegex(output, regex)\n",
                "\n",
                "    def parse_executed_tests(self, output):\n",
                "        regex = (r'^[0-9]+:[0-9]+:[0-9]+ \\[ *[0-9]+(?:/ *[0-9]+)?\\] (%s)'\n",
                "                 % self.TESTNAME_REGEX)\n",
                "        parser = re.finditer(regex, output, re.MULTILINE)\n",
                "        return list(match.group(1) for match in parser)\n",
                "\n",
                "    def check_executed_tests(self, output, tests, skipped=(), failed=(),\n"
            ],
            {
                "type": "replace",
                "before": [
                    "                             omitted=(), randomize=False):\n"
                ],
                "after": [
                    "                             omitted=(), randomize=False, interrupted=False):\n"
                ],
                "parent_version_range": {
                    "start": 351,
                    "end": 352
                },
                "child_version_range": {
                    "start": 351,
                    "end": 352
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "BaseTestCase",
                        "signature": "class BaseTestCase(unittest.TestCase):",
                        "at_line": 302
                    },
                    {
                        "type": "function",
                        "name": "check_executed_tests",
                        "signature": "def check_executed_tests(self, output, tests, skipped=(), failed=(),\n                             omitted=(), randomize=False):",
                        "at_line": 350
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: Lib/test/test_regrtest.py\nCode:\n           class BaseTestCase(unittest.TestCase):\n               ...\n348 348            return list(match.group(1) for match in parser)\n349 349    \n350 350        def check_executed_tests(self, output, tests, skipped=(), failed=(),\n351      -                              omitted=(), randomize=False):\n    351  +                              omitted=(), randomize=False, interrupted=False):\n352 352            if isinstance(tests, str):\n353 353                tests = [tests]\n354 354            if isinstance(skipped, str):\n         ...\n",
                "file_path": "Lib/test/test_regrtest.py",
                "identifiers_before": [
                    "omitted",
                    "randomize"
                ],
                "identifiers_after": [
                    "interrupted",
                    "omitted",
                    "randomize"
                ],
                "prefix": [
                    "        return list(match.group(1) for match in parser)\n",
                    "\n",
                    "    def check_executed_tests(self, output, tests, skipped=(), failed=(),\n"
                ],
                "suffix": [
                    "        if isinstance(tests, str):\n",
                    "            tests = [tests]\n",
                    "        if isinstance(skipped, str):\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "omitted",
                            "position": {
                                "start": {
                                    "line": 351,
                                    "column": 29
                                },
                                "end": {
                                    "line": 351,
                                    "column": 36
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "omitted",
                            "position": {
                                "start": {
                                    "line": 351,
                                    "column": 29
                                },
                                "end": {
                                    "line": 351,
                                    "column": 36
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "interrupted",
                            "position": {
                                "start": {
                                    "line": 351,
                                    "column": 58
                                },
                                "end": {
                                    "line": 351,
                                    "column": 69
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "interrupted",
                            "position": {
                                "start": {
                                    "line": 351,
                                    "column": 58
                                },
                                "end": {
                                    "line": 351,
                                    "column": 69
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "interrupted",
                            "position": {
                                "start": {
                                    "line": 351,
                                    "column": 58
                                },
                                "end": {
                                    "line": 351,
                                    "column": 69
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "interrupted",
                            "position": {
                                "start": {
                                    "line": 351,
                                    "column": 58
                                },
                                "end": {
                                    "line": 351,
                                    "column": 69
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "omitted",
                            "position": {
                                "start": {
                                    "line": 351,
                                    "column": 29
                                },
                                "end": {
                                    "line": 351,
                                    "column": 36
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "omitted",
                            "position": {
                                "start": {
                                    "line": 351,
                                    "column": 29
                                },
                                "end": {
                                    "line": 351,
                                    "column": 36
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "        if isinstance(tests, str):\n",
                "            tests = [tests]\n",
                "        if isinstance(skipped, str):\n",
                "            skipped = [skipped]\n",
                "        if isinstance(failed, str):\n",
                "            failed = [failed]\n",
                "        if isinstance(omitted, str):\n",
                "            omitted = [omitted]\n",
                "        ntest = len(tests)\n",
                "        nskipped = len(skipped)\n",
                "        nfailed = len(failed)\n",
                "        nomitted = len(omitted)\n",
                "\n",
                "        executed = self.parse_executed_tests(output)\n",
                "        if randomize:\n",
                "            self.assertEqual(set(executed), set(tests), output)\n",
                "        else:\n",
                "            self.assertEqual(executed, tests, output)\n",
                "\n",
                "        def plural(count):\n",
                "            return 's' if count != 1 else ''\n",
                "\n",
                "        def list_regex(line_format, tests):\n",
                "            count = len(tests)\n",
                "            names = ' '.join(sorted(tests))\n",
                "            regex = line_format % (count, plural(count))\n",
                "            regex = r'%s:\\n    %s$' % (regex, names)\n",
                "            return regex\n",
                "\n",
                "        if skipped:\n",
                "            regex = list_regex('%s test%s skipped', skipped)\n",
                "            self.check_line(output, regex)\n",
                "\n",
                "        if failed:\n",
                "            regex = list_regex('%s test%s failed', failed)\n",
                "            self.check_line(output, regex)\n",
                "\n",
                "        if omitted:\n",
                "            regex = list_regex('%s test%s omitted', omitted)\n",
                "            self.check_line(output, regex)\n",
                "\n",
                "        good = ntest - nskipped - nfailed - nomitted\n",
                "        if good:\n",
                "            regex = r'%s test%s OK\\.$' % (good, plural(good))\n",
                "            if not skipped and not failed and good > 1:\n",
                "                regex = 'All %s' % regex\n",
                "            self.check_line(output, regex)\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        if interrupted:\n",
                    "            self.check_line(output, 'Test suite interrupted by signal SIGINT.')\n",
                    "\n",
                    "        if nfailed:\n",
                    "            result = 'FAILURE'\n",
                    "        elif interrupted:\n",
                    "            result = 'INTERRUPTED'\n",
                    "        else:\n",
                    "            result = 'SUCCESS'\n",
                    "        self.check_line(output, 'Result: %s' % result)\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 400,
                    "end": 400
                },
                "child_version_range": {
                    "start": 400,
                    "end": 411
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "BaseTestCase",
                        "signature": "class BaseTestCase(unittest.TestCase):",
                        "at_line": 302
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: Lib/test/test_regrtest.py\nCode:\n           class BaseTestCase(unittest.TestCase):\n               ...\n397 397                    regex = 'All %s' % regex\n398 398                self.check_line(output, regex)\n399 399    \n    400  +         if interrupted:\n    401  +             self.check_line(output, 'Test suite interrupted by signal SIGINT.')\n    402  + \n    403  +         if nfailed:\n    404  +             result = 'FAILURE'\n    405  +         elif interrupted:\n    406  +             result = 'INTERRUPTED'\n    407  +         else:\n    408  +             result = 'SUCCESS'\n    409  +         self.check_line(output, 'Result: %s' % result)\n    410  + \n400 411        def parse_random_seed(self, output):\n401 412            match = self.regex_search(r'Using random seed ([0-9]+)', output)\n402 413            randseed = int(match.group(1))\n         ...\n",
                "file_path": "Lib/test/test_regrtest.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "check_line",
                    "interrupted",
                    "nfailed",
                    "output",
                    "result",
                    "self"
                ],
                "prefix": [
                    "                regex = 'All %s' % regex\n",
                    "            self.check_line(output, regex)\n",
                    "\n"
                ],
                "suffix": [
                    "    def parse_random_seed(self, output):\n",
                    "        match = self.regex_search(r'Using random seed ([0-9]+)', output)\n",
                    "        randseed = int(match.group(1))\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "interrupted",
                            "position": {
                                "start": {
                                    "line": 400,
                                    "column": 11
                                },
                                "end": {
                                    "line": 400,
                                    "column": 22
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "interrupted",
                            "position": {
                                "start": {
                                    "line": 405,
                                    "column": 13
                                },
                                "end": {
                                    "line": 405,
                                    "column": 24
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    def parse_random_seed(self, output):\n",
                "        match = self.regex_search(r'Using random seed ([0-9]+)', output)\n",
                "        randseed = int(match.group(1))\n",
                "        self.assertTrue(0 <= randseed <= 10000000, randseed)\n",
                "        return randseed\n",
                "\n",
                "    def run_command(self, args, input=None, exitcode=0, **kw):\n",
                "        if not input:\n",
                "            input = ''\n",
                "        if 'stderr' not in kw:\n",
                "            kw['stderr'] = subprocess.PIPE\n",
                "        proc = subprocess.run(args,\n",
                "                              universal_newlines=True,\n",
                "                              input=input,\n",
                "                              stdout=subprocess.PIPE,\n",
                "                              **kw)\n",
                "        if proc.returncode != exitcode:\n",
                "            msg = (\"Command %s failed with exit code %s\\n\"\n",
                "                   \"\\n\"\n",
                "                   \"stdout:\\n\"\n",
                "                   \"---\\n\"\n",
                "                   \"%s\\n\"\n",
                "                   \"---\\n\"\n",
                "                   % (str(args), proc.returncode, proc.stdout))\n",
                "            if proc.stderr:\n",
                "                msg += (\"\\n\"\n",
                "                        \"stderr:\\n\"\n",
                "                        \"---\\n\"\n",
                "                        \"%s\"\n",
                "                        \"---\\n\"\n",
                "                        % proc.stderr)\n",
                "            self.fail(msg)\n",
                "        return proc\n",
                "\n",
                "\n",
                "    def run_python(self, args, **kw):\n",
                "        args = [sys.executable, '-X', 'faulthandler', '-I', *args]\n",
                "        proc = self.run_command(args, **kw)\n",
                "        return proc.stdout\n",
                "\n",
                "\n",
                "class ProgramsTestCase(BaseTestCase):\n",
                "    \"\"\"\n",
                "    Test various ways to run the Python test suite. Use options close\n",
                "    to options used on the buildbot.\n",
                "    \"\"\"\n",
                "\n",
                "    NTEST = 4\n",
                "\n",
                "    def setUp(self):\n",
                "        super().setUp()\n",
                "\n",
                "        # Create NTEST tests doing nothing\n",
                "        self.tests = [self.create_test() for index in range(self.NTEST)]\n",
                "\n",
                "        self.python_args = ['-Wd', '-E', '-bb']\n",
                "        self.regrtest_args = ['-uall', '-rwW',\n",
                "                              '--testdir=%s' % self.tmptestdir]\n",
                "        if hasattr(faulthandler, 'dump_traceback_later'):\n",
                "            self.regrtest_args.extend(('--timeout', '3600', '-j4'))\n",
                "        if sys.platform == 'win32':\n",
                "            self.regrtest_args.append('-n')\n",
                "\n",
                "    def check_output(self, output):\n",
                "        self.parse_random_seed(output)\n",
                "        self.check_executed_tests(output, self.tests, randomize=True)\n",
                "\n",
                "    def run_tests(self, args):\n",
                "        output = self.run_python(args)\n",
                "        self.check_output(output)\n",
                "\n",
                "    def test_script_regrtest(self):\n",
                "        # Lib/test/regrtest.py\n",
                "        script = os.path.join(self.testdir, 'regrtest.py')\n",
                "\n",
                "        args = [*self.python_args, script, *self.regrtest_args, *self.tests]\n",
                "        self.run_tests(args)\n",
                "\n",
                "    def test_module_test(self):\n",
                "        # -m test\n",
                "        args = [*self.python_args, '-m', 'test',\n",
                "                *self.regrtest_args, *self.tests]\n",
                "        self.run_tests(args)\n",
                "\n",
                "    def test_module_regrtest(self):\n",
                "        # -m test.regrtest\n",
                "        args = [*self.python_args, '-m', 'test.regrtest',\n",
                "                *self.regrtest_args, *self.tests]\n",
                "        self.run_tests(args)\n",
                "\n",
                "    def test_module_autotest(self):\n",
                "        # -m test.autotest\n",
                "        args = [*self.python_args, '-m', 'test.autotest',\n",
                "                *self.regrtest_args, *self.tests]\n",
                "        self.run_tests(args)\n",
                "\n",
                "    def test_module_from_test_autotest(self):\n",
                "        # from test import autotest\n",
                "        code = 'from test import autotest'\n",
                "        args = [*self.python_args, '-c', code,\n",
                "                *self.regrtest_args, *self.tests]\n",
                "        self.run_tests(args)\n",
                "\n",
                "    def test_script_autotest(self):\n",
                "        # Lib/test/autotest.py\n",
                "        script = os.path.join(self.testdir, 'autotest.py')\n",
                "        args = [*self.python_args, script, *self.regrtest_args, *self.tests]\n",
                "        self.run_tests(args)\n",
                "\n",
                "    @unittest.skipUnless(sysconfig.is_python_build(),\n",
                "                         'run_tests.py script is not installed')\n",
                "    def test_tools_script_run_tests(self):\n",
                "        # Tools/scripts/run_tests.py\n",
                "        script = os.path.join(ROOT_DIR, 'Tools', 'scripts', 'run_tests.py')\n",
                "        args = [script, *self.regrtest_args, *self.tests]\n",
                "        self.run_tests(args)\n",
                "\n",
                "    def run_batch(self, *args):\n",
                "        proc = self.run_command(args)\n",
                "        self.check_output(proc.stdout)\n",
                "\n",
                "    @unittest.skipUnless(sysconfig.is_python_build(),\n",
                "                         'test.bat script is not installed')\n",
                "    @unittest.skipUnless(sys.platform == 'win32', 'Windows only')\n",
                "    def test_tools_buildbot_test(self):\n",
                "        # Tools\\buildbot\\test.bat\n",
                "        script = os.path.join(ROOT_DIR, 'Tools', 'buildbot', 'test.bat')\n",
                "        test_args = ['--testdir=%s' % self.tmptestdir]\n",
                "        if platform.architecture()[0] == '64bit':\n",
                "            test_args.append('-x64')   # 64-bit build\n",
                "        if not Py_DEBUG:\n",
                "            test_args.append('+d')     # Release build, use python.exe\n",
                "        self.run_batch(script, *test_args, *self.tests)\n",
                "\n",
                "    @unittest.skipUnless(sys.platform == 'win32', 'Windows only')\n",
                "    def test_pcbuild_rt(self):\n",
                "        # PCbuild\\rt.bat\n",
                "        script = os.path.join(ROOT_DIR, r'PCbuild\\rt.bat')\n",
                "        rt_args = [\"-q\"]             # Quick, don't run tests twice\n",
                "        if platform.architecture()[0] == '64bit':\n",
                "            rt_args.append('-x64')   # 64-bit build\n",
                "        if Py_DEBUG:\n",
                "            rt_args.append('-d')     # Debug build, use python_d.exe\n",
                "        self.run_batch(script, *rt_args, *self.regrtest_args, *self.tests)\n",
                "\n",
                "\n",
                "class ArgsTestCase(BaseTestCase):\n",
                "    \"\"\"\n",
                "    Test arguments of the Python test suite.\n",
                "    \"\"\"\n",
                "\n",
                "    def run_tests(self, *testargs, **kw):\n",
                "        cmdargs = ['-m', 'test', '--testdir=%s' % self.tmptestdir, *testargs]\n",
                "        return self.run_python(cmdargs, **kw)\n",
                "\n",
                "    def test_failing_test(self):\n",
                "        # test a failing test\n",
                "        code = textwrap.dedent(\"\"\"\n",
                "            import unittest\n",
                "\n",
                "            class FailingTest(unittest.TestCase):\n",
                "                def test_failing(self):\n",
                "                    self.fail(\"bug\")\n",
                "        \"\"\")\n",
                "        test_ok = self.create_test('ok')\n",
                "        test_failing = self.create_test('failing', code=code)\n",
                "        tests = [test_ok, test_failing]\n",
                "\n",
                "        output = self.run_tests(*tests, exitcode=1)\n",
                "        self.check_executed_tests(output, tests, failed=test_failing)\n",
                "\n",
                "    def test_resources(self):\n",
                "        # test -u command line option\n",
                "        tests = {}\n",
                "        for resource in ('audio', 'network'):\n",
                "            code = 'from test import support\\nsupport.requires(%r)' % resource\n",
                "            tests[resource] = self.create_test(resource, code)\n",
                "        test_names = sorted(tests.values())\n",
                "\n",
                "        # -u all: 2 resources enabled\n",
                "        output = self.run_tests('-u', 'all', *test_names)\n",
                "        self.check_executed_tests(output, test_names)\n",
                "\n",
                "        # -u audio: 1 resource enabled\n",
                "        output = self.run_tests('-uaudio', *test_names)\n",
                "        self.check_executed_tests(output, test_names,\n",
                "                                  skipped=tests['network'])\n",
                "\n",
                "        # no option: 0 resources enabled\n",
                "        output = self.run_tests(*test_names)\n",
                "        self.check_executed_tests(output, test_names,\n",
                "                                  skipped=test_names)\n",
                "\n",
                "    def test_random(self):\n",
                "        # test -r and --randseed command line option\n",
                "        code = textwrap.dedent(\"\"\"\n",
                "            import random\n",
                "            print(\"TESTRANDOM: %s\" % random.randint(1, 1000))\n",
                "        \"\"\")\n",
                "        test = self.create_test('random', code)\n",
                "\n",
                "        # first run to get the output with the random seed\n",
                "        output = self.run_tests('-r', test)\n",
                "        randseed = self.parse_random_seed(output)\n",
                "        match = self.regex_search(r'TESTRANDOM: ([0-9]+)', output)\n",
                "        test_random = int(match.group(1))\n",
                "\n",
                "        # try to reproduce with the random seed\n",
                "        output = self.run_tests('-r', '--randseed=%s' % randseed, test)\n",
                "        randseed2 = self.parse_random_seed(output)\n",
                "        self.assertEqual(randseed2, randseed)\n",
                "\n",
                "        match = self.regex_search(r'TESTRANDOM: ([0-9]+)', output)\n",
                "        test_random2 = int(match.group(1))\n",
                "        self.assertEqual(test_random2, test_random)\n",
                "\n",
                "    def test_fromfile(self):\n",
                "        # test --fromfile\n",
                "        tests = [self.create_test() for index in range(5)]\n",
                "\n",
                "        # Write the list of files using a format similar to regrtest output:\n",
                "        # [1/2] test_1\n",
                "        # [2/2] test_2\n",
                "        filename = support.TESTFN\n",
                "        self.addCleanup(support.unlink, filename)\n",
                "\n",
                "        # test format '0:00:00 [2/7] test_opcodes -- test_grammar took 0 sec'\n",
                "        with open(filename, \"w\") as fp:\n",
                "            previous = None\n",
                "            for index, name in enumerate(tests, 1):\n",
                "                line = (\"00:00:%02i [%s/%s] %s\"\n",
                "                        % (index, index, len(tests), name))\n",
                "                if previous:\n",
                "                    line += \" -- %s took 0 sec\" % previous\n",
                "                print(line, file=fp)\n",
                "                previous = name\n",
                "\n",
                "        output = self.run_tests('--fromfile', filename)\n",
                "        self.check_executed_tests(output, tests)\n",
                "\n",
                "        # test format '[2/7] test_opcodes'\n",
                "        with open(filename, \"w\") as fp:\n",
                "            for index, name in enumerate(tests, 1):\n",
                "                print(\"[%s/%s] %s\" % (index, len(tests), name), file=fp)\n",
                "\n",
                "        output = self.run_tests('--fromfile', filename)\n",
                "        self.check_executed_tests(output, tests)\n",
                "\n",
                "        # test format 'test_opcodes'\n",
                "        with open(filename, \"w\") as fp:\n",
                "            for name in tests:\n",
                "                print(name, file=fp)\n",
                "\n",
                "        output = self.run_tests('--fromfile', filename)\n",
                "        self.check_executed_tests(output, tests)\n",
                "\n",
                "    def test_interrupted(self):\n",
                "        code = TEST_INTERRUPTED\n",
                "        test = self.create_test('sigint', code=code)\n",
                "        output = self.run_tests(test, exitcode=1)\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        self.check_executed_tests(output, test, omitted=test)\n"
                ],
                "after": [
                    "        self.check_executed_tests(output, test, omitted=test,\n",
                    "                                  interrupted=True)\n"
                ],
                "parent_version_range": {
                    "start": 660,
                    "end": 661
                },
                "child_version_range": {
                    "start": 671,
                    "end": 673
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "ArgsTestCase",
                        "signature": "class ArgsTestCase(BaseTestCase):",
                        "at_line": 546
                    },
                    {
                        "type": "function",
                        "name": "test_interrupted",
                        "signature": "def test_interrupted(self):",
                        "at_line": 656
                    },
                    {
                        "type": "call",
                        "name": "self.check_executed_tests",
                        "signature": "self.check_executed_tests(output, test, omitted=test)",
                        "at_line": 660,
                        "argument": "output"
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: Lib/test/test_regrtest.py\nCode:\n           class ArgsTestCase(BaseTestCase):\n               ...\n               def test_interrupted(self):\n                   ...\n657 668            code = TEST_INTERRUPTED\n658 669            test = self.create_test('sigint', code=code)\n659 670            output = self.run_tests(test, exitcode=1)\n660      -         self.check_executed_tests(output, test, omitted=test)\n    671  +         self.check_executed_tests(output, test, omitted=test,\n    672  +                                   interrupted=True)\n661 673    \n662 674        def test_slowest(self):\n663 675            # test --slowest\n         ...\n",
                "file_path": "Lib/test/test_regrtest.py",
                "identifiers_before": [
                    "check_executed_tests",
                    "omitted",
                    "output",
                    "self",
                    "test"
                ],
                "identifiers_after": [
                    "check_executed_tests",
                    "interrupted",
                    "omitted",
                    "output",
                    "self",
                    "test"
                ],
                "prefix": [
                    "        code = TEST_INTERRUPTED\n",
                    "        test = self.create_test('sigint', code=code)\n",
                    "        output = self.run_tests(test, exitcode=1)\n"
                ],
                "suffix": [
                    "\n",
                    "    def test_slowest(self):\n",
                    "        # test --slowest\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "omitted",
                            "position": {
                                "start": {
                                    "line": 660,
                                    "column": 48
                                },
                                "end": {
                                    "line": 660,
                                    "column": 55
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "interrupted",
                            "position": {
                                "start": {
                                    "line": 672,
                                    "column": 34
                                },
                                "end": {
                                    "line": 672,
                                    "column": 45
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "omitted",
                            "position": {
                                "start": {
                                    "line": 671,
                                    "column": 48
                                },
                                "end": {
                                    "line": 671,
                                    "column": 55
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    4
                ]
            },
            [
                "\n",
                "    def test_slowest(self):\n",
                "        # test --slowest\n",
                "        tests = [self.create_test() for index in range(3)]\n",
                "        output = self.run_tests(\"--slowest\", *tests)\n",
                "        self.check_executed_tests(output, tests)\n",
                "        regex = ('10 slowest tests:\\n'\n",
                "                 '(?:- %s: .*\\n){%s}'\n",
                "                 % (self.TESTNAME_REGEX, len(tests)))\n",
                "        self.check_line(output, regex)\n",
                "\n",
                "    def test_slow_interrupted(self):\n",
                "        # Issue #25373: test --slowest with an interrupted test\n",
                "        code = TEST_INTERRUPTED\n",
                "        test = self.create_test(\"sigint\", code=code)\n",
                "\n",
                "        for multiprocessing in (False, True):\n",
                "            if multiprocessing:\n",
                "                args = (\"--slowest\", \"-j2\", test)\n",
                "            else:\n",
                "                args = (\"--slowest\", test)\n",
                "            output = self.run_tests(*args, exitcode=1)\n"
            ],
            {
                "type": "replace",
                "before": [
                    "            self.check_executed_tests(output, test, omitted=test)\n"
                ],
                "after": [
                    "            self.check_executed_tests(output, test,\n",
                    "                                      omitted=test, interrupted=True)\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 683,
                    "end": 684
                },
                "child_version_range": {
                    "start": 695,
                    "end": 698
                },
                "control_flow": [
                    {
                        "type": "for_statement",
                        "statement": "for multiprocessing in (False, True):",
                        "start_line": 677,
                        "end_line": 686
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "ArgsTestCase",
                        "signature": "class ArgsTestCase(BaseTestCase):",
                        "at_line": 546
                    },
                    {
                        "type": "function",
                        "name": "test_slow_interrupted",
                        "signature": "def test_slow_interrupted(self):",
                        "at_line": 672
                    },
                    {
                        "type": "call",
                        "name": "self.check_executed_tests",
                        "signature": "self.check_executed_tests(output, test, omitted=test)",
                        "at_line": 683,
                        "argument": "output"
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: Lib/test/test_regrtest.py\nCode:\n           class ArgsTestCase(BaseTestCase):\n               ...\n               def test_slow_interrupted(self):\n                   ...\n680 692                else:\n681 693                    args = (\"--slowest\", test)\n682 694                output = self.run_tests(*args, exitcode=1)\n683      -             self.check_executed_tests(output, test, omitted=test)\n    695  +             self.check_executed_tests(output, test,\n    696  +                                       omitted=test, interrupted=True)\n    697  + \n684 698                regex = ('10 slowest tests:\\n')\n685 699                self.check_line(output, regex)\n         ...\n",
                "file_path": "Lib/test/test_regrtest.py",
                "identifiers_before": [
                    "check_executed_tests",
                    "omitted",
                    "output",
                    "self",
                    "test"
                ],
                "identifiers_after": [
                    "check_executed_tests",
                    "interrupted",
                    "omitted",
                    "output",
                    "self",
                    "test"
                ],
                "prefix": [
                    "            else:\n",
                    "                args = (\"--slowest\", test)\n",
                    "            output = self.run_tests(*args, exitcode=1)\n"
                ],
                "suffix": [
                    "            regex = ('10 slowest tests:\\n')\n",
                    "            self.check_line(output, regex)\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "omitted",
                            "position": {
                                "start": {
                                    "line": 683,
                                    "column": 52
                                },
                                "end": {
                                    "line": 683,
                                    "column": 59
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "interrupted",
                            "position": {
                                "start": {
                                    "line": 696,
                                    "column": 52
                                },
                                "end": {
                                    "line": 696,
                                    "column": 63
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "omitted",
                            "position": {
                                "start": {
                                    "line": 696,
                                    "column": 38
                                },
                                "end": {
                                    "line": 696,
                                    "column": 45
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/cpython/Lib/test/test_regrtest.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    3
                ]
            },
            [
                "            regex = ('10 slowest tests:\\n')\n",
                "            self.check_line(output, regex)\n"
            ],
            {
                "type": "delete",
                "before": [
                    "            self.check_line(output, 'Test suite interrupted by signal SIGINT.')\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 686,
                    "end": 687
                },
                "child_version_range": {
                    "start": 700,
                    "end": 700
                },
                "control_flow": [
                    {
                        "type": "for_statement",
                        "statement": "for multiprocessing in (False, True):",
                        "start_line": 677,
                        "end_line": 686
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "ArgsTestCase",
                        "signature": "class ArgsTestCase(BaseTestCase):",
                        "at_line": 546
                    },
                    {
                        "type": "function",
                        "name": "test_slow_interrupted",
                        "signature": "def test_slow_interrupted(self):",
                        "at_line": 672
                    },
                    {
                        "type": "call",
                        "name": "self.check_line",
                        "signature": "self.check_line(output, 'Test suite interrupted by signal SIGINT.')",
                        "at_line": 686,
                        "argument": "output"
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: Lib/test/test_regrtest.py\nCode:\n           class ArgsTestCase(BaseTestCase):\n               ...\n               def test_slow_interrupted(self):\n                   ...\n684 698                regex = ('10 slowest tests:\\n')\n685 699                self.check_line(output, regex)\n686      -             self.check_line(output, 'Test suite interrupted by signal SIGINT.')\n687 700    \n688 701        def test_coverage(self):\n689 702            # test --coverage\n         ...\n",
                "file_path": "Lib/test/test_regrtest.py",
                "identifiers_before": [
                    "check_line",
                    "output",
                    "self"
                ],
                "identifiers_after": [],
                "prefix": [
                    "            regex = ('10 slowest tests:\\n')\n",
                    "            self.check_line(output, regex)\n"
                ],
                "suffix": [
                    "\n",
                    "    def test_coverage(self):\n",
                    "        # test --coverage\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "\n",
                "    def test_coverage(self):\n",
                "        # test --coverage\n",
                "        test = self.create_test('coverage')\n",
                "        output = self.run_tests(\"--coverage\", test)\n",
                "        self.check_executed_tests(output, [test])\n",
                "        regex = ('lines +cov% +module +\\(path\\)\\n'\n",
                "                 '(?: *[0-9]+ *[0-9]{1,2}% *[^ ]+ +\\([^)]+\\)+)+')\n",
                "        self.check_line(output, regex)\n",
                "\n",
                "    def test_wait(self):\n",
                "        # test --wait\n",
                "        test = self.create_test('wait')\n",
                "        output = self.run_tests(\"--wait\", test, input='key')\n",
                "        self.check_line(output, 'Press any key to continue')\n",
                "\n",
                "    def test_forever(self):\n",
                "        # test --forever\n",
                "        code = textwrap.dedent(\"\"\"\n",
                "            import builtins\n",
                "            import unittest\n",
                "\n",
                "            class ForeverTester(unittest.TestCase):\n",
                "                def test_run(self):\n",
                "                    # Store the state in the builtins module, because the test\n",
                "                    # module is reload at each run\n",
                "                    if 'RUN' in builtins.__dict__:\n",
                "                        builtins.__dict__['RUN'] += 1\n",
                "                        if builtins.__dict__['RUN'] >= 3:\n",
                "                            self.fail(\"fail at the 3rd runs\")\n",
                "                    else:\n",
                "                        builtins.__dict__['RUN'] = 1\n",
                "        \"\"\")\n",
                "        test = self.create_test('forever', code=code)\n",
                "        output = self.run_tests('--forever', test, exitcode=1)\n",
                "        self.check_executed_tests(output, [test]*3, failed=test)\n",
                "\n",
                "    @unittest.skipUnless(Py_DEBUG, 'need a debug build')\n",
                "    def test_huntrleaks_fd_leak(self):\n",
                "        # test --huntrleaks for file descriptor leak\n",
                "        code = textwrap.dedent(\"\"\"\n",
                "            import os\n",
                "            import unittest\n",
                "\n",
                "            # Issue #25306: Disable popups and logs to stderr on assertion\n",
                "            # failures in MSCRT\n",
                "            try:\n",
                "                import msvcrt\n",
                "                msvcrt.CrtSetReportMode\n",
                "            except (ImportError, AttributeError):\n",
                "                # no Windows, o release build\n",
                "                pass\n",
                "            else:\n",
                "                for m in [msvcrt.CRT_WARN, msvcrt.CRT_ERROR, msvcrt.CRT_ASSERT]:\n",
                "                    msvcrt.CrtSetReportMode(m, 0)\n",
                "\n",
                "            class FDLeakTest(unittest.TestCase):\n",
                "                def test_leak(self):\n",
                "                    fd = os.open(__file__, os.O_RDONLY)\n",
                "                    # bug: never cloes the file descriptor\n",
                "        \"\"\")\n",
                "        test = self.create_test('huntrleaks', code=code)\n",
                "\n",
                "        filename = 'reflog.txt'\n",
                "        self.addCleanup(support.unlink, filename)\n",
                "        output = self.run_tests('--huntrleaks', '3:3:', test,\n",
                "                                exitcode=1,\n",
                "                                stderr=subprocess.STDOUT)\n",
                "        self.check_executed_tests(output, [test], failed=test)\n",
                "\n",
                "        line = 'beginning 6 repetitions\\n123456\\n......\\n'\n",
                "        self.check_line(output, re.escape(line))\n",
                "\n",
                "        line2 = '%s leaked [1, 1, 1] file descriptors, sum=3\\n' % test\n",
                "        self.check_line(output, re.escape(line2))\n",
                "\n",
                "        with open(filename) as fp:\n",
                "            reflog = fp.read()\n",
                "            if hasattr(sys, 'getcounts'):\n",
                "                # Types are immportal if COUNT_ALLOCS is defined\n",
                "                reflog = reflog.splitlines(True)[-1]\n",
                "            self.assertEqual(reflog, line2)\n",
                "\n",
                "    def test_list_tests(self):\n",
                "        # test --list-tests\n",
                "        tests = [self.create_test() for i in range(5)]\n",
                "        output = self.run_tests('--list-tests', *tests)\n",
                "        self.assertEqual(output.rstrip().splitlines(),\n",
                "                         tests)\n",
                "\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    unittest.main()"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                1,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "arg def and use"
        },
        {
            "edit_hunk_pair": [
                1,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "func def and use"
        },
        {
            "edit_hunk_pair": [
                1,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "func def and use"
        },
        {
            "edit_hunk_pair": [
                2,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "implement and call"
        },
        {
            "edit_hunk_pair": [
                2,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "implement and call"
        },
        {
            "edit_hunk_pair": [
                3,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        },
        {
            "edit_hunk_pair": [
                2,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "refactor"
        }
    ]
}