{
    "language": "python",
    "commit_url": "https://github.com/localstack/localstack/commit/f79fb82e933053efbe3f479751772b1bfd063ea1",
    "commit_message": "fix text/xml content-type header in STS responses (#4942)",
    "commit_snapshots": {
        "localstack/services/s3/s3_listener.py": [
            [
                "import base64\n",
                "import codecs\n",
                "import collections\n",
                "import datetime\n",
                "import json\n",
                "import logging\n",
                "import random\n",
                "import re\n",
                "import urllib.parse\n",
                "import uuid\n",
                "from urllib.parse import parse_qs\n",
                "\n",
                "import botocore.config\n",
                "import dateutil.parser\n",
                "import six\n",
                "import xmltodict\n",
                "from botocore.client import ClientError\n",
                "from moto.s3.exceptions import InvalidFilterRuleName\n",
                "from moto.s3.models import s3_backend\n",
                "from pytz import timezone\n",
                "from requests.models import Request, Response\n",
                "from six.moves.urllib import parse as urlparse\n",
                "\n",
                "from localstack import config, constants\n",
                "from localstack.services.s3 import multipart_content\n",
                "from localstack.services.s3.s3_utils import (\n",
                "    ALLOWED_HEADER_OVERRIDES,\n",
                "    SIGNATURE_V2_PARAMS,\n",
                "    SIGNATURE_V4_PARAMS,\n",
                "    authenticate_presign_url,\n",
                "    extract_bucket_name,\n",
                "    extract_key_name,\n",
                "    get_forwarded_for_host,\n",
                "    is_expired,\n",
                "    is_static_website,\n",
                "    normalize_bucket_name,\n",
                "    uses_host_addressing,\n",
                "    validate_bucket_name,\n",
                ")\n",
                "from localstack.utils.analytics import event_publisher\n",
                "from localstack.utils.aws import aws_stack\n"
            ],
            {
                "type": "replace",
                "before": [
                    "from localstack.utils.aws.aws_responses import create_sqs_system_attributes, requests_response\n"
                ],
                "after": [
                    "from localstack.utils.aws.aws_responses import (\n",
                    "    create_sqs_system_attributes,\n",
                    "    is_invalid_html_response,\n",
                    "    requests_response,\n",
                    ")\n"
                ],
                "parent_version_range": {
                    "start": 41,
                    "end": 42
                },
                "child_version_range": {
                    "start": 41,
                    "end": 46
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 0,
                "hunk_diff": "File: localstack/services/s3/s3_listener.py\nCode:\n  ...\n38 38    )\n39 39    from localstack.utils.analytics import event_publisher\n40 40    from localstack.utils.aws import aws_stack\n41     - from localstack.utils.aws.aws_responses import create_sqs_system_attributes, requests_response\n   41  + from localstack.utils.aws.aws_responses import (\n   42  +     create_sqs_system_attributes,\n   43  +     is_invalid_html_response,\n   44  +     requests_response,\n   45  + )\n42 46    from localstack.utils.common import (\n43 47        clone,\n44 48        get_service_protocol,\n       ...\n",
                "file_path": "localstack/services/s3/s3_listener.py",
                "identifiers_before": [
                    "aws",
                    "aws_responses",
                    "create_sqs_system_attributes",
                    "localstack",
                    "requests_response",
                    "utils"
                ],
                "identifiers_after": [
                    "aws",
                    "aws_responses",
                    "create_sqs_system_attributes",
                    "is_invalid_html_response",
                    "localstack",
                    "requests_response",
                    "utils"
                ],
                "prefix": [
                    ")\n",
                    "from localstack.utils.analytics import event_publisher\n",
                    "from localstack.utils.aws import aws_stack\n"
                ],
                "suffix": [
                    "from localstack.utils.common import (\n",
                    "    clone,\n",
                    "    get_service_protocol,\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 43,
                                    "column": 4
                                },
                                "end": {
                                    "line": 43,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/services/s3/s3_listener.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 43,
                                    "column": 4
                                },
                                "end": {
                                    "line": 43,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/services/s3/s3_listener.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "from localstack.utils.common import (\n",
                "    clone,\n",
                "    get_service_protocol,\n",
                "    is_base64,\n",
                "    md5,\n",
                "    not_none_or,\n",
                "    short_uid,\n",
                "    timestamp_millis,\n",
                "    to_bytes,\n",
                "    to_str,\n",
                ")\n",
                "from localstack.utils.persistence import PersistingProxyListener\n",
                "\n",
                "CONTENT_SHA256_HEADER = \"x-amz-content-sha256\"\n",
                "STREAMING_HMAC_PAYLOAD = \"STREAMING-AWS4-HMAC-SHA256-PAYLOAD\"\n",
                "\n",
                "# backend port (configured in s3_starter.py on startup)\n",
                "PORT_S3_BACKEND = None\n",
                "\n",
                "# mappings for S3 bucket notifications\n",
                "S3_NOTIFICATIONS = s3_backend.S3_NOTIFICATIONS = getattr(s3_backend, \"S3_NOTIFICATIONS\", {})\n",
                "\n",
                "# mappings for bucket CORS settings\n",
                "BUCKET_CORS = s3_backend.BUCKET_CORS = getattr(s3_backend, \"BUCKET_CORS\", {})\n",
                "\n",
                "# maps bucket name to lifecycle settings\n",
                "BUCKET_LIFECYCLE = s3_backend.BUCKET_LIFECYCLE = getattr(s3_backend, \"BUCKET_LIFECYCLE\", {})\n",
                "\n",
                "# maps bucket name to replication settings\n",
                "BUCKET_REPLICATIONS = s3_backend.BUCKET_REPLICATIONS = getattr(\n",
                "    s3_backend, \"BUCKET_REPLICATIONS\", {}\n",
                ")\n",
                "\n",
                "# map to store the s3 expiry dates\n",
                "OBJECT_EXPIRY = s3_backend.OBJECT_EXPIRY = getattr(s3_backend, \"OBJECT_EXPIRY\", {})\n",
                "\n",
                "# set up logger\n",
                "LOGGER = logging.getLogger(__name__)\n",
                "\n",
                "# XML namespace constants\n",
                "XMLNS_S3 = \"http://s3.amazonaws.com/doc/2006-03-01/\"\n",
                "\n",
                "# see https://stackoverflow.com/questions/50480924/regex-for-s3-bucket-name#50484916\n",
                "BUCKET_NAME_REGEX = (\n",
                "    r\"(?=^.{3,63}$)(?!^(\\d+\\.)+\\d+$)\"\n",
                "    + r\"(^(([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])\\.)*([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])$)\"\n",
                ")\n",
                "\n",
                "# list of destination types for bucket notifications\n",
                "NOTIFICATION_DESTINATION_TYPES = (\"Queue\", \"Topic\", \"CloudFunction\", \"LambdaFunction\")\n",
                "\n",
                "# prefix for object metadata keys in headers and query params\n",
                "OBJECT_METADATA_KEY_PREFIX = \"x-amz-meta-\"\n",
                "\n",
                "# STS policy expiration date format\n",
                "POLICY_EXPIRATION_FORMAT1 = \"%Y-%m-%dT%H:%M:%SZ\"\n",
                "POLICY_EXPIRATION_FORMAT2 = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
                "\n",
                "# ignored_headers_lower conatins headers which don't get involved in signature calculations process\n",
                "# these headers are being sent by the localstack by default.\n",
                "IGNORED_HEADERS_LOWER = [\n",
                "    \"remote-addr\",\n",
                "    \"host\",\n",
                "    \"user-agent\",\n",
                "    \"accept-encoding\",\n",
                "    \"accept\",\n",
                "    \"connection\",\n",
                "    \"origin\",\n",
                "    \"x-forwarded-for\",\n",
                "    \"x-localstack-edge\",\n",
                "    \"authorization\",\n",
                "    \"date\",\n",
                "]\n",
                "\n",
                "CORS_HEADERS = [\n",
                "    \"Access-Control-Allow-Origin\",\n",
                "    \"Access-Control-Allow-Methods\",\n",
                "    \"Access-Control-Allow-Headers\",\n",
                "    \"Access-Control-Max-Age\",\n",
                "    \"Access-Control-Allow-Credentials\",\n",
                "    \"Access-Control-Expose-Headers\",\n",
                "    \"Access-Control-Request-Headers\",\n",
                "    \"Access-Control-Request-Method\",\n",
                "]\n",
                "\n",
                "\n",
                "def event_type_matches(events, action, api_method):\n",
                "    \"\"\"check whether any of the event types in `events` matches the\n",
                "    given `action` and `api_method`, and return the first match.\"\"\"\n",
                "    events = events or []\n",
                "    for event in events:\n",
                "        regex = event.replace(\"*\", \"[^:]*\")\n",
                "        action_string = \"s3:%s:%s\" % (action, api_method)\n",
                "        match = re.match(regex, action_string)\n",
                "        if match:\n",
                "            return match\n",
                "    return False\n",
                "\n",
                "\n",
                "def filter_rules_match(filters, object_path):\n",
                "    \"\"\"check whether the given object path matches all of the given filters\"\"\"\n",
                "    filters = filters or {}\n",
                "    s3_filter = _get_s3_filter(filters)\n",
                "    for rule in s3_filter.get(\"FilterRule\", []):\n",
                "        rule_name_lower = rule[\"Name\"].lower()\n",
                "        if rule_name_lower == \"prefix\":\n",
                "            if not prefix_with_slash(object_path).startswith(prefix_with_slash(rule[\"Value\"])):\n",
                "                return False\n",
                "        elif rule_name_lower == \"suffix\":\n",
                "            if not object_path.endswith(rule[\"Value\"]):\n",
                "                return False\n",
                "        else:\n",
                "            LOGGER.warning('Unknown filter name: \"%s\"' % rule[\"Name\"])\n",
                "    return True\n",
                "\n",
                "\n",
                "def _get_s3_filter(filters):\n",
                "    return filters.get(\"S3Key\", filters.get(\"Key\", {}))\n",
                "\n",
                "\n",
                "def prefix_with_slash(s):\n",
                "    return s if s and s[0] == \"/\" else \"/%s\" % s\n",
                "\n",
                "\n",
                "def get_event_message(\n",
                "    event_name,\n",
                "    bucket_name,\n",
                "    file_name=\"testfile.txt\",\n",
                "    etag=\"\",\n",
                "    version_id=None,\n",
                "    file_size=0,\n",
                "):\n",
                "    # Based on: http://docs.aws.amazon.com/AmazonS3/latest/dev/notification-content-structure.html\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "    return {\n",
                "        \"Records\": [\n",
                "            {\n",
                "                \"eventVersion\": \"2.1\",\n",
                "                \"eventSource\": \"aws:s3\",\n",
                "                \"awsRegion\": aws_stack.get_region(),\n",
                "                \"eventTime\": timestamp_millis(),\n",
                "                \"eventName\": event_name,\n",
                "                \"userIdentity\": {\"principalId\": \"AIDAJDPLRKLG7UEXAMPLE\"},\n",
                "                \"requestParameters\": {\n",
                "                    \"sourceIPAddress\": \"127.0.0.1\"\n",
                "                },  # TODO determine real source IP\n",
                "                \"responseElements\": {\n",
                "                    \"x-amz-request-id\": short_uid(),\n",
                "                    \"x-amz-id-2\": \"eftixk72aD6Ap51TnqcoF8eFidJG9Z/2\",  # Amazon S3 host that processed the request\n",
                "                },\n",
                "                \"s3\": {\n",
                "                    \"s3SchemaVersion\": \"1.0\",\n",
                "                    \"configurationId\": \"testConfigRule\",\n",
                "                    \"bucket\": {\n",
                "                        \"name\": bucket_name,\n",
                "                        \"ownerIdentity\": {\"principalId\": \"A3NL1KOZZKExample\"},\n",
                "                        \"arn\": \"arn:aws:s3:::%s\" % bucket_name,\n",
                "                    },\n",
                "                    \"object\": {\n",
                "                        \"key\": urllib.parse.quote(file_name),\n",
                "                        \"size\": file_size,\n",
                "                        \"eTag\": etag,\n",
                "                        \"versionId\": version_id,\n",
                "                        \"sequencer\": \"0055AED6DCD90281E5\",\n",
                "                    },\n",
                "                },\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "\n",
                "\n",
                "def send_notifications(method, bucket_name, object_path, version_id, headers):\n",
                "    for bucket, notifs in S3_NOTIFICATIONS.items():\n",
                "        if normalize_bucket_name(bucket) == normalize_bucket_name(bucket_name):\n",
                "            action = {\n",
                "                \"PUT\": \"ObjectCreated\",\n",
                "                \"POST\": \"ObjectCreated\",\n",
                "                \"DELETE\": \"ObjectRemoved\",\n",
                "            }[method]\n",
                "            # TODO: support more detailed methods, e.g., DeleteMarkerCreated\n",
                "            # http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\n",
                "            if action == \"ObjectCreated\" and method == \"PUT\" and \"x-amz-copy-source\" in headers:\n",
                "                api_method = \"Copy\"\n",
                "            elif (\n",
                "                action == \"ObjectCreated\"\n",
                "                and method == \"POST\"\n",
                "                and \"form-data\" in headers.get(\"Content-Type\", \"\")\n",
                "            ):\n",
                "                api_method = \"Post\"\n",
                "            elif action == \"ObjectCreated\" and method == \"POST\":\n",
                "                api_method = \"CompleteMultipartUpload\"\n",
                "            else:\n",
                "                api_method = {\"PUT\": \"Put\", \"POST\": \"Post\", \"DELETE\": \"Delete\"}[method]\n",
                "\n",
                "            event_name = \"%s:%s\" % (action, api_method)\n",
                "            for notif in notifs:\n",
                "                send_notification_for_subscriber(\n",
                "                    notif,\n",
                "                    bucket_name,\n",
                "                    object_path,\n",
                "                    version_id,\n",
                "                    api_method,\n",
                "                    action,\n",
                "                    event_name,\n",
                "                    headers,\n",
                "                )\n",
                "\n",
                "\n",
                "def send_notification_for_subscriber(\n",
                "    notif, bucket_name, object_path, version_id, api_method, action, event_name, headers\n",
                "):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "\n",
                "    if not event_type_matches(notif[\"Event\"], action, api_method) or not filter_rules_match(\n",
                "        notif.get(\"Filter\"), object_path\n",
                "    ):\n",
                "        return\n",
                "\n",
                "    key = urlparse.unquote(object_path.replace(\"//\", \"/\"))[1:]\n",
                "\n",
                "    s3_client = aws_stack.connect_to_service(\"s3\")\n",
                "    object_data = {}\n",
                "    try:\n",
                "        object_data = s3_client.head_object(Bucket=bucket_name, Key=key)\n",
                "\n",
                "    except botocore.exceptions.ClientError:\n",
                "        pass\n",
                "\n",
                "    # build event message\n",
                "    message = get_event_message(\n",
                "        event_name=event_name,\n",
                "        bucket_name=bucket_name,\n",
                "        file_name=key,\n",
                "        etag=object_data.get(\"ETag\", \"\"),\n",
                "        file_size=object_data.get(\"ContentLength\", 0),\n",
                "        version_id=version_id,\n",
                "    )\n",
                "    message = json.dumps(message)\n",
                "\n",
                "    if notif.get(\"Queue\"):\n",
                "        region = aws_stack.extract_region_from_arn(notif[\"Queue\"])\n",
                "        sqs_client = aws_stack.connect_to_service(\"sqs\", region_name=region)\n",
                "        try:\n",
                "            queue_url = aws_stack.sqs_queue_url_for_arn(notif[\"Queue\"])\n",
                "            sqs_client.send_message(\n",
                "                QueueUrl=queue_url,\n",
                "                MessageBody=message,\n",
                "                MessageSystemAttributes=create_sqs_system_attributes(headers),\n",
                "            )\n",
                "        except Exception as e:\n",
                "            LOGGER.warning(\n",
                "                'Unable to send notification for S3 bucket \"%s\" to SQS queue \"%s\": %s'\n",
                "                % (bucket_name, notif[\"Queue\"], e)\n",
                "            )\n",
                "    if notif.get(\"Topic\"):\n",
                "        region = aws_stack.extract_region_from_arn(notif[\"Topic\"])\n",
                "        sns_client = aws_stack.connect_to_service(\"sns\", region_name=region)\n",
                "        try:\n",
                "            sns_client.publish(\n",
                "                TopicArn=notif[\"Topic\"],\n",
                "                Message=message,\n",
                "                Subject=\"Amazon S3 Notification\",\n",
                "            )\n",
                "        except Exception as e:\n",
                "            LOGGER.warning(\n",
                "                'Unable to send notification for S3 bucket \"%s\" to SNS topic \"%s\": %s'\n",
                "                % (bucket_name, notif[\"Topic\"], e)\n",
                "            )\n",
                "    # CloudFunction and LambdaFunction are semantically identical\n",
                "    lambda_function_config = notif.get(\"CloudFunction\") or notif.get(\"LambdaFunction\")\n",
                "    if lambda_function_config:\n",
                "        # make sure we don't run into a socket timeout\n",
                "        region = aws_stack.extract_region_from_arn(lambda_function_config)\n",
                "        connection_config = botocore.config.Config(read_timeout=300)\n",
                "        lambda_client = aws_stack.connect_to_service(\n",
                "            \"lambda\", config=connection_config, region_name=region\n",
                "        )\n",
                "        try:\n",
                "            lambda_client.invoke(\n",
                "                FunctionName=lambda_function_config,\n",
                "                InvocationType=\"Event\",\n",
                "                Payload=message,\n",
                "            )\n",
                "        except Exception:\n",
                "            LOGGER.warning(\n",
                "                'Unable to send notification for S3 bucket \"%s\" to Lambda function \"%s\".'\n",
                "                % (bucket_name, lambda_function_config)\n",
                "            )\n",
                "\n",
                "    if not filter(lambda x: notif.get(x), NOTIFICATION_DESTINATION_TYPES):\n",
                "        LOGGER.warning(\n",
                "            \"Neither of %s defined for S3 notification.\" % \"/\".join(NOTIFICATION_DESTINATION_TYPES)\n",
                "        )\n",
                "\n",
                "\n",
                "# TODO: refactor/unify the 3 functions below...\n",
                "def get_cors(bucket_name):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "    response = Response()\n",
                "\n",
                "    exists, code = bucket_exists(bucket_name)\n",
                "    if not exists:\n",
                "        response.status_code = int(code)\n",
                "        return response\n",
                "\n",
                "    response.status_code = 200\n",
                "    cors = BUCKET_CORS.get(bucket_name)\n",
                "    if not cors:\n",
                "        response.status_code = 404\n",
                "        cors = {\n",
                "            \"Error\": {\n",
                "                \"Code\": \"NoSuchCORSConfiguration\",\n",
                "                \"Message\": \"The CORS configuration does not exist\",\n",
                "                \"BucketName\": bucket_name,\n",
                "                \"RequestId\": short_uid(),\n",
                "                \"HostId\": short_uid(),\n",
                "            }\n",
                "        }\n",
                "    body = xmltodict.unparse(cors)\n",
                "    response._content = body\n",
                "    return response\n",
                "\n",
                "\n",
                "def set_cors(bucket_name, cors):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "    response = Response()\n",
                "\n",
                "    exists, code = bucket_exists(bucket_name)\n",
                "    if not exists:\n",
                "        response.status_code = int(code)\n",
                "        return response\n",
                "\n",
                "    if not isinstance(cors, dict):\n",
                "        cors = xmltodict.parse(cors)\n",
                "\n",
                "    BUCKET_CORS[bucket_name] = cors\n",
                "    response.status_code = 200\n",
                "    return response\n",
                "\n",
                "\n",
                "def delete_cors(bucket_name):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "    response = Response()\n",
                "\n",
                "    exists, code = bucket_exists(bucket_name)\n",
                "    if not exists:\n",
                "        response.status_code = int(code)\n",
                "        return response\n",
                "\n",
                "    BUCKET_CORS.pop(bucket_name, {})\n",
                "    response.status_code = 200\n",
                "    return response\n",
                "\n",
                "\n",
                "def get_request_payment(bucket_name):\n",
                "    response = Response()\n",
                "\n",
                "    exists, code = bucket_exists(bucket_name)\n",
                "    if not exists:\n",
                "        response.status_code = int(code)\n",
                "        return response\n",
                "\n",
                "    content = {\n",
                "        \"RequestPaymentConfiguration\": {\n",
                "            \"@xmlns\": \"http://s3.amazonaws.com/doc/2006-03-01/\",\n",
                "            \"Payer\": s3_backend.buckets[bucket_name].payer,\n",
                "        }\n",
                "    }\n",
                "\n",
                "    body = xmltodict.unparse(content)\n",
                "    response.status_code = 200\n",
                "    response._content = body\n",
                "    return response\n",
                "\n",
                "\n",
                "def set_request_payment(bucket_name, payer):\n",
                "    response = Response()\n",
                "    exists, code = bucket_exists(bucket_name)\n",
                "    if not exists:\n",
                "        response.status_code = int(code)\n",
                "        return response\n",
                "\n",
                "    if not isinstance(payer, dict):\n",
                "        payer = xmltodict.parse(payer)\n",
                "        if payer[\"RequestPaymentConfiguration\"][\"Payer\"] not in [\n",
                "            \"Requester\",\n",
                "            \"BucketOwner\",\n",
                "        ]:\n",
                "            error = {\n",
                "                \"Error\": {\n",
                "                    \"Code\": \"MalformedXML\",\n",
                "                    \"Message\": \"The XML you provided was not well-formed \"\n",
                "                    + \"or did not validate against our published schema\",\n",
                "                    \"BucketName\": bucket_name,\n",
                "                    \"RequestId\": short_uid(),\n",
                "                    \"HostId\": short_uid(),\n",
                "                }\n",
                "            }\n",
                "            body = xmltodict.unparse(error)\n",
                "            response.status_code = 400\n",
                "            response._content = body\n",
                "            return response\n",
                "\n",
                "    s3_backend.buckets[bucket_name].payer = payer[\"RequestPaymentConfiguration\"][\"Payer\"]\n",
                "    response.status_code = 200\n",
                "    return response\n",
                "\n",
                "\n",
                "def convert_origins_into_list(allowed_origins):\n",
                "    if isinstance(allowed_origins, list):\n",
                "        return allowed_origins\n",
                "    return [allowed_origins]\n",
                "\n",
                "\n",
                "def get_origin_host(headers):\n",
                "    origin = headers.get(\"Origin\") or get_forwarded_for_host(headers)\n",
                "    return origin\n",
                "\n",
                "\n",
                "def append_cors_headers(bucket_name, request_method, request_headers, response):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "\n",
                "    # Checking CORS is allowed or not\n",
                "    cors = BUCKET_CORS.get(bucket_name)\n",
                "    if not cors:\n",
                "        return\n",
                "\n",
                "    # Cleaning headers\n",
                "    for header in CORS_HEADERS:\n",
                "        if header in response.headers:\n",
                "            del response.headers[header]\n",
                "\n",
                "    # Fetching origin of the request\n",
                "    origin = get_origin_host(request_headers)\n",
                "\n",
                "    rules = cors[\"CORSConfiguration\"][\"CORSRule\"]\n",
                "    if not isinstance(rules, list):\n",
                "        rules = [rules]\n",
                "\n",
                "    response.headers[\"Access-Control-Allow-Origin\"] = \"\"\n",
                "    response.headers[\"Access-Control-Allow-Methods\"] = \"\"\n",
                "    response.headers[\"Access-Control-Allow-Headers\"] = \"\"\n",
                "    response.headers[\"Access-Control-Expose-Headers\"] = \"\"\n",
                "\n",
                "    for rule in rules:\n",
                "        # add allow-origin header\n",
                "        allowed_methods = rule.get(\"AllowedMethod\", [])\n",
                "        if request_method in allowed_methods:\n",
                "            allowed_origins = rule.get(\"AllowedOrigin\", [])\n",
                "            # when only one origin is being set in cors then the allowed_origins is being\n",
                "            # reflected as a string here,so making it a list and then proceeding.\n",
                "            allowed_origins = convert_origins_into_list(allowed_origins)\n",
                "\n",
                "            for allowed in allowed_origins:\n",
                "                allowed = allowed or \"\"\n",
                "                if origin in allowed or re.match(allowed.replace(\"*\", \".*\"), origin):\n",
                "\n",
                "                    response.headers[\"Access-Control-Allow-Origin\"] = origin\n",
                "                    if \"AllowedMethod\" in rule:\n",
                "                        response.headers[\"Access-Control-Allow-Methods\"] = (\n",
                "                            \",\".join(allowed_methods)\n",
                "                            if isinstance(allowed_methods, list)\n",
                "                            else allowed_methods\n",
                "                        )\n",
                "                    if \"AllowedHeader\" in rule:\n",
                "                        allowed_headers = rule[\"AllowedHeader\"]\n",
                "                        response.headers[\"Access-Control-Allow-Headers\"] = (\n",
                "                            \",\".join(allowed_headers)\n",
                "                            if isinstance(allowed_headers, list)\n",
                "                            else allowed_headers\n",
                "                        )\n",
                "                    if \"ExposeHeader\" in rule:\n",
                "                        expose_headers = rule[\"ExposeHeader\"]\n",
                "                        response.headers[\"Access-Control-Expose-Headers\"] = (\n",
                "                            \",\".join(expose_headers)\n",
                "                            if isinstance(expose_headers, list)\n",
                "                            else expose_headers\n",
                "                        )\n",
                "                    if \"MaxAgeSeconds\" in rule:\n",
                "                        maxage_header = rule[\"MaxAgeSeconds\"]\n",
                "                        response.headers[\"Access-Control-Max-Age\"] = maxage_header\n",
                "                    break\n",
                "\n",
                "    if response.headers[\"Access-Control-Allow-Origin\"] != \"*\":\n",
                "        response.headers[\"Access-Control-Allow-Credentials\"] = \"true\"\n",
                "\n",
                "\n",
                "def append_aws_request_troubleshooting_headers(response):\n",
                "    gen_amz_request_id = \"\".join(random.choice(\"0123456789ABCDEF\") for i in range(16))\n",
                "    if response.headers.get(\"x-amz-request-id\") is None:\n",
                "        response.headers[\"x-amz-request-id\"] = gen_amz_request_id\n",
                "    if response.headers.get(\"x-amz-id-2\") is None:\n",
                "        response.headers[\"x-amz-id-2\"] = (\n",
                "            \"MzRISOwyjmnup\" + gen_amz_request_id + \"7/JypPGXLh0OVFGcJaaO3KW/hRAqKOpIEEp\"\n",
                "        )\n",
                "\n",
                "\n",
                "def add_accept_range_header(response):\n",
                "    if response.headers.get(\"accept-ranges\") is None:\n",
                "        response.headers[\"accept-ranges\"] = \"bytes\"\n",
                "\n",
                "\n",
                "def is_object_expired(path):\n",
                "    object_expiry = get_object_expiry(path)\n",
                "    if not object_expiry:\n",
                "        return False\n",
                "    if dateutil.parser.parse(object_expiry) > datetime.datetime.now(\n",
                "        timezone(dateutil.parser.parse(object_expiry).tzname())\n",
                "    ):\n",
                "        return False\n",
                "    return True\n",
                "\n",
                "\n",
                "def set_object_expiry(path, headers):\n",
                "    OBJECT_EXPIRY[path] = headers.get(\"expires\")\n",
                "\n",
                "\n",
                "def get_object_expiry(path):\n",
                "    return OBJECT_EXPIRY.get(path)\n",
                "\n",
                "\n",
                "def add_response_metadata_headers(response):\n",
                "    if response.headers.get(\"content-language\") is None:\n",
                "        response.headers[\"content-language\"] = \"en-US\"\n",
                "\n",
                "\n",
                "def append_last_modified_headers(response, content=None):\n",
                "    \"\"\"Add Last-Modified header with current time\n",
                "    (if the response content is an XML containing <LastModified>, add that instead)\"\"\"\n",
                "\n",
                "    time_format = \"%a, %d %b %Y %H:%M:%S GMT\"  # TimeFormat\n",
                "    try:\n",
                "        if content:\n",
                "            last_modified_str = re.findall(r\"<LastModified>([^<]*)</LastModified>\", content)\n",
                "            if last_modified_str:\n",
                "                last_modified_str = last_modified_str[0]\n",
                "                last_modified_time_format = dateutil.parser.parse(last_modified_str).strftime(\n",
                "                    time_format\n",
                "                )\n",
                "                response.headers[\"Last-Modified\"] = last_modified_time_format\n",
                "    except TypeError as err:\n",
                "        LOGGER.debug(\"No parsable content: %s\" % err)\n",
                "    except ValueError as err:\n",
                "        LOGGER.error(\"Failed to parse LastModified: %s\" % err)\n",
                "    except Exception as err:\n",
                "        LOGGER.error(\"Caught generic exception (parsing LastModified): %s\" % err)\n",
                "    # if cannot parse any LastModified, just continue\n",
                "\n",
                "    try:\n",
                "        if response.headers.get(\"Last-Modified\", \"\") == \"\":\n",
                "            response.headers[\"Last-Modified\"] = datetime.datetime.now().strftime(time_format)\n",
                "    except Exception as err:\n",
                "        LOGGER.error(\"Caught generic exception (setting LastModified header): %s\" % err)\n",
                "\n",
                "\n",
                "def append_list_objects_marker(method, path, data, response):\n",
                "    if \"marker=\" in path:\n",
                "        marker = \"\"\n",
                "        content = to_str(response.content)\n",
                "        if \"<ListBucketResult\" in content and \"<Marker>\" not in content:\n",
                "            parsed = urlparse.urlparse(path)\n",
                "            query_map = urlparse.parse_qs(parsed.query)\n",
                "            if query_map.get(\"marker\") and query_map.get(\"marker\")[0]:\n",
                "                marker = query_map.get(\"marker\")[0]\n",
                "            insert = \"<Marker>%s</Marker>\" % marker\n",
                "            response._content = content.replace(\n",
                "                \"</ListBucketResult>\", \"%s</ListBucketResult>\" % insert\n",
                "            )\n",
                "            response.headers.pop(\"Content-Length\", None)\n",
                "\n",
                "\n",
                "def append_metadata_headers(method, query_map, headers):\n",
                "    for key, value in query_map.items():\n",
                "        if key.lower().startswith(OBJECT_METADATA_KEY_PREFIX):\n",
                "            if headers.get(key) is None:\n",
                "                headers[key] = value[0]\n",
                "\n",
                "\n",
                "def fix_location_constraint(response):\n",
                "    \"\"\"Make sure we return a valid non-empty LocationConstraint, as this otherwise breaks Serverless.\"\"\"\n",
                "    try:\n",
                "        content = to_str(response.content or \"\") or \"\"\n",
                "    except Exception:\n",
                "        content = \"\"\n",
                "    if \"LocationConstraint\" in content:\n",
                "        pattern = r\"<LocationConstraint([^>]*)>\\s*</LocationConstraint>\"\n",
                "        replace = r\"<LocationConstraint\\1>%s</LocationConstraint>\" % aws_stack.get_region()\n",
                "        response._content = re.sub(pattern, replace, content)\n",
                "        remove_xml_preamble(response)\n",
                "\n",
                "\n",
                "def fix_range_content_type(bucket_name, path, headers, response):\n",
                "    # Fix content type for Range requests - https://github.com/localstack/localstack/issues/1259\n",
                "    if \"Range\" not in headers:\n",
                "        return\n",
                "\n",
                "    if response.status_code >= 400:\n",
                "        return\n",
                "\n",
                "    s3_client = aws_stack.connect_to_service(\"s3\")\n",
                "    path = urlparse.urlparse(urlparse.unquote(path)).path\n",
                "    key_name = extract_key_name(headers, path)\n",
                "    result = s3_client.head_object(Bucket=bucket_name, Key=key_name)\n",
                "    content_type = result[\"ContentType\"]\n",
                "    if response.headers.get(\"Content-Type\") == \"text/html; charset=utf-8\":\n",
                "        response.headers[\"Content-Type\"] = content_type\n",
                "\n",
                "\n",
                "def fix_delete_objects_response(bucket_name, method, parsed_path, data, headers, response):\n",
                "    # Deleting non-existing keys should not result in errors.\n",
                "    # Fixes https://github.com/localstack/localstack/issues/1893\n",
                "    if not (method == \"POST\" and parsed_path.query == \"delete\" and \"<Delete\" in to_str(data or \"\")):\n",
                "        return\n",
                "    content = to_str(response._content)\n",
                "    if \"<Error>\" not in content:\n",
                "        return\n",
                "    result = xmltodict.parse(content).get(\"DeleteResult\")\n",
                "    errors = result.get(\"Error\")\n",
                "    errors = errors if isinstance(errors, list) else [errors]\n",
                "    deleted = result.get(\"Deleted\")\n",
                "    if not isinstance(result.get(\"Deleted\"), list):\n",
                "        deleted = result[\"Deleted\"] = [deleted] if deleted else []\n",
                "    for entry in list(errors):\n",
                "        if set(entry.keys()) == set([\"Key\"]):\n",
                "            errors.remove(entry)\n",
                "            deleted.append(entry)\n",
                "    if not errors:\n",
                "        result.pop(\"Error\")\n",
                "    response._content = xmltodict.unparse({\"DeleteResult\": result})\n",
                "\n",
                "\n",
                "def fix_metadata_key_underscores(request_headers={}, response=None):\n",
                "    # fix for https://github.com/localstack/localstack/issues/1790\n",
                "    underscore_replacement = \"---\"\n",
                "    meta_header_prefix = \"x-amz-meta-\"\n",
                "    prefix_len = len(meta_header_prefix)\n",
                "    updated = False\n",
                "    for key in list(request_headers.keys()):\n",
                "        if key.lower().startswith(meta_header_prefix):\n",
                "            key_new = meta_header_prefix + key[prefix_len:].replace(\"_\", underscore_replacement)\n",
                "            if key != key_new:\n",
                "                request_headers[key_new] = request_headers.pop(key)\n",
                "                updated = True\n",
                "    if response is not None:\n",
                "        for key in list(response.headers.keys()):\n",
                "            if key.lower().startswith(meta_header_prefix):\n",
                "                key_new = meta_header_prefix + key[prefix_len:].replace(underscore_replacement, \"_\")\n",
                "                if key != key_new:\n",
                "                    response.headers[key_new] = response.headers.pop(key)\n",
                "    return updated\n",
                "\n",
                "\n",
                "def fix_creation_date(method, path, response):\n",
                "    if method != \"GET\" or path != \"/\":\n",
                "        return\n",
                "    response._content = re.sub(\n",
                "        r\"(\\.[0-9]+)(\\+00:00)?</CreationDate>\",\n",
                "        r\"\\1Z</CreationDate>\",\n",
                "        to_str(response._content),\n",
                "    )\n",
                "\n",
                "\n",
                "def fix_delimiter(data, headers, response):\n",
                "    if response.status_code == 200 and response._content:\n",
                "        c, xml_prefix, delimiter = response._content, \"<?xml\", \"<Delimiter><\"\n",
                "        pattern = \"[<]Delimiter[>]None[<]\"\n",
                "        if isinstance(c, bytes):\n",
                "            xml_prefix, delimiter = xml_prefix.encode(), delimiter.encode()\n",
                "            pattern = pattern.encode()\n",
                "        if c.startswith(xml_prefix):\n",
                "            response._content = re.compile(pattern).sub(delimiter, c)\n",
                "\n",
                "\n",
                "def convert_to_chunked_encoding(method, path, response):\n",
                "    if method != \"GET\" or path != \"/\":\n",
                "        return\n",
                "    if response.headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\":\n",
                "        return\n",
                "    response.headers[\"Transfer-Encoding\"] = \"chunked\"\n",
                "    response.headers.pop(\"Content-Encoding\", None)\n",
                "    response.headers.pop(\"Content-Length\", None)\n",
                "\n",
                "\n",
                "def unquote(s):\n",
                "    if (s[0], s[-1]) in (('\"', '\"'), (\"'\", \"'\")):\n",
                "        return s[1:-1]\n",
                "    return s\n",
                "\n",
                "\n",
                "def ret304_on_etag(data, headers, response):\n",
                "    etag = response.headers.get(\"ETag\")\n",
                "    if etag:\n",
                "        match = headers.get(\"If-None-Match\")\n",
                "        if match and unquote(match) == unquote(etag):\n",
                "            response.status_code = 304\n",
                "            response._content = \"\"\n",
                "\n",
                "\n",
                "def remove_xml_preamble(response):\n",
                "    \"\"\"Removes <?xml ... ?> from a response content\"\"\"\n",
                "    response._content = re.sub(r\"^<\\?[^\\?]+\\?>\", \"\", to_str(response._content))\n",
                "\n",
                "\n",
                "# --------------\n",
                "# HELPER METHODS\n",
                "#   for lifecycle/replication/...\n",
                "# --------------\n",
                "\n",
                "\n",
                "def get_lifecycle(bucket_name):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "    exists, code, body = is_bucket_available(bucket_name)\n",
                "    if not exists:\n",
                "        return xml_response(body, status_code=code)\n",
                "\n",
                "    lifecycle = BUCKET_LIFECYCLE.get(bucket_name)\n",
                "    status_code = 200\n",
                "\n",
                "    if not lifecycle:\n",
                "        lifecycle = {\n",
                "            \"Error\": {\n",
                "                \"Code\": \"NoSuchLifecycleConfiguration\",\n",
                "                \"Message\": \"The lifecycle configuration does not exist\",\n",
                "                \"BucketName\": bucket_name,\n",
                "            }\n",
                "        }\n",
                "        status_code = 404\n",
                "    body = xmltodict.unparse(lifecycle)\n",
                "    return xml_response(body, status_code=status_code)\n",
                "\n",
                "\n",
                "def get_replication(bucket_name):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "    exists, code, body = is_bucket_available(bucket_name)\n",
                "    if not exists:\n",
                "        return xml_response(body, status_code=code)\n",
                "\n",
                "    replication = BUCKET_REPLICATIONS.get(bucket_name)\n",
                "    status_code = 200\n",
                "    if not replication:\n",
                "        replication = {\n",
                "            \"Error\": {\n",
                "                \"Code\": \"ReplicationConfigurationNotFoundError\",\n",
                "                \"Message\": \"The replication configuration was not found\",\n",
                "                \"BucketName\": bucket_name,\n",
                "            }\n",
                "        }\n",
                "        status_code = 404\n",
                "    body = xmltodict.unparse(replication)\n",
                "    return xml_response(body, status_code=status_code)\n",
                "\n",
                "\n",
                "def set_lifecycle(bucket_name, lifecycle):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "    exists, code, body = is_bucket_available(bucket_name)\n",
                "    if not exists:\n",
                "        return xml_response(body, status_code=code)\n",
                "\n",
                "    if isinstance(to_str(lifecycle), six.string_types):\n",
                "        lifecycle = xmltodict.parse(lifecycle)\n",
                "    BUCKET_LIFECYCLE[bucket_name] = lifecycle\n",
                "    return 200\n",
                "\n",
                "\n",
                "def delete_lifecycle(bucket_name):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "    exists, code, body = is_bucket_available(bucket_name)\n",
                "    if not exists:\n",
                "        return xml_response(body, status_code=code)\n",
                "\n",
                "    if BUCKET_LIFECYCLE.get(bucket_name):\n",
                "        BUCKET_LIFECYCLE.pop(bucket_name)\n",
                "\n",
                "\n",
                "def set_replication(bucket_name, replication):\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "    exists, code, body = is_bucket_available(bucket_name)\n",
                "    if not exists:\n",
                "        return xml_response(body, status_code=code)\n",
                "\n",
                "    if isinstance(to_str(replication), six.string_types):\n",
                "        replication = xmltodict.parse(replication)\n",
                "    BUCKET_REPLICATIONS[bucket_name] = replication\n",
                "    return 200\n",
                "\n",
                "\n",
                "# -------------\n",
                "# UTIL METHODS\n",
                "# -------------\n",
                "\n",
                "\n",
                "def is_bucket_available(bucket_name):\n",
                "    body = {\"Code\": \"200\"}\n",
                "    exists, code = bucket_exists(bucket_name)\n",
                "    if not exists:\n",
                "        body = {\n",
                "            \"Error\": {\n",
                "                \"Code\": code,\n",
                "                \"Message\": \"The bucket does not exist\",\n",
                "                \"BucketName\": bucket_name,\n",
                "            }\n",
                "        }\n",
                "        return exists, code, body\n",
                "\n",
                "    return True, 200, body\n",
                "\n",
                "\n",
                "def bucket_exists(bucket_name):\n",
                "    \"\"\"Tests for the existence of the specified bucket. Returns the error code\n",
                "    if the bucket does not exist (200 if the bucket does exist).\n",
                "    \"\"\"\n",
                "    bucket_name = normalize_bucket_name(bucket_name)\n",
                "\n",
                "    s3_client = aws_stack.connect_to_service(\"s3\")\n",
                "    try:\n",
                "        s3_client.head_bucket(Bucket=bucket_name)\n",
                "    except ClientError as err:\n",
                "        error_code = err.response.get(\"Error\").get(\"Code\")\n",
                "        return False, error_code\n",
                "\n",
                "    return True, 200\n",
                "\n",
                "\n",
                "def check_content_md5(data, headers):\n",
                "    actual = md5(data)\n",
                "    try:\n",
                "        md5_header = headers[\"Content-MD5\"]\n",
                "        if not is_base64(md5_header):\n",
                "            raise Exception('Content-MD5 header is not in Base64 format: \"%s\"' % md5_header)\n",
                "        expected = to_str(codecs.encode(base64.b64decode(md5_header), \"hex\"))\n",
                "    except Exception:\n",
                "        return error_response(\n",
                "            \"The Content-MD5 you specified is not valid.\",\n",
                "            \"InvalidDigest\",\n",
                "            status_code=400,\n",
                "        )\n",
                "    if actual != expected:\n",
                "        return error_response(\n",
                "            \"The Content-MD5 you specified did not match what we received.\",\n",
                "            \"BadDigest\",\n",
                "            status_code=400,\n",
                "        )\n",
                "\n",
                "\n",
                "def error_response(message, code, status_code=400):\n",
                "    result = {\"Error\": {\"Code\": code, \"Message\": message}}\n",
                "    content = xmltodict.unparse(result)\n",
                "    return xml_response(content, status_code=status_code)\n",
                "\n",
                "\n",
                "def xml_response(content, status_code=200):\n",
                "    headers = {\"Content-Type\": \"application/xml\"}\n",
                "    return requests_response(content, status_code=status_code, headers=headers)\n",
                "\n",
                "\n",
                "def no_such_key_error(resource, requestId=None, status_code=400):\n",
                "    result = {\n",
                "        \"Error\": {\n",
                "            \"Code\": \"NoSuchKey\",\n",
                "            \"Message\": \"The resource you requested does not exist\",\n",
                "            \"Resource\": resource,\n",
                "            \"RequestId\": requestId,\n",
                "        }\n",
                "    }\n",
                "    content = xmltodict.unparse(result)\n",
                "    return xml_response(content, status_code=status_code)\n",
                "\n",
                "\n",
                "def no_such_bucket(bucket_name, requestId=None, status_code=404):\n",
                "    # TODO: fix the response to match AWS bucket response when the webconfig is not set and bucket not exists\n",
                "    result = {\n",
                "        \"Error\": {\n",
                "            \"Code\": \"NoSuchBucket\",\n",
                "            \"Message\": \"The specified bucket does not exist\",\n",
                "            \"BucketName\": bucket_name,\n",
                "            \"RequestId\": requestId,\n",
                "            \"HostId\": short_uid(),\n",
                "        }\n",
                "    }\n",
                "    content = xmltodict.unparse(result)\n",
                "    return xml_response(content, status_code=status_code)\n",
                "\n",
                "\n",
                "def token_expired_error(resource, requestId=None, status_code=400):\n",
                "    result = {\n",
                "        \"Error\": {\n",
                "            \"Code\": \"ExpiredToken\",\n",
                "            \"Message\": \"The provided token has expired.\",\n",
                "            \"Resource\": resource,\n",
                "            \"RequestId\": requestId,\n",
                "        }\n",
                "    }\n",
                "    content = xmltodict.unparse(result)\n",
                "    return xml_response(content, status_code=status_code)\n",
                "\n",
                "\n",
                "def expand_redirect_url(starting_url, key, bucket):\n",
                "    \"\"\"Add key and bucket parameters to starting URL query string.\"\"\"\n",
                "    parsed = urlparse.urlparse(starting_url)\n",
                "    query = collections.OrderedDict(urlparse.parse_qsl(parsed.query))\n",
                "    query.update([(\"key\", key), (\"bucket\", bucket)])\n",
                "\n",
                "    redirect_url = urlparse.urlunparse(\n",
                "        (\n",
                "            parsed.scheme,\n",
                "            parsed.netloc,\n",
                "            parsed.path,\n",
                "            parsed.params,\n",
                "            urlparse.urlencode(query),\n",
                "            None,\n",
                "        )\n",
                "    )\n",
                "\n",
                "    return redirect_url\n",
                "\n",
                "\n",
                "def is_bucket_specified_in_domain_name(path, headers):\n",
                "    host = headers.get(\"host\", \"\")\n",
                "    return re.match(r\".*s3(\\-website)?\\.([^\\.]+\\.)?amazonaws.com\", host)\n",
                "\n",
                "\n",
                "def is_object_specific_request(path, headers):\n",
                "    \"\"\"Return whether the given request is specific to a certain S3 object.\n",
                "    Note: the bucket name is usually specified as a path parameter,\n",
                "    but may also be part of the domain name!\"\"\"\n",
                "    bucket_in_domain = is_bucket_specified_in_domain_name(path, headers)\n",
                "    parts = len(path.split(\"/\"))\n",
                "    return parts > (1 if bucket_in_domain else 2)\n",
                "\n",
                "\n",
                "def empty_response():\n",
                "    response = Response()\n",
                "    response.status_code = 200\n",
                "    response._content = \"\"\n",
                "    return response\n",
                "\n",
                "\n",
                "def handle_notification_request(bucket, method, data):\n",
                "    if method == \"GET\":\n",
                "        return handle_get_bucket_notification(bucket)\n",
                "    if method == \"PUT\":\n",
                "        return handle_put_bucket_notification(bucket, data)\n",
                "\n",
                "    return empty_response()\n",
                "\n",
                "\n",
                "def handle_get_bucket_notification(bucket):\n",
                "    response = Response()\n",
                "    response.status_code = 200\n",
                "    response._content = \"\"\n",
                "\n",
                "    # TODO check if bucket exists\n",
                "    result = '<NotificationConfiguration xmlns=\"%s\">' % XMLNS_S3\n",
                "    if bucket in S3_NOTIFICATIONS:\n",
                "        notifs = S3_NOTIFICATIONS[bucket]\n",
                "        for notif in notifs:\n",
                "            for dest in NOTIFICATION_DESTINATION_TYPES:\n",
                "                if dest in notif:\n",
                "                    dest_dict = {\n",
                "                        \"%sConfiguration\"\n",
                "                        % dest: {\n",
                "                            \"Id\": notif[\"Id\"],\n",
                "                            dest: notif[dest],\n",
                "                            \"Event\": notif[\"Event\"],\n",
                "                            \"Filter\": notif[\"Filter\"],\n",
                "                        }\n",
                "                    }\n",
                "                    result += xmltodict.unparse(dest_dict, full_document=False)\n",
                "    result += \"</NotificationConfiguration>\"\n",
                "    response._content = result\n",
                "    return response\n",
                "\n",
                "\n",
                "def _validate_filter_rules(filter_doc):\n",
                "    rules = filter_doc.get(\"FilterRule\")\n",
                "    if not rules:\n",
                "        return\n",
                "\n",
                "    for rule in rules:\n",
                "        name = rule.get(\"Name\", \"\")\n",
                "        if name.lower() not in [\"suffix\", \"prefix\"]:\n",
                "            raise InvalidFilterRuleName(name)\n",
                "\n",
                "        # TODO: check what other rules there are\n",
                "\n",
                "\n",
                "def _sanitize_notification_filter_rules(filter_doc):\n",
                "    rules = filter_doc.get(\"FilterRule\")\n",
                "    if not rules:\n",
                "        return\n",
                "\n",
                "    for rule in rules:\n",
                "        name = rule.get(\"Name\", \"\")\n",
                "        if name.lower() not in [\"suffix\", \"prefix\"]:\n",
                "            raise InvalidFilterRuleName(name)\n",
                "\n",
                "        rule[\"Name\"] = name.title()\n",
                "\n",
                "\n",
                "def handle_put_bucket_notification(bucket, data):\n",
                "    parsed = xmltodict.parse(data)\n",
                "    notif_config = parsed.get(\"NotificationConfiguration\")\n",
                "\n",
                "    notifications = []\n",
                "\n",
                "    for dest in NOTIFICATION_DESTINATION_TYPES:\n",
                "        config = notif_config.get(\"%sConfiguration\" % dest)\n",
                "        configs = config if isinstance(config, list) else [config] if config else []\n",
                "        for config in configs:\n",
                "            events = config.get(\"Event\")\n",
                "            if isinstance(events, six.string_types):\n",
                "                events = [events]\n",
                "            event_filter = config.get(\"Filter\", {})\n",
                "            # make sure FilterRule is an array\n",
                "            s3_filter = _get_s3_filter(event_filter)\n",
                "\n",
                "            if s3_filter and not isinstance(s3_filter.get(\"FilterRule\", []), list):\n",
                "                s3_filter[\"FilterRule\"] = [s3_filter[\"FilterRule\"]]\n",
                "\n",
                "            # make sure FilterRules are valid and sanitize if necessary\n",
                "            _sanitize_notification_filter_rules(s3_filter)\n",
                "\n",
                "            # create final details dict\n",
                "            notification_details = {\n",
                "                \"Id\": config.get(\"Id\", str(uuid.uuid4())),\n",
                "                \"Event\": events,\n",
                "                dest: config.get(dest),\n",
                "                \"Filter\": event_filter,\n",
                "            }\n",
                "\n",
                "            notifications.append(clone(notification_details))\n",
                "\n",
                "    S3_NOTIFICATIONS[bucket] = notifications\n",
                "\n",
                "    return empty_response()\n",
                "\n",
                "\n",
                "def remove_bucket_notification(bucket):\n",
                "    if bucket in S3_NOTIFICATIONS:\n",
                "        del S3_NOTIFICATIONS[bucket]\n",
                "\n",
                "\n",
                "class ProxyListenerS3(PersistingProxyListener):\n",
                "    def api_name(self):\n",
                "        return \"s3\"\n",
                "\n",
                "    @staticmethod\n",
                "    def is_s3_copy_request(headers, path):\n",
                "        return \"x-amz-copy-source\" in headers or \"x-amz-copy-source\" in path\n",
                "\n",
                "    @staticmethod\n",
                "    def is_create_multipart_request(query):\n",
                "        return query.startswith(\"uploads\")\n",
                "\n",
                "    @staticmethod\n",
                "    def is_multipart_upload(query):\n",
                "        return query.startswith(\"uploadId\")\n",
                "\n",
                "    @staticmethod\n",
                "    def get_201_response(key, bucket_name):\n",
                "        return \"\"\"\n",
                "                <PostResponse>\n",
                "                    <Location>{protocol}://{host}/{encoded_key}</Location>\n",
                "                    <Bucket>{bucket}</Bucket>\n",
                "                    <Key>{key}</Key>\n",
                "                    <ETag>{etag}</ETag>\n",
                "                </PostResponse>\n",
                "                \"\"\".format(\n",
                "            protocol=get_service_protocol(),\n",
                "            host=config.HOSTNAME_EXTERNAL,\n",
                "            encoded_key=urlparse.quote(key, safe=\"\"),\n",
                "            key=key,\n",
                "            bucket=bucket_name,\n",
                "            etag=\"d41d8cd98f00b204e9800998ecf8427f\",\n",
                "        )\n",
                "\n",
                "    @staticmethod\n",
                "    def _update_location(content, bucket_name):\n",
                "        bucket_name = normalize_bucket_name(bucket_name)\n",
                "\n",
                "        host = config.HOSTNAME_EXTERNAL\n",
                "        if \":\" not in host:\n",
                "            host = \"%s:%s\" % (host, config.PORT_S3)\n",
                "        return re.sub(\n",
                "            r\"<Location>\\s*([a-zA-Z0-9\\-]+)://[^/]+/([^<]+)\\s*</Location>\",\n",
                "            r\"<Location>%s://%s/%s/\\2</Location>\" % (get_service_protocol(), host, bucket_name),\n",
                "            content,\n",
                "            flags=re.MULTILINE,\n",
                "        )\n",
                "\n",
                "    @staticmethod\n",
                "    def is_query_allowable(method, query):\n",
                "        # Generally if there is a query (some/path/with?query) we don't want to send notifications\n",
                "        if not query:\n",
                "            return True\n",
                "        # Except we do want to notify on multipart and presigned url upload completion\n",
                "        contains_cred = \"X-Amz-Credential\" in query and \"X-Amz-Signature\" in query\n",
                "        contains_key = \"AWSAccessKeyId\" in query and \"Signature\" in query\n",
                "        # nodejs sdk putObjectCommand is adding x-id=putobject in the query\n",
                "        allowed_query = \"x-id=\" in query.lower()\n",
                "        if (\n",
                "            (method == \"POST\" and query.startswith(\"uploadId\"))\n",
                "            or contains_cred\n",
                "            or contains_key\n",
                "            or allowed_query\n",
                "        ):\n",
                "            return True\n",
                "\n",
                "    @staticmethod\n",
                "    def parse_policy_expiration_date(expiration_string):\n",
                "        try:\n",
                "            dt = datetime.datetime.strptime(expiration_string, POLICY_EXPIRATION_FORMAT1)\n",
                "        except Exception:\n",
                "            dt = datetime.datetime.strptime(expiration_string, POLICY_EXPIRATION_FORMAT2)\n",
                "\n",
                "        # both date formats assume a UTC timezone ('Z' suffix), but it's not parsed as tzinfo into the datetime object\n",
                "        dt = dt.replace(tzinfo=datetime.timezone.utc)\n",
                "        return dt\n",
                "\n",
                "    def forward_request(self, method, path, data, headers):\n",
                "        # Create list of query parameteres from the url\n",
                "        parsed = urlparse.urlparse(\"{}{}\".format(config.get_edge_url(), path))\n",
                "        query_params = parse_qs(parsed.query)\n",
                "        path_orig = path\n",
                "        path = path.replace(\n",
                "            \"#\", \"%23\"\n",
                "        )  # support key names containing hashes (e.g., required by Amplify)\n",
                "        # extracting bucket name from the request\n",
                "        parsed_path = urlparse.urlparse(path)\n",
                "        bucket_name = extract_bucket_name(headers, parsed_path.path)\n",
                "\n",
                "        if method == \"PUT\" and bucket_name and not re.match(BUCKET_NAME_REGEX, bucket_name):\n",
                "            if len(parsed_path.path) <= 1:\n",
                "                return error_response(\n",
                "                    \"Unable to extract valid bucket name. Please ensure that your AWS SDK is \"\n",
                "                    + \"configured to use path style addressing, or send a valid \"\n",
                "                    + '<Bucket>.s3.localhost.localstack.cloud \"Host\" header',\n",
                "                    \"InvalidBucketName\",\n",
                "                    status_code=400,\n",
                "                )\n",
                "\n",
                "            return error_response(\n",
                "                \"The specified bucket is not valid.\",\n",
                "                \"InvalidBucketName\",\n",
                "                status_code=400,\n",
                "            )\n",
                "\n",
                "        # Detecting pre-sign url and checking signature\n",
                "        if any([p in query_params for p in SIGNATURE_V2_PARAMS]) or any(\n",
                "            [p in query_params for p in SIGNATURE_V4_PARAMS]\n",
                "        ):\n",
                "            response = authenticate_presign_url(\n",
                "                method=method, path=path, data=data, headers=headers\n",
                "            )\n",
                "            if response is not None:\n",
                "                return response\n",
                "\n",
                "        # handling s3 website hosting requests\n",
                "        if is_static_website(headers) and method == \"GET\":\n",
                "            return serve_static_website(headers=headers, path=path, bucket_name=bucket_name)\n",
                "\n",
                "        # check content md5 hash integrity if not a copy request or multipart initialization\n",
                "        if (\n",
                "            \"Content-MD5\" in headers\n",
                "            and not self.is_s3_copy_request(headers, path)\n",
                "            and not self.is_create_multipart_request(parsed_path.query)\n",
                "        ):\n",
                "            response = check_content_md5(data, headers)\n",
                "            if response is not None:\n",
                "                return response\n",
                "\n",
                "        modified_data = None\n",
                "\n",
                "        # TODO: For some reason, moto doesn't allow us to put a location constraint on us-east-1\n",
                "        to_find1 = to_bytes(\"<LocationConstraint>us-east-1</LocationConstraint>\")\n",
                "        to_find2 = to_bytes(\"<CreateBucketConfiguration\")\n",
                "        if data and data.startswith(to_bytes(\"<\")) and to_find1 in data and to_find2 in data:\n",
                "            # Note: with the latest version, <CreateBucketConfiguration> must either\n",
                "            # contain a valid <LocationConstraint>, or not be present at all in the body.\n",
                "            modified_data = b\"\"\n",
                "\n",
                "        # POST requests to S3 may include a \"${filename}\" placeholder in the\n",
                "        # key, which should be replaced with an actual file name before storing.\n",
                "        if method == \"POST\":\n",
                "            original_data = not_none_or(modified_data, data)\n",
                "            expanded_data = multipart_content.expand_multipart_filename(original_data, headers)\n",
                "            if expanded_data is not original_data:\n",
                "                modified_data = expanded_data\n",
                "\n",
                "        # If no content-type is provided, 'binary/octet-stream' should be used\n",
                "        # src: https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html\n",
                "        if method == \"PUT\" and not headers.get(\"content-type\"):\n",
                "            headers[\"content-type\"] = \"binary/octet-stream\"\n",
                "\n",
                "        # parse query params\n",
                "        query = parsed_path.query\n",
                "        path = parsed_path.path\n",
                "        query_map = urlparse.parse_qs(query, keep_blank_values=True)\n",
                "\n",
                "        # remap metadata query params (not supported in moto) to request headers\n",
                "        append_metadata_headers(method, query_map, headers)\n",
                "\n",
                "        # apply fixes\n",
                "        headers_changed = fix_metadata_key_underscores(request_headers=headers)\n",
                "\n",
                "        if query == \"notification\" or \"notification\" in query_map:\n",
                "            # handle and return response for ?notification request\n",
                "            response = handle_notification_request(bucket_name, method, data)\n",
                "            return response\n",
                "\n",
                "        # if the Expires key in the url is already expired then return error\n",
                "        if method == \"GET\" and \"Expires\" in query_map:\n",
                "            ts = datetime.datetime.fromtimestamp(\n",
                "                int(query_map.get(\"Expires\")[0]), tz=datetime.timezone.utc\n",
                "            )\n",
                "            if is_expired(ts):\n",
                "                return token_expired_error(path, headers.get(\"x-amz-request-id\"), 400)\n",
                "\n",
                "        # If multipart POST with policy in the params, return error if the policy has expired\n",
                "        if method == \"POST\":\n",
                "            policy_key, policy_value = multipart_content.find_multipart_key_value(\n",
                "                data, headers, \"policy\"\n",
                "            )\n",
                "            if policy_key and policy_value:\n",
                "                policy = json.loads(base64.b64decode(policy_value).decode(\"utf-8\"))\n",
                "                expiration_string = policy.get(\"expiration\", None)  # Example: 2020-06-05T13:37:12Z\n",
                "                if expiration_string:\n",
                "                    expiration_datetime = self.parse_policy_expiration_date(expiration_string)\n",
                "                    if is_expired(expiration_datetime):\n",
                "                        return token_expired_error(path, headers.get(\"x-amz-request-id\"), 400)\n",
                "\n",
                "        if query == \"cors\" or \"cors\" in query_map:\n",
                "            if method == \"GET\":\n",
                "                return get_cors(bucket_name)\n",
                "            if method == \"PUT\":\n",
                "                return set_cors(bucket_name, data)\n",
                "            if method == \"DELETE\":\n",
                "                return delete_cors(bucket_name)\n",
                "\n",
                "        if query == \"requestPayment\" or \"requestPayment\" in query_map:\n",
                "            if method == \"GET\":\n",
                "                return get_request_payment(bucket_name)\n",
                "            if method == \"PUT\":\n",
                "                return set_request_payment(bucket_name, data)\n",
                "\n",
                "        if query == \"lifecycle\" or \"lifecycle\" in query_map:\n",
                "            if method == \"GET\":\n",
                "                return get_lifecycle(bucket_name)\n",
                "            if method == \"PUT\":\n",
                "                return set_lifecycle(bucket_name, data)\n",
                "            if method == \"DELETE\":\n",
                "                delete_lifecycle(bucket_name)\n",
                "\n",
                "        if query == \"replication\" or \"replication\" in query_map:\n",
                "            if method == \"GET\":\n",
                "                return get_replication(bucket_name)\n",
                "            if method == \"PUT\":\n",
                "                return set_replication(bucket_name, data)\n",
                "\n",
                "        if method == \"DELETE\" and validate_bucket_name(bucket_name):\n",
                "            delete_lifecycle(bucket_name)\n",
                "\n",
                "        path_orig_escaped = path_orig.replace(\"#\", \"%23\")\n",
                "        if modified_data is not None or headers_changed or path_orig != path_orig_escaped:\n",
                "            data_to_return = not_none_or(modified_data, data)\n",
                "            if modified_data is not None:\n",
                "                headers[\"Content-Length\"] = str(len(data_to_return or \"\"))\n",
                "            return Request(\n",
                "                url=path_orig_escaped,\n",
                "                data=data_to_return,\n",
                "                headers=headers,\n",
                "                method=method,\n",
                "            )\n",
                "        return True\n",
                "\n",
                "    def return_response(self, method, path, data, headers, response, request_handler=None):\n",
                "        path = to_str(path)\n",
                "        method = to_str(method)\n",
                "        path = path.replace(\"#\", \"%23\")\n",
                "\n",
                "        # persist this API call to disk\n",
                "        super(ProxyListenerS3, self).return_response(\n",
                "            method, path, data, headers, response, request_handler\n",
                "        )\n",
                "\n",
                "        bucket_name = extract_bucket_name(headers, path)\n",
                "\n",
                "        # POST requests to S3 may include a success_action_redirect or\n",
                "        # success_action_status field, which should be used to redirect a\n",
                "        # client to a new location.\n",
                "        key = None\n",
                "        if method == \"POST\":\n",
                "            key, redirect_url = multipart_content.find_multipart_key_value(data, headers)\n",
                "            if key and redirect_url:\n",
                "                response.status_code = 303\n",
                "                response.headers[\"Location\"] = expand_redirect_url(redirect_url, key, bucket_name)\n",
                "                LOGGER.debug(\n",
                "                    \"S3 POST {} to {}\".format(response.status_code, response.headers[\"Location\"])\n",
                "                )\n",
                "\n",
                "            expanded_data = multipart_content.expand_multipart_filename(data, headers)\n",
                "            key, status_code = multipart_content.find_multipart_key_value(\n",
                "                expanded_data, headers, \"success_action_status\"\n",
                "            )\n",
                "\n",
                "            if response.status_code == 201 and key:\n",
                "                response._content = self.get_201_response(key, bucket_name)\n",
                "                response.headers[\"Content-Length\"] = str(len(response._content or \"\"))\n",
                "                response.headers[\"Content-Type\"] = \"application/xml; charset=utf-8\"\n",
                "                return response\n",
                "        if response.status_code == 416:\n",
                "            if method == \"GET\":\n",
                "                return error_response(\n",
                "                    \"The requested range cannot be satisfied.\", \"InvalidRange\", 416\n",
                "                )\n",
                "            elif method == \"HEAD\":\n",
                "                response.status_code = 200\n",
                "                return response\n",
                "\n",
                "        parsed = urlparse.urlparse(path)\n",
                "        bucket_name_in_host = uses_host_addressing(headers)\n",
                "        should_send_notifications = all(\n",
                "            [\n",
                "                method in (\"PUT\", \"POST\", \"DELETE\"),\n",
                "                \"/\" in path[1:] or bucket_name_in_host or key,\n",
                "                # check if this is an actual put object request, because it could also be\n",
                "                # a put bucket request with a path like this: /bucket_name/\n",
                "                bucket_name_in_host\n",
                "                or key\n",
                "                or (len(path[1:].split(\"/\")) > 1 and len(path[1:].split(\"/\")[1]) > 0),\n",
                "                self.is_query_allowable(method, parsed.query),\n",
                "            ]\n",
                "        )\n",
                "\n",
                "        # get subscribers and send bucket notifications\n",
                "        if should_send_notifications:\n",
                "            # if we already have a good key, use it, otherwise examine the path\n",
                "            if key:\n",
                "                object_path = \"/\" + key\n",
                "            elif bucket_name_in_host:\n",
                "                object_path = parsed.path\n",
                "            else:\n",
                "                parts = parsed.path[1:].split(\"/\", 1)\n",
                "                object_path = parts[1] if parts[1][0] == \"/\" else \"/%s\" % parts[1]\n",
                "            version_id = response.headers.get(\"x-amz-version-id\", None)\n",
                "\n",
                "            send_notifications(method, bucket_name, object_path, version_id, headers)\n",
                "\n",
                "        # publish event for creation/deletion of buckets:\n",
                "        if method in (\"PUT\", \"DELETE\") and (\n",
                "            \"/\" not in path[1:] or len(path[1:].split(\"/\")[1]) <= 0\n",
                "        ):\n",
                "            event_type = (\n",
                "                event_publisher.EVENT_S3_CREATE_BUCKET\n",
                "                if method == \"PUT\"\n",
                "                else event_publisher.EVENT_S3_DELETE_BUCKET\n",
                "            )\n",
                "            event_publisher.fire_event(\n",
                "                event_type, payload={\"n\": event_publisher.get_hash(bucket_name)}\n",
                "            )\n",
                "\n",
                "        # fix an upstream issue in moto S3 (see https://github.com/localstack/localstack/issues/382)\n",
                "        if method == \"PUT\":\n",
                "            if parsed.query == \"policy\":\n",
                "                response._content = \"\"\n",
                "                response.status_code = 204\n",
                "                return response\n",
                "            # when creating s3 bucket using aws s3api the return header contains 'Location' param\n",
                "            if key is None:\n",
                "                # if the bucket is created in 'us-east-1' the location header contains bucket as path\n",
                "                # else the the header contains bucket url\n",
                "                if aws_stack.get_region() == \"us-east-1\":\n",
                "                    response.headers[\"Location\"] = \"/{}\".format(bucket_name)\n",
                "                else:\n",
                "                    # Note: we need to set the correct protocol here\n",
                "                    protocol = (\n",
                "                        headers.get(constants.HEADER_LOCALSTACK_EDGE_URL, \"\").split(\"://\")[0]\n",
                "                        or \"http\"\n",
                "                    )\n",
                "                    response.headers[\"Location\"] = \"{}://{}.{}:{}/\".format(\n",
                "                        protocol,\n",
                "                        bucket_name,\n",
                "                        constants.S3_VIRTUAL_HOSTNAME,\n",
                "                        config.EDGE_PORT,\n",
                "                    )\n",
                "\n",
                "        if response is not None:\n",
                "            reset_content_length = False\n",
                "            # append CORS headers and other annotations/patches to response\n",
                "            append_cors_headers(\n",
                "                bucket_name,\n",
                "                request_method=method,\n",
                "                request_headers=headers,\n",
                "                response=response,\n",
                "            )\n",
                "            append_last_modified_headers(response=response)\n",
                "            append_list_objects_marker(method, path, data, response)\n",
                "            fix_location_constraint(response)\n",
                "            fix_range_content_type(bucket_name, path, headers, response)\n",
                "            fix_delete_objects_response(bucket_name, method, parsed, data, headers, response)\n",
                "            fix_metadata_key_underscores(response=response)\n",
                "            fix_creation_date(method, path, response=response)\n",
                "            ret304_on_etag(data, headers, response)\n",
                "            append_aws_request_troubleshooting_headers(response)\n",
                "            fix_delimiter(data, headers, response)\n",
                "\n",
                "            if method == \"PUT\":\n",
                "                set_object_expiry(path, headers)\n",
                "\n",
                "            # Remove body from PUT response on presigned URL\n",
                "            # https://github.com/localstack/localstack/issues/1317\n",
                "            if (\n",
                "                method == \"PUT\"\n",
                "                and int(response.status_code) < 400\n",
                "                and (\n",
                "                    \"X-Amz-Security-Token=\" in path\n",
                "                    or \"X-Amz-Credential=\" in path\n",
                "                    or \"AWSAccessKeyId=\" in path\n",
                "                )\n",
                "            ):\n",
                "                response._content = \"\"\n",
                "                reset_content_length = True\n",
                "\n",
                "            response_content_str = None\n",
                "            try:\n",
                "                response_content_str = to_str(response._content)\n",
                "            except Exception:\n",
                "                pass\n",
                "\n",
                "            # Honor response header overrides\n",
                "            # https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html\n",
                "            if method == \"GET\":\n",
                "                add_accept_range_header(response)\n",
                "                add_response_metadata_headers(response)\n",
                "                if is_object_expired(path):\n",
                "                    return no_such_key_error(path, headers.get(\"x-amz-request-id\"), 400)\n",
                "                # AWS C# SDK uses get bucket acl to check the existence of the bucket\n",
                "                # If not exists, raises a NoSuchBucket Error\n",
                "                if bucket_name and \"/?acl\" in path:\n",
                "                    exists, code, body = is_bucket_available(bucket_name)\n",
                "                    if not exists:\n",
                "                        return no_such_bucket(bucket_name, headers.get(\"x-amz-request-id\"), 404)\n",
                "                query_map = urlparse.parse_qs(parsed.query, keep_blank_values=True)\n",
                "                for param_name, header_name in ALLOWED_HEADER_OVERRIDES.items():\n",
                "                    if param_name in query_map:\n",
                "                        response.headers[header_name] = query_map[param_name][0]\n",
                "\n",
                "            if response_content_str and response_content_str.startswith(\"<\"):\n",
                "                is_bytes = isinstance(response._content, six.binary_type)\n",
                "                response._content = response_content_str\n",
                "\n",
                "                append_last_modified_headers(response=response, content=response_content_str)\n",
                "\n",
                "                # We need to un-pretty-print the XML, otherwise we run into this issue with Spark:\n",
                "                # https://github.com/jserver/mock-s3/pull/9/files\n",
                "                # https://github.com/localstack/localstack/issues/183\n",
                "                # Note: yet, we need to make sure we have a newline after the first line: <?xml ...>\\n\n",
                "                # Note: make sure to return XML docs verbatim: https://github.com/localstack/localstack/issues/1037\n",
                "                if method != \"GET\" or not is_object_specific_request(path, headers):\n",
                "                    response._content = re.sub(\n",
                "                        r\"([^\\?])>\\n\\s*<\",\n",
                "                        r\"\\1><\",\n",
                "                        response_content_str,\n",
                "                        flags=re.MULTILINE,\n",
                "                    )\n",
                "\n",
                "                # update Location information in response payload\n",
                "                response._content = self._update_location(response._content, bucket_name)\n",
                "\n",
                "                # convert back to bytes\n",
                "                if is_bytes:\n",
                "                    response._content = to_bytes(response._content)\n",
                "\n",
                "                # fix content-type: https://github.com/localstack/localstack/issues/618\n",
                "                #                   https://github.com/localstack/localstack/issues/549\n",
                "                #                   https://github.com/localstack/localstack/issues/854\n"
            ],
            {
                "type": "replace",
                "before": [
                    "                if \"text/html\" in response.headers.get(\n",
                    "                    \"Content-Type\", \"\"\n",
                    "                ) and not response_content_str.lower().startswith(\"<!doctype html\"):\n"
                ],
                "after": [
                    "\n",
                    "                if is_invalid_html_response(response.headers, response_content_str):\n"
                ],
                "parent_version_range": {
                    "start": 1518,
                    "end": 1521
                },
                "child_version_range": {
                    "start": 1522,
                    "end": 1524
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if response is not None:",
                        "start_line": 1428,
                        "end_line": 1533
                    },
                    {
                        "type": "if_statement",
                        "statement": "if response_content_str and response_content_str.startswith(\"<\"):",
                        "start_line": 1489,
                        "end_line": 1523
                    },
                    {
                        "type": "if_statement",
                        "statement": "if \"text/html\" in response.headers.get(\n                    \"Content-Type\", \"\"\n                ) and not response_content_str.lower().startswith(\"<!doctype html\"):",
                        "start_line": 1518,
                        "end_line": 1521
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "ProxyListenerS3",
                        "signature": "class ProxyListenerS3(PersistingProxyListener):",
                        "at_line": 1084
                    },
                    {
                        "type": "function",
                        "name": "return_response",
                        "signature": "def return_response(self, method, path, data, headers, response, request_handler=None):",
                        "at_line": 1317
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: localstack/services/s3/s3_listener.py\nCode:\n             class ProxyListenerS3(PersistingProxyListener):\n                 ...\n                 def return_response(self, method, path, data, headers, response, request_handler=None):\n                     ...\n1515 1519                    # fix content-type: https://github.com/localstack/localstack/issues/618\n1516 1520                    #                   https://github.com/localstack/localstack/issues/549\n1517 1521                    #                   https://github.com/localstack/localstack/issues/854\n1518       -                 if \"text/html\" in response.headers.get(\n1519       -                     \"Content-Type\", \"\"\n1520       -                 ) and not response_content_str.lower().startswith(\"<!doctype html\"):\n     1522  + \n     1523  +                 if is_invalid_html_response(response.headers, response_content_str):\n1521 1524                        response.headers[\"Content-Type\"] = \"application/xml; charset=utf-8\"\n1522 1525    \n1523 1526                    reset_content_length = True\n           ...\n",
                "file_path": "localstack/services/s3/s3_listener.py",
                "identifiers_before": [
                    "get",
                    "headers",
                    "lower",
                    "response",
                    "response_content_str",
                    "startswith"
                ],
                "identifiers_after": [
                    "headers",
                    "is_invalid_html_response",
                    "response",
                    "response_content_str"
                ],
                "prefix": [
                    "                # fix content-type: https://github.com/localstack/localstack/issues/618\n",
                    "                #                   https://github.com/localstack/localstack/issues/549\n",
                    "                #                   https://github.com/localstack/localstack/issues/854\n"
                ],
                "suffix": [
                    "                    response.headers[\"Content-Type\"] = \"application/xml; charset=utf-8\"\n",
                    "\n",
                    "                reset_content_length = True\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 1523,
                                    "column": 19
                                },
                                "end": {
                                    "line": 1523,
                                    "column": 43
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/services/s3/s3_listener.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 1523,
                                    "column": 19
                                },
                                "end": {
                                    "line": 1523,
                                    "column": 43
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/services/s3/s3_listener.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "                    response.headers[\"Content-Type\"] = \"application/xml; charset=utf-8\"\n",
                "\n",
                "                reset_content_length = True\n",
                "\n",
                "            # update Content-Length headers (fix https://github.com/localstack/localstack/issues/541)\n",
                "            if method == \"DELETE\":\n",
                "                reset_content_length = True\n",
                "\n",
                "            if reset_content_length:\n",
                "                response.headers[\"Content-Length\"] = str(len(response._content or \"\"))\n",
                "\n",
                "            # convert to chunked encoding, for compatibility with certain SDKs (e.g., AWS PHP SDK)\n",
                "            convert_to_chunked_encoding(method, path, response)\n",
                "\n",
                "\n",
                "def serve_static_website(headers, path, bucket_name):\n",
                "    s3_client = aws_stack.connect_to_service(\"s3\")\n",
                "\n",
                "    # check if bucket exists\n",
                "    try:\n",
                "        s3_client.head_bucket(Bucket=bucket_name)\n",
                "    except ClientError:\n",
                "        return no_such_bucket(bucket_name, headers.get(\"x-amz-request-id\"), 404)\n",
                "\n",
                "    def respond_with_key(status_code, key):\n",
                "        obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
                "        response_headers = {}\n",
                "\n",
                "        if \"if-none-match\" in headers and \"ETag\" in obj and obj[\"ETag\"] in headers[\"if-none-match\"]:\n",
                "            return requests_response(status_code=304, content=\"\", headers=response_headers)\n",
                "        if \"WebsiteRedirectLocation\" in obj:\n",
                "            response_headers[\"location\"] = obj[\"WebsiteRedirectLocation\"]\n",
                "            return requests_response(status_code=301, content=\"\", headers=response_headers)\n",
                "        if \"ContentType\" in obj:\n",
                "            response_headers[\"content-type\"] = obj[\"ContentType\"]\n",
                "        if \"ETag\" in obj:\n",
                "            response_headers[\"etag\"] = obj[\"ETag\"]\n",
                "        return requests_response(\n",
                "            status_code=status_code, content=obj[\"Body\"].read(), headers=response_headers\n",
                "        )\n",
                "\n",
                "    try:\n",
                "        if path != \"/\":\n",
                "            path = path.lstrip(\"/\")\n",
                "            return respond_with_key(status_code=200, key=path)\n",
                "    except ClientError:\n",
                "        LOGGER.debug(\"No such key found. %s\" % path)\n",
                "\n",
                "    website_config = s3_client.get_bucket_website(Bucket=bucket_name)\n",
                "    path_suffix = website_config.get(\"IndexDocument\", {}).get(\"Suffix\", \"\").lstrip(\"/\")\n",
                "    index_document = \"%s/%s\" % (path.rstrip(\"/\"), path_suffix)\n",
                "    try:\n",
                "        return respond_with_key(status_code=200, key=index_document)\n",
                "    except ClientError:\n",
                "        error_document = website_config.get(\"ErrorDocument\", {}).get(\"Key\", \"\").lstrip(\"/\")\n",
                "        try:\n",
                "            return respond_with_key(status_code=404, key=error_document)\n",
                "        except ClientError:\n",
                "            return requests_response(status_code=404, content=\"\")\n",
                "\n",
                "\n",
                "# instantiate listener\n",
                "UPDATE_S3 = ProxyListenerS3()"
            ]
        ],
        "localstack/services/sts/sts_listener.py": [
            [
                "from requests.models import Request\n",
                "\n",
                "from localstack.services.generic_proxy import ProxyListener\n"
            ],
            {
                "type": "replace",
                "before": [
                    "from localstack.utils.aws.aws_responses import MessageConversion\n"
                ],
                "after": [
                    "from localstack.utils.aws.aws_responses import MessageConversion, is_invalid_html_response\n"
                ],
                "parent_version_range": {
                    "start": 3,
                    "end": 4
                },
                "child_version_range": {
                    "start": 3,
                    "end": 4
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 2,
                "hunk_diff": "File: localstack/services/sts/sts_listener.py\nCode:\n  ...\n0 0    from requests.models import Request\n1 1    \n2 2    from localstack.services.generic_proxy import ProxyListener\n3    - from localstack.utils.aws.aws_responses import MessageConversion\n  3  + from localstack.utils.aws.aws_responses import MessageConversion, is_invalid_html_response\n4 4    \n5 5    \n6 6    class ProxyListenerSTS(ProxyListener):\n     ...\n",
                "file_path": "localstack/services/sts/sts_listener.py",
                "identifiers_before": [
                    "MessageConversion",
                    "aws",
                    "aws_responses",
                    "localstack",
                    "utils"
                ],
                "identifiers_after": [
                    "MessageConversion",
                    "aws",
                    "aws_responses",
                    "is_invalid_html_response",
                    "localstack",
                    "utils"
                ],
                "prefix": [
                    "from requests.models import Request\n",
                    "\n",
                    "from localstack.services.generic_proxy import ProxyListener\n"
                ],
                "suffix": [
                    "\n",
                    "\n",
                    "class ProxyListenerSTS(ProxyListener):\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 3,
                                    "column": 66
                                },
                                "end": {
                                    "line": 3,
                                    "column": 90
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/services/sts/sts_listener.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 3,
                                    "column": 66
                                },
                                "end": {
                                    "line": 3,
                                    "column": 90
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/services/sts/sts_listener.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "\n",
                "\n",
                "class ProxyListenerSTS(ProxyListener):\n",
                "    def forward_request(self, method, path, data, headers):\n",
                "        if method == \"POST\" and path == \"/\":\n",
                "            data = MessageConversion._reset_account_id(data)\n",
                "            return Request(data=data, headers=headers, method=method)\n",
                "\n",
                "        return True\n",
                "\n",
                "    def return_response(self, method, path, data, headers, response):\n",
                "        if response.content:\n",
                "            # fix hardcoded account ID in ARNs returned from this API\n",
                "            MessageConversion.fix_account_id(response)\n",
                "            # fix dates returned from this API (fixes an issue with Terraform)\n",
                "            MessageConversion.fix_date_format(response)\n",
                "            # fix returned error codes\n",
                "            MessageConversion.fix_error_codes(method, data, response)\n",
                "            # fix content-length header\n",
                "            response.headers[\"Content-Length\"] = str(len(response._content))\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "            # fix content-type header\n",
                    "            if is_invalid_html_response(response.headers, response._content):\n",
                    "                response.headers[\"Content-Type\"] = \"text/xml\"\n"
                ],
                "parent_version_range": {
                    "start": 24,
                    "end": 24
                },
                "child_version_range": {
                    "start": 24,
                    "end": 27
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if response.content:",
                        "start_line": 15,
                        "end_line": 23
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "ProxyListenerSTS",
                        "signature": "class ProxyListenerSTS(ProxyListener):",
                        "at_line": 6
                    },
                    {
                        "type": "function",
                        "name": "return_response",
                        "signature": "def return_response(self, method, path, data, headers, response):",
                        "at_line": 14
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: localstack/services/sts/sts_listener.py\nCode:\n         class ProxyListenerSTS(ProxyListener):\n             ...\n             def return_response(self, method, path, data, headers, response):\n                 ...\n21 21                MessageConversion.fix_error_codes(method, data, response)\n22 22                # fix content-length header\n23 23                response.headers[\"Content-Length\"] = str(len(response._content))\n   24  +             # fix content-type header\n   25  +             if is_invalid_html_response(response.headers, response._content):\n   26  +                 response.headers[\"Content-Type\"] = \"text/xml\"\n24 27    \n25 28    \n26 29    # instantiate listener\n       ...\n",
                "file_path": "localstack/services/sts/sts_listener.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "_content",
                    "headers",
                    "is_invalid_html_response",
                    "response"
                ],
                "prefix": [
                    "            MessageConversion.fix_error_codes(method, data, response)\n",
                    "            # fix content-length header\n",
                    "            response.headers[\"Content-Length\"] = str(len(response._content))\n"
                ],
                "suffix": [
                    "\n",
                    "\n",
                    "# instantiate listener\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 25,
                                    "column": 15
                                },
                                "end": {
                                    "line": 25,
                                    "column": 39
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/services/sts/sts_listener.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 25,
                                    "column": 15
                                },
                                "end": {
                                    "line": 25,
                                    "column": 39
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/services/sts/sts_listener.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "\n",
                "\n",
                "# instantiate listener\n",
                "UPDATE_STS = ProxyListenerSTS()"
            ]
        ],
        "localstack/utils/aws/aws_responses.py": [
            [
                "import binascii\n",
                "import datetime\n",
                "import json\n",
                "import re\n",
                "import xml.etree.ElementTree as ET\n",
                "from binascii import crc32\n",
                "from struct import pack\n",
                "from typing import Dict, Optional, Union\n",
                "from urllib.parse import parse_qs\n",
                "\n",
                "import xmltodict\n",
                "from flask import Response as FlaskResponse\n",
                "from moto.core.exceptions import JsonRESTError\n",
                "from requests.models import CaseInsensitiveDict\n",
                "from requests.models import Response as RequestsResponse\n",
                "\n",
                "from localstack.config import DEFAULT_ENCODING\n",
                "from localstack.constants import (\n",
                "    APPLICATION_JSON,\n",
                "    HEADER_CONTENT_TYPE,\n",
                "    MOTO_ACCOUNT_ID,\n",
                "    TEST_AWS_ACCOUNT_ID,\n",
                ")\n",
                "from localstack.utils.aws import aws_stack\n",
                "from localstack.utils.common import (\n",
                "    json_safe,\n",
                "    replace_response_content,\n",
                "    short_uid,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "    str_startswith_ignore_case,\n"
                ],
                "parent_version_range": {
                    "start": 28,
                    "end": 28
                },
                "child_version_range": {
                    "start": 28,
                    "end": 29
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 4,
                "hunk_diff": "File: localstack/utils/aws/aws_responses.py\nCode:\n  ...\n25 25        json_safe,\n26 26        replace_response_content,\n27 27        short_uid,\n   28  +     str_startswith_ignore_case,\n28 29        to_bytes,\n29 30        to_str,\n30 31        truncate,\n       ...\n",
                "file_path": "localstack/utils/aws/aws_responses.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "str_startswith_ignore_case"
                ],
                "prefix": [
                    "    json_safe,\n",
                    "    replace_response_content,\n",
                    "    short_uid,\n"
                ],
                "suffix": [
                    "    to_bytes,\n",
                    "    to_str,\n",
                    "    truncate,\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "str_startswith_ignore_case",
                            "position": {
                                "start": {
                                    "line": 28,
                                    "column": 4
                                },
                                "end": {
                                    "line": 28,
                                    "column": 30
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/aws/aws_responses.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "str_startswith_ignore_case",
                            "position": {
                                "start": {
                                    "line": 28,
                                    "column": 4
                                },
                                "end": {
                                    "line": 28,
                                    "column": 30
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/aws/aws_responses.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "    to_bytes,\n",
                "    to_str,\n",
                "    truncate,\n",
                ")\n",
                "\n",
                "REGEX_FLAGS = re.MULTILINE | re.DOTALL\n",
                "\n",
                "AWS_BINARY_DATA_TYPE_STRING = 7\n",
                "\n",
                "\n",
                "class ErrorResponse(Exception):\n",
                "    def __init__(self, response):\n",
                "        self.response = response\n",
                "\n",
                "\n",
                "class ResourceNotFoundException(JsonRESTError):\n",
                "    \"\"\"Generic ResourceNotFoundException used when processing requests in Flask contexts.\"\"\"\n",
                "\n",
                "    code = 404\n",
                "\n",
                "    def __init__(self, message=None):\n",
                "        message = message or \"The given resource cannot be found\"\n",
                "        super(ResourceNotFoundException, self).__init__(\"ResourceNotFoundException\", message)\n",
                "\n",
                "\n",
                "def flask_error_response_json(\n",
                "    msg: str, code: Optional[int] = 500, error_type: Optional[str] = \"InternalFailure\"\n",
                "):\n",
                "    result = {\n",
                "        \"Type\": \"User\" if code < 500 else \"Server\",\n",
                "        \"message\": msg,\n",
                "        \"__type\": error_type,\n",
                "    }\n",
                "    headers = {\"x-amzn-errortype\": error_type}\n",
                "    # Note: don't use flask's make_response(..) or jsonify(..) here as they\n",
                "    # can lead to \"RuntimeError: working outside of application context\".\n",
                "    return FlaskResponse(json.dumps(result), status=code, headers=headers)\n",
                "\n",
                "\n",
                "def requests_error_response_json(message, code=500, error_type=\"InternalFailure\"):\n",
                "    response = flask_error_response_json(message, code=code, error_type=error_type)\n",
                "    return flask_to_requests_response(response)\n",
                "\n",
                "\n",
                "def requests_error_response_xml(\n",
                "    message: str,\n",
                "    code: Optional[int] = 400,\n",
                "    code_string: Optional[str] = \"InvalidParameter\",\n",
                "    service: Optional[str] = None,\n",
                "    xmlns: Optional[str] = None,\n",
                "):\n",
                "    response = RequestsResponse()\n",
                "    xmlns = xmlns or \"http://%s.amazonaws.com/doc/2010-03-31/\" % service\n",
                "    response._content = \"\"\"<ErrorResponse xmlns=\"{xmlns}\"><Error>\n",
                "        <Type>Sender</Type>\n",
                "        <Code>{code_string}</Code>\n",
                "        <Message>{message}</Message>\n",
                "        </Error><RequestId>{req_id}</RequestId>\n",
                "        </ErrorResponse>\"\"\".format(\n",
                "        xmlns=xmlns, message=message, code_string=code_string, req_id=short_uid()\n",
                "    )\n",
                "    response.status_code = code\n",
                "    return response\n",
                "\n",
                "\n",
                "def to_xml(data: dict, memberize: bool = True) -> ET.Element:\n",
                "    \"\"\"Generate XML element hierarchy out of dict. Wraps list items in <member> tags by default\"\"\"\n",
                "    if not isinstance(data, dict) or len(data.keys()) != 1:\n",
                "        raise Exception(\"Expected data to be a dict with a single root element\")\n",
                "\n",
                "    def _to_xml(parent_el: ET.Element, data_rest) -> None:\n",
                "        if isinstance(data_rest, list):\n",
                "            for i in data_rest:\n",
                "                member_el = ET.SubElement(parent_el, \"member\") if memberize else parent_el\n",
                "                _to_xml(member_el, i)\n",
                "        elif isinstance(data_rest, dict):\n",
                "            for key in data_rest:\n",
                "                value = data_rest[key]\n",
                "                curr_el = ET.SubElement(parent_el, key)\n",
                "                _to_xml(curr_el, value)\n",
                "        elif isinstance(data_rest, str):\n",
                "            parent_el.text = data_rest\n",
                "        elif any(\n",
                "            [isinstance(data_rest, i) for i in [bool, str, int, float]]\n",
                "        ):  # limit types for text serialization\n",
                "            parent_el.text = str(data_rest)\n",
                "        else:\n",
                "            if data_rest is not None:  # None is just ignored and omitted\n",
                "                raise Exception(f\"Unexpected type for value encountered: {type(data_rest)}\")\n",
                "\n",
                "    root_key = list(data.keys())[0]\n",
                "    root = ET.Element(root_key)\n",
                "    _to_xml(root, data[root_key])\n",
                "    return root\n",
                "\n",
                "\n",
                "def requests_response_xml(action, response, xmlns=None, service=None, memberize=True):\n",
                "    xmlns = xmlns or \"http://%s.amazonaws.com/doc/2010-03-31/\" % service\n",
                "    response = json_safe(response)\n",
                "    response = {\"{action}Result\".format(action=action): response}\n",
                "    response = ET.tostring(to_xml(response, memberize=memberize), short_empty_elements=True)\n",
                "    response = to_str(response)\n",
                "    result = (\n",
                "        \"\"\"\n",
                "        <{action}Response xmlns=\"{xmlns}\">\n",
                "            {response}\n",
                "        </{action}Response>\n",
                "        \"\"\"\n",
                "    ).strip()\n",
                "    result = result.format(action=action, xmlns=xmlns, response=response)\n",
                "    result = requests_response(result)\n",
                "    return result\n",
                "\n",
                "\n",
                "def requests_error_response_xml_signature_calculation(\n",
                "    message,\n",
                "    string_to_sign=None,\n",
                "    signature=None,\n",
                "    expires=None,\n",
                "    code=400,\n",
                "    code_string=\"AccessDenied\",\n",
                "    aws_access_token=\"temp\",\n",
                "):\n",
                "    response = RequestsResponse()\n",
                "    response_template = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
                "        <Error>\n",
                "            <Code>{code_string}</Code>\n",
                "            <Message>{message}</Message>\n",
                "            <RequestId>{req_id}</RequestId>\n",
                "            <HostId>{host_id}</HostId>\n",
                "        </Error>\"\"\".format(\n",
                "        message=message,\n",
                "        code_string=code_string,\n",
                "        req_id=short_uid(),\n",
                "        host_id=short_uid(),\n",
                "    )\n",
                "\n",
                "    parsed_response = xmltodict.parse(response_template)\n",
                "    response.status_code = code\n",
                "\n",
                "    if signature and string_to_sign or code_string == \"SignatureDoesNotMatch\":\n",
                "        bytes_signature = binascii.hexlify(bytes(signature, encoding=\"utf-8\"))\n",
                "        parsed_response[\"Error\"][\"Code\"] = code_string\n",
                "        parsed_response[\"Error\"][\"AWSAccessKeyId\"] = aws_access_token\n",
                "        parsed_response[\"Error\"][\"StringToSign\"] = string_to_sign\n",
                "        parsed_response[\"Error\"][\"SignatureProvided\"] = signature\n",
                "        parsed_response[\"Error\"][\"StringToSignBytes\"] = \"{}\".format(bytes_signature.decode(\"utf-8\"))\n",
                "        set_response_content(response, xmltodict.unparse(parsed_response))\n",
                "\n",
                "    if expires and code_string == \"AccessDenied\":\n",
                "        server_time = datetime.datetime.utcnow().isoformat()[:-4]\n",
                "        expires_isoformat = datetime.datetime.fromtimestamp(int(expires)).isoformat()[:-4]\n",
                "        parsed_response[\"Error\"][\"Code\"] = code_string\n",
                "        parsed_response[\"Error\"][\"Expires\"] = \"{}Z\".format(expires_isoformat)\n",
                "        parsed_response[\"Error\"][\"ServerTime\"] = \"{}Z\".format(server_time)\n",
                "        set_response_content(response, xmltodict.unparse(parsed_response))\n",
                "\n",
                "    if not signature and not expires and code_string == \"AccessDenied\":\n",
                "        set_response_content(response, xmltodict.unparse(parsed_response))\n",
                "\n",
                "    if response._content:\n",
                "        return response\n",
                "\n",
                "\n",
                "def flask_error_response_xml(\n",
                "    message: str,\n",
                "    code: Optional[int] = 500,\n",
                "    code_string: Optional[str] = \"InternalFailure\",\n",
                "    service: Optional[str] = None,\n",
                "    xmlns: Optional[str] = None,\n",
                "):\n",
                "    response = requests_error_response_xml(\n",
                "        message, code=code, code_string=code_string, service=service, xmlns=xmlns\n",
                "    )\n",
                "    return requests_to_flask_response(response)\n",
                "\n",
                "\n",
                "def requests_error_response(\n",
                "    req_headers: Dict,\n",
                "    message: Union[str, bytes],\n",
                "    code: int = 500,\n",
                "    error_type: str = \"InternalFailure\",\n",
                "    service: str = None,\n",
                "    xmlns: str = None,\n",
                "):\n",
                "    is_json = is_json_request(req_headers)\n",
                "    if is_json:\n",
                "        return requests_error_response_json(message=message, code=code, error_type=error_type)\n",
                "    return requests_error_response_xml(\n",
                "        message, code=code, code_string=error_type, service=service, xmlns=xmlns\n",
                "    )\n",
                "\n",
                "\n",
                "def is_json_request(req_headers: Dict) -> bool:\n",
                "    ctype = req_headers.get(\"Content-Type\", \"\")\n",
                "    accept = req_headers.get(\"Accept\", \"\")\n",
                "    return \"json\" in ctype or \"json\" in accept\n",
                "\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "def is_invalid_html_response(headers, content) -> bool:\n",
                    "    content_type = headers.get(\"Content-Type\", \"\")\n",
                    "    return \"text/html\" in content_type and not str_startswith_ignore_case(content, \"<!doctype html\")\n",
                    "\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 227,
                    "end": 227
                },
                "child_version_range": {
                    "start": 228,
                    "end": 233
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 5,
                "hunk_diff": "File: localstack/utils/aws/aws_responses.py\nCode:\n  ...\n224 225        return \"json\" in ctype or \"json\" in accept\n225 226    \n226 227    \n    228  + def is_invalid_html_response(headers, content) -> bool:\n    229  +     content_type = headers.get(\"Content-Type\", \"\")\n    230  +     return \"text/html\" in content_type and not str_startswith_ignore_case(content, \"<!doctype html\")\n    231  + \n    232  + \n227 233    def raise_exception_if_error_response(response):\n228 234        if not is_response_obj(response):\n229 235            return\n         ...\n",
                "file_path": "localstack/utils/aws/aws_responses.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "bool",
                    "content",
                    "content_type",
                    "get",
                    "headers",
                    "is_invalid_html_response",
                    "str_startswith_ignore_case"
                ],
                "prefix": [
                    "    return \"json\" in ctype or \"json\" in accept\n",
                    "\n",
                    "\n"
                ],
                "suffix": [
                    "def raise_exception_if_error_response(response):\n",
                    "    if not is_response_obj(response):\n",
                    "        return\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "str_startswith_ignore_case",
                            "position": {
                                "start": {
                                    "line": 230,
                                    "column": 47
                                },
                                "end": {
                                    "line": 230,
                                    "column": 73
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/aws/aws_responses.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "str_startswith_ignore_case",
                            "position": {
                                "start": {
                                    "line": 230,
                                    "column": 47
                                },
                                "end": {
                                    "line": 230,
                                    "column": 73
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/aws/aws_responses.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 228,
                                    "column": 4
                                },
                                "end": {
                                    "line": 228,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/aws/aws_responses.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 228,
                                    "column": 4
                                },
                                "end": {
                                    "line": 228,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/aws/aws_responses.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 228,
                                    "column": 4
                                },
                                "end": {
                                    "line": 228,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/aws/aws_responses.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "is_invalid_html_response",
                            "position": {
                                "start": {
                                    "line": 228,
                                    "column": 4
                                },
                                "end": {
                                    "line": 228,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/aws/aws_responses.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "def raise_exception_if_error_response(response):\n",
                "    if not is_response_obj(response):\n",
                "        return\n",
                "    if response.status_code < 400:\n",
                "        return\n",
                "    content = \"...\"\n",
                "    try:\n",
                "        content = truncate(to_str(response.content or \"\"))\n",
                "    except Exception:\n",
                "        pass  # ignore if content has non-printable bytes\n",
                "    raise Exception(\"Received error response (code %s): %s\" % (response.status_code, content))\n",
                "\n",
                "\n",
                "def is_response_obj(result, include_lambda_response=False):\n",
                "    types = (RequestsResponse, FlaskResponse)\n",
                "    if include_lambda_response:\n",
                "        types += (LambdaResponse,)\n",
                "    return isinstance(result, types)\n",
                "\n",
                "\n",
                "def get_response_payload(response, as_json=False):\n",
                "    result = (\n",
                "        response.content\n",
                "        if isinstance(response, RequestsResponse)\n",
                "        else response.data\n",
                "        if isinstance(response, FlaskResponse)\n",
                "        else None\n",
                "    )\n",
                "    result = \"\" if result is None else result\n",
                "    if as_json:\n",
                "        result = result or \"{}\"\n",
                "        result = json.loads(to_str(result))\n",
                "    return result\n",
                "\n",
                "\n",
                "def requests_response(content, status_code=200, headers={}):\n",
                "    resp = RequestsResponse()\n",
                "    headers = CaseInsensitiveDict(dict(headers or {}))\n",
                "    if isinstance(content, dict):\n",
                "        content = json.dumps(content)\n",
                "        if not headers.get(HEADER_CONTENT_TYPE):\n",
                "            headers[HEADER_CONTENT_TYPE] = APPLICATION_JSON\n",
                "    resp._content = content\n",
                "    resp.status_code = int(status_code)\n",
                "    # Note: update headers (instead of assigning directly), to ensure we're using a case-insensitive dict\n",
                "    resp.headers.update(headers)\n",
                "    return resp\n",
                "\n",
                "\n",
                "def request_response_stream(stream, status_code=200, headers={}):\n",
                "    resp = RequestsResponse()\n",
                "    resp.raw = stream\n",
                "    resp.status_code = int(status_code)\n",
                "    # Note: update headers (instead of assigning directly), to ensure we're using a case-insensitive dict\n",
                "    resp.headers.update(headers or {})\n",
                "    return resp\n",
                "\n",
                "\n",
                "def flask_to_requests_response(r):\n",
                "    return requests_response(r.data, status_code=r.status_code, headers=r.headers)\n",
                "\n",
                "\n",
                "def requests_to_flask_response(r):\n",
                "    return FlaskResponse(r.content, status=r.status_code, headers=dict(r.headers))\n",
                "\n",
                "\n",
                "def flask_not_found_error(msg=None):\n",
                "    msg = msg or \"The specified resource doesnt exist.\"\n",
                "    return flask_error_response_json(msg, code=404, error_type=\"ResourceNotFoundException\")\n",
                "\n",
                "\n",
                "def response_regex_replace(response, search, replace):\n",
                "    content = re.sub(search, replace, to_str(response._content), flags=re.DOTALL | re.MULTILINE)\n",
                "    set_response_content(response, content)\n",
                "\n",
                "\n",
                "def set_response_content(response, content, headers=None):\n",
                "    if isinstance(content, dict):\n",
                "        content = json.dumps(json_safe(content))\n",
                "    elif isinstance(content, RequestsResponse):\n",
                "        response.status_code = content.status_code\n",
                "        content = content.content\n",
                "    response._content = content or \"\"\n",
                "    response.headers.update(headers or {})\n",
                "    response.headers[\"Content-Length\"] = str(len(response._content))\n",
                "\n",
                "\n",
                "def make_requests_error(*args, **kwargs):\n",
                "    return flask_to_requests_response(flask_error_response_xml(*args, **kwargs))\n",
                "\n",
                "\n",
                "def make_error(*args, **kwargs):\n",
                "    return flask_error_response_xml(*args, **kwargs)\n",
                "\n",
                "\n",
                "def create_sqs_system_attributes(headers):\n",
                "    system_attributes = {}\n",
                "    if \"X-Amzn-Trace-Id\" in headers:\n",
                "        system_attributes[\"AWSTraceHeader\"] = {\n",
                "            \"DataType\": \"String\",\n",
                "            \"StringValue\": str(headers[\"X-Amzn-Trace-Id\"]),\n",
                "        }\n",
                "    return system_attributes\n",
                "\n",
                "\n",
                "def extract_tags(req_data):\n",
                "    for param_name in [\"Tag\", \"member\"]:\n",
                "        keys = extract_url_encoded_param_list(req_data, \"Tags.{}.%s.Key\".format(param_name))\n",
                "        values = extract_url_encoded_param_list(req_data, \"Tags.{}.%s.Value\".format(param_name))\n",
                "        if keys and values:\n",
                "            break\n",
                "    entries = zip(keys, values)\n",
                "    tags = [{\"Key\": entry[0], \"Value\": entry[1]} for entry in entries]\n",
                "    return tags\n",
                "\n",
                "\n",
                "def extract_url_encoded_param_list(req_data, pattern):\n",
                "    result = []\n",
                "    for i in range(1, 200):\n",
                "        key = pattern % i\n",
                "        value = req_data.get(key)\n",
                "        if value is None:\n",
                "            break\n",
                "        result.append(value)\n",
                "    return result\n",
                "\n",
                "\n",
                "def parse_urlencoded_data(qs_data, top_level_attribute):\n",
                "    # TODO: potentially find a better way than calling moto here...\n",
                "    from moto.core.responses import BaseResponse\n",
                "\n",
                "    if qs_data and isinstance(qs_data, dict):\n",
                "        # make sure we're using the array form of query string dict here\n",
                "        qs_data = {k: v if isinstance(v, list) else [v] for k, v in qs_data.items()}\n",
                "    if isinstance(qs_data, (str, bytes)):\n",
                "        qs_data = parse_qs(qs_data)\n",
                "    response = BaseResponse()\n",
                "    response.querystring = qs_data\n",
                "    result = response._get_multi_param(top_level_attribute, skip_result_conversion=True)\n",
                "    return result\n",
                "\n",
                "\n",
                "def calculate_crc32(content):\n",
                "    return crc32(to_bytes(content)) & 0xFFFFFFFF\n",
                "\n",
                "\n",
                "def convert_to_binary_event_payload(result, event_type=None, message_type=None):\n",
                "    # e.g.: https://docs.aws.amazon.com/AmazonS3/latest/API/RESTSelectObjectAppendix.html\n",
                "    # e.g.: https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html\n",
                "\n",
                "    header_descriptors = {\n",
                "        \":event-type\": event_type or \"Records\",\n",
                "        \":message-type\": message_type or \"event\",\n",
                "    }\n",
                "\n",
                "    # construct headers\n",
                "    headers = b\"\"\n",
                "    for key, value in header_descriptors.items():\n",
                "        header_name = key.encode(DEFAULT_ENCODING)\n",
                "        header_value = to_bytes(value)\n",
                "        headers += pack(\"!B\", len(header_name))\n",
                "        headers += header_name\n",
                "        headers += pack(\"!B\", AWS_BINARY_DATA_TYPE_STRING)\n",
                "        headers += pack(\"!H\", len(header_value))\n",
                "        headers += header_value\n",
                "\n",
                "    # construct body\n",
                "    body = bytes(result, DEFAULT_ENCODING)\n",
                "\n",
                "    # calculate lengths\n",
                "    headers_length = len(headers)\n",
                "    body_length = len(body)\n",
                "\n",
                "    # construct message\n",
                "    result = pack(\"!I\", body_length + headers_length + 16)\n",
                "    result += pack(\"!I\", headers_length)\n",
                "    prelude_crc = binascii.crc32(result)\n",
                "    result += pack(\"!I\", prelude_crc)\n",
                "    result += headers\n",
                "    result += body\n",
                "    payload_crc = binascii.crc32(result)\n",
                "    result += pack(\"!I\", payload_crc)\n",
                "\n",
                "    return result\n",
                "\n",
                "\n",
                "class LambdaResponse(object):\n",
                "    \"\"\"Helper class to support multi_value_headers in Lambda responses\"\"\"\n",
                "\n",
                "    def __init__(self):\n",
                "        self._content = False\n",
                "        self.status_code = None\n",
                "        self.multi_value_headers = CaseInsensitiveDict()\n",
                "        self.headers = CaseInsensitiveDict()\n",
                "\n",
                "    @property\n",
                "    def content(self):\n",
                "        return self._content\n",
                "\n",
                "\n",
                "class MessageConversion(object):\n",
                "    @staticmethod\n",
                "    def fix_date_format(response):\n",
                "        \"\"\"Normalize date to format '2019-06-13T18:10:09.1234Z'\"\"\"\n",
                "        pattern = r\"<CreateDate>([^<]+) ([^<+]+)(\\+[^<]*)?</CreateDate>\"\n",
                "        replacement = r\"<CreateDate>\\1T\\2Z</CreateDate>\"\n",
                "        replace_response_content(response, pattern, replacement)\n",
                "\n",
                "    @staticmethod\n",
                "    def fix_account_id(response):\n",
                "        return aws_stack.fix_account_id_in_arns(response, replace=TEST_AWS_ACCOUNT_ID)\n",
                "\n",
                "    @staticmethod\n",
                "    def fix_error_codes(method, data, response):\n",
                "        regex = r\"<Errors>\\s*(<Error>(\\s|.)*</Error>)\\s*</Errors>\"\n",
                "        if method == \"POST\" and \"Action=CreateRole\" in to_str(data) and response.status_code >= 400:\n",
                "            content = to_str(response.content)\n",
                "            # remove the <Errors> wrapper element, as this breaks AWS Java SDKs (issue #2231)\n",
                "            response._content = re.sub(regex, r\"\\1\", content, flags=REGEX_FLAGS)\n",
                "\n",
                "    @staticmethod\n",
                "    def fix_xml_empty_boolean(response, tag_names):\n",
                "        for tag_name in tag_names:\n",
                "            regex = r\"<{tag}>\\s*([Nn]one|null)\\s*</{tag}>\".format(tag=tag_name)\n",
                "            replace = r\"<{tag}>false</{tag}>\".format(tag=tag_name)\n",
                "            response._content = re.sub(regex, replace, to_str(response.content), flags=REGEX_FLAGS)\n",
                "\n",
                "    @staticmethod\n",
                "    def booleans_to_lowercase(response, tag_names):\n",
                "        for tag_name in tag_names:\n",
                "            regex_true = r\"<{tag}>\\s*True\\s*</{tag}>\".format(tag=tag_name)\n",
                "            replace_true = r\"<{tag}>true</{tag}>\".format(tag=tag_name)\n",
                "            response._content = re.sub(\n",
                "                regex_true, replace_true, to_str(response.content), flags=REGEX_FLAGS\n",
                "            )\n",
                "\n",
                "            regex_false = r\"<{tag}>\\s*False\\s*</{tag}>\".format(tag=tag_name)\n",
                "            replace_false = r\"<{tag}>false</{tag}>\".format(tag=tag_name)\n",
                "            response._content = re.sub(\n",
                "                regex_false, replace_false, to_str(response.content), flags=REGEX_FLAGS\n",
                "            )\n",
                "\n",
                "    @staticmethod\n",
                "    def _reset_account_id(data):\n",
                "        \"\"\"Fix account ID in request payload. All external-facing responses contain our\n",
                "        predefined account ID (defaults to 000000000000), whereas the backend endpoint\n",
                "        from moto expects a different hardcoded account ID (123456789012).\"\"\"\n",
                "        return aws_stack.fix_account_id_in_arns(\n",
                "            data,\n",
                "            colon_delimiter=\"%3A\",\n",
                "            existing=TEST_AWS_ACCOUNT_ID,\n",
                "            replace=MOTO_ACCOUNT_ID,\n",
                "        )"
            ]
        ],
        "localstack/utils/common.py": [
            [
                "import base64\n",
                "import binascii\n",
                "import decimal\n",
                "import functools\n",
                "import glob\n",
                "import hashlib\n",
                "import inspect\n",
                "import io\n",
                "import itertools\n",
                "import json\n",
                "import logging\n",
                "import os\n",
                "import platform\n",
                "import re\n",
                "import shutil\n",
                "import socket\n",
                "import subprocess\n",
                "import sys\n",
                "import tarfile\n",
                "import tempfile\n",
                "import threading\n",
                "import time\n",
                "import uuid\n",
                "import zipfile\n",
                "from contextlib import closing\n",
                "from datetime import date, datetime, timezone, tzinfo\n",
                "from multiprocessing.dummy import Pool\n",
                "from queue import Queue\n",
                "from typing import Any, Callable, Dict, List, Optional, Sized, Tuple, Type, Union\n",
                "from urllib.parse import parse_qs, urlparse\n",
                "\n",
                "import dns.resolver\n",
                "import requests\n",
                "import six\n",
                "from requests import Response\n",
                "\n",
                "import localstack.utils.run\n",
                "from localstack import config\n",
                "from localstack.config import DEFAULT_ENCODING\n",
                "from localstack.constants import ENV_DEV\n",
                "from localstack.utils.run import FuncThread\n",
                "\n",
                "# arrays for temporary files and resources\n",
                "TMP_FILES = []\n",
                "TMP_THREADS = []\n",
                "TMP_PROCESSES = []\n",
                "\n",
                "# cache clean variables\n",
                "CACHE_CLEAN_TIMEOUT = 60 * 5\n",
                "CACHE_MAX_AGE = 60 * 60\n",
                "CACHE_FILE_PATTERN = os.path.join(tempfile.gettempdir(), \"_random_dir_\", \"cache.*.json\")\n",
                "last_cache_clean_time = {\"time\": 0}\n",
                "MUTEX_CLEAN = threading.Lock()\n",
                "\n",
                "# misc. constants\n",
                "TIMESTAMP_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
                "TIMESTAMP_FORMAT_TZ = \"%Y-%m-%dT%H:%M:%SZ\"\n",
                "TIMESTAMP_FORMAT_MICROS = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
                "CODEC_HANDLER_UNDERSCORE = \"underscore\"\n",
                "\n",
                "# chunk size for file downloads\n",
                "DOWNLOAD_CHUNK_SIZE = 1024 * 1024\n",
                "\n",
                "# set up logger\n",
                "LOG = logging.getLogger(__name__)\n",
                "\n",
                "# flag to indicate whether we've received and processed the stop signal\n",
                "INFRA_STOPPED = False\n",
                "\n",
                "# generic cache object\n",
                "CACHE = {}\n",
                "\n",
                "# lock for creating certificate files\n",
                "SSL_CERT_LOCK = threading.RLock()\n",
                "\n",
                "# regular expression for unprintable characters\n",
                "# Based on https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SendMessage.html\n",
                "#     #x9 | #xA | #xD | #x20 to #xD7FF | #xE000 to #xFFFD | #x10000 to #x10FFFF\n",
                "_unprintables = (\n",
                "    range(0x00, 0x09),\n",
                "    range(0x0A, 0x0A),\n",
                "    range(0x0B, 0x0D),\n",
                "    range(0x0E, 0x20),\n",
                "    range(0xD800, 0xE000),\n",
                "    range(0xFFFE, 0x10000),\n",
                ")\n",
                "REGEX_UNPRINTABLE_CHARS = re.compile(\n",
                "    f\"[{re.escape(''.join(map(chr, itertools.chain(*_unprintables))))}]\"\n",
                ")\n",
                "\n",
                "# user of the currently running process\n",
                "CACHED_USER = None\n",
                "\n",
                "# type definitions for JSON-serializable objects\n",
                "JsonComplexType = Union[Dict, List]\n",
                "JsonType = Union[JsonComplexType, str, int, float, bool, None]\n",
                "SerializableObj = JsonType\n",
                "\n",
                "\n",
                "class Mock(object):\n",
                "    \"\"\"Dummy class that can be used for mocking custom attributes.\"\"\"\n",
                "\n",
                "    pass\n",
                "\n",
                "\n",
                "class CustomEncoder(json.JSONEncoder):\n",
                "    \"\"\"Helper class to convert JSON documents with datetime, decimals, or bytes.\"\"\"\n",
                "\n",
                "    def default(self, o):\n",
                "        import yaml  # leave import here, to avoid breaking our Lambda tests!\n",
                "\n",
                "        if isinstance(o, decimal.Decimal):\n",
                "            if o % 1 > 0:\n",
                "                return float(o)\n",
                "            else:\n",
                "                return int(o)\n",
                "        if isinstance(o, (datetime, date)):\n",
                "            return timestamp_millis(o)\n",
                "        if isinstance(o, yaml.ScalarNode):\n",
                "            if o.tag == \"tag:yaml.org,2002:int\":\n",
                "                return int(o.value)\n",
                "            if o.tag == \"tag:yaml.org,2002:float\":\n",
                "                return float(o.value)\n",
                "            if o.tag == \"tag:yaml.org,2002:bool\":\n",
                "                return bool(o.value)\n",
                "            return str(o.value)\n",
                "        try:\n",
                "            if isinstance(o, six.binary_type):\n",
                "                return to_str(o)\n",
                "            return super(CustomEncoder, self).default(o)\n",
                "        except Exception:\n",
                "            return None\n",
                "\n",
                "\n",
                "class ShellCommandThread(FuncThread):\n",
                "    \"\"\"Helper class to run a shell command in a background thread.\"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        cmd: Union[str, List[str]],\n",
                "        params: Any = None,\n",
                "        outfile: Union[str, int] = None,\n",
                "        env_vars: Dict[str, str] = None,\n",
                "        stdin: bool = False,\n",
                "        auto_restart: bool = False,\n",
                "        quiet: bool = True,\n",
                "        inherit_cwd: bool = False,\n",
                "        inherit_env: bool = True,\n",
                "        log_listener: Callable = None,\n",
                "        stop_listener: Callable = None,\n",
                "        strip_color: bool = False,\n",
                "    ):\n",
                "        params = not_none_or(params, {})\n",
                "        env_vars = not_none_or(env_vars, {})\n",
                "        self.stopped = False\n",
                "        self.cmd = cmd\n",
                "        self.process = None\n",
                "        self.outfile = outfile\n",
                "        self.stdin = stdin\n",
                "        self.env_vars = env_vars\n",
                "        self.inherit_cwd = inherit_cwd\n",
                "        self.inherit_env = inherit_env\n",
                "        self.auto_restart = auto_restart\n",
                "        self.log_listener = log_listener\n",
                "        self.stop_listener = stop_listener\n",
                "        self.strip_color = strip_color\n",
                "        FuncThread.__init__(self, self.run_cmd, params, quiet=quiet)\n",
                "\n",
                "    def run_cmd(self, params):\n",
                "        while True:\n",
                "            self.do_run_cmd()\n",
                "            if (\n",
                "                INFRA_STOPPED\n",
                "                or not self.auto_restart\n",
                "                or not self.process\n",
                "                or self.process.returncode == 0\n",
                "            ):\n",
                "                return self.process.returncode if self.process else None\n",
                "            LOG.info(\n",
                "                \"Restarting process (received exit code %s): %s\"\n",
                "                % (self.process.returncode, self.cmd)\n",
                "            )\n",
                "\n",
                "    def do_run_cmd(self):\n",
                "        def convert_line(line):\n",
                "            line = to_str(line or \"\")\n",
                "            if self.strip_color:\n",
                "                # strip color codes\n",
                "                line = re.sub(r\"\\x1b(\\[.*?[@-~]|\\].*?(\\x07|\\x1b\\\\))\", \"\", line)\n",
                "            return \"%s\\r\\n\" % line.strip()\n",
                "\n",
                "        def filter_line(line):\n",
                "            \"\"\"Return True if this line should be filtered, i.e., not printed\"\"\"\n",
                "            return \"(Press CTRL+C to quit)\" in line\n",
                "\n",
                "        outfile = self.outfile or os.devnull\n",
                "        if self.log_listener and outfile == os.devnull:\n",
                "            outfile = subprocess.PIPE\n",
                "        try:\n",
                "            self.process = run(\n",
                "                self.cmd,\n",
                "                asynchronous=True,\n",
                "                stdin=self.stdin,\n",
                "                outfile=outfile,\n",
                "                env_vars=self.env_vars,\n",
                "                inherit_cwd=self.inherit_cwd,\n",
                "                inherit_env=self.inherit_env,\n",
                "            )\n",
                "            if outfile:\n",
                "                if outfile == subprocess.PIPE:\n",
                "                    # get stdout/stderr from child process and write to parent output\n",
                "                    streams = (\n",
                "                        (self.process.stdout, sys.stdout),\n",
                "                        (self.process.stderr, sys.stderr),\n",
                "                    )\n",
                "                    for instream, outstream in streams:\n",
                "                        if not instream:\n",
                "                            continue\n",
                "                        for line in iter(instream.readline, None):\n",
                "                            # `line` should contain a newline at the end as we're iterating,\n",
                "                            # hence we can safely break the loop if `line` is None or empty string\n",
                "                            if line in [None, \"\", b\"\"]:\n",
                "                                break\n",
                "                            if not (line and line.strip()) and self.is_killed():\n",
                "                                break\n",
                "                            line = convert_line(line)\n",
                "                            if filter_line(line):\n",
                "                                continue\n",
                "                            if self.log_listener:\n",
                "                                self.log_listener(line, stream=instream)\n",
                "                            if self.outfile not in [None, os.devnull]:\n",
                "                                outstream.write(line)\n",
                "                                outstream.flush()\n",
                "                if self.process:\n",
                "                    self.process.wait()\n",
                "            else:\n",
                "                self.process.communicate()\n",
                "        except Exception as e:\n",
                "            self.result_future.set_exception(e)\n",
                "            if self.process and not self.quiet:\n",
                "                LOG.warning('Shell command error \"%s\": %s' % (e, self.cmd))\n",
                "        if self.process and not self.quiet and self.process.returncode != 0:\n",
                "            LOG.warning('Shell command exit code \"%s\": %s' % (self.process.returncode, self.cmd))\n",
                "\n",
                "    def is_killed(self):\n",
                "        if not self.process:\n",
                "            return True\n",
                "        if INFRA_STOPPED:\n",
                "            return True\n",
                "        # Note: Do NOT import \"psutil\" at the root scope, as this leads\n",
                "        # to problems when importing this file from our test Lambdas in Docker\n",
                "        # (Error: libc.musl-x86_64.so.1: cannot open shared object file)\n",
                "        import psutil\n",
                "\n",
                "        return not psutil.pid_exists(self.process.pid)\n",
                "\n",
                "    def stop(self, quiet=False):\n",
                "        if self.stopped:\n",
                "            return\n",
                "        if not self.process:\n",
                "            LOG.warning(\"No process found for command '%s'\" % self.cmd)\n",
                "            return\n",
                "\n",
                "        parent_pid = self.process.pid\n",
                "        try:\n",
                "            kill_process_tree(parent_pid)\n",
                "            self.process = None\n",
                "        except Exception as e:\n",
                "            if not quiet:\n",
                "                LOG.warning(\"Unable to kill process with pid %s: %s\", parent_pid, e)\n",
                "        try:\n",
                "            self.stop_listener and self.stop_listener(self)\n",
                "        except Exception as e:\n",
                "            if not quiet:\n",
                "                LOG.warning(\"Unable to run stop handler for shell command thread %s: %s\", self, e)\n",
                "        self.stopped = True\n",
                "\n",
                "\n",
                "class JsonObject(object):\n",
                "    \"\"\"Generic JSON serializable object for simplified subclassing\"\"\"\n",
                "\n",
                "    def to_json(self, indent=None):\n",
                "        return json.dumps(\n",
                "            self,\n",
                "            default=lambda o: (\n",
                "                (float(o) if o % 1 > 0 else int(o))\n",
                "                if isinstance(o, decimal.Decimal)\n",
                "                else o.__dict__\n",
                "            ),\n",
                "            sort_keys=True,\n",
                "            indent=indent,\n",
                "        )\n",
                "\n",
                "    def apply_json(self, j):\n",
                "        if isinstance(j, str):\n",
                "            j = json.loads(j)\n",
                "        self.__dict__.update(j)\n",
                "\n",
                "    def to_dict(self):\n",
                "        return json.loads(self.to_json())\n",
                "\n",
                "    @classmethod\n",
                "    def from_json(cls, j):\n",
                "        j = JsonObject.as_dict(j)\n",
                "        result = cls()\n",
                "        result.apply_json(j)\n",
                "        return result\n",
                "\n",
                "    @classmethod\n",
                "    def from_json_list(cls, json_list):\n",
                "        return [cls.from_json(j) for j in json_list]\n",
                "\n",
                "    @classmethod\n",
                "    def as_dict(cls, obj):\n",
                "        if isinstance(obj, dict):\n",
                "            return obj\n",
                "        return obj.to_dict()\n",
                "\n",
                "    def __str__(self):\n",
                "        return self.to_json()\n",
                "\n",
                "    def __repr__(self):\n",
                "        return self.__str__()\n",
                "\n",
                "\n",
                "class DelSafeDict(dict):\n",
                "    \"\"\"Useful when applying jsonpatch. Use it as follows:\n",
                "\n",
                "    obj.__dict__ = DelSafeDict(obj.__dict__)\n",
                "    apply_patch(obj.__dict__, patch)\n",
                "    \"\"\"\n",
                "\n",
                "    def __delitem__(self, key, *args, **kwargs):\n",
                "        self[key] = None\n",
                "\n",
                "\n",
                "class CaptureOutput(object):\n",
                "    \"\"\"A context manager that captures stdout/stderr of the current thread. Use it as follows:\n",
                "\n",
                "    with CaptureOutput() as c:\n",
                "        ...\n",
                "    print(c.stdout(), c.stderr())\n",
                "    \"\"\"\n",
                "\n",
                "    orig_stdout = sys.stdout\n",
                "    orig_stderr = sys.stderr\n",
                "    orig___stdout = sys.__stdout__\n",
                "    orig___stderr = sys.__stderr__\n",
                "    CONTEXTS_BY_THREAD = {}\n",
                "\n",
                "    class LogStreamIO(io.StringIO):\n",
                "        def write(self, s):\n",
                "            if isinstance(s, str) and hasattr(s, \"decode\"):\n",
                "                s = s.decode(\"unicode-escape\")\n",
                "            return super(CaptureOutput.LogStreamIO, self).write(s)\n",
                "\n",
                "    def __init__(self):\n",
                "        self._stdout = self.LogStreamIO()\n",
                "        self._stderr = self.LogStreamIO()\n",
                "\n",
                "    def __enter__(self):\n",
                "        # Note: import werkzeug here (not at top of file) to allow dependency pruning\n",
                "        from werkzeug.local import LocalProxy\n",
                "\n",
                "        ident = self._ident()\n",
                "        if ident not in self.CONTEXTS_BY_THREAD:\n",
                "            self.CONTEXTS_BY_THREAD[ident] = self\n",
                "            self._set(\n",
                "                LocalProxy(self._proxy(sys.stdout, \"stdout\")),\n",
                "                LocalProxy(self._proxy(sys.stderr, \"stderr\")),\n",
                "                LocalProxy(self._proxy(sys.__stdout__, \"stdout\")),\n",
                "                LocalProxy(self._proxy(sys.__stderr__, \"stderr\")),\n",
                "            )\n",
                "        return self\n",
                "\n",
                "    def __exit__(self, type, value, traceback):\n",
                "        ident = self._ident()\n",
                "        removed = self.CONTEXTS_BY_THREAD.pop(ident, None)\n",
                "        if not self.CONTEXTS_BY_THREAD:\n",
                "            # reset pointers\n",
                "            self._set(\n",
                "                self.orig_stdout,\n",
                "                self.orig_stderr,\n",
                "                self.orig___stdout,\n",
                "                self.orig___stderr,\n",
                "            )\n",
                "        # get value from streams\n",
                "        removed._stdout.flush()\n",
                "        removed._stderr.flush()\n",
                "        out = removed._stdout.getvalue()\n",
                "        err = removed._stderr.getvalue()\n",
                "        # close handles\n",
                "        removed._stdout.close()\n",
                "        removed._stderr.close()\n",
                "        removed._stdout = out\n",
                "        removed._stderr = err\n",
                "\n",
                "    def _set(self, out, err, __out, __err):\n",
                "        sys.stdout, sys.stderr, sys.__stdout__, sys.__stderr__ = (\n",
                "            out,\n",
                "            err,\n",
                "            __out,\n",
                "            __err,\n",
                "        )\n",
                "\n",
                "    def _proxy(self, original_stream, type):\n",
                "        def proxy():\n",
                "            ident = self._ident()\n",
                "            ctx = self.CONTEXTS_BY_THREAD.get(ident)\n",
                "            if ctx:\n",
                "                return ctx._stdout if type == \"stdout\" else ctx._stderr\n",
                "            return original_stream\n",
                "\n",
                "        return proxy\n",
                "\n",
                "    def _ident(self):\n",
                "        # TODO: On some systems we seem to be running into a stack overflow with LAMBDA_EXECUTOR=local here!\n",
                "        return threading.current_thread().ident\n",
                "\n",
                "    def stdout(self):\n",
                "        return self._stream_value(self._stdout)\n",
                "\n",
                "    def stderr(self):\n",
                "        return self._stream_value(self._stderr)\n",
                "\n",
                "    def _stream_value(self, stream):\n",
                "        return stream.getvalue() if hasattr(stream, \"getvalue\") else stream\n",
                "\n",
                "\n",
                "class ObjectIdHashComparator:\n",
                "    \"\"\"Simple wrapper class that allows us to create a hashset using the object id(..) as the entries' hash value\"\"\"\n",
                "\n",
                "    def __init__(self, obj):\n",
                "        self.obj = obj\n",
                "        self._hash = id(obj)\n",
                "\n",
                "    def __hash__(self):\n",
                "        return self._hash\n",
                "\n",
                "    def __eq__(self, other):\n",
                "        # assumption here is that we're comparing only against ObjectIdHash instances!\n",
                "        return self.obj == other.obj\n",
                "\n",
                "\n",
                "class ArbitraryAccessObj:\n",
                "    \"\"\"Dummy object that can be arbitrarily accessed - any attributes, as a callable, item assignment, ...\"\"\"\n",
                "\n",
                "    def __init__(self, name=None):\n",
                "        self.name = name\n",
                "\n",
                "    def __getattr__(self, name, *args, **kwargs):\n",
                "        return ArbitraryAccessObj(name)\n",
                "\n",
                "    def __call__(self, *args, **kwargs):\n",
                "        if self.name in [\"items\", \"keys\", \"values\"] and not args and not kwargs:\n",
                "            return []\n",
                "        return ArbitraryAccessObj()\n",
                "\n",
                "    def __getitem__(self, *args, **kwargs):\n",
                "        return ArbitraryAccessObj()\n",
                "\n",
                "    def __setitem__(self, *args, **kwargs):\n",
                "        return ArbitraryAccessObj()\n",
                "\n",
                "\n",
                "# ----------------\n",
                "# UTILITY METHODS\n",
                "# ----------------\n",
                "\n",
                "\n",
                "def start_thread(method, *args, **kwargs) -> FuncThread:\n",
                "    \"\"\"Start the given method in a background thread, and add the thread to the TMP_THREADS shutdown hook\"\"\"\n",
                "    _shutdown_hook = kwargs.pop(\"_shutdown_hook\", True)\n",
                "    thread = FuncThread(method, *args, **kwargs)\n",
                "    thread.start()\n",
                "    if _shutdown_hook:\n",
                "        TMP_THREADS.append(thread)\n",
                "    return thread\n",
                "\n",
                "\n",
                "def start_worker_thread(method, *args, **kwargs):\n",
                "    return start_thread(method, *args, _shutdown_hook=False, **kwargs)\n",
                "\n",
                "\n",
                "def empty_context_manager():\n",
                "    import contextlib\n",
                "\n",
                "    return contextlib.nullcontext()\n",
                "\n",
                "\n",
                "def synchronized(lock=None):\n",
                "    \"\"\"\n",
                "    Synchronization decorator as described in\n",
                "    http://blog.dscpl.com.au/2014/01/the-missing-synchronized-decorator.html.\n",
                "    \"\"\"\n",
                "\n",
                "    def _decorator(wrapped):\n",
                "        @functools.wraps(wrapped)\n",
                "        def _wrapper(*args, **kwargs):\n",
                "            with lock:\n",
                "                return wrapped(*args, **kwargs)\n",
                "\n",
                "        return _wrapper\n",
                "\n",
                "    return _decorator\n",
                "\n",
                "\n",
                "def prevent_stack_overflow(match_parameters=False):\n",
                "    \"\"\"Function decorator to protect a function from stack overflows -\n",
                "    raises an exception if a (potential) infinite recursion is detected.\"\"\"\n",
                "\n",
                "    def _decorator(wrapped):\n",
                "        @functools.wraps(wrapped)\n",
                "        def func(*args, **kwargs):\n",
                "            def _matches(frame):\n",
                "                if frame.function != wrapped.__name__:\n",
                "                    return False\n",
                "                frame = frame.frame\n",
                "\n",
                "                if not match_parameters:\n",
                "                    return False\n",
                "\n",
                "                # construct dict of arguments this stack frame has been called with\n",
                "                prev_call_args = {\n",
                "                    frame.f_code.co_varnames[i]: frame.f_locals[frame.f_code.co_varnames[i]]\n",
                "                    for i in range(frame.f_code.co_argcount)\n",
                "                }\n",
                "\n",
                "                # construct dict of arguments the original function has been called with\n",
                "                sig = inspect.signature(wrapped)\n",
                "                this_call_args = dict(zip(sig.parameters.keys(), args))\n",
                "                this_call_args.update(kwargs)\n",
                "\n",
                "                return prev_call_args == this_call_args\n",
                "\n",
                "            matching_frames = [frame[2] for frame in inspect.stack(context=1) if _matches(frame)]\n",
                "            if matching_frames:\n",
                "                raise RecursionError(\"(Potential) infinite recursion detected\")\n",
                "            return wrapped(*args, **kwargs)\n",
                "\n",
                "        return func\n",
                "\n",
                "    return _decorator\n",
                "\n",
                "\n",
                "def is_string(s, include_unicode=True, exclude_binary=False):\n",
                "    if isinstance(s, six.binary_type) and exclude_binary:\n",
                "        return False\n",
                "    if isinstance(s, str):\n",
                "        return True\n",
                "    if include_unicode and isinstance(s, six.text_type):\n",
                "        return True\n",
                "    return False\n",
                "\n",
                "\n",
                "def is_string_or_bytes(s):\n",
                "    return is_string(s) or isinstance(s, six.string_types) or isinstance(s, bytes)\n",
                "\n",
                "\n",
                "def is_base64(s):\n",
                "    regex = r\"^(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?$\"\n",
                "    return is_string(s) and re.match(regex, s)\n",
                "\n",
                "\n",
                "def md5(string: Union[str, bytes]) -> str:\n",
                "    m = hashlib.md5()\n",
                "    m.update(to_bytes(string))\n",
                "    return m.hexdigest()\n",
                "\n",
                "\n",
                "def select_attributes(obj: Dict, attributes: List[str]) -> Dict:\n",
                "    \"\"\"Select a subset of attributes from the given dict (returns a copy)\"\"\"\n",
                "    attributes = attributes if is_list_or_tuple(attributes) else [attributes]\n",
                "    return dict([(k, v) for k, v in obj.items() if k in attributes])\n",
                "\n",
                "\n",
                "def remove_attributes(obj: Dict, attributes: List[str], recursive: bool = False) -> Dict:\n",
                "    \"\"\"Remove a set of attributes from the given dict (in-place)\"\"\"\n",
                "    if recursive:\n",
                "\n",
                "        def _remove(o, **kwargs):\n",
                "            if isinstance(o, dict):\n",
                "                remove_attributes(o, attributes)\n",
                "            return o\n",
                "\n",
                "        return recurse_object(obj, _remove)\n",
                "    attributes = attributes if is_list_or_tuple(attributes) else [attributes]\n",
                "    for attr in attributes:\n",
                "        obj.pop(attr, None)\n",
                "    return obj\n",
                "\n",
                "\n",
                "def rename_attributes(\n",
                "    obj: Dict, old_to_new_attributes: Dict[str, str], in_place: bool = False\n",
                ") -> Dict:\n",
                "    \"\"\"Rename a set of attributes in the given dict object. Second parameter is a dict that maps old to\n",
                "    new attribute names. Default is to return a copy, but can also pass in_place=True.\"\"\"\n",
                "    if not in_place:\n",
                "        obj = dict(obj)\n",
                "    for old_name, new_name in old_to_new_attributes.items():\n",
                "        if old_name in obj:\n",
                "            obj[new_name] = obj.pop(old_name)\n",
                "    return obj\n",
                "\n",
                "\n",
                "def is_list_or_tuple(obj) -> bool:\n",
                "    return isinstance(obj, (list, tuple))\n",
                "\n",
                "\n",
                "def ensure_list(obj: Any, wrap_none=False) -> List:\n",
                "    \"\"\"Wrap the given object in a list, or return the object itself if it already is a list.\"\"\"\n",
                "    if obj is None and not wrap_none:\n",
                "        return obj\n",
                "    return obj if isinstance(obj, list) else [obj]\n",
                "\n",
                "\n",
                "def in_docker() -> bool:\n",
                "    return config.in_docker()\n",
                "\n",
                "\n",
                "def path_from_url(url: str) -> str:\n",
                "    return \"/%s\" % str(url).partition(\"://\")[2].partition(\"/\")[2] if \"://\" in url else url\n",
                "\n",
                "\n",
                "def is_port_open(port_or_url, http_path=None, expect_success=True, protocols=None):\n",
                "    protocols = protocols or [\"tcp\"]\n",
                "    port = port_or_url\n",
                "    if is_number(port):\n",
                "        port = int(port)\n",
                "    host = \"localhost\"\n",
                "    protocol = \"http\"\n",
                "    protocols = protocols if isinstance(protocols, list) else [protocols]\n",
                "    if isinstance(port, six.string_types):\n",
                "        url = urlparse(port_or_url)\n",
                "        port = url.port\n",
                "        host = url.hostname\n",
                "        protocol = url.scheme\n",
                "    nw_protocols = []\n",
                "    nw_protocols += [socket.SOCK_STREAM] if \"tcp\" in protocols else []\n",
                "    nw_protocols += [socket.SOCK_DGRAM] if \"udp\" in protocols else []\n",
                "    for nw_protocol in nw_protocols:\n",
                "        with closing(socket.socket(socket.AF_INET, nw_protocol)) as sock:\n",
                "            sock.settimeout(1)\n",
                "            if nw_protocol == socket.SOCK_DGRAM:\n",
                "                try:\n",
                "                    if port == 53:\n",
                "                        dnshost = \"127.0.0.1\" if host == \"localhost\" else host\n",
                "                        resolver = dns.resolver.Resolver()\n",
                "                        resolver.nameservers = [dnshost]\n",
                "                        resolver.timeout = 1\n",
                "                        resolver.lifetime = 1\n",
                "                        answers = resolver.query(\"google.com\", \"A\")\n",
                "                        assert len(answers) > 0\n",
                "                    else:\n",
                "                        sock.sendto(bytes(), (host, port))\n",
                "                        sock.recvfrom(1024)\n",
                "                except Exception:\n",
                "                    return False\n",
                "            elif nw_protocol == socket.SOCK_STREAM:\n",
                "                result = sock.connect_ex((host, port))\n",
                "                if result != 0:\n",
                "                    return False\n",
                "    if \"tcp\" not in protocols or not http_path:\n",
                "        return True\n",
                "    url = \"%s://%s:%s%s\" % (protocol, host, port, http_path)\n",
                "    try:\n",
                "        response = safe_requests.get(url, verify=False)\n",
                "        return not expect_success or response.status_code < 400\n",
                "    except Exception:\n",
                "        return False\n",
                "\n",
                "\n",
                "def wait_for_port_open(port, http_path=None, expect_success=True, retries=10, sleep_time=0.5):\n",
                "    \"\"\"Ping the given network port until it becomes available (for a given number of retries).\n",
                "    If 'http_path' is set, make a GET request to this path and assert a non-error response.\"\"\"\n",
                "    return wait_for_port_status(\n",
                "        port,\n",
                "        http_path=http_path,\n",
                "        expect_success=expect_success,\n",
                "        retries=retries,\n",
                "        sleep_time=sleep_time,\n",
                "    )\n",
                "\n",
                "\n",
                "def wait_for_port_closed(port, http_path=None, expect_success=True, retries=10, sleep_time=0.5):\n",
                "    return wait_for_port_status(\n",
                "        port,\n",
                "        http_path=http_path,\n",
                "        expect_success=expect_success,\n",
                "        retries=retries,\n",
                "        sleep_time=sleep_time,\n",
                "        expect_closed=True,\n",
                "    )\n",
                "\n",
                "\n",
                "def wait_for_port_status(\n",
                "    port, http_path=None, expect_success=True, retries=10, sleep_time=0.5, expect_closed=False\n",
                "):\n",
                "    \"\"\"Ping the given network port until it becomes (un)available (for a given number of retries).\"\"\"\n",
                "\n",
                "    def check():\n",
                "        status = is_port_open(port, http_path=http_path, expect_success=expect_success)\n",
                "        if bool(status) != (not expect_closed):\n",
                "            raise Exception(\n",
                "                \"Port %s (path: %s) was not %s\"\n",
                "                % (port, http_path, \"closed\" if expect_closed else \"open\")\n",
                "            )\n",
                "\n",
                "    return retry(check, sleep=sleep_time, retries=retries)\n",
                "\n",
                "\n",
                "def port_can_be_bound(port):\n",
                "    \"\"\"Return whether a local port can be bound to. Note that this is a stricter check\n",
                "    than is_port_open(...) above, as is_port_open() may return False if the port is\n",
                "    not accessible (i.e., does not respond), yet cannot be bound to.\"\"\"\n",
                "    try:\n",
                "        tcp = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
                "        tcp.bind((\"\", port))\n",
                "        return True\n",
                "    except Exception:\n",
                "        return False\n",
                "\n",
                "\n",
                "def get_free_tcp_port(blacklist=None):\n",
                "    blacklist = blacklist or []\n",
                "    for i in range(10):\n",
                "        tcp = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
                "        tcp.bind((\"\", 0))\n",
                "        addr, port = tcp.getsockname()\n",
                "        tcp.close()\n",
                "        if port not in blacklist:\n",
                "            return port\n",
                "    raise Exception(\"Unable to determine free TCP port with blacklist %s\" % blacklist)\n",
                "\n",
                "\n",
                "def sleep_forever():\n",
                "    while True:\n",
                "        time.sleep(1)\n",
                "\n",
                "\n",
                "def get_service_protocol():\n",
                "    return \"https\" if config.USE_SSL else \"http\"\n",
                "\n",
                "\n",
                "def edge_ports_info():\n",
                "    if config.EDGE_PORT_HTTP:\n",
                "        result = \"ports %s/%s\" % (config.EDGE_PORT, config.EDGE_PORT_HTTP)\n",
                "    else:\n",
                "        result = \"port %s\" % config.EDGE_PORT\n",
                "    result = \"%s %s\" % (get_service_protocol(), result)\n",
                "    return result\n",
                "\n",
                "\n",
                "def to_unique_items_list(inputs, comparator=None):\n",
                "    \"\"\"Return a list of unique items from the given input iterable.\n",
                "    The comparator(item1, item2) returns True/False or an int for comparison.\"\"\"\n",
                "\n",
                "    def contained(item):\n",
                "        for r in result:\n",
                "            if comparator:\n",
                "                cmp_res = comparator(item, r)\n",
                "                if cmp_res is True or str(cmp_res) == \"0\":\n",
                "                    return True\n",
                "            elif item == r:\n",
                "                return True\n",
                "\n",
                "    result = []\n",
                "    for it in inputs:\n",
                "        if not contained(it):\n",
                "            result.append(it)\n",
                "    return result\n",
                "\n",
                "\n",
                "def timestamp(time=None, format: str = TIMESTAMP_FORMAT) -> str:\n",
                "    if not time:\n",
                "        time = datetime.utcnow()\n",
                "    if isinstance(time, six.integer_types + (float,)):\n",
                "        time = datetime.fromtimestamp(time)\n",
                "    return time.strftime(format)\n",
                "\n",
                "\n",
                "def timestamp_millis(time=None) -> str:\n",
                "    microsecond_time = timestamp(time=time, format=TIMESTAMP_FORMAT_MICROS)\n",
                "    # truncating microseconds to milliseconds, while leaving the \"Z\" indicator\n",
                "    return microsecond_time[:-4] + microsecond_time[-1]\n",
                "\n",
                "\n",
                "def epoch_timestamp() -> float:\n",
                "    return time.time()\n",
                "\n",
                "\n",
                "def parse_timestamp(ts_str: str) -> datetime:\n",
                "    for ts_format in [TIMESTAMP_FORMAT, TIMESTAMP_FORMAT_TZ, TIMESTAMP_FORMAT_MICROS]:\n",
                "        try:\n",
                "            return datetime.strptime(ts_str, ts_format)\n",
                "        except ValueError:\n",
                "            pass\n",
                "    raise Exception(\"Unable to parse timestamp string with any known formats: %s\" % ts_str)\n",
                "\n",
                "\n",
                "def retry(function, retries=3, sleep=1.0, sleep_before=0, **kwargs):\n",
                "    raise_error = None\n",
                "    if sleep_before > 0:\n",
                "        time.sleep(sleep_before)\n",
                "    retries = int(retries)\n",
                "    for i in range(0, retries + 1):\n",
                "        try:\n",
                "            return function(**kwargs)\n",
                "        except Exception as error:\n",
                "            raise_error = error\n",
                "            time.sleep(sleep)\n",
                "    raise raise_error\n",
                "\n",
                "\n",
                "def poll_condition(condition, timeout: float = None, interval: float = 0.5) -> bool:\n",
                "    \"\"\"\n",
                "    Poll evaluates the given condition until a truthy value is returned. It does this every `interval` seconds\n",
                "    (0.5 by default), until the timeout (in seconds, if any) is reached.\n",
                "\n",
                "    Poll returns True once `condition()` returns a truthy value, or False if the timeout is reached.\n",
                "    \"\"\"\n",
                "    remaining = 0\n",
                "    if timeout is not None:\n",
                "        remaining = timeout\n",
                "\n",
                "    while not condition():\n",
                "        if timeout is not None:\n",
                "            remaining -= interval\n",
                "\n",
                "            if remaining <= 0:\n",
                "                return False\n",
                "\n",
                "        time.sleep(interval)\n",
                "\n",
                "    return True\n",
                "\n",
                "\n",
                "def merge_recursive(source, destination, none_values=[None], overwrite=False):\n",
                "    for key, value in source.items():\n",
                "        if isinstance(value, dict):\n",
                "            # get node or create one\n",
                "            node = destination.setdefault(key, {})\n",
                "            merge_recursive(value, node, none_values=none_values, overwrite=overwrite)\n",
                "        else:\n",
                "            if not isinstance(destination, dict):\n",
                "                LOG.warning(\n",
                "                    \"Destination for merging %s=%s is not dict: %s\", key, value, destination\n",
                "                )\n",
                "            if overwrite or destination.get(key) in none_values:\n",
                "                destination[key] = value\n",
                "    return destination\n",
                "\n",
                "\n",
                "def merge_dicts(*dicts, **kwargs):\n",
                "    \"\"\"Merge all dicts in `*dicts` into a single dict, and return the result. If any of the entries\n",
                "    in `*dicts` is None, and `default` is specified as keyword argument, then return `default`.\"\"\"\n",
                "    result = {}\n",
                "    for d in dicts:\n",
                "        if d is None and \"default\" in kwargs:\n",
                "            return kwargs[\"default\"]\n",
                "        if d:\n",
                "            result.update(d)\n",
                "    return result\n",
                "\n",
                "\n",
                "def recurse_object(obj: JsonType, func: Callable, path: str = \"\") -> Any:\n",
                "    \"\"\"Recursively apply `func` to `obj` (may be a list, dict, or other object).\"\"\"\n",
                "    obj = func(obj, path=path)\n",
                "    if isinstance(obj, list):\n",
                "        for i in range(len(obj)):\n",
                "            tmp_path = \"%s[%s]\" % (path or \".\", i)\n",
                "            obj[i] = recurse_object(obj[i], func, tmp_path)\n",
                "    elif isinstance(obj, dict):\n",
                "        for k, v in obj.items():\n",
                "            tmp_path = \"%s%s\" % ((path + \".\") if path else \"\", k)\n",
                "            obj[k] = recurse_object(v, func, tmp_path)\n",
                "    return obj\n",
                "\n",
                "\n",
                "def keys_to_lower(obj: JsonComplexType, skip_children_of: List[str] = None) -> JsonComplexType:\n",
                "    \"\"\"Recursively changes all dict keys to first character lowercase. Skip children\n",
                "    of any elements whose names are contained in skip_children_of (e.g., ['Tags'])\"\"\"\n",
                "    skip_children_of = ensure_list(skip_children_of or [])\n",
                "\n",
                "    def fix_keys(o, path=\"\", **kwargs):\n",
                "        if any([re.match(r\"(^|.*\\.)%s($|[.\\[].*)\" % k, path) for k in skip_children_of]):\n",
                "            return o\n",
                "        if isinstance(o, dict):\n",
                "            for k, v in dict(o).items():\n",
                "                o.pop(k)\n",
                "                o[first_char_to_lower(k)] = v\n",
                "        return o\n",
                "\n",
                "    result = recurse_object(obj, fix_keys)\n",
                "    return result\n",
                "\n",
                "\n",
                "def camel_to_snake_case(string: str) -> str:\n",
                "    return re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", string).replace(\"__\", \"_\").lower()\n",
                "\n",
                "\n",
                "def snake_to_camel_case(string: str, capitalize_first: bool = True) -> str:\n",
                "    components = string.split(\"_\")\n",
                "    start_idx = 0 if capitalize_first else 1\n",
                "    components = [x.title() for x in components[start_idx:]]\n",
                "    return \"\".join(components)\n",
                "\n",
                "\n",
                "def base64_to_hex(b64_string: str) -> bytes:\n",
                "    return binascii.hexlify(base64.b64decode(b64_string))\n",
                "\n",
                "\n",
                "def obj_to_xml(obj: SerializableObj) -> str:\n",
                "    \"\"\"Return an XML representation of the given object (dict, list, or primitive).\n",
                "    Does NOT add a common root element if the given obj is a list.\n",
                "    Does NOT work for nested dict structures.\"\"\"\n",
                "    if isinstance(obj, list):\n",
                "        return \"\".join([obj_to_xml(o) for o in obj])\n",
                "    if isinstance(obj, dict):\n",
                "        return \"\".join([\"<{k}>{v}</{k}>\".format(k=k, v=obj_to_xml(v)) for (k, v) in obj.items()])\n",
                "    return str(obj)\n",
                "\n",
                "\n",
                "def now(millis: bool = False, tz: Optional[tzinfo] = None) -> int:\n",
                "    return mktime(datetime.now(tz=tz), millis=millis)\n",
                "\n",
                "\n",
                "def now_utc(millis: bool = False) -> int:\n",
                "    return now(millis, timezone.utc)\n",
                "\n",
                "\n",
                "def mktime(ts: datetime, millis: bool = False) -> int:\n",
                "    if millis:\n",
                "        return int(ts.timestamp() * 1000)\n",
                "    return int(ts.timestamp())\n",
                "\n",
                "\n",
                "def mkdir(folder: str):\n",
                "    if not os.path.exists(folder):\n",
                "        os.makedirs(folder, exist_ok=True)\n",
                "\n",
                "\n",
                "def is_empty_dir(directory: str, ignore_hidden: bool = False) -> bool:\n",
                "    \"\"\"Return whether the given directory contains any entries (files/folders), including hidden\n",
                "    entries whose name starts with a dot (.), unless ignore_hidden=True is passed.\"\"\"\n",
                "    if not os.path.isdir(directory):\n",
                "        raise Exception(f\"Path is not a directory: {directory}\")\n",
                "    entries = os.listdir(directory)\n",
                "    if ignore_hidden:\n",
                "        entries = [e for e in entries if not e.startswith(\".\")]\n",
                "    return not bool(entries)\n",
                "\n",
                "\n",
                "def ensure_readable(file_path: str, default_perms: int = None):\n",
                "    if default_perms is None:\n",
                "        default_perms = 0o644\n",
                "    try:\n",
                "        with open(file_path, \"rb\"):\n",
                "            pass\n",
                "    except Exception:\n",
                "        LOG.info(\"Updating permissions as file is currently not readable: %s\" % file_path)\n",
                "        os.chmod(file_path, default_perms)\n",
                "\n",
                "\n",
                "def chown_r(path: str, user: str):\n",
                "    \"\"\"Recursive chown on the given file/directory path.\"\"\"\n",
                "    # keep these imports here for Windows compatibility\n",
                "    import grp\n",
                "    import pwd\n",
                "\n",
                "    uid = pwd.getpwnam(user).pw_uid\n",
                "    gid = grp.getgrnam(user).gr_gid\n",
                "    os.chown(path, uid, gid)\n",
                "    for root, dirs, files in os.walk(path):\n",
                "        for dirname in dirs:\n",
                "            os.chown(os.path.join(root, dirname), uid, gid)\n",
                "        for filename in files:\n",
                "            os.chown(os.path.join(root, filename), uid, gid)\n",
                "\n",
                "\n",
                "def chmod_r(path: str, mode: int):\n",
                "    \"\"\"Recursive chmod\"\"\"\n",
                "    if not os.path.exists(path):\n",
                "        return\n",
                "    os.chmod(path, mode)\n",
                "    for root, dirnames, filenames in os.walk(path):\n",
                "        for dirname in dirnames:\n",
                "            os.chmod(os.path.join(root, dirname), mode)\n",
                "        for filename in filenames:\n",
                "            os.chmod(os.path.join(root, filename), mode)\n",
                "\n",
                "\n",
                "def rm_rf(path: str):\n",
                "    \"\"\"\n",
                "    Recursively removes a file or directory\n",
                "    \"\"\"\n",
                "    if not path or not os.path.exists(path):\n",
                "        return\n",
                "    # Running the native command can be an order of magnitude faster in Alpine on Travis-CI\n",
                "    if is_debian():\n",
                "        try:\n",
                "            return run('rm -rf \"%s\"' % path)\n",
                "        except Exception:\n",
                "            pass\n",
                "    # Make sure all files are writeable and dirs executable to remove\n",
                "    try:\n",
                "        chmod_r(path, 0o777)\n",
                "    except PermissionError:\n",
                "        pass  # todo log\n",
                "    # check if the file is either a normal file, or, e.g., a fifo\n",
                "    exists_but_non_dir = os.path.exists(path) and not os.path.isdir(path)\n",
                "    if os.path.isfile(path) or exists_but_non_dir:\n",
                "        os.remove(path)\n",
                "    else:\n",
                "        shutil.rmtree(path)\n",
                "\n",
                "\n",
                "def cp_r(src: str, dst: str, rm_dest_on_conflict=False, ignore_copystat_errors=False, **kwargs):\n",
                "    \"\"\"Recursively copies file/directory\"\"\"\n",
                "    # attention: this patch is not threadsafe\n",
                "    copystat_orig = shutil.copystat\n",
                "    if ignore_copystat_errors:\n",
                "\n",
                "        def _copystat(*args, **kwargs):\n",
                "            try:\n",
                "                return copystat_orig(*args, **kwargs)\n",
                "            except Exception:\n",
                "                pass\n",
                "\n",
                "        shutil.copystat = _copystat\n",
                "    try:\n",
                "        if os.path.isfile(src):\n",
                "            if os.path.isdir(dst):\n",
                "                dst = os.path.join(dst, os.path.basename(src))\n",
                "            return shutil.copyfile(src, dst)\n",
                "        if \"dirs_exist_ok\" in inspect.getfullargspec(shutil.copytree).args:\n",
                "            kwargs[\"dirs_exist_ok\"] = True\n",
                "        try:\n",
                "            return shutil.copytree(src, dst, **kwargs)\n",
                "        except FileExistsError:\n",
                "            if rm_dest_on_conflict:\n",
                "                rm_rf(dst)\n",
                "                return shutil.copytree(src, dst, **kwargs)\n",
                "            raise\n",
                "    except Exception as e:\n",
                "\n",
                "        def _info(_path):\n",
                "            return \"%s (file=%s, symlink=%s)\" % (\n",
                "                _path,\n",
                "                os.path.isfile(_path),\n",
                "                os.path.islink(_path),\n",
                "            )\n",
                "\n",
                "        LOG.debug(\n",
                "            \"Error copying files from %s to %s: %s\"\n",
                "            % (\n",
                "                _info(src),\n",
                "                _info(dst),\n",
                "                e,\n",
                "            )\n",
                "        )\n",
                "        raise\n",
                "    finally:\n",
                "        shutil.copystat = copystat_orig\n",
                "\n",
                "\n",
                "def disk_usage(path: str) -> int:\n",
                "    if not os.path.exists(path):\n",
                "        return 0\n",
                "\n",
                "    if os.path.isfile(path):\n",
                "        return os.path.getsize(path)\n",
                "\n",
                "    total_size = 0\n",
                "    for dirpath, dirnames, filenames in os.walk(path):\n",
                "        for f in filenames:\n",
                "            fp = os.path.join(dirpath, f)\n",
                "            # skip if it is symbolic link\n",
                "            if not os.path.islink(fp):\n",
                "                total_size += os.path.getsize(fp)\n",
                "    return total_size\n",
                "\n",
                "\n",
                "def format_bytes(count: float, default: str = \"n/a\"):\n",
                "    \"\"\"Format a bytes number as a human-readable unit, e.g., 1.3GB or 21.53MB\"\"\"\n",
                "    if not is_number(count):\n",
                "        return default\n",
                "    cnt = float(count)\n",
                "    if cnt < 0:\n",
                "        return default\n",
                "    units = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\")\n",
                "    for unit in units:\n",
                "        if cnt < 1000 or unit == units[-1]:\n",
                "            # FIXME: will return '1e+03TB' for 1000TB\n",
                "            return \"%s%s\" % (format_number(cnt, decimals=3), unit)\n",
                "        cnt = cnt / 1000.0\n",
                "    return count\n",
                "\n",
                "\n",
                "def get_proxies() -> Dict[str, str]:\n",
                "    proxy_map = {}\n",
                "    if config.OUTBOUND_HTTP_PROXY:\n",
                "        proxy_map[\"http\"] = config.OUTBOUND_HTTP_PROXY\n",
                "    if config.OUTBOUND_HTTPS_PROXY:\n",
                "        proxy_map[\"https\"] = config.OUTBOUND_HTTPS_PROXY\n",
                "    return proxy_map\n",
                "\n",
                "\n",
                "def download(url: str, path: str, verify_ssl=True):\n",
                "    \"\"\"Downloads file at url to the given path\"\"\"\n",
                "    # make sure we're creating a new session here to\n",
                "    # enable parallel file downloads during installation!\n",
                "    s = requests.Session()\n",
                "    proxies = get_proxies()\n",
                "    if proxies:\n",
                "        s.proxies.update(proxies)\n",
                "    # Use REQUESTS_CA_BUNDLE path. If it doesn't exist, use the method provided settings.\n",
                "    # Note that a value that is not False, will result to True and will get the bundle file.\n",
                "    r = s.get(url, stream=True, verify=os.getenv(\"REQUESTS_CA_BUNDLE\", verify_ssl))\n",
                "    # check status code before attempting to read body\n",
                "    if r.status_code >= 400:\n",
                "        raise Exception(\"Failed to download %s, response code %s\" % (url, r.status_code))\n",
                "\n",
                "    total = 0\n",
                "    try:\n",
                "        if not os.path.exists(os.path.dirname(path)):\n",
                "            os.makedirs(os.path.dirname(path))\n",
                "        LOG.debug(\n",
                "            \"Starting download from %s to %s (%s bytes)\"\n",
                "            % (url, path, r.headers.get(\"Content-Length\"))\n",
                "        )\n",
                "        with open(path, \"wb\") as f:\n",
                "            iter_length = 0\n",
                "            iter_limit = 1000000  # print a log line for every 1MB chunk\n",
                "            for chunk in r.iter_content(DOWNLOAD_CHUNK_SIZE):\n",
                "                total += len(chunk)\n",
                "                iter_length += len(chunk)\n",
                "                if chunk:  # filter out keep-alive new chunks\n",
                "                    f.write(chunk)\n",
                "                else:\n",
                "                    LOG.debug(\"Empty chunk %s (total %s) from %s\" % (chunk, total, url))\n",
                "                if iter_length >= iter_limit:\n",
                "                    LOG.debug(\"Written %s bytes (total %s) to %s\" % (iter_length, total, path))\n",
                "                    iter_length = 0\n",
                "            f.flush()\n",
                "            os.fsync(f)\n",
                "        if os.path.getsize(path) == 0:\n",
                "            LOG.warning(\"Zero bytes downloaded from %s, retrying\" % url)\n",
                "            download(url, path, verify_ssl)\n",
                "            return\n",
                "        LOG.debug(\n",
                "            \"Done downloading %s, response code %s, total bytes %d\" % (url, r.status_code, total)\n",
                "        )\n",
                "    finally:\n",
                "        r.close()\n",
                "        s.close()\n",
                "\n",
                "\n",
                "def parse_request_data(method: str, path: str, data=None, headers=None) -> Dict:\n",
                "    \"\"\"Extract request data either from query string as well as request body (e.g., for POST).\"\"\"\n",
                "    result = {}\n",
                "    headers = headers or {}\n",
                "    content_type = headers.get(\"Content-Type\", \"\")\n",
                "\n",
                "    # add query params to result\n",
                "    parsed_path = urlparse(path)\n",
                "    result.update(parse_qs(parsed_path.query))\n",
                "\n",
                "    # add params from url-encoded payload\n",
                "    if method in [\"POST\", \"PUT\", \"PATCH\"] and (not content_type or \"form-\" in content_type):\n",
                "        # content-type could be either \"application/x-www-form-urlencoded\" or \"multipart/form-data\"\n",
                "        try:\n",
                "            params = parse_qs(to_str(data or \"\"))\n",
                "            result.update(params)\n",
                "        except Exception:\n",
                "            pass  # probably binary / JSON / non-URL encoded payload - ignore\n",
                "\n",
                "    # select first elements from result lists (this is assuming we are not using parameter lists!)\n",
                "    result = dict([(k, v[0]) for k, v in result.items()])\n",
                "    return result\n",
                "\n",
                "\n",
                "def first_char_to_lower(s: str) -> str:\n",
                "    return s and \"%s%s\" % (s[0].lower(), s[1:])\n",
                "\n",
                "\n",
                "def first_char_to_upper(s: str) -> str:\n",
                "    return s and \"%s%s\" % (s[0].upper(), s[1:])\n",
                "\n",
                "\n",
                "def format_number(number: float, decimals: int = 2):\n",
                "    # Note: interestingly, f\"{number:.3g}\" seems to yield incorrect results in some cases.\n",
                "    # The logic below seems to be the most stable/reliable.\n",
                "    result = f\"{number:.{decimals}f}\"\n",
                "    if \".\" in result:\n",
                "        result = result.rstrip(\"0\").rstrip(\".\")\n",
                "    return result\n",
                "\n",
                "\n",
                "def is_number(s: Any) -> bool:\n",
                "    try:\n",
                "        float(s)  # for int, long and float\n",
                "        return True\n",
                "    except (TypeError, ValueError):\n",
                "        return False\n",
                "\n",
                "\n",
                "def to_number(s: Any) -> Union[int, float]:\n",
                "    \"\"\"Cast the string representation of the given object to a number (int or float), or raise ValueError.\"\"\"\n",
                "    try:\n",
                "        return int(str(s))\n",
                "    except ValueError:\n",
                "        return float(str(s))\n",
                "\n",
                "\n",
                "def is_mac_os() -> bool:\n",
                "    return localstack.utils.run.is_mac_os()\n",
                "\n",
                "\n",
                "def is_linux() -> bool:\n",
                "    return localstack.utils.run.is_linux()\n",
                "\n",
                "\n",
                "def is_windows() -> bool:\n",
                "    return platform.system().lower() == \"windows\"\n",
                "\n",
                "\n",
                "def is_debian() -> bool:\n",
                "    cache_key = \"_is_debian_\"\n",
                "    try:\n",
                "        with MUTEX_CLEAN:\n",
                "            if cache_key not in CACHE:\n",
                "                CACHE[cache_key] = False\n",
                "                if not os.path.exists(\"/etc/issue\"):\n",
                "                    return False\n",
                "                out = to_str(subprocess.check_output([\"cat\", \"/etc/issue\"]))\n",
                "                CACHE[cache_key] = \"Debian\" in out\n",
                "    except subprocess.CalledProcessError:\n",
                "        return False\n",
                "    return CACHE[cache_key]\n",
                "\n",
                "\n",
                "def get_arch() -> str:\n",
                "    \"\"\"\n",
                "    Returns the current machine architecture\n",
                "    :return: \"amd64\" when x86_64, \"arm64\" if aarch64, platform.machine() otherwise\n",
                "    \"\"\"\n",
                "    arch = platform.machine()\n",
                "    if arch == \"x86_64\":\n",
                "        return \"amd64\"\n",
                "    if arch == \"aarch64\":\n",
                "        return \"arm64\"\n",
                "    return arch\n",
                "\n",
                "\n",
                "def get_os() -> str:\n",
                "    if is_mac_os():\n",
                "        return \"osx\"\n",
                "    if is_linux():\n",
                "        return \"linux\"\n",
                "    if is_windows():\n",
                "        return \"windows\"\n",
                "    raise Exception(\"Unable to determine local operating system\")\n",
                "\n",
                "\n",
                "def is_command_available(cmd: str) -> bool:\n",
                "    try:\n",
                "        run(\"which %s\" % cmd, print_error=False)\n",
                "        return True\n",
                "    except Exception:\n",
                "        return False\n",
                "\n",
                "\n",
                "def short_uid() -> str:\n",
                "    return str(uuid.uuid4())[0:8]\n",
                "\n",
                "\n",
                "def long_uid() -> str:\n",
                "    return str(uuid.uuid4())\n",
                "\n",
                "\n",
                "def parse_json_or_yaml(markup: str) -> JsonComplexType:\n",
                "    import yaml  # leave import here, to avoid breaking our Lambda tests!\n",
                "\n",
                "    try:\n",
                "        return json.loads(markup)\n",
                "    except Exception:\n",
                "        try:\n",
                "            return clone_safe(yaml.safe_load(markup))\n",
                "        except Exception:\n",
                "            try:\n",
                "                return clone_safe(yaml.load(markup, Loader=yaml.SafeLoader))\n",
                "            except Exception:\n",
                "                raise\n",
                "\n",
                "\n",
                "def json_safe(item: JsonType) -> JsonType:\n",
                "    \"\"\"Return a copy of the given object (e.g., dict) that is safe for JSON dumping\"\"\"\n",
                "    try:\n",
                "        return json.loads(json.dumps(item, cls=CustomEncoder))\n",
                "    except Exception:\n",
                "        item = fix_json_keys(item)\n",
                "        return json.loads(json.dumps(item, cls=CustomEncoder))\n",
                "\n",
                "\n",
                "def fix_json_keys(item: JsonType):\n",
                "    \"\"\"make sure the keys of a JSON are strings (not binary type or other)\"\"\"\n",
                "    item_copy = item\n",
                "    if isinstance(item, list):\n",
                "        item_copy = []\n",
                "        for i in item:\n",
                "            item_copy.append(fix_json_keys(i))\n",
                "    if isinstance(item, dict):\n",
                "        item_copy = {}\n",
                "        for k, v in item.items():\n",
                "            item_copy[to_str(k)] = fix_json_keys(v)\n",
                "    return item_copy\n",
                "\n",
                "\n",
                "def canonical_json(obj):\n",
                "    return json.dumps(obj, sort_keys=True)\n",
                "\n",
                "\n",
                "def extract_jsonpath(value, path):\n",
                "    from jsonpath_rw import parse\n",
                "\n",
                "    jsonpath_expr = parse(path)\n",
                "    result = [match.value for match in jsonpath_expr.find(value)]\n",
                "    result = result[0] if len(result) == 1 else result\n",
                "    return result\n",
                "\n",
                "\n",
                "def assign_to_path(target, path: str, value, delimiter: str = \".\"):\n",
                "    parts = path.strip(delimiter).split(delimiter)\n",
                "    path_to_parent = delimiter.join(parts[:-1])\n",
                "    parent = extract_from_jsonpointer_path(target, path_to_parent, auto_create=True)\n",
                "    if not isinstance(parent, dict):\n",
                "        LOG.debug(\n",
                "            'Unable to find parent (type %s) for path \"%s\" in object: %s'\n",
                "            % (type(parent), path, target)\n",
                "        )\n",
                "        return\n",
                "    path_end = int(parts[-1]) if is_number(parts[-1]) else parts[-1]\n",
                "    parent[path_end] = value\n",
                "    return target\n",
                "\n",
                "\n",
                "def extract_from_jsonpointer_path(target, path: str, delimiter: str = \"/\", auto_create=False):\n",
                "    parts = path.strip(delimiter).split(delimiter)\n",
                "    for part in parts:\n",
                "        path_part = int(part) if is_number(part) else part\n",
                "        if isinstance(target, list) and not is_number(path_part):\n",
                "            if path_part == \"-\":\n",
                "                # special case where path is like /path/to/list/- where \"/-\" means \"append to list\"\n",
                "                continue\n",
                "            LOG.warning(\n",
                "                'Attempting to extract non-int index \"%s\" from list: %s' % (path_part, target)\n",
                "            )\n",
                "            return None\n",
                "        target_new = target[path_part] if isinstance(target, list) else target.get(path_part)\n",
                "        if target_new is None:\n",
                "            if not auto_create:\n",
                "                return\n",
                "            target[path_part] = target_new = {}\n",
                "        target = target_new\n",
                "    return target\n",
                "\n",
                "\n",
                "def save_file(file, content, append=False, permissions=None):\n",
                "    mode = \"a\" if append else \"w+\"\n",
                "    if not isinstance(content, six.string_types):\n",
                "        mode = mode + \"b\"\n",
                "\n",
                "    def _opener(path, flags):\n",
                "        return os.open(path, flags, permissions)\n",
                "\n",
                "    # make sure that the parent dir exsits\n",
                "    mkdir(os.path.dirname(file))\n",
                "    # store file contents\n",
                "    with open(file, mode, opener=_opener if permissions else None) as f:\n",
                "        f.write(content)\n",
                "        f.flush()\n",
                "\n",
                "\n",
                "def load_file(file_path, default=None, mode=None):\n",
                "    if not os.path.isfile(file_path):\n",
                "        return default\n",
                "    if not mode:\n",
                "        mode = \"r\"\n",
                "    with open(file_path, mode) as f:\n",
                "        result = f.read()\n",
                "    return result\n",
                "\n",
                "\n",
                "def get_or_create_file(file_path, content=None, permissions=None):\n",
                "    if os.path.exists(file_path):\n",
                "        return load_file(file_path)\n",
                "    content = \"{}\" if content is None else content\n",
                "    try:\n",
                "        save_file(file_path, content, permissions=permissions)\n",
                "        return content\n",
                "    except Exception:\n",
                "        pass\n",
                "\n",
                "\n",
                "def replace_in_file(search, replace, file_path):\n",
                "    \"\"\"Replace all occurrences of `search` with `replace` in the given file (overwrites in place!)\"\"\"\n",
                "    content = load_file(file_path) or \"\"\n",
                "    content_new = content.replace(search, replace)\n",
                "    if content != content_new:\n",
                "        save_file(file_path, content_new)\n",
                "\n",
                "\n",
                "def to_str(obj: Union[str, bytes], encoding: str = DEFAULT_ENCODING, errors=\"strict\") -> str:\n",
                "    \"\"\"If ``obj`` is an instance of ``binary_type``, return\n",
                "    ``obj.decode(encoding, errors)``, otherwise return ``obj``\"\"\"\n",
                "    return obj.decode(encoding, errors) if isinstance(obj, six.binary_type) else obj\n",
                "\n",
                "\n",
                "def to_bytes(obj: Union[str, bytes], encoding: str = DEFAULT_ENCODING, errors=\"strict\") -> bytes:\n",
                "    \"\"\"If ``obj`` is an instance of ``text_type``, return\n",
                "    ``obj.encode(encoding, errors)``, otherwise return ``obj``\"\"\"\n",
                "    return obj.encode(encoding, errors) if isinstance(obj, six.text_type) else obj\n",
                "\n",
                "\n",
                "def str_to_bool(value):\n",
                "    \"\"\"Return the boolean value of the given string, or the verbatim value if it is not a string\"\"\"\n",
                "    true_strings = [\"true\", \"True\"]\n",
                "    if isinstance(value, str):\n",
                "        return value in true_strings\n",
                "    return value\n",
                "\n",
                "\n",
                "def str_insert(string, index, content):\n",
                "    \"\"\"Insert a substring into an existing string at a certain index.\"\"\"\n",
                "    return \"%s%s%s\" % (string[:index], content, string[index:])\n",
                "\n",
                "\n",
                "def str_remove(string, index, end_index=None):\n",
                "    \"\"\"Remove a substring from an existing string at a certain from-to index range.\"\"\"\n",
                "    end_index = end_index or (index + 1)\n",
                "    return \"%s%s\" % (string[:index], string[end_index:])\n",
                "\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "def str_startswith_ignore_case(value: str, prefix: str) -> bool:\n",
                    "    return value[: len(prefix)].lower() == prefix.lower()\n",
                    "\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 1444,
                    "end": 1444
                },
                "child_version_range": {
                    "start": 1444,
                    "end": 1448
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 6,
                "hunk_diff": "File: localstack/utils/common.py\nCode:\n  ...\n1441 1441        return \"%s%s\" % (string[:index], string[end_index:])\n1442 1442    \n1443 1443    \n     1444  + def str_startswith_ignore_case(value: str, prefix: str) -> bool:\n     1445  +     return value[: len(prefix)].lower() == prefix.lower()\n     1446  + \n     1447  + \n1444 1448    def last_index_of(array, value):\n1445 1449        \"\"\"Return the last index of `value` in the given list, or -1 if it does not exist.\"\"\"\n1446 1450        result = -1\n           ...\n",
                "file_path": "localstack/utils/common.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "bool",
                    "len",
                    "lower",
                    "prefix",
                    "str",
                    "str_startswith_ignore_case",
                    "value"
                ],
                "prefix": [
                    "    return \"%s%s\" % (string[:index], string[end_index:])\n",
                    "\n",
                    "\n"
                ],
                "suffix": [
                    "def last_index_of(array, value):\n",
                    "    \"\"\"Return the last index of `value` in the given list, or -1 if it does not exist.\"\"\"\n",
                    "    result = -1\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "str_startswith_ignore_case",
                            "position": {
                                "start": {
                                    "line": 1444,
                                    "column": 4
                                },
                                "end": {
                                    "line": 1444,
                                    "column": 30
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/common.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "str_startswith_ignore_case",
                            "position": {
                                "start": {
                                    "line": 1444,
                                    "column": 4
                                },
                                "end": {
                                    "line": 1444,
                                    "column": 30
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/localstack/localstack/utils/common.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "def last_index_of(array, value):\n",
                "    \"\"\"Return the last index of `value` in the given list, or -1 if it does not exist.\"\"\"\n",
                "    result = -1\n",
                "    for i in reversed(range(len(array))):\n",
                "        entry = array[i]\n",
                "        if entry == value or (callable(value) and value(entry)):\n",
                "            return i\n",
                "    return result\n",
                "\n",
                "\n",
                "def is_sub_dict(child_dict: Dict, parent_dict: Dict) -> bool:\n",
                "    \"\"\"Returns whether the first dict is a sub-dict (subset) of the second dict.\"\"\"\n",
                "    return all(parent_dict.get(key) == val for key, val in child_dict.items())\n",
                "\n",
                "\n",
                "def not_none_or(value: Any, alternative: Any) -> Any:\n",
                "    \"\"\"Return 'value' if it is not None, or 'alternative' otherwise.\"\"\"\n",
                "    return value if value is not None else alternative\n",
                "\n",
                "\n",
                "def cleanup(files=True, env=ENV_DEV, quiet=True):\n",
                "    if files:\n",
                "        cleanup_tmp_files()\n",
                "\n",
                "\n",
                "def cleanup_threads_and_processes(quiet=True):\n",
                "    for thread in TMP_THREADS:\n",
                "        if thread:\n",
                "            try:\n",
                "                # LOG.debug('[shutdown] Cleaning up thread: %s', thread)\n",
                "                if hasattr(thread, \"shutdown\"):\n",
                "                    thread.shutdown()\n",
                "                    continue\n",
                "                if hasattr(thread, \"kill\"):\n",
                "                    thread.kill()\n",
                "                    continue\n",
                "                thread.stop(quiet=quiet)\n",
                "            except Exception as e:\n",
                "                print(e)\n",
                "    for proc in TMP_PROCESSES:\n",
                "        try:\n",
                "            # LOG.debug('[shutdown] Cleaning up process: %s', proc)\n",
                "            kill_process_tree(proc.pid)\n",
                "            # proc.terminate()\n",
                "        except Exception as e:\n",
                "            print(e)\n",
                "    # clean up async tasks\n",
                "    try:\n",
                "        import asyncio\n",
                "\n",
                "        for task in asyncio.all_tasks():\n",
                "            try:\n",
                "                # LOG.debug('[shutdown] Canceling asyncio task: %s', task)\n",
                "                task.cancel()\n",
                "            except Exception as e:\n",
                "                print(e)\n",
                "    except Exception:\n",
                "        pass\n",
                "    LOG.debug(\"[shutdown] Done cleaning up threads / processes / tasks\")\n",
                "    # clear lists\n",
                "    TMP_THREADS.clear()\n",
                "    TMP_PROCESSES.clear()\n",
                "\n",
                "\n",
                "def kill_process_tree(parent_pid):\n",
                "    # Note: Do NOT import \"psutil\" at the root scope\n",
                "    import psutil\n",
                "\n",
                "    parent_pid = getattr(parent_pid, \"pid\", None) or parent_pid\n",
                "    parent = psutil.Process(parent_pid)\n",
                "    for child in parent.children(recursive=True):\n",
                "        try:\n",
                "            child.kill()\n",
                "        except Exception:\n",
                "            pass\n",
                "    parent.kill()\n",
                "\n",
                "\n",
                "def items_equivalent(list1, list2, comparator):\n",
                "    \"\"\"Returns whether two lists are equivalent (i.e., same items contained in both lists,\n",
                "    irrespective of the items' order) with respect to a comparator function.\"\"\"\n",
                "\n",
                "    def contained(item):\n",
                "        for _item in list2:\n",
                "            if comparator(item, _item):\n",
                "                return True\n",
                "\n",
                "    if len(list1) != len(list2):\n",
                "        return False\n",
                "    for item in list1:\n",
                "        if not contained(item):\n",
                "            return False\n",
                "    return True\n",
                "\n",
                "\n",
                "def cleanup_tmp_files():\n",
                "    for tmp in TMP_FILES:\n",
                "        try:\n",
                "            rm_rf(tmp)\n",
                "        except Exception:\n",
                "            pass  # file likely doesn't exist, or permission denied\n",
                "    del TMP_FILES[:]\n",
                "\n",
                "\n",
                "def new_tmp_file() -> str:\n",
                "    \"\"\"Return a path to a new temporary file.\"\"\"\n",
                "    tmp_file, tmp_path = tempfile.mkstemp()\n",
                "    os.close(tmp_file)\n",
                "    TMP_FILES.append(tmp_path)\n",
                "    return tmp_path\n",
                "\n",
                "\n",
                "def new_tmp_dir():\n",
                "    folder = new_tmp_file()\n",
                "    rm_rf(folder)\n",
                "    mkdir(folder)\n",
                "    return folder\n",
                "\n",
                "\n",
                "def is_ip_address(addr):\n",
                "    try:\n",
                "        socket.inet_aton(addr)\n",
                "        return True\n",
                "    except socket.error:\n",
                "        return False\n",
                "\n",
                "\n",
                "def is_zip_file(content):\n",
                "    stream = io.BytesIO(content)\n",
                "    return zipfile.is_zipfile(stream)\n",
                "\n",
                "\n",
                "def unzip(path, target_dir, overwrite=True):\n",
                "    is_in_debian = is_debian()\n",
                "    if is_in_debian:\n",
                "        # Running the native command can be an order of magnitude faster in Alpine on Travis-CI\n",
                "        flags = \"-o\" if overwrite else \"\"\n",
                "        flags += \" -q\"\n",
                "        try:\n",
                "            return run(\"cd %s; unzip %s %s\" % (target_dir, flags, path), print_error=False)\n",
                "        except Exception as e:\n",
                "            error_str = truncate(str(e), max_length=200)\n",
                "            LOG.info(\n",
                "                'Unable to use native \"unzip\" command (using fallback mechanism): %s' % error_str\n",
                "            )\n",
                "\n",
                "    try:\n",
                "        zip_ref = zipfile.ZipFile(path, \"r\")\n",
                "    except Exception as e:\n",
                "        LOG.warning(\"Unable to open zip file: %s: %s\" % (path, e))\n",
                "        raise e\n",
                "\n",
                "    def _unzip_file_entry(zip_ref, file_entry, target_dir):\n",
                "        \"\"\"Extracts a Zipfile entry and preserves permissions\"\"\"\n",
                "        out_path = os.path.join(target_dir, file_entry.filename)\n",
                "        if is_in_debian and os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n",
                "            # this can happen under certain circumstances if the native \"unzip\" command\n",
                "            # fails with a non-zero exit code, yet manages to extract parts of the zip file\n",
                "            return\n",
                "        zip_ref.extract(file_entry.filename, path=target_dir)\n",
                "        perm = file_entry.external_attr >> 16\n",
                "        # Make sure to preserve file permissions in the zip file\n",
                "        # https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module\n",
                "        os.chmod(out_path, perm or 0o777)\n",
                "\n",
                "    try:\n",
                "        for file_entry in zip_ref.infolist():\n",
                "            _unzip_file_entry(zip_ref, file_entry, target_dir)\n",
                "    finally:\n",
                "        zip_ref.close()\n",
                "\n",
                "\n",
                "def untar(path, target_dir):\n",
                "    mode = \"r:gz\" if path.endswith(\"gz\") else \"r\"\n",
                "    with tarfile.open(path, mode) as tar:\n",
                "        tar.extractall(path=target_dir)\n",
                "\n",
                "\n",
                "def is_root():\n",
                "    return get_os_user() == \"root\"\n",
                "\n",
                "\n",
                "def get_os_user():\n",
                "    global CACHED_USER\n",
                "    if not CACHED_USER:\n",
                "        # TODO: using getpass.getuser() seems to be reporting a different/invalid user in Docker/MacOS\n",
                "        # import getpass\n",
                "        # CACHED_USER = getpass.getuser()\n",
                "        CACHED_USER = run(\"whoami\").strip()\n",
                "    return CACHED_USER\n",
                "\n",
                "\n",
                "def cleanup_resources():\n",
                "    cleanup_tmp_files()\n",
                "    cleanup_threads_and_processes()\n",
                "\n",
                "\n",
                "@synchronized(lock=SSL_CERT_LOCK)\n",
                "def generate_ssl_cert(\n",
                "    target_file=None,\n",
                "    overwrite=False,\n",
                "    random=False,\n",
                "    return_content=False,\n",
                "    serial_number=None,\n",
                "):\n",
                "    # Note: Do NOT import \"OpenSSL\" at the root scope\n",
                "    # (Our test Lambdas are importing this file but don't have the module installed)\n",
                "    from OpenSSL import crypto\n",
                "\n",
                "    def all_exist(*files):\n",
                "        return all([os.path.exists(f) for f in files])\n",
                "\n",
                "    def store_cert_key_files(base_filename):\n",
                "        key_file_name = \"%s.key\" % base_filename\n",
                "        cert_file_name = \"%s.crt\" % base_filename\n",
                "        # TODO: Cleaner code to load the cert dinamically\n",
                "        # extract key and cert from target_file and store into separate files\n",
                "        content = load_file(target_file)\n",
                "        key_start = re.search(r\"-----BEGIN(.*)PRIVATE KEY-----\", content)\n",
                "        key_start = key_start.group(0)\n",
                "        key_end = re.search(r\"-----END(.*)PRIVATE KEY-----\", content)\n",
                "        key_end = key_end.group(0)\n",
                "        cert_start = \"-----BEGIN CERTIFICATE-----\"\n",
                "        cert_end = \"-----END CERTIFICATE-----\"\n",
                "        key_content = content[content.index(key_start) : content.index(key_end) + len(key_end)]\n",
                "        cert_content = content[content.index(cert_start) : content.rindex(cert_end) + len(cert_end)]\n",
                "        save_file(key_file_name, key_content)\n",
                "        save_file(cert_file_name, cert_content)\n",
                "        return cert_file_name, key_file_name\n",
                "\n",
                "    if target_file and not overwrite and os.path.exists(target_file):\n",
                "        try:\n",
                "            cert_file_name, key_file_name = store_cert_key_files(target_file)\n",
                "        except Exception as e:\n",
                "            # fall back to temporary files if we cannot store/overwrite the files above\n",
                "            LOG.info(\n",
                "                \"Error storing key/cert SSL files (falling back to random tmp file names): %s\" % e\n",
                "            )\n",
                "            target_file_tmp = new_tmp_file()\n",
                "            cert_file_name, key_file_name = store_cert_key_files(target_file_tmp)\n",
                "        if all_exist(cert_file_name, key_file_name):\n",
                "            return target_file, cert_file_name, key_file_name\n",
                "    if random and target_file:\n",
                "        if \".\" in target_file:\n",
                "            target_file = target_file.replace(\".\", \".%s.\" % short_uid(), 1)\n",
                "        else:\n",
                "            target_file = \"%s.%s\" % (target_file, short_uid())\n",
                "\n",
                "    # create a key pair\n",
                "    k = crypto.PKey()\n",
                "    k.generate_key(crypto.TYPE_RSA, 2048)\n",
                "\n",
                "    # create a self-signed cert\n",
                "    cert = crypto.X509()\n",
                "    subj = cert.get_subject()\n",
                "    subj.C = \"AU\"\n",
                "    subj.ST = \"Some-State\"\n",
                "    subj.L = \"Some-Locality\"\n",
                "    subj.O = \"LocalStack Org\"  # noqa\n",
                "    subj.OU = \"Testing\"\n",
                "    subj.CN = \"localhost\"\n",
                "    # Note: new requirements for recent OSX versions: https://support.apple.com/en-us/HT210176\n",
                "    # More details: https://www.iol.unh.edu/blog/2019/10/10/macos-catalina-and-chrome-trust\n",
                "    serial_number = serial_number or 1001\n",
                "    cert.set_version(2)\n",
                "    cert.set_serial_number(serial_number)\n",
                "    cert.gmtime_adj_notBefore(0)\n",
                "    cert.gmtime_adj_notAfter(2 * 365 * 24 * 60 * 60)\n",
                "    cert.set_issuer(cert.get_subject())\n",
                "    cert.set_pubkey(k)\n",
                "    alt_names = (\n",
                "        b\"DNS:localhost,DNS:test.localhost.atlassian.io,DNS:localhost.localstack.cloud,IP:127.0.0.1\"\n",
                "    )\n",
                "    cert.add_extensions(\n",
                "        [\n",
                "            crypto.X509Extension(b\"subjectAltName\", False, alt_names),\n",
                "            crypto.X509Extension(b\"basicConstraints\", True, b\"CA:false\"),\n",
                "            crypto.X509Extension(\n",
                "                b\"keyUsage\", True, b\"nonRepudiation,digitalSignature,keyEncipherment\"\n",
                "            ),\n",
                "            crypto.X509Extension(b\"extendedKeyUsage\", True, b\"serverAuth\"),\n",
                "        ]\n",
                "    )\n",
                "    cert.sign(k, \"SHA256\")\n",
                "\n",
                "    cert_file = io.StringIO()\n",
                "    key_file = io.StringIO()\n",
                "    cert_file.write(to_str(crypto.dump_certificate(crypto.FILETYPE_PEM, cert)))\n",
                "    key_file.write(to_str(crypto.dump_privatekey(crypto.FILETYPE_PEM, k)))\n",
                "    cert_file_content = cert_file.getvalue().strip()\n",
                "    key_file_content = key_file.getvalue().strip()\n",
                "    file_content = \"%s\\n%s\" % (key_file_content, cert_file_content)\n",
                "    if target_file:\n",
                "        key_file_name = \"%s.key\" % target_file\n",
                "        cert_file_name = \"%s.crt\" % target_file\n",
                "        # check existence to avoid permission denied issues:\n",
                "        # https://github.com/localstack/localstack/issues/1607\n",
                "        if not all_exist(target_file, key_file_name, cert_file_name):\n",
                "            for i in range(2):\n",
                "                try:\n",
                "                    save_file(target_file, file_content)\n",
                "                    save_file(key_file_name, key_file_content)\n",
                "                    save_file(cert_file_name, cert_file_content)\n",
                "                    break\n",
                "                except Exception as e:\n",
                "                    if i > 0:\n",
                "                        raise\n",
                "                    LOG.info(\n",
                "                        \"Unable to store certificate file under %s, using tmp file instead: %s\"\n",
                "                        % (target_file, e)\n",
                "                    )\n",
                "                    # Fix for https://github.com/localstack/localstack/issues/1743\n",
                "                    target_file = \"%s.pem\" % new_tmp_file()\n",
                "                    key_file_name = \"%s.key\" % target_file\n",
                "                    cert_file_name = \"%s.crt\" % target_file\n",
                "            TMP_FILES.append(target_file)\n",
                "            TMP_FILES.append(key_file_name)\n",
                "            TMP_FILES.append(cert_file_name)\n",
                "        if not return_content:\n",
                "            return target_file, cert_file_name, key_file_name\n",
                "    return file_content\n",
                "\n",
                "\n",
                "def call_safe(\n",
                "    func: Callable, args: Tuple = None, kwargs: Dict = None, exception_message: str = None\n",
                ") -> Optional[Any]:\n",
                "    \"\"\"\n",
                "    Call the given function with the given arguments, and if it fails, log the given exception_message.\n",
                "    If logging.DEBUG is set for the logger, then we also log the traceback.\n",
                "\n",
                "    :param func: function to call\n",
                "    :param args: arguments to pass\n",
                "    :param kwargs: keyword arguments to pass\n",
                "    :param exception_message: message to log on exception\n",
                "    :return: whatever the func returns\n",
                "    \"\"\"\n",
                "    if exception_message is None:\n",
                "        exception_message = \"error calling function %s\" % func.__name__\n",
                "    if args is None:\n",
                "        args = ()\n",
                "    if kwargs is None:\n",
                "        kwargs = {}\n",
                "\n",
                "    try:\n",
                "        return func(*args, **kwargs)\n",
                "    except Exception as e:\n",
                "        if LOG.isEnabledFor(logging.DEBUG):\n",
                "            LOG.exception(exception_message)\n",
                "        else:\n",
                "            LOG.warning(\"%s: %s\", exception_message, e)\n",
                "\n",
                "\n",
                "def run_safe(_python_lambda, *args, _default=None, **kwargs):\n",
                "    print_error = kwargs.get(\"print_error\", False)\n",
                "    try:\n",
                "        return _python_lambda(*args, **kwargs)\n",
                "    except Exception as e:\n",
                "        if print_error:\n",
                "            LOG.warning(\"Unable to execute function: %s\" % e)\n",
                "        return _default\n",
                "\n",
                "\n",
                "def run_cmd_safe(**kwargs):\n",
                "    return run_safe(run, print_error=False, **kwargs)\n",
                "\n",
                "\n",
                "def run_for_max_seconds(max_secs, _function, *args, **kwargs):\n",
                "    \"\"\"Run the given function for a maximum of `max_secs` seconds - continue running\n",
                "    in a background thread if the function does not finish in time.\"\"\"\n",
                "\n",
                "    def _worker(*_args):\n",
                "        try:\n",
                "            fn_result = _function(*args, **kwargs)\n",
                "        except Exception as e:\n",
                "            fn_result = e\n",
                "\n",
                "        fn_result = True if fn_result is None else fn_result\n",
                "        q.put(fn_result)\n",
                "        return fn_result\n",
                "\n",
                "    start = now()\n",
                "    q = Queue()\n",
                "    start_worker_thread(_worker)\n",
                "    for i in range(max_secs * 2):\n",
                "        result = None\n",
                "        try:\n",
                "            result = q.get_nowait()\n",
                "        except Exception:\n",
                "            pass\n",
                "        if result is not None:\n",
                "            if isinstance(result, Exception):\n",
                "                raise result\n",
                "            return result\n",
                "        if now() - start >= max_secs:\n",
                "            return\n",
                "        time.sleep(0.5)\n",
                "\n",
                "\n",
                "def do_run(cmd: str, run_cmd: Callable, cache_duration_secs: float):\n",
                "    if cache_duration_secs <= 0:\n",
                "        return run_cmd()\n",
                "\n",
                "    hashcode = md5(cmd)\n",
                "    cache_file = CACHE_FILE_PATTERN.replace(\"*\", hashcode)\n",
                "    mkdir(os.path.dirname(CACHE_FILE_PATTERN))\n",
                "    if os.path.isfile(cache_file):\n",
                "        # check file age\n",
                "        mod_time = os.path.getmtime(cache_file)\n",
                "        time_now = time.time()\n",
                "        if mod_time > (time_now - cache_duration_secs):\n",
                "            with open(cache_file) as fd:\n",
                "                return fd.read()\n",
                "    result = run_cmd()\n",
                "    with open(cache_file, \"w+\") as fd:\n",
                "        fd.write(result)\n",
                "    clean_cache()\n",
                "    return result\n",
                "\n",
                "\n",
                "def run(\n",
                "    cmd: Union[str, List[str]], cache_duration_secs=0, **kwargs\n",
                ") -> Union[str, subprocess.Popen]:\n",
                "    # TODO: should be unified and replaced with safe_run(..) over time! (allowing only lists for cmd parameter)\n",
                "    def run_cmd():\n",
                "        return localstack.utils.run.run(cmd, **kwargs)\n",
                "\n",
                "    return do_run(cmd, run_cmd, cache_duration_secs)\n",
                "\n",
                "\n",
                "def safe_run(cmd: List[str], cache_duration_secs=0, **kwargs) -> Union[str, subprocess.Popen]:\n",
                "    def run_cmd():\n",
                "        return localstack.utils.run.run(cmd, shell=False, **kwargs)\n",
                "\n",
                "    return do_run(\" \".join(cmd), run_cmd, cache_duration_secs)\n",
                "\n",
                "\n",
                "def clone(item):\n",
                "    return json.loads(json.dumps(item))\n",
                "\n",
                "\n",
                "def clone_safe(item):\n",
                "    return clone(json_safe(item))\n",
                "\n",
                "\n",
                "class NetrcBypassAuth(requests.auth.AuthBase):\n",
                "    def __call__(self, r):\n",
                "        return r\n",
                "\n",
                "\n",
                "class _RequestsSafe(type):\n",
                "    \"\"\"Wrapper around requests library, which can prevent it from verifying\n",
                "    SSL certificates or reading credentials from ~/.netrc file\"\"\"\n",
                "\n",
                "    verify_ssl = True\n",
                "\n",
                "    def __getattr__(self, name):\n",
                "        method = requests.__dict__.get(name.lower())\n",
                "        if not method:\n",
                "            return method\n",
                "\n",
                "        def _wrapper(*args, **kwargs):\n",
                "            if \"auth\" not in kwargs:\n",
                "                kwargs[\"auth\"] = NetrcBypassAuth()\n",
                "            url = kwargs.get(\"url\") or (args[1] if name == \"request\" else args[0])\n",
                "            if not self.verify_ssl and url.startswith(\"https://\") and \"verify\" not in kwargs:\n",
                "                kwargs[\"verify\"] = False\n",
                "            return method(*args, **kwargs)\n",
                "\n",
                "        return _wrapper\n",
                "\n",
                "\n",
                "# create class-of-a-class\n",
                "class safe_requests(six.with_metaclass(_RequestsSafe)):\n",
                "    pass\n",
                "\n",
                "\n",
                "def make_http_request(\n",
                "    url: str, data: Union[bytes, str] = None, headers: Dict[str, str] = None, method: str = \"GET\"\n",
                ") -> Response:\n",
                "    return requests.request(\n",
                "        url=url, method=method, headers=headers, data=data, auth=NetrcBypassAuth(), verify=False\n",
                "    )\n",
                "\n",
                "\n",
                "class SafeStringIO(io.StringIO):\n",
                "    \"\"\"Safe StringIO implementation that doesn't fail if str is passed in Python 2.\"\"\"\n",
                "\n",
                "    def write(self, obj):\n",
                "        if six.PY2 and isinstance(obj, str):\n",
                "            obj = obj.decode(\"unicode-escape\")\n",
                "        return super(SafeStringIO, self).write(obj)\n",
                "\n",
                "\n",
                "def clean_cache(file_pattern=CACHE_FILE_PATTERN, last_clean_time=None, max_age=CACHE_MAX_AGE):\n",
                "    if last_clean_time is None:\n",
                "        last_clean_time = last_cache_clean_time\n",
                "\n",
                "    with MUTEX_CLEAN:\n",
                "        time_now = now()\n",
                "        if last_clean_time[\"time\"] > time_now - CACHE_CLEAN_TIMEOUT:\n",
                "            return\n",
                "        for cache_file in set(glob.glob(file_pattern)):\n",
                "            mod_time = os.path.getmtime(cache_file)\n",
                "            if time_now > mod_time + max_age:\n",
                "                rm_rf(cache_file)\n",
                "        last_clean_time[\"time\"] = time_now\n",
                "    return time_now\n",
                "\n",
                "\n",
                "def truncate(data: str, max_length: int = 100) -> str:\n",
                "    data = str(data or \"\")\n",
                "    return (\"%s...\" % data[:max_length]) if len(data) > max_length else data\n",
                "\n",
                "\n",
                "# this requires that all subclasses have been imported before(!)\n",
                "def get_all_subclasses(clazz: Type) -> List[Type]:\n",
                "    \"\"\"Recursively get all subclasses of the given class.\"\"\"\n",
                "    result = set()\n",
                "    subs = clazz.__subclasses__()\n",
                "    for sub in subs:\n",
                "        result.add(sub)\n",
                "        result.update(get_all_subclasses(sub))\n",
                "    return result\n",
                "\n",
                "\n",
                "def parallelize(func: Callable, arr: List, size: int = None):\n",
                "    if not size:\n",
                "        size = len(arr)\n",
                "    if size <= 0:\n",
                "        return None\n",
                "\n",
                "    with Pool(size) as pool:\n",
                "        return pool.map(func, arr)\n",
                "\n",
                "\n",
                "def isoformat_milliseconds(t) -> str:\n",
                "    try:\n",
                "        return t.isoformat(timespec=\"milliseconds\")\n",
                "    except TypeError:\n",
                "        return t.isoformat()[:-3]\n",
                "\n",
                "\n",
                "# TODO move to aws_responses.py?\n",
                "def replace_response_content(response, pattern, replacement):\n",
                "    content = to_str(response.content or \"\")\n",
                "    response._content = re.sub(pattern, replacement, content)\n",
                "\n",
                "\n",
                "def is_none_or_empty(obj: Union[Optional[str], Optional[list]]) -> bool:\n",
                "    return (\n",
                "        obj is None\n",
                "        or (isinstance(obj, str) and obj.strip() == \"\")\n",
                "        or (isinstance(obj, Sized) and len(obj) == 0)\n",
                "    )\n",
                "\n",
                "\n",
                "def canonicalize_bool_to_str(val: bool) -> str:\n",
                "    return \"true\" if str(val).lower() == \"true\" else \"false\"\n",
                "\n",
                "\n",
                "def convert_to_printable_chars(value: Union[List, Dict, str]) -> str:\n",
                "    \"\"\"Removes all unprintable characters from the given string.\"\"\"\n",
                "    if isinstance(value, (dict, list)):\n",
                "\n",
                "        def _convert(obj, **kwargs):\n",
                "            if isinstance(obj, str):\n",
                "                return convert_to_printable_chars(obj)\n",
                "            return obj\n",
                "\n",
                "        return recurse_object(value, _convert)\n",
                "\n",
                "    result = REGEX_UNPRINTABLE_CHARS.sub(\"\", value)\n",
                "    return result\n",
                "\n",
                "\n",
                "# Code that requires util functions from above\n",
                "CACHE_FILE_PATTERN = CACHE_FILE_PATTERN.replace(\"_random_dir_\", short_uid())"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "bi-directional",
            "reason": "import and use"
        },
        {
            "edit_hunk_pair": [
                0,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "import and def"
        },
        {
            "edit_hunk_pair": [
                1,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "use and def"
        },
        {
            "edit_hunk_pair": [
                2,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "import and use"
        },
        {
            "edit_hunk_pair": [
                2,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "import and def"
        },
        {
            "edit_hunk_pair": [
                3,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "use and def"
        },
        {
            "edit_hunk_pair": [
                4,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "use and import"
        },
        {
            "edit_hunk_pair": [
                4,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "def and import"
        },
        {
            "edit_hunk_pair": [
                5,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        }
    ]
}