{
    "language": "python",
    "commit_url": "https://github.com/comfyanonymous/ComfyUI/commit/6d281b4ff4ad3918a4f3b4ca4a8b547a2ba3bf80",
    "commit_message": "Add a /free route to unload models or free all memory.\n\nA POST request to /free with: {\"unload_models\":true}\nwill unload models from vram.\n\nA POST request to /free with: {\"free_memory\":true}\nwill unload models and free all cached data from the last run workflow.",
    "commit_snapshots": {
        "execution.py": [
            [
                "import os\n",
                "import sys\n",
                "import copy\n",
                "import json\n",
                "import logging\n",
                "import threading\n",
                "import heapq\n",
                "import traceback\n",
                "import gc\n",
                "import inspect\n",
                "\n",
                "import torch\n",
                "import nodes\n",
                "\n",
                "import comfy.model_management\n",
                "\n",
                "def get_input_data(inputs, class_def, unique_id, outputs={}, prompt={}, extra_data={}):\n",
                "    valid_inputs = class_def.INPUT_TYPES()\n",
                "    input_data_all = {}\n",
                "    for x in inputs:\n",
                "        input_data = inputs[x]\n",
                "        if isinstance(input_data, list):\n",
                "            input_unique_id = input_data[0]\n",
                "            output_index = input_data[1]\n",
                "            if input_unique_id not in outputs:\n",
                "                input_data_all[x] = (None,)\n",
                "                continue\n",
                "            obj = outputs[input_unique_id][output_index]\n",
                "            input_data_all[x] = obj\n",
                "        else:\n",
                "            if (\"required\" in valid_inputs and x in valid_inputs[\"required\"]) or (\"optional\" in valid_inputs and x in valid_inputs[\"optional\"]):\n",
                "                input_data_all[x] = [input_data]\n",
                "\n",
                "    if \"hidden\" in valid_inputs:\n",
                "        h = valid_inputs[\"hidden\"]\n",
                "        for x in h:\n",
                "            if h[x] == \"PROMPT\":\n",
                "                input_data_all[x] = [prompt]\n",
                "            if h[x] == \"EXTRA_PNGINFO\":\n",
                "                if \"extra_pnginfo\" in extra_data:\n",
                "                    input_data_all[x] = [extra_data['extra_pnginfo']]\n",
                "            if h[x] == \"UNIQUE_ID\":\n",
                "                input_data_all[x] = [unique_id]\n",
                "    return input_data_all\n",
                "\n",
                "def map_node_over_list(obj, input_data_all, func, allow_interrupt=False):\n",
                "    # check if node wants the lists\n",
                "    input_is_list = False\n",
                "    if hasattr(obj, \"INPUT_IS_LIST\"):\n",
                "        input_is_list = obj.INPUT_IS_LIST\n",
                "\n",
                "    if len(input_data_all) == 0:\n",
                "        max_len_input = 0\n",
                "    else:\n",
                "        max_len_input = max([len(x) for x in input_data_all.values()])\n",
                "     \n",
                "    # get a slice of inputs, repeat last input when list isn't long enough\n",
                "    def slice_dict(d, i):\n",
                "        d_new = dict()\n",
                "        for k,v in d.items():\n",
                "            d_new[k] = v[i if len(v) > i else -1]\n",
                "        return d_new\n",
                "    \n",
                "    results = []\n",
                "    if input_is_list:\n",
                "        if allow_interrupt:\n",
                "            nodes.before_node_execution()\n",
                "        results.append(getattr(obj, func)(**input_data_all))\n",
                "    elif max_len_input == 0:\n",
                "        if allow_interrupt:\n",
                "            nodes.before_node_execution()\n",
                "        results.append(getattr(obj, func)())\n",
                "    else:\n",
                "        for i in range(max_len_input):\n",
                "            if allow_interrupt:\n",
                "                nodes.before_node_execution()\n",
                "            results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))\n",
                "    return results\n",
                "\n",
                "def get_output_data(obj, input_data_all):\n",
                "    \n",
                "    results = []\n",
                "    uis = []\n",
                "    return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)\n",
                "\n",
                "    for r in return_values:\n",
                "        if isinstance(r, dict):\n",
                "            if 'ui' in r:\n",
                "                uis.append(r['ui'])\n",
                "            if 'result' in r:\n",
                "                results.append(r['result'])\n",
                "        else:\n",
                "            results.append(r)\n",
                "    \n",
                "    output = []\n",
                "    if len(results) > 0:\n",
                "        # check which outputs need concatenating\n",
                "        output_is_list = [False] * len(results[0])\n",
                "        if hasattr(obj, \"OUTPUT_IS_LIST\"):\n",
                "            output_is_list = obj.OUTPUT_IS_LIST\n",
                "\n",
                "        # merge node execution results\n",
                "        for i, is_list in zip(range(len(results[0])), output_is_list):\n",
                "            if is_list:\n",
                "                output.append([x for o in results for x in o[i]])\n",
                "            else:\n",
                "                output.append([o[i] for o in results])\n",
                "\n",
                "    ui = dict()    \n",
                "    if len(uis) > 0:\n",
                "        ui = {k: [y for x in uis for y in x[k]] for k in uis[0].keys()}\n",
                "    return output, ui\n",
                "\n",
                "def format_value(x):\n",
                "    if x is None:\n",
                "        return None\n",
                "    elif isinstance(x, (int, float, bool, str)):\n",
                "        return x\n",
                "    else:\n",
                "        return str(x)\n",
                "\n",
                "def recursive_execute(server, prompt, outputs, current_item, extra_data, executed, prompt_id, outputs_ui, object_storage):\n",
                "    unique_id = current_item\n",
                "    inputs = prompt[unique_id]['inputs']\n",
                "    class_type = prompt[unique_id]['class_type']\n",
                "    class_def = nodes.NODE_CLASS_MAPPINGS[class_type]\n",
                "    if unique_id in outputs:\n",
                "        return (True, None, None)\n",
                "\n",
                "    for x in inputs:\n",
                "        input_data = inputs[x]\n",
                "\n",
                "        if isinstance(input_data, list):\n",
                "            input_unique_id = input_data[0]\n",
                "            output_index = input_data[1]\n",
                "            if input_unique_id not in outputs:\n",
                "                result = recursive_execute(server, prompt, outputs, input_unique_id, extra_data, executed, prompt_id, outputs_ui, object_storage)\n",
                "                if result[0] is not True:\n",
                "                    # Another node failed further upstream\n",
                "                    return result\n",
                "\n",
                "    input_data_all = None\n",
                "    try:\n",
                "        input_data_all = get_input_data(inputs, class_def, unique_id, outputs, prompt, extra_data)\n",
                "        if server.client_id is not None:\n",
                "            server.last_node_id = unique_id\n",
                "            server.send_sync(\"executing\", { \"node\": unique_id, \"prompt_id\": prompt_id }, server.client_id)\n",
                "\n",
                "        obj = object_storage.get((unique_id, class_type), None)\n",
                "        if obj is None:\n",
                "            obj = class_def()\n",
                "            object_storage[(unique_id, class_type)] = obj\n",
                "\n",
                "        output_data, output_ui = get_output_data(obj, input_data_all)\n",
                "        outputs[unique_id] = output_data\n",
                "        if len(output_ui) > 0:\n",
                "            outputs_ui[unique_id] = output_ui\n",
                "            if server.client_id is not None:\n",
                "                server.send_sync(\"executed\", { \"node\": unique_id, \"output\": output_ui, \"prompt_id\": prompt_id }, server.client_id)\n",
                "    except comfy.model_management.InterruptProcessingException as iex:\n",
                "        logging.info(\"Processing interrupted\")\n",
                "\n",
                "        # skip formatting inputs/outputs\n",
                "        error_details = {\n",
                "            \"node_id\": unique_id,\n",
                "        }\n",
                "\n",
                "        return (False, error_details, iex)\n",
                "    except Exception as ex:\n",
                "        typ, _, tb = sys.exc_info()\n",
                "        exception_type = full_type_name(typ)\n",
                "        input_data_formatted = {}\n",
                "        if input_data_all is not None:\n",
                "            input_data_formatted = {}\n",
                "            for name, inputs in input_data_all.items():\n",
                "                input_data_formatted[name] = [format_value(x) for x in inputs]\n",
                "\n",
                "        output_data_formatted = {}\n",
                "        for node_id, node_outputs in outputs.items():\n",
                "            output_data_formatted[node_id] = [[format_value(x) for x in l] for l in node_outputs]\n",
                "\n",
                "        logging.error(\"!!! Exception during processing !!!\")\n",
                "        logging.error(traceback.format_exc())\n",
                "\n",
                "        error_details = {\n",
                "            \"node_id\": unique_id,\n",
                "            \"exception_message\": str(ex),\n",
                "            \"exception_type\": exception_type,\n",
                "            \"traceback\": traceback.format_tb(tb),\n",
                "            \"current_inputs\": input_data_formatted,\n",
                "            \"current_outputs\": output_data_formatted\n",
                "        }\n",
                "        return (False, error_details, ex)\n",
                "\n",
                "    executed.add(unique_id)\n",
                "\n",
                "    return (True, None, None)\n",
                "\n",
                "def recursive_will_execute(prompt, outputs, current_item):\n",
                "    unique_id = current_item\n",
                "    inputs = prompt[unique_id]['inputs']\n",
                "    will_execute = []\n",
                "    if unique_id in outputs:\n",
                "        return []\n",
                "\n",
                "    for x in inputs:\n",
                "        input_data = inputs[x]\n",
                "        if isinstance(input_data, list):\n",
                "            input_unique_id = input_data[0]\n",
                "            output_index = input_data[1]\n",
                "            if input_unique_id not in outputs:\n",
                "                will_execute += recursive_will_execute(prompt, outputs, input_unique_id)\n",
                "\n",
                "    return will_execute + [unique_id]\n",
                "\n",
                "def recursive_output_delete_if_changed(prompt, old_prompt, outputs, current_item):\n",
                "    unique_id = current_item\n",
                "    inputs = prompt[unique_id]['inputs']\n",
                "    class_type = prompt[unique_id]['class_type']\n",
                "    class_def = nodes.NODE_CLASS_MAPPINGS[class_type]\n",
                "\n",
                "    is_changed_old = ''\n",
                "    is_changed = ''\n",
                "    to_delete = False\n",
                "    if hasattr(class_def, 'IS_CHANGED'):\n",
                "        if unique_id in old_prompt and 'is_changed' in old_prompt[unique_id]:\n",
                "            is_changed_old = old_prompt[unique_id]['is_changed']\n",
                "        if 'is_changed' not in prompt[unique_id]:\n",
                "            input_data_all = get_input_data(inputs, class_def, unique_id, outputs)\n",
                "            if input_data_all is not None:\n",
                "                try:\n",
                "                    #is_changed = class_def.IS_CHANGED(**input_data_all)\n",
                "                    is_changed = map_node_over_list(class_def, input_data_all, \"IS_CHANGED\")\n",
                "                    prompt[unique_id]['is_changed'] = is_changed\n",
                "                except:\n",
                "                    to_delete = True\n",
                "        else:\n",
                "            is_changed = prompt[unique_id]['is_changed']\n",
                "\n",
                "    if unique_id not in outputs:\n",
                "        return True\n",
                "\n",
                "    if not to_delete:\n",
                "        if is_changed != is_changed_old:\n",
                "            to_delete = True\n",
                "        elif unique_id not in old_prompt:\n",
                "            to_delete = True\n",
                "        elif inputs == old_prompt[unique_id]['inputs']:\n",
                "            for x in inputs:\n",
                "                input_data = inputs[x]\n",
                "\n",
                "                if isinstance(input_data, list):\n",
                "                    input_unique_id = input_data[0]\n",
                "                    output_index = input_data[1]\n",
                "                    if input_unique_id in outputs:\n",
                "                        to_delete = recursive_output_delete_if_changed(prompt, old_prompt, outputs, input_unique_id)\n",
                "                    else:\n",
                "                        to_delete = True\n",
                "                    if to_delete:\n",
                "                        break\n",
                "        else:\n",
                "            to_delete = True\n",
                "\n",
                "    if to_delete:\n",
                "        d = outputs.pop(unique_id)\n",
                "        del d\n",
                "    return to_delete\n",
                "\n",
                "class PromptExecutor:\n",
                "    def __init__(self, server):\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        self.server = server\n",
                    "        self.reset()\n",
                    "\n",
                    "    def reset(self):\n"
                ],
                "parent_version_range": {
                    "start": 270,
                    "end": 270
                },
                "child_version_range": {
                    "start": 270,
                    "end": 274
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "PromptExecutor",
                        "signature": "class PromptExecutor:",
                        "at_line": 268
                    },
                    {
                        "type": "function",
                        "name": "__init__",
                        "signature": "def __init__(self, server):",
                        "at_line": 269
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: execution.py\nCode:\n267 267    \n268 268    class PromptExecutor:\n269 269        def __init__(self, server):\n    270  +         self.server = server\n    271  +         self.reset()\n    272  + \n    273  +     def reset(self):\n270 274            self.outputs = {}\n271 275            self.object_storage = {}\n272 276            self.outputs_ui = {}\n         ...\n",
                "file_path": "execution.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "reset",
                    "self",
                    "server"
                ],
                "prefix": [
                    "\n",
                    "class PromptExecutor:\n",
                    "    def __init__(self, server):\n"
                ],
                "suffix": [
                    "        self.outputs = {}\n",
                    "        self.object_storage = {}\n",
                    "        self.outputs_ui = {}\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "reset",
                            "position": {
                                "start": {
                                    "line": 273,
                                    "column": 8
                                },
                                "end": {
                                    "line": 273,
                                    "column": 13
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/execution.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "        self.outputs = {}\n",
                "        self.object_storage = {}\n",
                "        self.outputs_ui = {}\n",
                "        self.old_prompt = {}\n"
            ],
            {
                "type": "delete",
                "before": [
                    "        self.server = server\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 274,
                    "end": 275
                },
                "child_version_range": {
                    "start": 278,
                    "end": 278
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "PromptExecutor",
                        "signature": "class PromptExecutor:",
                        "at_line": 268
                    },
                    {
                        "type": "function",
                        "name": "__init__",
                        "signature": "def __init__(self, server):",
                        "at_line": 269
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: execution.py\nCode:\n           class PromptExecutor:\n               ...\n               def __init__(self, server):\n                   ...\n271 275            self.object_storage = {}\n272 276            self.outputs_ui = {}\n273 277            self.old_prompt = {}\n274      -         self.server = server\n275 278    \n276 279        def handle_execution_error(self, prompt_id, prompt, current_outputs, executed, error, ex):\n277 280            node_id = error[\"node_id\"]\n         ...\n",
                "file_path": "execution.py",
                "identifiers_before": [
                    "self",
                    "server"
                ],
                "identifiers_after": [],
                "prefix": [
                    "        self.object_storage = {}\n",
                    "        self.outputs_ui = {}\n",
                    "        self.old_prompt = {}\n"
                ],
                "suffix": [
                    "\n",
                    "    def handle_execution_error(self, prompt_id, prompt, current_outputs, executed, error, ex):\n",
                    "        node_id = error[\"node_id\"]\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "\n",
                "    def handle_execution_error(self, prompt_id, prompt, current_outputs, executed, error, ex):\n",
                "        node_id = error[\"node_id\"]\n",
                "        class_type = prompt[node_id][\"class_type\"]\n",
                "\n",
                "        # First, send back the status to the frontend depending\n",
                "        # on the exception type\n",
                "        if isinstance(ex, comfy.model_management.InterruptProcessingException):\n",
                "            mes = {\n",
                "                \"prompt_id\": prompt_id,\n",
                "                \"node_id\": node_id,\n",
                "                \"node_type\": class_type,\n",
                "                \"executed\": list(executed),\n",
                "            }\n",
                "            self.server.send_sync(\"execution_interrupted\", mes, self.server.client_id)\n",
                "        else:\n",
                "            if self.server.client_id is not None:\n",
                "                mes = {\n",
                "                    \"prompt_id\": prompt_id,\n",
                "                    \"node_id\": node_id,\n",
                "                    \"node_type\": class_type,\n",
                "                    \"executed\": list(executed),\n",
                "\n",
                "                    \"exception_message\": error[\"exception_message\"],\n",
                "                    \"exception_type\": error[\"exception_type\"],\n",
                "                    \"traceback\": error[\"traceback\"],\n",
                "                    \"current_inputs\": error[\"current_inputs\"],\n",
                "                    \"current_outputs\": error[\"current_outputs\"],\n",
                "                }\n",
                "                self.server.send_sync(\"execution_error\", mes, self.server.client_id)\n",
                "\n",
                "        # Next, remove the subsequent outputs since they will not be executed\n",
                "        to_delete = []\n",
                "        for o in self.outputs:\n",
                "            if (o not in current_outputs) and (o not in executed):\n",
                "                to_delete += [o]\n",
                "                if o in self.old_prompt:\n",
                "                    d = self.old_prompt.pop(o)\n",
                "                    del d\n",
                "        for o in to_delete:\n",
                "            d = self.outputs.pop(o)\n",
                "            del d\n",
                "\n",
                "    def execute(self, prompt, prompt_id, extra_data={}, execute_outputs=[]):\n",
                "        nodes.interrupt_processing(False)\n",
                "\n",
                "        if \"client_id\" in extra_data:\n",
                "            self.server.client_id = extra_data[\"client_id\"]\n",
                "        else:\n",
                "            self.server.client_id = None\n",
                "\n",
                "        if self.server.client_id is not None:\n",
                "            self.server.send_sync(\"execution_start\", { \"prompt_id\": prompt_id}, self.server.client_id)\n",
                "\n",
                "        with torch.inference_mode():\n",
                "            #delete cached outputs if nodes don't exist for them\n",
                "            to_delete = []\n",
                "            for o in self.outputs:\n",
                "                if o not in prompt:\n",
                "                    to_delete += [o]\n",
                "            for o in to_delete:\n",
                "                d = self.outputs.pop(o)\n",
                "                del d\n",
                "            to_delete = []\n",
                "            for o in self.object_storage:\n",
                "                if o[0] not in prompt:\n",
                "                    to_delete += [o]\n",
                "                else:\n",
                "                    p = prompt[o[0]]\n",
                "                    if o[1] != p['class_type']:\n",
                "                        to_delete += [o]\n",
                "            for o in to_delete:\n",
                "                d = self.object_storage.pop(o)\n",
                "                del d\n",
                "\n",
                "            for x in prompt:\n",
                "                recursive_output_delete_if_changed(prompt, self.old_prompt, self.outputs, x)\n",
                "\n",
                "            current_outputs = set(self.outputs.keys())\n",
                "            for x in list(self.outputs_ui.keys()):\n",
                "                if x not in current_outputs:\n",
                "                    d = self.outputs_ui.pop(x)\n",
                "                    del d\n",
                "\n",
                "            comfy.model_management.cleanup_models()\n",
                "            if self.server.client_id is not None:\n",
                "                self.server.send_sync(\"execution_cached\", { \"nodes\": list(current_outputs) , \"prompt_id\": prompt_id}, self.server.client_id)\n",
                "            executed = set()\n",
                "            output_node_id = None\n",
                "            to_execute = []\n",
                "\n",
                "            for node_id in list(execute_outputs):\n",
                "                to_execute += [(0, node_id)]\n",
                "\n",
                "            while len(to_execute) > 0:\n",
                "                #always execute the output that depends on the least amount of unexecuted nodes first\n",
                "                to_execute = sorted(list(map(lambda a: (len(recursive_will_execute(prompt, self.outputs, a[-1])), a[-1]), to_execute)))\n",
                "                output_node_id = to_execute.pop(0)[-1]\n",
                "\n",
                "                # This call shouldn't raise anything if there's an error deep in\n",
                "                # the actual SD code, instead it will report the node where the\n",
                "                # error was raised\n",
                "                success, error, ex = recursive_execute(self.server, prompt, self.outputs, output_node_id, extra_data, executed, prompt_id, self.outputs_ui, self.object_storage)\n",
                "                if success is not True:\n",
                "                    self.handle_execution_error(prompt_id, prompt, current_outputs, executed, error, ex)\n",
                "                    break\n",
                "\n",
                "            for x in executed:\n",
                "                self.old_prompt[x] = copy.deepcopy(prompt[x])\n",
                "            self.server.last_node_id = None\n",
                "            if comfy.model_management.DISABLE_SMART_MEMORY:\n",
                "                comfy.model_management.unload_all_models()\n",
                "\n",
                "\n",
                "\n",
                "def validate_inputs(prompt, item, validated):\n",
                "    unique_id = item\n",
                "    if unique_id in validated:\n",
                "        return validated[unique_id]\n",
                "\n",
                "    inputs = prompt[unique_id]['inputs']\n",
                "    class_type = prompt[unique_id]['class_type']\n",
                "    obj_class = nodes.NODE_CLASS_MAPPINGS[class_type]\n",
                "\n",
                "    class_inputs = obj_class.INPUT_TYPES()\n",
                "    required_inputs = class_inputs['required']\n",
                "\n",
                "    errors = []\n",
                "    valid = True\n",
                "\n",
                "    validate_function_inputs = []\n",
                "    if hasattr(obj_class, \"VALIDATE_INPUTS\"):\n",
                "        validate_function_inputs = inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args\n",
                "\n",
                "    for x in required_inputs:\n",
                "        if x not in inputs:\n",
                "            error = {\n",
                "                \"type\": \"required_input_missing\",\n",
                "                \"message\": \"Required input is missing\",\n",
                "                \"details\": f\"{x}\",\n",
                "                \"extra_info\": {\n",
                "                    \"input_name\": x\n",
                "                }\n",
                "            }\n",
                "            errors.append(error)\n",
                "            continue\n",
                "\n",
                "        val = inputs[x]\n",
                "        info = required_inputs[x]\n",
                "        type_input = info[0]\n",
                "        if isinstance(val, list):\n",
                "            if len(val) != 2:\n",
                "                error = {\n",
                "                    \"type\": \"bad_linked_input\",\n",
                "                    \"message\": \"Bad linked input, must be a length-2 list of [node_id, slot_index]\",\n",
                "                    \"details\": f\"{x}\",\n",
                "                    \"extra_info\": {\n",
                "                        \"input_name\": x,\n",
                "                        \"input_config\": info,\n",
                "                        \"received_value\": val\n",
                "                    }\n",
                "                }\n",
                "                errors.append(error)\n",
                "                continue\n",
                "\n",
                "            o_id = val[0]\n",
                "            o_class_type = prompt[o_id]['class_type']\n",
                "            r = nodes.NODE_CLASS_MAPPINGS[o_class_type].RETURN_TYPES\n",
                "            if r[val[1]] != type_input:\n",
                "                received_type = r[val[1]]\n",
                "                details = f\"{x}, {received_type} != {type_input}\"\n",
                "                error = {\n",
                "                    \"type\": \"return_type_mismatch\",\n",
                "                    \"message\": \"Return type mismatch between linked nodes\",\n",
                "                    \"details\": details,\n",
                "                    \"extra_info\": {\n",
                "                        \"input_name\": x,\n",
                "                        \"input_config\": info,\n",
                "                        \"received_type\": received_type,\n",
                "                        \"linked_node\": val\n",
                "                    }\n",
                "                }\n",
                "                errors.append(error)\n",
                "                continue\n",
                "            try:\n",
                "                r = validate_inputs(prompt, o_id, validated)\n",
                "                if r[0] is False:\n",
                "                    # `r` will be set in `validated[o_id]` already\n",
                "                    valid = False\n",
                "                    continue\n",
                "            except Exception as ex:\n",
                "                typ, _, tb = sys.exc_info()\n",
                "                valid = False\n",
                "                exception_type = full_type_name(typ)\n",
                "                reasons = [{\n",
                "                    \"type\": \"exception_during_inner_validation\",\n",
                "                    \"message\": \"Exception when validating inner node\",\n",
                "                    \"details\": str(ex),\n",
                "                    \"extra_info\": {\n",
                "                        \"input_name\": x,\n",
                "                        \"input_config\": info,\n",
                "                        \"exception_message\": str(ex),\n",
                "                        \"exception_type\": exception_type,\n",
                "                        \"traceback\": traceback.format_tb(tb),\n",
                "                        \"linked_node\": val\n",
                "                    }\n",
                "                }]\n",
                "                validated[o_id] = (False, reasons, o_id)\n",
                "                continue\n",
                "        else:\n",
                "            try:\n",
                "                if type_input == \"INT\":\n",
                "                    val = int(val)\n",
                "                    inputs[x] = val\n",
                "                if type_input == \"FLOAT\":\n",
                "                    val = float(val)\n",
                "                    inputs[x] = val\n",
                "                if type_input == \"STRING\":\n",
                "                    val = str(val)\n",
                "                    inputs[x] = val\n",
                "            except Exception as ex:\n",
                "                error = {\n",
                "                    \"type\": \"invalid_input_type\",\n",
                "                    \"message\": f\"Failed to convert an input value to a {type_input} value\",\n",
                "                    \"details\": f\"{x}, {val}, {ex}\",\n",
                "                    \"extra_info\": {\n",
                "                        \"input_name\": x,\n",
                "                        \"input_config\": info,\n",
                "                        \"received_value\": val,\n",
                "                        \"exception_message\": str(ex)\n",
                "                    }\n",
                "                }\n",
                "                errors.append(error)\n",
                "                continue\n",
                "\n",
                "            if len(info) > 1:\n",
                "                if \"min\" in info[1] and val < info[1][\"min\"]:\n",
                "                    error = {\n",
                "                        \"type\": \"value_smaller_than_min\",\n",
                "                        \"message\": \"Value {} smaller than min of {}\".format(val, info[1][\"min\"]),\n",
                "                        \"details\": f\"{x}\",\n",
                "                        \"extra_info\": {\n",
                "                            \"input_name\": x,\n",
                "                            \"input_config\": info,\n",
                "                            \"received_value\": val,\n",
                "                        }\n",
                "                    }\n",
                "                    errors.append(error)\n",
                "                    continue\n",
                "                if \"max\" in info[1] and val > info[1][\"max\"]:\n",
                "                    error = {\n",
                "                        \"type\": \"value_bigger_than_max\",\n",
                "                        \"message\": \"Value {} bigger than max of {}\".format(val, info[1][\"max\"]),\n",
                "                        \"details\": f\"{x}\",\n",
                "                        \"extra_info\": {\n",
                "                            \"input_name\": x,\n",
                "                            \"input_config\": info,\n",
                "                            \"received_value\": val,\n",
                "                        }\n",
                "                    }\n",
                "                    errors.append(error)\n",
                "                    continue\n",
                "\n",
                "            if x not in validate_function_inputs:\n",
                "                if isinstance(type_input, list):\n",
                "                    if val not in type_input:\n",
                "                        input_config = info\n",
                "                        list_info = \"\"\n",
                "\n",
                "                        # Don't send back gigantic lists like if they're lots of\n",
                "                        # scanned model filepaths\n",
                "                        if len(type_input) > 20:\n",
                "                            list_info = f\"(list of length {len(type_input)})\"\n",
                "                            input_config = None\n",
                "                        else:\n",
                "                            list_info = str(type_input)\n",
                "\n",
                "                        error = {\n",
                "                            \"type\": \"value_not_in_list\",\n",
                "                            \"message\": \"Value not in list\",\n",
                "                            \"details\": f\"{x}: '{val}' not in {list_info}\",\n",
                "                            \"extra_info\": {\n",
                "                                \"input_name\": x,\n",
                "                                \"input_config\": input_config,\n",
                "                                \"received_value\": val,\n",
                "                            }\n",
                "                        }\n",
                "                        errors.append(error)\n",
                "                        continue\n",
                "\n",
                "    if len(validate_function_inputs) > 0:\n",
                "        input_data_all = get_input_data(inputs, obj_class, unique_id)\n",
                "        input_filtered = {}\n",
                "        for x in input_data_all:\n",
                "            if x in validate_function_inputs:\n",
                "                input_filtered[x] = input_data_all[x]\n",
                "\n",
                "        #ret = obj_class.VALIDATE_INPUTS(**input_filtered)\n",
                "        ret = map_node_over_list(obj_class, input_filtered, \"VALIDATE_INPUTS\")\n",
                "        for x in input_filtered:\n",
                "            for i, r in enumerate(ret):\n",
                "                if r is not True:\n",
                "                    details = f\"{x}\"\n",
                "                    if r is not False:\n",
                "                        details += f\" - {str(r)}\"\n",
                "\n",
                "                    error = {\n",
                "                        \"type\": \"custom_validation_failed\",\n",
                "                        \"message\": \"Custom validation failed for node\",\n",
                "                        \"details\": details,\n",
                "                        \"extra_info\": {\n",
                "                            \"input_name\": x,\n",
                "                            \"input_config\": info,\n",
                "                            \"received_value\": val,\n",
                "                        }\n",
                "                    }\n",
                "                    errors.append(error)\n",
                "                    continue\n",
                "\n",
                "    if len(errors) > 0 or valid is not True:\n",
                "        ret = (False, errors, unique_id)\n",
                "    else:\n",
                "        ret = (True, [], unique_id)\n",
                "\n",
                "    validated[unique_id] = ret\n",
                "    return ret\n",
                "\n",
                "def full_type_name(klass):\n",
                "    module = klass.__module__\n",
                "    if module == 'builtins':\n",
                "        return klass.__qualname__\n",
                "    return module + '.' + klass.__qualname__\n",
                "\n",
                "def validate_prompt(prompt):\n",
                "    outputs = set()\n",
                "    for x in prompt:\n",
                "        class_ = nodes.NODE_CLASS_MAPPINGS[prompt[x]['class_type']]\n",
                "        if hasattr(class_, 'OUTPUT_NODE') and class_.OUTPUT_NODE == True:\n",
                "            outputs.add(x)\n",
                "\n",
                "    if len(outputs) == 0:\n",
                "        error = {\n",
                "            \"type\": \"prompt_no_outputs\",\n",
                "            \"message\": \"Prompt has no outputs\",\n",
                "            \"details\": \"\",\n",
                "            \"extra_info\": {}\n",
                "        }\n",
                "        return (False, error, [], [])\n",
                "\n",
                "    good_outputs = set()\n",
                "    errors = []\n",
                "    node_errors = {}\n",
                "    validated = {}\n",
                "    for o in outputs:\n",
                "        valid = False\n",
                "        reasons = []\n",
                "        try:\n",
                "            m = validate_inputs(prompt, o, validated)\n",
                "            valid = m[0]\n",
                "            reasons = m[1]\n",
                "        except Exception as ex:\n",
                "            typ, _, tb = sys.exc_info()\n",
                "            valid = False\n",
                "            exception_type = full_type_name(typ)\n",
                "            reasons = [{\n",
                "                \"type\": \"exception_during_validation\",\n",
                "                \"message\": \"Exception when validating node\",\n",
                "                \"details\": str(ex),\n",
                "                \"extra_info\": {\n",
                "                    \"exception_type\": exception_type,\n",
                "                    \"traceback\": traceback.format_tb(tb)\n",
                "                }\n",
                "            }]\n",
                "            validated[o] = (False, reasons, o)\n",
                "\n",
                "        if valid is True:\n",
                "            good_outputs.add(o)\n",
                "        else:\n",
                "            logging.error(f\"Failed to validate prompt for output {o}:\")\n",
                "            if len(reasons) > 0:\n",
                "                logging.error(\"* (prompt):\")\n",
                "                for reason in reasons:\n",
                "                    logging.error(f\"  - {reason['message']}: {reason['details']}\")\n",
                "            errors += [(o, reasons)]\n",
                "            for node_id, result in validated.items():\n",
                "                valid = result[0]\n",
                "                reasons = result[1]\n",
                "                # If a node upstream has errors, the nodes downstream will also\n",
                "                # be reported as invalid, but there will be no errors attached.\n",
                "                # So don't return those nodes as having errors in the response.\n",
                "                if valid is not True and len(reasons) > 0:\n",
                "                    if node_id not in node_errors:\n",
                "                        class_type = prompt[node_id]['class_type']\n",
                "                        node_errors[node_id] = {\n",
                "                            \"errors\": reasons,\n",
                "                            \"dependent_outputs\": [],\n",
                "                            \"class_type\": class_type\n",
                "                        }\n",
                "                        logging.error(f\"* {class_type} {node_id}:\")\n",
                "                        for reason in reasons:\n",
                "                            logging.error(f\"  - {reason['message']}: {reason['details']}\")\n",
                "                    node_errors[node_id][\"dependent_outputs\"].append(o)\n",
                "            logging.error(\"Output will be ignored\")\n",
                "\n",
                "    if len(good_outputs) == 0:\n",
                "        errors_list = []\n",
                "        for o, errors in errors:\n",
                "            for error in errors:\n",
                "                errors_list.append(f\"{error['message']}: {error['details']}\")\n",
                "        errors_list = \"\\n\".join(errors_list)\n",
                "\n",
                "        error = {\n",
                "            \"type\": \"prompt_outputs_failed_validation\",\n",
                "            \"message\": \"Prompt outputs failed validation\",\n",
                "            \"details\": errors_list,\n",
                "            \"extra_info\": {}\n",
                "        }\n",
                "\n",
                "        return (False, error, list(good_outputs), node_errors)\n",
                "\n",
                "    return (True, None, list(good_outputs), node_errors)\n",
                "\n",
                "MAXIMUM_HISTORY_SIZE = 10000\n",
                "\n",
                "class PromptQueue:\n",
                "    def __init__(self, server):\n",
                "        self.server = server\n",
                "        self.mutex = threading.RLock()\n",
                "        self.not_empty = threading.Condition(self.mutex)\n",
                "        self.task_counter = 0\n",
                "        self.queue = []\n",
                "        self.currently_running = {}\n",
                "        self.history = {}\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        self.flags = {}\n"
                ],
                "parent_version_range": {
                    "start": 708,
                    "end": 708
                },
                "child_version_range": {
                    "start": 711,
                    "end": 712
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "PromptQueue",
                        "signature": "class PromptQueue:",
                        "at_line": 699
                    },
                    {
                        "type": "function",
                        "name": "__init__",
                        "signature": "def __init__(self, server):",
                        "at_line": 700
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: execution.py\nCode:\n           class PromptQueue:\n               ...\n               def __init__(self, server):\n                   ...\n705 708            self.queue = []\n706 709            self.currently_running = {}\n707 710            self.history = {}\n    711  +         self.flags = {}\n708 712            server.prompt_queue = self\n709 713    \n710 714        def put(self, item):\n         ...\n",
                "file_path": "execution.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "flags",
                    "self"
                ],
                "prefix": [
                    "        self.queue = []\n",
                    "        self.currently_running = {}\n",
                    "        self.history = {}\n"
                ],
                "suffix": [
                    "        server.prompt_queue = self\n",
                    "\n",
                    "    def put(self, item):\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        server.prompt_queue = self\n",
                "\n",
                "    def put(self, item):\n",
                "        with self.mutex:\n",
                "            heapq.heappush(self.queue, item)\n",
                "            self.server.queue_updated()\n",
                "            self.not_empty.notify()\n",
                "\n",
                "    def get(self, timeout=None):\n",
                "        with self.not_empty:\n",
                "            while len(self.queue) == 0:\n",
                "                self.not_empty.wait(timeout=timeout)\n",
                "                if timeout is not None and len(self.queue) == 0:\n",
                "                    return None\n",
                "            item = heapq.heappop(self.queue)\n",
                "            i = self.task_counter\n",
                "            self.currently_running[i] = copy.deepcopy(item)\n",
                "            self.task_counter += 1\n",
                "            self.server.queue_updated()\n",
                "            return (item, i)\n",
                "\n",
                "    def task_done(self, item_id, outputs):\n",
                "        with self.mutex:\n",
                "            prompt = self.currently_running.pop(item_id)\n",
                "            if len(self.history) > MAXIMUM_HISTORY_SIZE:\n",
                "                self.history.pop(next(iter(self.history)))\n",
                "            self.history[prompt[1]] = { \"prompt\": prompt, \"outputs\": {} }\n",
                "            for o in outputs:\n",
                "                self.history[prompt[1]][\"outputs\"][o] = outputs[o]\n",
                "            self.server.queue_updated()\n",
                "\n",
                "    def get_current_queue(self):\n",
                "        with self.mutex:\n",
                "            out = []\n",
                "            for x in self.currently_running.values():\n",
                "                out += [x]\n",
                "            return (out, copy.deepcopy(self.queue))\n",
                "\n",
                "    def get_tasks_remaining(self):\n",
                "        with self.mutex:\n",
                "            return len(self.queue) + len(self.currently_running)\n",
                "\n",
                "    def wipe_queue(self):\n",
                "        with self.mutex:\n",
                "            self.queue = []\n",
                "            self.server.queue_updated()\n",
                "\n",
                "    def delete_queue_item(self, function):\n",
                "        with self.mutex:\n",
                "            for x in range(len(self.queue)):\n",
                "                if function(self.queue[x]):\n",
                "                    if len(self.queue) == 1:\n",
                "                        self.wipe_queue()\n",
                "                    else:\n",
                "                        self.queue.pop(x)\n",
                "                        heapq.heapify(self.queue)\n",
                "                    self.server.queue_updated()\n",
                "                    return True\n",
                "        return False\n",
                "\n",
                "    def get_history(self, prompt_id=None, max_items=None, offset=-1):\n",
                "        with self.mutex:\n",
                "            if prompt_id is None:\n",
                "                out = {}\n",
                "                i = 0\n",
                "                if offset < 0 and max_items is not None:\n",
                "                    offset = len(self.history) - max_items\n",
                "                for k in self.history:\n",
                "                    if i >= offset:\n",
                "                        out[k] = self.history[k]\n",
                "                        if max_items is not None and len(out) >= max_items:\n",
                "                            break\n",
                "                    i += 1\n",
                "                return out\n",
                "            elif prompt_id in self.history:\n",
                "                return {prompt_id: copy.deepcopy(self.history[prompt_id])}\n",
                "            else:\n",
                "                return {}\n",
                "\n",
                "    def wipe_history(self):\n",
                "        with self.mutex:\n",
                "            self.history = {}\n",
                "\n",
                "    def delete_history_item(self, id_to_delete):\n",
                "        with self.mutex:\n",
                "            self.history.pop(id_to_delete, None)\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "\n",
                    "    def set_flag(self, name, data):\n",
                    "        with self.mutex:\n",
                    "            self.flags[name] = data\n",
                    "            self.not_empty.notify()\n",
                    "\n",
                    "    def get_flags(self, reset=True):\n",
                    "        with self.mutex:\n",
                    "            if reset:\n",
                    "                ret = self.flags\n",
                    "                self.flags = {}\n",
                    "                return ret\n",
                    "            else:\n",
                    "                return self.flags.copy()"
                ],
                "parent_version_range": {
                    "start": 794,
                    "end": 794
                },
                "child_version_range": {
                    "start": 798,
                    "end": 812
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "with self.mutex:",
                        "start_line": 792,
                        "end_line": 793
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "PromptQueue",
                        "signature": "class PromptQueue:",
                        "at_line": 699
                    },
                    {
                        "type": "function",
                        "name": "delete_history_item",
                        "signature": "def delete_history_item(self, id_to_delete):",
                        "at_line": 791
                    },
                    {
                        "type": "call",
                        "name": "self.history.pop",
                        "signature": "self.history.pop(id_to_delete, None)",
                        "at_line": 793,
                        "argument": "id_to_delete"
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: execution.py\nCode:\n           class PromptQueue:\n               ...\n791 795        def delete_history_item(self, id_to_delete):\n792 796            with self.mutex:\n793 797                self.history.pop(id_to_delete, None)\n    798  + \n    799  +     def set_flag(self, name, data):\n    800  +         with self.mutex:\n    801  +             self.flags[name] = data\n    802  +             self.not_empty.notify()\n    803  + \n    804  +     def get_flags(self, reset=True):\n    805  +         with self.mutex:\n    806  +             if reset:\n    807  +                 ret = self.flags\n    808  +                 self.flags = {}\n    809  +                 return ret\n    810  +             else:\n    811  +                 return self.flags.copy()\n         ...\n",
                "file_path": "execution.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "copy",
                    "data",
                    "flags",
                    "get_flags",
                    "mutex",
                    "name",
                    "not_empty",
                    "notify",
                    "reset",
                    "ret",
                    "self",
                    "set_flag"
                ],
                "prefix": [
                    "    def delete_history_item(self, id_to_delete):\n",
                    "        with self.mutex:\n",
                    "            self.history.pop(id_to_delete, None)\n"
                ],
                "suffix": [],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            }
        ],
        "main.py": [
            [
                "import comfy.options\n",
                "comfy.options.enable_args_parsing()\n",
                "\n",
                "import os\n",
                "import importlib.util\n",
                "import folder_paths\n",
                "import time\n",
                "\n",
                "def execute_prestartup_script():\n",
                "    def execute_script(script_path):\n",
                "        module_name = os.path.splitext(script_path)[0]\n",
                "        try:\n",
                "            spec = importlib.util.spec_from_file_location(module_name, script_path)\n",
                "            module = importlib.util.module_from_spec(spec)\n",
                "            spec.loader.exec_module(module)\n",
                "            return True\n",
                "        except Exception as e:\n",
                "            print(f\"Failed to execute startup-script: {script_path} / {e}\")\n",
                "        return False\n",
                "\n",
                "    node_paths = folder_paths.get_folder_paths(\"custom_nodes\")\n",
                "    for custom_node_path in node_paths:\n",
                "        possible_modules = os.listdir(custom_node_path)\n",
                "        node_prestartup_times = []\n",
                "\n",
                "        for possible_module in possible_modules:\n",
                "            module_path = os.path.join(custom_node_path, possible_module)\n",
                "            if os.path.isfile(module_path) or module_path.endswith(\".disabled\") or module_path == \"__pycache__\":\n",
                "                continue\n",
                "\n",
                "            script_path = os.path.join(module_path, \"prestartup_script.py\")\n",
                "            if os.path.exists(script_path):\n",
                "                time_before = time.perf_counter()\n",
                "                success = execute_script(script_path)\n",
                "                node_prestartup_times.append((time.perf_counter() - time_before, module_path, success))\n",
                "    if len(node_prestartup_times) > 0:\n",
                "        print(\"\\nPrestartup times for custom nodes:\")\n",
                "        for n in sorted(node_prestartup_times):\n",
                "            if n[2]:\n",
                "                import_message = \"\"\n",
                "            else:\n",
                "                import_message = \" (PRESTARTUP FAILED)\"\n",
                "            print(\"{:6.1f} seconds{}:\".format(n[0], import_message), n[1])\n",
                "        print()\n",
                "\n",
                "execute_prestartup_script()\n",
                "\n",
                "\n",
                "# Main code\n",
                "import asyncio\n",
                "import itertools\n",
                "import shutil\n",
                "import threading\n",
                "import gc\n",
                "\n",
                "from comfy.cli_args import args\n",
                "\n",
                "if os.name == \"nt\":\n",
                "    import logging\n",
                "    logging.getLogger(\"xformers\").addFilter(lambda record: 'A matching Triton is not available' not in record.getMessage())\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    if args.cuda_device is not None:\n",
                "        os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda_device)\n",
                "        print(\"Set cuda device to:\", args.cuda_device)\n",
                "\n",
                "    if args.deterministic:\n",
                "        if 'CUBLAS_WORKSPACE_CONFIG' not in os.environ:\n",
                "            os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
                "\n",
                "    import cuda_malloc\n",
                "\n",
                "import comfy.utils\n",
                "import yaml\n",
                "\n",
                "import execution\n",
                "import server\n",
                "from server import BinaryEventTypes\n",
                "from nodes import init_custom_nodes\n",
                "import comfy.model_management\n",
                "\n",
                "def cuda_malloc_warning():\n",
                "    device = comfy.model_management.get_torch_device()\n",
                "    device_name = comfy.model_management.get_torch_device_name(device)\n",
                "    cuda_malloc_warning = False\n",
                "    if \"cudaMallocAsync\" in device_name:\n",
                "        for b in cuda_malloc.blacklist:\n",
                "            if b in device_name:\n",
                "                cuda_malloc_warning = True\n",
                "        if cuda_malloc_warning:\n",
                "            print(\"\\nWARNING: this card most likely does not support cuda-malloc, if you get \\\"CUDA error\\\" please run ComfyUI with: --disable-cuda-malloc\\n\")\n",
                "\n",
                "def prompt_worker(q, server):\n",
                "    e = execution.PromptExecutor(server)\n",
                "    last_gc_collect = 0\n",
                "    need_gc = False\n",
                "    gc_collect_interval = 10.0\n",
                "\n",
                "    while True:\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        timeout = None\n"
                ],
                "after": [
                    "        timeout = 1000.0\n"
                ],
                "parent_version_range": {
                    "start": 99,
                    "end": 100
                },
                "child_version_range": {
                    "start": 99,
                    "end": 100
                },
                "control_flow": [
                    {
                        "type": "while_statement",
                        "statement": "while True:",
                        "start_line": 98,
                        "end_line": 126
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "prompt_worker",
                        "signature": "def prompt_worker(q, server):",
                        "at_line": 92
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: main.py\nCode:\n           def prompt_worker(q, server):\n               ...\n 96  96        gc_collect_interval = 10.0\n 97  97    \n 98  98        while True:\n 99      -         timeout = None\n     99  +         timeout = 1000.0\n100 100            if need_gc:\n101 101                timeout = max(gc_collect_interval - (current_time - last_gc_collect), 0.0)\n102 102    \n         ...\n",
                "file_path": "main.py",
                "identifiers_before": [
                    "timeout"
                ],
                "identifiers_after": [
                    "timeout"
                ],
                "prefix": [
                    "    gc_collect_interval = 10.0\n",
                    "\n",
                    "    while True:\n"
                ],
                "suffix": [
                    "        if need_gc:\n",
                    "            timeout = max(gc_collect_interval - (current_time - last_gc_collect), 0.0)\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        if need_gc:\n",
                "            timeout = max(gc_collect_interval - (current_time - last_gc_collect), 0.0)\n",
                "\n",
                "        queue_item = q.get(timeout=timeout)\n",
                "        if queue_item is not None:\n",
                "            item, item_id = queue_item\n",
                "            execution_start_time = time.perf_counter()\n",
                "            prompt_id = item[1]\n",
                "            server.last_prompt_id = prompt_id\n",
                "\n",
                "            e.execute(item[2], prompt_id, item[3], item[4])\n",
                "            need_gc = True\n",
                "            q.task_done(item_id, e.outputs_ui)\n",
                "            if server.client_id is not None:\n",
                "                server.send_sync(\"executing\", { \"node\": None, \"prompt_id\": prompt_id }, server.client_id)\n",
                "\n",
                "            current_time = time.perf_counter()\n",
                "            execution_time = current_time - execution_start_time\n",
                "            print(\"Prompt executed in {:.2f} seconds\".format(execution_time))\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        flags = q.get_flags()\n",
                    "        free_memory = flags.get(\"free_memory\", False)\n",
                    "\n",
                    "        if flags.get(\"unload_models\", free_memory):\n",
                    "            comfy.model_management.unload_all_models()\n",
                    "            need_gc = True\n",
                    "            last_gc_collect = 0\n",
                    "\n",
                    "        if free_memory:\n",
                    "            e.reset()\n",
                    "            need_gc = True\n",
                    "            last_gc_collect = 0\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 120,
                    "end": 120
                },
                "child_version_range": {
                    "start": 120,
                    "end": 133
                },
                "control_flow": [
                    {
                        "type": "while_statement",
                        "statement": "while True:",
                        "start_line": 98,
                        "end_line": 126
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "prompt_worker",
                        "signature": "def prompt_worker(q, server):",
                        "at_line": 92
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: main.py\nCode:\n           def prompt_worker(q, server):\n               ...\n117 117                execution_time = current_time - execution_start_time\n118 118                print(\"Prompt executed in {:.2f} seconds\".format(execution_time))\n119 119    \n    120  +         flags = q.get_flags()\n    121  +         free_memory = flags.get(\"free_memory\", False)\n    122  + \n    123  +         if flags.get(\"unload_models\", free_memory):\n    124  +             comfy.model_management.unload_all_models()\n    125  +             need_gc = True\n    126  +             last_gc_collect = 0\n    127  + \n    128  +         if free_memory:\n    129  +             e.reset()\n    130  +             need_gc = True\n    131  +             last_gc_collect = 0\n    132  + \n120 133            if need_gc:\n121 134                current_time = time.perf_counter()\n122 135                if (current_time - last_gc_collect) > gc_collect_interval:\n         ...\n",
                "file_path": "main.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "comfy",
                    "e",
                    "flags",
                    "free_memory",
                    "get",
                    "get_flags",
                    "last_gc_collect",
                    "model_management",
                    "need_gc",
                    "q",
                    "reset",
                    "unload_all_models"
                ],
                "prefix": [
                    "            execution_time = current_time - execution_start_time\n",
                    "            print(\"Prompt executed in {:.2f} seconds\".format(execution_time))\n",
                    "\n"
                ],
                "suffix": [
                    "        if need_gc:\n",
                    "            current_time = time.perf_counter()\n",
                    "            if (current_time - last_gc_collect) > gc_collect_interval:\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "reset",
                            "position": {
                                "start": {
                                    "line": 129,
                                    "column": 14
                                },
                                "end": {
                                    "line": 129,
                                    "column": 19
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/ComfyUI/main.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        if need_gc:\n",
                "            current_time = time.perf_counter()\n",
                "            if (current_time - last_gc_collect) > gc_collect_interval:\n",
                "                gc.collect()\n",
                "                comfy.model_management.soft_empty_cache()\n",
                "                last_gc_collect = current_time\n",
                "                need_gc = False\n",
                "\n",
                "async def run(server, address='', port=8188, verbose=True, call_on_start=None):\n",
                "    await asyncio.gather(server.start(address, port, verbose, call_on_start), server.publish_loop())\n",
                "\n",
                "\n",
                "def hijack_progress(server):\n",
                "    def hook(value, total, preview_image):\n",
                "        comfy.model_management.throw_exception_if_processing_interrupted()\n",
                "        progress = {\"value\": value, \"max\": total, \"prompt_id\": server.last_prompt_id, \"node\": server.last_node_id}\n",
                "\n",
                "        server.send_sync(\"progress\", progress, server.client_id)\n",
                "        if preview_image is not None:\n",
                "            server.send_sync(BinaryEventTypes.UNENCODED_PREVIEW_IMAGE, preview_image, server.client_id)\n",
                "    comfy.utils.set_progress_bar_global_hook(hook)\n",
                "\n",
                "\n",
                "def cleanup_temp():\n",
                "    temp_dir = folder_paths.get_temp_directory()\n",
                "    if os.path.exists(temp_dir):\n",
                "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
                "\n",
                "\n",
                "def load_extra_path_config(yaml_path):\n",
                "    with open(yaml_path, 'r') as stream:\n",
                "        config = yaml.safe_load(stream)\n",
                "    for c in config:\n",
                "        conf = config[c]\n",
                "        if conf is None:\n",
                "            continue\n",
                "        base_path = None\n",
                "        if \"base_path\" in conf:\n",
                "            base_path = conf.pop(\"base_path\")\n",
                "        for x in conf:\n",
                "            for y in conf[x].split(\"\\n\"):\n",
                "                if len(y) == 0:\n",
                "                    continue\n",
                "                full_path = y\n",
                "                if base_path is not None:\n",
                "                    full_path = os.path.join(base_path, full_path)\n",
                "                print(\"Adding extra search path\", x, full_path)\n",
                "                folder_paths.add_model_folder_path(x, full_path)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    if args.temp_directory:\n",
                "        temp_dir = os.path.join(os.path.abspath(args.temp_directory), \"temp\")\n",
                "        print(f\"Setting temp directory to: {temp_dir}\")\n",
                "        folder_paths.set_temp_directory(temp_dir)\n",
                "    cleanup_temp()\n",
                "\n",
                "    loop = asyncio.new_event_loop()\n",
                "    asyncio.set_event_loop(loop)\n",
                "    server = server.PromptServer(loop)\n",
                "    q = execution.PromptQueue(server)\n",
                "\n",
                "    extra_model_paths_config_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"extra_model_paths.yaml\")\n",
                "    if os.path.isfile(extra_model_paths_config_path):\n",
                "        load_extra_path_config(extra_model_paths_config_path)\n",
                "\n",
                "    if args.extra_model_paths_config:\n",
                "        for config_path in itertools.chain(*args.extra_model_paths_config):\n",
                "            load_extra_path_config(config_path)\n",
                "\n",
                "    init_custom_nodes()\n",
                "\n",
                "    cuda_malloc_warning()\n",
                "\n",
                "    server.add_routes()\n",
                "    hijack_progress(server)\n",
                "\n",
                "    threading.Thread(target=prompt_worker, daemon=True, args=(q, server,)).start()\n",
                "\n",
                "    if args.output_directory:\n",
                "        output_dir = os.path.abspath(args.output_directory)\n",
                "        print(f\"Setting output directory to: {output_dir}\")\n",
                "        folder_paths.set_output_directory(output_dir)\n",
                "\n",
                "    #These are the default folders that checkpoints, clip and vae models will be saved to when using CheckpointSave, etc.. nodes\n",
                "    folder_paths.add_model_folder_path(\"checkpoints\", os.path.join(folder_paths.get_output_directory(), \"checkpoints\"))\n",
                "    folder_paths.add_model_folder_path(\"clip\", os.path.join(folder_paths.get_output_directory(), \"clip\"))\n",
                "    folder_paths.add_model_folder_path(\"vae\", os.path.join(folder_paths.get_output_directory(), \"vae\"))\n",
                "\n",
                "    if args.input_directory:\n",
                "        input_dir = os.path.abspath(args.input_directory)\n",
                "        print(f\"Setting input directory to: {input_dir}\")\n",
                "        folder_paths.set_input_directory(input_dir)\n",
                "\n",
                "    if args.quick_test_for_ci:\n",
                "        exit(0)\n",
                "\n",
                "    call_on_start = None\n",
                "    if args.auto_launch:\n",
                "        def startup_server(address, port):\n",
                "            import webbrowser\n",
                "            if os.name == 'nt' and address == '0.0.0.0':\n",
                "                address = '127.0.0.1'\n",
                "            webbrowser.open(f\"http://{address}:{port}\")\n",
                "        call_on_start = startup_server\n",
                "\n",
                "    try:\n",
                "        loop.run_until_complete(run(server, address=args.listen, port=args.port, verbose=not args.dont_print_server, call_on_start=call_on_start))\n",
                "    except KeyboardInterrupt:\n",
                "        print(\"\\nStopped server\")\n",
                "\n",
                "    cleanup_temp()"
            ]
        ],
        "server.py": [
            [
                "import os\n",
                "import sys\n",
                "import asyncio\n",
                "import traceback\n",
                "\n",
                "import nodes\n",
                "import folder_paths\n",
                "import execution\n",
                "import uuid\n",
                "import urllib\n",
                "import json\n",
                "import glob\n",
                "import struct\n",
                "from PIL import Image, ImageOps\n",
                "from PIL.PngImagePlugin import PngInfo\n",
                "from io import BytesIO\n",
                "\n",
                "try:\n",
                "    import aiohttp\n",
                "    from aiohttp import web\n",
                "except ImportError:\n",
                "    print(\"Module 'aiohttp' not installed. Please install it via:\")\n",
                "    print(\"pip install aiohttp\")\n",
                "    print(\"or\")\n",
                "    print(\"pip install -r requirements.txt\")\n",
                "    sys.exit()\n",
                "\n",
                "import mimetypes\n",
                "from comfy.cli_args import args\n",
                "import comfy.utils\n",
                "import comfy.model_management\n",
                "\n",
                "\n",
                "class BinaryEventTypes:\n",
                "    PREVIEW_IMAGE = 1\n",
                "    UNENCODED_PREVIEW_IMAGE = 2\n",
                "\n",
                "async def send_socket_catch_exception(function, message):\n",
                "    try:\n",
                "        await function(message)\n",
                "    except (aiohttp.ClientError, aiohttp.ClientPayloadError, ConnectionResetError) as err:\n",
                "        print(\"send error:\", err)\n",
                "\n",
                "@web.middleware\n",
                "async def cache_control(request: web.Request, handler):\n",
                "    response: web.Response = await handler(request)\n",
                "    if request.path.endswith('.js') or request.path.endswith('.css'):\n",
                "        response.headers.setdefault('Cache-Control', 'no-cache')\n",
                "    return response\n",
                "\n",
                "def create_cors_middleware(allowed_origin: str):\n",
                "    @web.middleware\n",
                "    async def cors_middleware(request: web.Request, handler):\n",
                "        if request.method == \"OPTIONS\":\n",
                "            # Pre-flight request. Reply successfully:\n",
                "            response = web.Response()\n",
                "        else:\n",
                "            response = await handler(request)\n",
                "\n",
                "        response.headers['Access-Control-Allow-Origin'] = allowed_origin\n",
                "        response.headers['Access-Control-Allow-Methods'] = 'POST, GET, DELETE, PUT, OPTIONS'\n",
                "        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'\n",
                "        response.headers['Access-Control-Allow-Credentials'] = 'true'\n",
                "        return response\n",
                "\n",
                "    return cors_middleware\n",
                "\n",
                "class PromptServer():\n",
                "    def __init__(self, loop):\n",
                "        PromptServer.instance = self\n",
                "\n",
                "        mimetypes.init()\n",
                "        mimetypes.types_map['.js'] = 'application/javascript; charset=utf-8'\n",
                "\n",
                "        self.supports = [\"custom_nodes_from_web\"]\n",
                "        self.prompt_queue = None\n",
                "        self.loop = loop\n",
                "        self.messages = asyncio.Queue()\n",
                "        self.number = 0\n",
                "\n",
                "        middlewares = [cache_control]\n",
                "        if args.enable_cors_header:\n",
                "            middlewares.append(create_cors_middleware(args.enable_cors_header))\n",
                "\n",
                "        max_upload_size = round(args.max_upload_size * 1024 * 1024)\n",
                "        self.app = web.Application(client_max_size=max_upload_size, middlewares=middlewares)\n",
                "        self.sockets = dict()\n",
                "        self.web_root = os.path.join(os.path.dirname(\n",
                "            os.path.realpath(__file__)), \"web\")\n",
                "        routes = web.RouteTableDef()\n",
                "        self.routes = routes\n",
                "        self.last_node_id = None\n",
                "        self.client_id = None\n",
                "\n",
                "        self.on_prompt_handlers = []\n",
                "\n",
                "        @routes.get('/ws')\n",
                "        async def websocket_handler(request):\n",
                "            ws = web.WebSocketResponse()\n",
                "            await ws.prepare(request)\n",
                "            sid = request.rel_url.query.get('clientId', '')\n",
                "            if sid:\n",
                "                # Reusing existing session, remove old\n",
                "                self.sockets.pop(sid, None)\n",
                "            else:\n",
                "                sid = uuid.uuid4().hex\n",
                "\n",
                "            self.sockets[sid] = ws\n",
                "\n",
                "            try:\n",
                "                # Send initial state to the new client\n",
                "                await self.send(\"status\", { \"status\": self.get_queue_info(), 'sid': sid }, sid)\n",
                "                # On reconnect if we are the currently executing client send the current node\n",
                "                if self.client_id == sid and self.last_node_id is not None:\n",
                "                    await self.send(\"executing\", { \"node\": self.last_node_id }, sid)\n",
                "                    \n",
                "                async for msg in ws:\n",
                "                    if msg.type == aiohttp.WSMsgType.ERROR:\n",
                "                        print('ws connection closed with exception %s' % ws.exception())\n",
                "            finally:\n",
                "                self.sockets.pop(sid, None)\n",
                "            return ws\n",
                "\n",
                "        @routes.get(\"/\")\n",
                "        async def get_root(request):\n",
                "            return web.FileResponse(os.path.join(self.web_root, \"index.html\"))\n",
                "\n",
                "        @routes.get(\"/embeddings\")\n",
                "        def get_embeddings(self):\n",
                "            embeddings = folder_paths.get_filename_list(\"embeddings\")\n",
                "            return web.json_response(list(map(lambda a: os.path.splitext(a)[0], embeddings)))\n",
                "\n",
                "        @routes.get(\"/extensions\")\n",
                "        async def get_extensions(request):\n",
                "            files = glob.glob(os.path.join(\n",
                "                glob.escape(self.web_root), 'extensions/**/*.js'), recursive=True)\n",
                "            \n",
                "            extensions = list(map(lambda f: \"/\" + os.path.relpath(f, self.web_root).replace(\"\\\\\", \"/\"), files))\n",
                "            \n",
                "            for name, dir in nodes.EXTENSION_WEB_DIRS.items():\n",
                "                files = glob.glob(os.path.join(glob.escape(dir), '**/*.js'), recursive=True)\n",
                "                extensions.extend(list(map(lambda f: \"/extensions/\" + urllib.parse.quote(\n",
                "                    name) + \"/\" + os.path.relpath(f, dir).replace(\"\\\\\", \"/\"), files)))\n",
                "\n",
                "            return web.json_response(extensions)\n",
                "\n",
                "        def get_dir_by_type(dir_type):\n",
                "            if dir_type is None:\n",
                "                dir_type = \"input\"\n",
                "\n",
                "            if dir_type == \"input\":\n",
                "                type_dir = folder_paths.get_input_directory()\n",
                "            elif dir_type == \"temp\":\n",
                "                type_dir = folder_paths.get_temp_directory()\n",
                "            elif dir_type == \"output\":\n",
                "                type_dir = folder_paths.get_output_directory()\n",
                "\n",
                "            return type_dir, dir_type\n",
                "\n",
                "        def image_upload(post, image_save_function=None):\n",
                "            image = post.get(\"image\")\n",
                "            overwrite = post.get(\"overwrite\")\n",
                "\n",
                "            image_upload_type = post.get(\"type\")\n",
                "            upload_dir, image_upload_type = get_dir_by_type(image_upload_type)\n",
                "\n",
                "            if image and image.file:\n",
                "                filename = image.filename\n",
                "                if not filename:\n",
                "                    return web.Response(status=400)\n",
                "\n",
                "                subfolder = post.get(\"subfolder\", \"\")\n",
                "                full_output_folder = os.path.join(upload_dir, os.path.normpath(subfolder))\n",
                "                filepath = os.path.abspath(os.path.join(full_output_folder, filename))\n",
                "\n",
                "                if os.path.commonpath((upload_dir, filepath)) != upload_dir:\n",
                "                    return web.Response(status=400)\n",
                "\n",
                "                if not os.path.exists(full_output_folder):\n",
                "                    os.makedirs(full_output_folder)\n",
                "\n",
                "                split = os.path.splitext(filename)\n",
                "\n",
                "                if overwrite is not None and (overwrite == \"true\" or overwrite == \"1\"):\n",
                "                    pass\n",
                "                else:\n",
                "                    i = 1\n",
                "                    while os.path.exists(filepath):\n",
                "                        filename = f\"{split[0]} ({i}){split[1]}\"\n",
                "                        filepath = os.path.join(full_output_folder, filename)\n",
                "                        i += 1\n",
                "\n",
                "                if image_save_function is not None:\n",
                "                    image_save_function(image, post, filepath)\n",
                "                else:\n",
                "                    with open(filepath, \"wb\") as f:\n",
                "                        f.write(image.file.read())\n",
                "\n",
                "                return web.json_response({\"name\" : filename, \"subfolder\": subfolder, \"type\": image_upload_type})\n",
                "            else:\n",
                "                return web.Response(status=400)\n",
                "\n",
                "        @routes.post(\"/upload/image\")\n",
                "        async def upload_image(request):\n",
                "            post = await request.post()\n",
                "            return image_upload(post)\n",
                "\n",
                "\n",
                "        @routes.post(\"/upload/mask\")\n",
                "        async def upload_mask(request):\n",
                "            post = await request.post()\n",
                "\n",
                "            def image_save_function(image, post, filepath):\n",
                "                original_ref = json.loads(post.get(\"original_ref\"))\n",
                "                filename, output_dir = folder_paths.annotated_filepath(original_ref['filename'])\n",
                "\n",
                "                # validation for security: prevent accessing arbitrary path\n",
                "                if filename[0] == '/' or '..' in filename:\n",
                "                    return web.Response(status=400)\n",
                "\n",
                "                if output_dir is None:\n",
                "                    type = original_ref.get(\"type\", \"output\")\n",
                "                    output_dir = folder_paths.get_directory_by_type(type)\n",
                "\n",
                "                if output_dir is None:\n",
                "                    return web.Response(status=400)\n",
                "\n",
                "                if original_ref.get(\"subfolder\", \"\") != \"\":\n",
                "                    full_output_dir = os.path.join(output_dir, original_ref[\"subfolder\"])\n",
                "                    if os.path.commonpath((os.path.abspath(full_output_dir), output_dir)) != output_dir:\n",
                "                        return web.Response(status=403)\n",
                "                    output_dir = full_output_dir\n",
                "\n",
                "                file = os.path.join(output_dir, filename)\n",
                "\n",
                "                if os.path.isfile(file):\n",
                "                    with Image.open(file) as original_pil:\n",
                "                        metadata = PngInfo()\n",
                "                        if hasattr(original_pil,'text'):\n",
                "                            for key in original_pil.text:\n",
                "                                metadata.add_text(key, original_pil.text[key])\n",
                "                        original_pil = original_pil.convert('RGBA')\n",
                "                        mask_pil = Image.open(image.file).convert('RGBA')\n",
                "\n",
                "                        # alpha copy\n",
                "                        new_alpha = mask_pil.getchannel('A')\n",
                "                        original_pil.putalpha(new_alpha)\n",
                "                        original_pil.save(filepath, compress_level=4, pnginfo=metadata)\n",
                "\n",
                "            return image_upload(post, image_save_function)\n",
                "\n",
                "        @routes.get(\"/view\")\n",
                "        async def view_image(request):\n",
                "            if \"filename\" in request.rel_url.query:\n",
                "                filename = request.rel_url.query[\"filename\"]\n",
                "                filename,output_dir = folder_paths.annotated_filepath(filename)\n",
                "\n",
                "                # validation for security: prevent accessing arbitrary path\n",
                "                if filename[0] == '/' or '..' in filename:\n",
                "                    return web.Response(status=400)\n",
                "\n",
                "                if output_dir is None:\n",
                "                    type = request.rel_url.query.get(\"type\", \"output\")\n",
                "                    output_dir = folder_paths.get_directory_by_type(type)\n",
                "\n",
                "                if output_dir is None:\n",
                "                    return web.Response(status=400)\n",
                "\n",
                "                if \"subfolder\" in request.rel_url.query:\n",
                "                    full_output_dir = os.path.join(output_dir, request.rel_url.query[\"subfolder\"])\n",
                "                    if os.path.commonpath((os.path.abspath(full_output_dir), output_dir)) != output_dir:\n",
                "                        return web.Response(status=403)\n",
                "                    output_dir = full_output_dir\n",
                "\n",
                "                filename = os.path.basename(filename)\n",
                "                file = os.path.join(output_dir, filename)\n",
                "\n",
                "                if os.path.isfile(file):\n",
                "                    if 'preview' in request.rel_url.query:\n",
                "                        with Image.open(file) as img:\n",
                "                            preview_info = request.rel_url.query['preview'].split(';')\n",
                "                            image_format = preview_info[0]\n",
                "                            if image_format not in ['webp', 'jpeg'] or 'a' in request.rel_url.query.get('channel', ''):\n",
                "                                image_format = 'webp'\n",
                "\n",
                "                            quality = 90\n",
                "                            if preview_info[-1].isdigit():\n",
                "                                quality = int(preview_info[-1])\n",
                "\n",
                "                            buffer = BytesIO()\n",
                "                            if image_format in ['jpeg'] or request.rel_url.query.get('channel', '') == 'rgb':\n",
                "                                img = img.convert(\"RGB\")\n",
                "                            img.save(buffer, format=image_format, quality=quality)\n",
                "                            buffer.seek(0)\n",
                "\n",
                "                            return web.Response(body=buffer.read(), content_type=f'image/{image_format}',\n",
                "                                                headers={\"Content-Disposition\": f\"filename=\\\"{filename}\\\"\"})\n",
                "\n",
                "                    if 'channel' not in request.rel_url.query:\n",
                "                        channel = 'rgba'\n",
                "                    else:\n",
                "                        channel = request.rel_url.query[\"channel\"]\n",
                "\n",
                "                    if channel == 'rgb':\n",
                "                        with Image.open(file) as img:\n",
                "                            if img.mode == \"RGBA\":\n",
                "                                r, g, b, a = img.split()\n",
                "                                new_img = Image.merge('RGB', (r, g, b))\n",
                "                            else:\n",
                "                                new_img = img.convert(\"RGB\")\n",
                "\n",
                "                            buffer = BytesIO()\n",
                "                            new_img.save(buffer, format='PNG')\n",
                "                            buffer.seek(0)\n",
                "\n",
                "                            return web.Response(body=buffer.read(), content_type='image/png',\n",
                "                                                headers={\"Content-Disposition\": f\"filename=\\\"{filename}\\\"\"})\n",
                "\n",
                "                    elif channel == 'a':\n",
                "                        with Image.open(file) as img:\n",
                "                            if img.mode == \"RGBA\":\n",
                "                                _, _, _, a = img.split()\n",
                "                            else:\n",
                "                                a = Image.new('L', img.size, 255)\n",
                "\n",
                "                            # alpha img\n",
                "                            alpha_img = Image.new('RGBA', img.size)\n",
                "                            alpha_img.putalpha(a)\n",
                "                            alpha_buffer = BytesIO()\n",
                "                            alpha_img.save(alpha_buffer, format='PNG')\n",
                "                            alpha_buffer.seek(0)\n",
                "\n",
                "                            return web.Response(body=alpha_buffer.read(), content_type='image/png',\n",
                "                                                headers={\"Content-Disposition\": f\"filename=\\\"{filename}\\\"\"})\n",
                "                    else:\n",
                "                        return web.FileResponse(file, headers={\"Content-Disposition\": f\"filename=\\\"{filename}\\\"\"})\n",
                "\n",
                "            return web.Response(status=404)\n",
                "\n",
                "        @routes.get(\"/view_metadata/{folder_name}\")\n",
                "        async def view_metadata(request):\n",
                "            folder_name = request.match_info.get(\"folder_name\", None)\n",
                "            if folder_name is None:\n",
                "                return web.Response(status=404)\n",
                "            if not \"filename\" in request.rel_url.query:\n",
                "                return web.Response(status=404)\n",
                "\n",
                "            filename = request.rel_url.query[\"filename\"]\n",
                "            if not filename.endswith(\".safetensors\"):\n",
                "                return web.Response(status=404)\n",
                "\n",
                "            safetensors_path = folder_paths.get_full_path(folder_name, filename)\n",
                "            if safetensors_path is None:\n",
                "                return web.Response(status=404)\n",
                "            out = comfy.utils.safetensors_header(safetensors_path, max_size=1024*1024)\n",
                "            if out is None:\n",
                "                return web.Response(status=404)\n",
                "            dt = json.loads(out)\n",
                "            if not \"__metadata__\" in dt:\n",
                "                return web.Response(status=404)\n",
                "            return web.json_response(dt[\"__metadata__\"])\n",
                "\n",
                "        @routes.get(\"/system_stats\")\n",
                "        async def get_queue(request):\n",
                "            device = comfy.model_management.get_torch_device()\n",
                "            device_name = comfy.model_management.get_torch_device_name(device)\n",
                "            vram_total, torch_vram_total = comfy.model_management.get_total_memory(device, torch_total_too=True)\n",
                "            vram_free, torch_vram_free = comfy.model_management.get_free_memory(device, torch_free_too=True)\n",
                "            system_stats = {\n",
                "                \"system\": {\n",
                "                    \"os\": os.name,\n",
                "                    \"python_version\": sys.version,\n",
                "                    \"embedded_python\": os.path.split(os.path.split(sys.executable)[0])[1] == \"python_embeded\"\n",
                "                },\n",
                "                \"devices\": [\n",
                "                    {\n",
                "                        \"name\": device_name,\n",
                "                        \"type\": device.type,\n",
                "                        \"index\": device.index,\n",
                "                        \"vram_total\": vram_total,\n",
                "                        \"vram_free\": vram_free,\n",
                "                        \"torch_vram_total\": torch_vram_total,\n",
                "                        \"torch_vram_free\": torch_vram_free,\n",
                "                    }\n",
                "                ]\n",
                "            }\n",
                "            return web.json_response(system_stats)\n",
                "\n",
                "        @routes.get(\"/prompt\")\n",
                "        async def get_prompt(request):\n",
                "            return web.json_response(self.get_queue_info())\n",
                "\n",
                "        def node_info(node_class):\n",
                "            obj_class = nodes.NODE_CLASS_MAPPINGS[node_class]\n",
                "            info = {}\n",
                "            info['input'] = obj_class.INPUT_TYPES()\n",
                "            info['output'] = obj_class.RETURN_TYPES\n",
                "            info['output_is_list'] = obj_class.OUTPUT_IS_LIST if hasattr(obj_class, 'OUTPUT_IS_LIST') else [False] * len(obj_class.RETURN_TYPES)\n",
                "            info['output_name'] = obj_class.RETURN_NAMES if hasattr(obj_class, 'RETURN_NAMES') else info['output']\n",
                "            info['name'] = node_class\n",
                "            info['display_name'] = nodes.NODE_DISPLAY_NAME_MAPPINGS[node_class] if node_class in nodes.NODE_DISPLAY_NAME_MAPPINGS.keys() else node_class\n",
                "            info['description'] = obj_class.DESCRIPTION if hasattr(obj_class,'DESCRIPTION') else ''\n",
                "            info['category'] = 'sd'\n",
                "            if hasattr(obj_class, 'OUTPUT_NODE') and obj_class.OUTPUT_NODE == True:\n",
                "                info['output_node'] = True\n",
                "            else:\n",
                "                info['output_node'] = False\n",
                "\n",
                "            if hasattr(obj_class, 'CATEGORY'):\n",
                "                info['category'] = obj_class.CATEGORY\n",
                "            return info\n",
                "\n",
                "        @routes.get(\"/object_info\")\n",
                "        async def get_object_info(request):\n",
                "            out = {}\n",
                "            for x in nodes.NODE_CLASS_MAPPINGS:\n",
                "                try:\n",
                "                    out[x] = node_info(x)\n",
                "                except Exception as e:\n",
                "                    print(f\"[ERROR] An error occurred while retrieving information for the '{x}' node.\", file=sys.stderr)\n",
                "                    traceback.print_exc()\n",
                "            return web.json_response(out)\n",
                "\n",
                "        @routes.get(\"/object_info/{node_class}\")\n",
                "        async def get_object_info_node(request):\n",
                "            node_class = request.match_info.get(\"node_class\", None)\n",
                "            out = {}\n",
                "            if (node_class is not None) and (node_class in nodes.NODE_CLASS_MAPPINGS):\n",
                "                out[node_class] = node_info(node_class)\n",
                "            return web.json_response(out)\n",
                "\n",
                "        @routes.get(\"/history\")\n",
                "        async def get_history(request):\n",
                "            max_items = request.rel_url.query.get(\"max_items\", None)\n",
                "            if max_items is not None:\n",
                "                max_items = int(max_items)\n",
                "            return web.json_response(self.prompt_queue.get_history(max_items=max_items))\n",
                "\n",
                "        @routes.get(\"/history/{prompt_id}\")\n",
                "        async def get_history(request):\n",
                "            prompt_id = request.match_info.get(\"prompt_id\", None)\n",
                "            return web.json_response(self.prompt_queue.get_history(prompt_id=prompt_id))\n",
                "\n",
                "        @routes.get(\"/queue\")\n",
                "        async def get_queue(request):\n",
                "            queue_info = {}\n",
                "            current_queue = self.prompt_queue.get_current_queue()\n",
                "            queue_info['queue_running'] = current_queue[0]\n",
                "            queue_info['queue_pending'] = current_queue[1]\n",
                "            return web.json_response(queue_info)\n",
                "\n",
                "        @routes.post(\"/prompt\")\n",
                "        async def post_prompt(request):\n",
                "            print(\"got prompt\")\n",
                "            resp_code = 200\n",
                "            out_string = \"\"\n",
                "            json_data =  await request.json()\n",
                "            json_data = self.trigger_on_prompt(json_data)\n",
                "\n",
                "            if \"number\" in json_data:\n",
                "                number = float(json_data['number'])\n",
                "            else:\n",
                "                number = self.number\n",
                "                if \"front\" in json_data:\n",
                "                    if json_data['front']:\n",
                "                        number = -number\n",
                "\n",
                "                self.number += 1\n",
                "\n",
                "            if \"prompt\" in json_data:\n",
                "                prompt = json_data[\"prompt\"]\n",
                "                valid = execution.validate_prompt(prompt)\n",
                "                extra_data = {}\n",
                "                if \"extra_data\" in json_data:\n",
                "                    extra_data = json_data[\"extra_data\"]\n",
                "\n",
                "                if \"client_id\" in json_data:\n",
                "                    extra_data[\"client_id\"] = json_data[\"client_id\"]\n",
                "                if valid[0]:\n",
                "                    prompt_id = str(uuid.uuid4())\n",
                "                    outputs_to_execute = valid[2]\n",
                "                    self.prompt_queue.put((number, prompt_id, prompt, extra_data, outputs_to_execute))\n",
                "                    response = {\"prompt_id\": prompt_id, \"number\": number, \"node_errors\": valid[3]}\n",
                "                    return web.json_response(response)\n",
                "                else:\n",
                "                    print(\"invalid prompt:\", valid[1])\n",
                "                    return web.json_response({\"error\": valid[1], \"node_errors\": valid[3]}, status=400)\n",
                "            else:\n",
                "                return web.json_response({\"error\": \"no prompt\", \"node_errors\": []}, status=400)\n",
                "\n",
                "        @routes.post(\"/queue\")\n",
                "        async def post_queue(request):\n",
                "            json_data =  await request.json()\n",
                "            if \"clear\" in json_data:\n",
                "                if json_data[\"clear\"]:\n",
                "                    self.prompt_queue.wipe_queue()\n",
                "            if \"delete\" in json_data:\n",
                "                to_delete = json_data['delete']\n",
                "                for id_to_delete in to_delete:\n",
                "                    delete_func = lambda a: a[1] == id_to_delete\n",
                "                    self.prompt_queue.delete_queue_item(delete_func)\n",
                "\n",
                "            return web.Response(status=200)\n",
                "\n",
                "        @routes.post(\"/interrupt\")\n",
                "        async def post_interrupt(request):\n",
                "            nodes.interrupt_processing()\n",
                "            return web.Response(status=200)\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        @routes.post(\"/free\")\n",
                    "        async def post_interrupt(request):\n",
                    "            json_data = await request.json()\n",
                    "            unload_models = json_data.get(\"unload_models\", False)\n",
                    "            free_memory = json_data.get(\"free_memory\", False)\n",
                    "            if unload_models:\n",
                    "                self.prompt_queue.set_flag(\"unload_models\", unload_models)\n",
                    "            if free_memory:\n",
                    "                self.prompt_queue.set_flag(\"free_memory\", free_memory)\n",
                    "            return web.Response(status=200)\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 509,
                    "end": 509
                },
                "child_version_range": {
                    "start": 509,
                    "end": 520
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "PromptServer",
                        "signature": "class PromptServer():",
                        "at_line": 67
                    },
                    {
                        "type": "function",
                        "name": "__init__",
                        "signature": "def __init__(self, loop):",
                        "at_line": 68
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: server.py\nCode:\n           class PromptServer():\n               ...\n               def __init__(self, loop):\n                   ...\n506 506                nodes.interrupt_processing()\n507 507                return web.Response(status=200)\n508 508    \n    509  +         @routes.post(\"/free\")\n    510  +         async def post_interrupt(request):\n    511  +             json_data = await request.json()\n    512  +             unload_models = json_data.get(\"unload_models\", False)\n    513  +             free_memory = json_data.get(\"free_memory\", False)\n    514  +             if unload_models:\n    515  +                 self.prompt_queue.set_flag(\"unload_models\", unload_models)\n    516  +             if free_memory:\n    517  +                 self.prompt_queue.set_flag(\"free_memory\", free_memory)\n    518  +             return web.Response(status=200)\n    519  + \n509 520            @routes.post(\"/history\")\n510 521            async def post_history(request):\n511 522                json_data =  await request.json()\n         ...\n",
                "file_path": "server.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "Response",
                    "free_memory",
                    "get",
                    "json",
                    "json_data",
                    "post",
                    "post_interrupt",
                    "prompt_queue",
                    "request",
                    "routes",
                    "self",
                    "set_flag",
                    "status",
                    "unload_models",
                    "web"
                ],
                "prefix": [
                    "            nodes.interrupt_processing()\n",
                    "            return web.Response(status=200)\n",
                    "\n"
                ],
                "suffix": [
                    "        @routes.post(\"/history\")\n",
                    "        async def post_history(request):\n",
                    "            json_data =  await request.json()\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        @routes.post(\"/history\")\n",
                "        async def post_history(request):\n",
                "            json_data =  await request.json()\n",
                "            if \"clear\" in json_data:\n",
                "                if json_data[\"clear\"]:\n",
                "                    self.prompt_queue.wipe_history()\n",
                "            if \"delete\" in json_data:\n",
                "                to_delete = json_data['delete']\n",
                "                for id_to_delete in to_delete:\n",
                "                    self.prompt_queue.delete_history_item(id_to_delete)\n",
                "\n",
                "            return web.Response(status=200)\n",
                "        \n",
                "    def add_routes(self):\n",
                "        self.app.add_routes(self.routes)\n",
                "\n",
                "        for name, dir in nodes.EXTENSION_WEB_DIRS.items():\n",
                "            self.app.add_routes([\n",
                "                web.static('/extensions/' + urllib.parse.quote(name), dir, follow_symlinks=True),\n",
                "            ])\n",
                "\n",
                "        self.app.add_routes([\n",
                "            web.static('/', self.web_root, follow_symlinks=True),\n",
                "        ])\n",
                "\n",
                "    def get_queue_info(self):\n",
                "        prompt_info = {}\n",
                "        exec_info = {}\n",
                "        exec_info['queue_remaining'] = self.prompt_queue.get_tasks_remaining()\n",
                "        prompt_info['exec_info'] = exec_info\n",
                "        return prompt_info\n",
                "\n",
                "    async def send(self, event, data, sid=None):\n",
                "        if event == BinaryEventTypes.UNENCODED_PREVIEW_IMAGE:\n",
                "            await self.send_image(data, sid=sid)\n",
                "        elif isinstance(data, (bytes, bytearray)):\n",
                "            await self.send_bytes(event, data, sid)\n",
                "        else:\n",
                "            await self.send_json(event, data, sid)\n",
                "\n",
                "    def encode_bytes(self, event, data):\n",
                "        if not isinstance(event, int):\n",
                "            raise RuntimeError(f\"Binary event types must be integers, got {event}\")\n",
                "\n",
                "        packed = struct.pack(\">I\", event)\n",
                "        message = bytearray(packed)\n",
                "        message.extend(data)\n",
                "        return message\n",
                "\n",
                "    async def send_image(self, image_data, sid=None):\n",
                "        image_type = image_data[0]\n",
                "        image = image_data[1]\n",
                "        max_size = image_data[2]\n",
                "        if max_size is not None:\n",
                "            if hasattr(Image, 'Resampling'):\n",
                "                resampling = Image.Resampling.BILINEAR\n",
                "            else:\n",
                "                resampling = Image.ANTIALIAS\n",
                "\n",
                "            image = ImageOps.contain(image, (max_size, max_size), resampling)\n",
                "        type_num = 1\n",
                "        if image_type == \"JPEG\":\n",
                "            type_num = 1\n",
                "        elif image_type == \"PNG\":\n",
                "            type_num = 2\n",
                "\n",
                "        bytesIO = BytesIO()\n",
                "        header = struct.pack(\">I\", type_num)\n",
                "        bytesIO.write(header)\n",
                "        image.save(bytesIO, format=image_type, quality=95, compress_level=1)\n",
                "        preview_bytes = bytesIO.getvalue()\n",
                "        await self.send_bytes(BinaryEventTypes.PREVIEW_IMAGE, preview_bytes, sid=sid)\n",
                "\n",
                "    async def send_bytes(self, event, data, sid=None):\n",
                "        message = self.encode_bytes(event, data)\n",
                "\n",
                "        if sid is None:\n",
                "            sockets = list(self.sockets.values())\n",
                "            for ws in sockets:\n",
                "                await send_socket_catch_exception(ws.send_bytes, message)\n",
                "        elif sid in self.sockets:\n",
                "            await send_socket_catch_exception(self.sockets[sid].send_bytes, message)\n",
                "\n",
                "    async def send_json(self, event, data, sid=None):\n",
                "        message = {\"type\": event, \"data\": data}\n",
                "\n",
                "        if sid is None:\n",
                "            sockets = list(self.sockets.values())\n",
                "            for ws in sockets:\n",
                "                await send_socket_catch_exception(ws.send_json, message)\n",
                "        elif sid in self.sockets:\n",
                "            await send_socket_catch_exception(self.sockets[sid].send_json, message)\n",
                "\n",
                "    def send_sync(self, event, data, sid=None):\n",
                "        self.loop.call_soon_threadsafe(\n",
                "            self.messages.put_nowait, (event, data, sid))\n",
                "\n",
                "    def queue_updated(self):\n",
                "        self.send_sync(\"status\", { \"status\": self.get_queue_info() })\n",
                "\n",
                "    async def publish_loop(self):\n",
                "        while True:\n",
                "            msg = await self.messages.get()\n",
                "            await self.send(*msg)\n",
                "\n",
                "    async def start(self, address, port, verbose=True, call_on_start=None):\n",
                "        runner = web.AppRunner(self.app, access_log=None)\n",
                "        await runner.setup()\n",
                "        site = web.TCPSite(runner, address, port)\n",
                "        await site.start()\n",
                "\n",
                "        if address == '':\n",
                "            address = '0.0.0.0'\n",
                "        if verbose:\n",
                "            print(\"Starting server\\n\")\n",
                "            print(\"To see the GUI go to: http://{}:{}\".format(address, port))\n",
                "        if call_on_start is not None:\n",
                "            call_on_start(address, port)\n",
                "\n",
                "    def add_on_prompt_handler(self, handler):\n",
                "        self.on_prompt_handlers.append(handler)\n",
                "\n",
                "    def trigger_on_prompt(self, json_data):\n",
                "        for handler in self.on_prompt_handlers:\n",
                "            try:\n",
                "                json_data = handler(json_data)\n",
                "            except Exception as e:\n",
                "                print(f\"[ERROR] An error occurred during the on_prompt_handler processing\")\n",
                "                traceback.print_exc()\n",
                "\n",
                "        return json_data"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "bi-directional",
            "reason": "same action"
        },
        {
            "edit_hunk_pair": [
                0,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                2,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "data flow"
        },
        {
            "edit_hunk_pair": [
                3,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "def use"
        },
        {
            "edit_hunk_pair": [
                3,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "def use"
        }
    ]
}