{
    "language": "python",
    "commit_url": "https://github.com/jax-ml/jax/commit/0b1b812e3be92c1731ef84af2e5dbe8fe4ef03c4",
    "commit_message": "bugfix for sm3 optimizer\n\nEnsure atleast 1d input. Optimizer should work now with scalar data.\nfixes #7421",
    "commit_snapshots": {
        "jax/experimental/optimizers.py": [
            [
                "# Copyright 2018 Google LLC\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     https://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\n",
                "\"\"\"Optimizers for use with JAX.\n",
                "\n",
                "This module contains some convenient optimizer definitions, specifically\n",
                "initialization and update functions, which can be used with ndarrays or\n",
                "arbitrarily-nested tuple/list/dicts of ndarrays.\n",
                "\n",
                "An optimizer is modeled as an ``(init_fun, update_fun, get_params)`` triple of\n",
                "functions, where the component functions have these signatures:\n",
                "\n",
                "::\n",
                "\n",
                "  init_fun(params)\n",
                "\n",
                "  Args:\n",
                "    params: pytree representing the initial parameters.\n",
                "\n",
                "  Returns:\n",
                "    A pytree representing the initial optimizer state, which includes the\n",
                "    initial parameters and may also include auxiliary values like initial\n",
                "    momentum. The optimizer state pytree structure generally differs from that\n",
                "    of `params`.\n",
                "\n",
                "::\n",
                "\n",
                "  update_fun(step, grads, opt_state)\n",
                "\n",
                "  Args:\n",
                "    step: integer representing the step index.\n",
                "    grads: a pytree with the same structure as `get_params(opt_state)`\n",
                "      representing the gradients to be used in updating the optimizer state.\n",
                "    opt_state: a pytree representing the optimizer state to be updated.\n",
                "\n",
                "  Returns:\n",
                "    A pytree with the same structure as the `opt_state` argument representing\n",
                "    the updated optimizer state.\n",
                "\n",
                "::\n",
                "\n",
                "  get_params(opt_state)\n",
                "\n",
                "  Args:\n",
                "    opt_state: pytree representing an optimizer state.\n",
                "\n",
                "  Returns:\n",
                "    A pytree representing the parameters extracted from `opt_state`, such that\n",
                "    the invariant `params == get_params(init_fun(params))` holds true.\n",
                "\n",
                "\n",
                "Notice that an optimizer implementation has a lot of flexibility in the form of\n",
                "opt_state: it just has to be a pytree of JaxTypes (so that it can be passed to\n",
                "the JAX transforms defined in api.py) and it has to be consumable by update_fun\n",
                "and get_params.\n",
                "\n",
                "Example Usage:\n",
                "\n",
                ".. code-block:: python\n",
                "\n",
                "  opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
                "  opt_state = opt_init(params)\n",
                "\n",
                "  def step(step, opt_state):\n",
                "    value, grads = jax.value_and_grad(loss_fn)(get_params(opt_state))\n",
                "    opt_state = opt_update(step, grads, opt_state)\n",
                "    return value, opt_state\n",
                "\n",
                "  for i in range(num_steps):\n",
                "    value, opt_state = step(i, opt_state)\n",
                "\"\"\"\n",
                "\n",
                "from typing import Any, Callable, NamedTuple, Tuple, Union\n",
                "\n",
                "from collections import namedtuple\n",
                "import functools\n",
                "\n",
                "import jax.numpy as jnp\n",
                "from jax._src.util import partial, safe_zip, safe_map, unzip2\n",
                "from jax import tree_util\n",
                "from jax.tree_util import (tree_map, tree_flatten, tree_unflatten,\n",
                "                           register_pytree_node)\n",
                "\n",
                "map = safe_map\n",
                "zip = safe_zip\n",
                "\n",
                "\n",
                "# The implementation here basically works by flattening pytrees. There are two\n",
                "# levels of pytrees to think about: the pytree of params, which we can think of\n",
                "# as defining an \"outer pytree\", and a pytree produced by applying init_fun to\n",
                "# each leaf of the params pytree, which we can think of as the \"inner pytrees\".\n",
                "# Since pytrees can be flattened, that structure is isomorphic to a list of\n",
                "# lists (with no further nesting).\n",
                "\n",
                "OptimizerState = namedtuple(\"OptimizerState\",\n",
                "                            [\"packed_state\", \"tree_def\", \"subtree_defs\"])\n",
                "register_pytree_node(\n",
                "    OptimizerState,\n",
                "    lambda xs: ((xs.packed_state,), (xs.tree_def, xs.subtree_defs)),\n",
                "    lambda data, xs: OptimizerState(xs[0], data[0], data[1]))\n",
                "\n",
                "\n",
                "Array = Any\n",
                "Params = Any  # Parameters are arbitrary nests of `jnp.ndarrays`.\n",
                "State = Any   # internal State\n",
                "Updates = Params  # Gradient updates are of the same type as parameters.\n",
                "\n",
                "InitFn = Callable[[Params], OptimizerState]\n",
                "Step = int\n",
                "UpdateFn = Callable[[Step, Updates, OptimizerState], OptimizerState]\n",
                "ParamsFn = Callable[[OptimizerState], Params]\n",
                "\n",
                "class Optimizer(NamedTuple):\n",
                "  init_fn: InitFn\n",
                "  update_fn: UpdateFn\n",
                "  params_fn: ParamsFn\n",
                "\n",
                "Schedule = Callable[[Step], float]\n",
                "\n",
                "def optimizer(opt_maker: Callable[...,\n",
                "  Tuple[Callable[[Params], State],\n",
                "        Callable[[Step, Updates, Params], Params],\n",
                "        Callable[[State], Params]]]) -> Callable[..., Optimizer]:\n",
                "  \"\"\"Decorator to make an optimizer defined for arrays generalize to containers.\n",
                "\n",
                "  With this decorator, you can write init, update, and get_params functions that\n",
                "  each operate only on single arrays, and convert them to corresponding\n",
                "  functions that operate on pytrees of parameters. See the optimizers defined in\n",
                "  optimizers.py for examples.\n",
                "\n",
                "  Args:\n",
                "    opt_maker: a function that returns an ``(init_fun, update_fun, get_params)``\n",
                "      triple of functions that might only work with ndarrays, as per\n",
                "\n",
                "      .. code-block:: haskell\n",
                "\n",
                "          init_fun :: ndarray -> OptStatePytree ndarray\n",
                "          update_fun :: OptStatePytree ndarray -> OptStatePytree ndarray\n",
                "          get_params :: OptStatePytree ndarray -> ndarray\n",
                "\n",
                "  Returns:\n",
                "    An ``(init_fun, update_fun, get_params)`` triple of functions that work on\n",
                "    arbitrary pytrees, as per\n",
                "\n",
                "    .. code-block:: haskell\n",
                "\n",
                "          init_fun :: ParameterPytree ndarray -> OptimizerState\n",
                "          update_fun :: OptimizerState -> OptimizerState\n",
                "          get_params :: OptimizerState -> ParameterPytree ndarray\n",
                "\n",
                "    The OptimizerState pytree type used by the returned functions is isomorphic\n",
                "    to ``ParameterPytree (OptStatePytree ndarray)``, but may store the state\n",
                "    instead as e.g. a partially-flattened data structure for performance.\n",
                "  \"\"\"\n",
                "\n",
                "  @functools.wraps(opt_maker)\n",
                "  def tree_opt_maker(*args, **kwargs):\n",
                "    init, update, get_params = opt_maker(*args, **kwargs)\n",
                "\n",
                "    @functools.wraps(init)\n",
                "    def tree_init(x0_tree):\n",
                "      x0_flat, tree = tree_flatten(x0_tree)\n",
                "      initial_states = [init(x0) for x0 in x0_flat]\n",
                "      states_flat, subtrees = unzip2(map(tree_flatten, initial_states))\n",
                "      return OptimizerState(states_flat, tree, subtrees)\n",
                "\n",
                "    @functools.wraps(update)\n",
                "    def tree_update(i, grad_tree, opt_state):\n",
                "      states_flat, tree, subtrees = opt_state\n",
                "      grad_flat, tree2 = tree_flatten(grad_tree)\n",
                "      if tree2 != tree:\n",
                "        msg = (\"optimizer update function was passed a gradient tree that did \"\n",
                "               \"not match the parameter tree structure with which it was \"\n",
                "               \"initialized: parameter tree {} and grad tree {}.\")\n",
                "        raise TypeError(msg.format(tree, tree2))\n",
                "      states = map(tree_unflatten, subtrees, states_flat)\n",
                "      new_states = map(partial(update, i), grad_flat, states)\n",
                "      new_states_flat, subtrees2 = unzip2(map(tree_flatten, new_states))\n",
                "      for subtree, subtree2 in zip(subtrees, subtrees2):\n",
                "        if subtree2 != subtree:\n",
                "          msg = (\"optimizer update function produced an output structure that \"\n",
                "                 \"did not match its input structure: input {} and output {}.\")\n",
                "          raise TypeError(msg.format(subtree, subtree2))\n",
                "      return OptimizerState(new_states_flat, tree, subtrees)\n",
                "\n",
                "    @functools.wraps(get_params)\n",
                "    def tree_get_params(opt_state):\n",
                "      states_flat, tree, subtrees = opt_state\n",
                "      states = map(tree_unflatten, subtrees, states_flat)\n",
                "      params = map(get_params, states)\n",
                "      return tree_unflatten(tree, params)\n",
                "\n",
                "    return Optimizer(tree_init, tree_update, tree_get_params)\n",
                "  return tree_opt_maker\n",
                "\n",
                "\n",
                "### optimizers\n",
                "\n",
                "@optimizer\n",
                "def sgd(step_size):\n",
                "  \"\"\"Construct optimizer triple for stochastic gradient descent.\n",
                "\n",
                "  Args:\n",
                "    step_size: positive scalar, or a callable representing a step size schedule\n",
                "      that maps the iteration index to positive scalar.\n",
                "\n",
                "  Returns:\n",
                "    An (init_fun, update_fun, get_params) triple.\n",
                "  \"\"\"\n",
                "  step_size = make_schedule(step_size)\n",
                "  def init(x0):\n",
                "    return x0\n",
                "  def update(i, g, x):\n",
                "    return x - step_size(i) * g\n",
                "  def get_params(x):\n",
                "    return x\n",
                "  return Optimizer(init, update, get_params)\n",
                "\n",
                "@optimizer\n",
                "def momentum(step_size: Schedule, mass: float):\n",
                "  \"\"\"Construct optimizer triple for SGD with momentum.\n",
                "\n",
                "  Args:\n",
                "    step_size: positive scalar, or a callable representing a step size schedule\n",
                "      that maps the iteration index to positive scalar.\n",
                "    mass: positive scalar representing the momentum coefficient.\n",
                "\n",
                "  Returns:\n",
                "    An (init_fun, update_fun, get_params) triple.\n",
                "  \"\"\"\n",
                "  step_size = make_schedule(step_size)\n",
                "  def init(x0):\n",
                "    v0 = jnp.zeros_like(x0)\n",
                "    return x0, v0\n",
                "  def update(i, g, state):\n",
                "    x, velocity = state\n",
                "    velocity = mass * velocity + g\n",
                "    x = x - step_size(i) * velocity\n",
                "    return x, velocity\n",
                "  def get_params(state):\n",
                "    x, _ = state\n",
                "    return x\n",
                "  return init, update, get_params\n",
                "\n",
                "\n",
                "@optimizer\n",
                "def nesterov(step_size: Schedule, mass: float):\n",
                "  \"\"\"Construct optimizer triple for SGD with Nesterov momentum.\n",
                "\n",
                "  Args:\n",
                "    step_size: positive scalar, or a callable representing a step size schedule\n",
                "      that maps the iteration index to positive scalar.\n",
                "    mass: positive scalar representing the momentum coefficient.\n",
                "\n",
                "  Returns:\n",
                "    An (init_fun, update_fun, get_params) triple.\n",
                "  \"\"\"\n",
                "  step_size = make_schedule(step_size)\n",
                "  def init(x0):\n",
                "    v0 = jnp.zeros_like(x0)\n",
                "    return x0, v0\n",
                "  def update(i, g, state):\n",
                "    x, velocity = state\n",
                "    velocity = mass * velocity + g\n",
                "    x = x - step_size(i) * (mass * velocity + g)\n",
                "    return x, velocity\n",
                "  def get_params(state):\n",
                "    x, _ = state\n",
                "    return x\n",
                "  return init, update, get_params\n",
                "\n",
                "\n",
                "@optimizer\n",
                "def adagrad(step_size, momentum=0.9):\n",
                "  \"\"\"Construct optimizer triple for Adagrad.\n",
                "\n",
                "  Adaptive Subgradient Methods for Online Learning and Stochastic Optimization:\n",
                "  http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n",
                "\n",
                "  Args:\n",
                "    step_size: positive scalar, or a callable representing a step size schedule\n",
                "      that maps the iteration index to positive scalar.\n",
                "    momentum: optional, a positive scalar value for momentum\n",
                "\n",
                "  Returns:\n",
                "    An (init_fun, update_fun, get_params) triple.\n",
                "  \"\"\"\n",
                "  step_size = make_schedule(step_size)\n",
                "\n",
                "  def init(x0):\n",
                "    g_sq = jnp.zeros_like(x0)\n",
                "    m = jnp.zeros_like(x0)\n",
                "    return x0, g_sq, m\n",
                "\n",
                "  def update(i, g, state):\n",
                "    x, g_sq, m = state\n",
                "    g_sq += jnp.square(g)\n",
                "    g_sq_inv_sqrt = jnp.where(g_sq > 0, 1. / jnp.sqrt(g_sq), 0.0)\n",
                "    m = (1. - momentum) * (g * g_sq_inv_sqrt) + momentum * m\n",
                "    x = x - step_size(i) * m\n",
                "    return x, g_sq, m\n",
                "\n",
                "  def get_params(state):\n",
                "    x, _, _ = state\n",
                "    return x\n",
                "\n",
                "  return init, update, get_params\n",
                "\n",
                "\n",
                "@optimizer\n",
                "def rmsprop(step_size, gamma=0.9, eps=1e-8):\n",
                "  \"\"\"Construct optimizer triple for RMSProp.\n",
                "\n",
                "  Args:\n",
                "    step_size: positive scalar, or a callable representing a step size schedule\n",
                "      that maps the iteration index to positive scalar.\n",
                "      gamma: Decay parameter.\n",
                "      eps: Epsilon parameter.\n",
                "\n",
                "  Returns:\n",
                "    An (init_fun, update_fun, get_params) triple.\n",
                "  \"\"\"\n",
                "  step_size = make_schedule(step_size)\n",
                "  def init(x0):\n",
                "    avg_sq_grad = jnp.zeros_like(x0)\n",
                "    return x0, avg_sq_grad\n",
                "  def update(i, g, state):\n",
                "    x, avg_sq_grad = state\n",
                "    avg_sq_grad = avg_sq_grad * gamma + jnp.square(g) * (1. - gamma)\n",
                "    x = x - step_size(i) * g / jnp.sqrt(avg_sq_grad + eps)\n",
                "    return x, avg_sq_grad\n",
                "  def get_params(state):\n",
                "    x, _ = state\n",
                "    return x\n",
                "  return init, update, get_params\n",
                "\n",
                "\n",
                "@optimizer\n",
                "def rmsprop_momentum(step_size, gamma=0.9, eps=1e-8, momentum=0.9):\n",
                "  \"\"\"Construct optimizer triple for RMSProp with momentum.\n",
                "\n",
                "  This optimizer is separate from the rmsprop optimizer because it needs to\n",
                "  keep track of additional parameters.\n",
                "\n",
                "  Args:\n",
                "    step_size: positive scalar, or a callable representing a step size schedule\n",
                "      that maps the iteration index to positive scalar.\n",
                "    gamma: Decay parameter.\n",
                "    eps: Epsilon parameter.\n",
                "    momentum: Momentum parameter.\n",
                "\n",
                "  Returns:\n",
                "    An (init_fun, update_fun, get_params) triple.\n",
                "  \"\"\"\n",
                "  step_size = make_schedule(step_size)\n",
                "  def init(x0):\n",
                "    avg_sq_grad = jnp.zeros_like(x0)\n",
                "    mom = jnp.zeros_like(x0)\n",
                "    return x0, avg_sq_grad, mom\n",
                "  def update(i, g, state):\n",
                "    x, avg_sq_grad, mom = state\n",
                "    avg_sq_grad = avg_sq_grad * gamma + jnp.square(g) * (1. - gamma)\n",
                "    mom = momentum * mom + step_size(i) * g / jnp.sqrt(avg_sq_grad + eps)\n",
                "    x = x - mom\n",
                "    return x, avg_sq_grad, mom\n",
                "  def get_params(state):\n",
                "    x, _, _ = state\n",
                "    return x\n",
                "  return init, update, get_params\n",
                "\n",
                "\n",
                "@optimizer\n",
                "def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):\n",
                "  \"\"\"Construct optimizer triple for Adam.\n",
                "\n",
                "  Args:\n",
                "    step_size: positive scalar, or a callable representing a step size schedule\n",
                "      that maps the iteration index to positive scalar.\n",
                "    b1: optional, a positive scalar value for beta_1, the exponential decay rate\n",
                "      for the first moment estimates (default 0.9).\n",
                "    b2: optional, a positive scalar value for beta_2, the exponential decay rate\n",
                "      for the second moment estimates (default 0.999).\n",
                "    eps: optional, a positive scalar value for epsilon, a small constant for\n",
                "      numerical stability (default 1e-8).\n",
                "\n",
                "  Returns:\n",
                "    An (init_fun, update_fun, get_params) triple.\n",
                "  \"\"\"\n",
                "  step_size = make_schedule(step_size)\n",
                "  def init(x0):\n",
                "    m0 = jnp.zeros_like(x0)\n",
                "    v0 = jnp.zeros_like(x0)\n",
                "    return x0, m0, v0\n",
                "  def update(i, g, state):\n",
                "    x, m, v = state\n",
                "    m = (1 - b1) * g + b1 * m  # First  moment estimate.\n",
                "    v = (1 - b2) * jnp.square(g) + b2 * v  # Second moment estimate.\n",
                "    mhat = m / (1 - jnp.asarray(b1, m.dtype) ** (i + 1))  # Bias correction.\n",
                "    vhat = v / (1 - jnp.asarray(b2, m.dtype) ** (i + 1))\n",
                "    x = x - step_size(i) * mhat / (jnp.sqrt(vhat) + eps)\n",
                "    return x, m, v\n",
                "  def get_params(state):\n",
                "    x, _, _ = state\n",
                "    return x\n",
                "  return init, update, get_params\n",
                "\n",
                "\n",
                "@optimizer\n",
                "def adamax(step_size, b1=0.9, b2=0.999, eps=1e-8):\n",
                "  \"\"\"Construct optimizer triple for AdaMax (a variant of Adam based on infinity norm).\n",
                "\n",
                "  Args:\n",
                "    step_size: positive scalar, or a callable representing a step size schedule\n",
                "      that maps the iteration index to positive scalar.\n",
                "    b1: optional, a positive scalar value for beta_1, the exponential decay rate\n",
                "      for the first moment estimates (default 0.9).\n",
                "    b2: optional, a positive scalar value for beta_2, the exponential decay rate\n",
                "      for the second moment estimates (default 0.999).\n",
                "    eps: optional, a positive scalar value for epsilon, a small constant for\n",
                "      numerical stability (default 1e-8).\n",
                "\n",
                "  Returns:\n",
                "    An (init_fun, update_fun, get_params) triple.\n",
                "  \"\"\"\n",
                "  step_size = make_schedule(step_size)\n",
                "  def init(x0):\n",
                "    m0 = jnp.zeros_like(x0)\n",
                "    u0 = jnp.zeros_like(x0)\n",
                "    return x0, m0, u0\n",
                "  def update(i, g, state):\n",
                "    x, m, u = state\n",
                "    m = (1 - b1) * g + b1 * m  # First  moment estimate.\n",
                "    u = jnp.maximum(b2 * u, jnp.abs(g))  # Update exponentially weighted infinity norm.\n",
                "    x = (x - (step_size(i) / (1 - jnp.asarray(b1, m.dtype) ** (i + 1))) * m\n",
                "         / (u + eps))\n",
                "    return x, m, u\n",
                "  def get_params(state):\n",
                "    x, _, _ = state\n",
                "    return x\n",
                "  return init, update, get_params\n",
                "\n",
                "\n",
                "@optimizer\n",
                "def sm3(step_size, momentum=0.9):\n",
                "  \"\"\"Construct optimizer triple for SM3.\n",
                "\n",
                "  Memory-Efficient Adaptive Optimization for Large-Scale Learning.\n",
                "  https://arxiv.org/abs/1901.11150\n",
                "\n",
                "  Args:\n",
                "    step_size: positive scalar, or a callable representing a step size schedule\n",
                "      that maps the iteration index to positive scalar.\n",
                "    momentum: optional, a positive scalar value for momentum\n",
                "\n",
                "  Returns:\n",
                "    An (init_fun, update_fun, get_params) triple.\n",
                "  \"\"\"\n",
                "  step_size = make_schedule(step_size)\n",
                "\n",
                "  def splice(seq, i, x):\n",
                "    lst = list(seq)\n",
                "    lst[i:i+1] = x\n",
                "    return lst\n",
                "\n",
                "  def broadcast_into(ndim, x, axis):\n",
                "    idx = splice([None] * ndim, axis, [slice(None)])\n",
                "    return x[tuple(idx)]\n",
                "\n",
                "  def init(x0):\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "    x_shape = x0.shape\n",
                    "    x0 = jnp.atleast_1d(x0)\n"
                ],
                "parent_version_range": {
                    "start": 480,
                    "end": 480
                },
                "child_version_range": {
                    "start": 480,
                    "end": 482
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "sm3",
                        "signature": "def sm3(step_size, momentum=0.9):",
                        "at_line": 454
                    },
                    {
                        "type": "function",
                        "name": "init",
                        "signature": "def init(x0):",
                        "at_line": 479
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: jax/experimental/optimizers.py\nCode:\n           def sm3(step_size, momentum=0.9):\n               ...\n477 477        return x[tuple(idx)]\n478 478    \n479 479      def init(x0):\n    480  +     x_shape = x0.shape\n    481  +     x0 = jnp.atleast_1d(x0)\n480 482        vs = [jnp.zeros(sz, dtype=x0.dtype) for sz in x0.shape]\n         ...\n",
                "file_path": "jax/experimental/optimizers.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "atleast_1d",
                    "jnp",
                    "shape",
                    "x0",
                    "x_shape"
                ],
                "prefix": [
                    "    return x[tuple(idx)]\n",
                    "\n",
                    "  def init(x0):\n"
                ],
                "suffix": [
                    "    vs = [jnp.zeros(sz, dtype=x0.dtype) for sz in x0.shape]\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "x_shape",
                            "position": {
                                "start": {
                                    "line": 480,
                                    "column": 4
                                },
                                "end": {
                                    "line": 480,
                                    "column": 11
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/jax/jax/experimental/optimizers.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "    vs = [jnp.zeros(sz, dtype=x0.dtype) for sz in x0.shape]\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    return x0, jnp.zeros_like(x0), vs\n"
                ],
                "after": [
                    "    return x0, jnp.zeros_like(x0), vs, x_shape\n"
                ],
                "parent_version_range": {
                    "start": 481,
                    "end": 482
                },
                "child_version_range": {
                    "start": 483,
                    "end": 484
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "sm3",
                        "signature": "def sm3(step_size, momentum=0.9):",
                        "at_line": 454
                    },
                    {
                        "type": "function",
                        "name": "init",
                        "signature": "def init(x0):",
                        "at_line": 479
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: jax/experimental/optimizers.py\nCode:\n           def sm3(step_size, momentum=0.9):\n               ...\n               def init(x0):\n                   ...\n480 482        vs = [jnp.zeros(sz, dtype=x0.dtype) for sz in x0.shape]\n481      -     return x0, jnp.zeros_like(x0), vs\n    483  +     return x0, jnp.zeros_like(x0), vs, x_shape\n482 484    \n483 485      def update(i, g, state):\n         ...\n",
                "file_path": "jax/experimental/optimizers.py",
                "identifiers_before": [
                    "jnp",
                    "vs",
                    "x0",
                    "zeros_like"
                ],
                "identifiers_after": [
                    "jnp",
                    "vs",
                    "x0",
                    "x_shape",
                    "zeros_like"
                ],
                "prefix": [
                    "    vs = [jnp.zeros(sz, dtype=x0.dtype) for sz in x0.shape]\n"
                ],
                "suffix": [
                    "\n",
                    "  def update(i, g, state):\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "x_shape",
                            "position": {
                                "start": {
                                    "line": 483,
                                    "column": 39
                                },
                                "end": {
                                    "line": 483,
                                    "column": 46
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/jax/jax/experimental/optimizers.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    2,
                    3
                ]
            },
            [
                "\n",
                "  def update(i, g, state):\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    x, m, vs = state\n"
                ],
                "after": [
                    "    x, m, vs, x_shape = state\n"
                ],
                "parent_version_range": {
                    "start": 484,
                    "end": 485
                },
                "child_version_range": {
                    "start": 486,
                    "end": 487
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "sm3",
                        "signature": "def sm3(step_size, momentum=0.9):",
                        "at_line": 454
                    },
                    {
                        "type": "function",
                        "name": "update",
                        "signature": "def update(i, g, state):",
                        "at_line": 483
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: jax/experimental/optimizers.py\nCode:\n           def sm3(step_size, momentum=0.9):\n               ...\n482 484    \n483 485      def update(i, g, state):\n484      -     x, m, vs = state\n    486  +     x, m, vs, x_shape = state\n485 487        vs = [broadcast_into(g.ndim, v, i) for i, v in enumerate(vs)]\n486 488        accum = functools.reduce(jnp.minimum, vs) + jnp.square(g)\n487 489        accum_inv_sqrt = jnp.where(accum > 0, 1. / jnp.sqrt(accum), 0)\n         ...\n",
                "file_path": "jax/experimental/optimizers.py",
                "identifiers_before": [
                    "m",
                    "state",
                    "vs",
                    "x"
                ],
                "identifiers_after": [
                    "m",
                    "state",
                    "vs",
                    "x",
                    "x_shape"
                ],
                "prefix": [
                    "\n",
                    "  def update(i, g, state):\n"
                ],
                "suffix": [
                    "    vs = [broadcast_into(g.ndim, v, i) for i, v in enumerate(vs)]\n",
                    "    accum = functools.reduce(jnp.minimum, vs) + jnp.square(g)\n",
                    "    accum_inv_sqrt = jnp.where(accum > 0, 1. / jnp.sqrt(accum), 0)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "x_shape",
                            "position": {
                                "start": {
                                    "line": 486,
                                    "column": 14
                                },
                                "end": {
                                    "line": 486,
                                    "column": 21
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/jax/jax/experimental/optimizers.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    1,
                    3
                ]
            },
            [
                "    vs = [broadcast_into(g.ndim, v, i) for i, v in enumerate(vs)]\n",
                "    accum = functools.reduce(jnp.minimum, vs) + jnp.square(g)\n",
                "    accum_inv_sqrt = jnp.where(accum > 0, 1. / jnp.sqrt(accum), 0)\n",
                "    m = (1. - momentum) * (g * accum_inv_sqrt) + momentum * m\n",
                "    x = x - step_size(i) * m\n",
                "    vs = [accum.max(splice(range(x.ndim), j, [])) for j in range(x.ndim)]\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    return x, m, vs\n"
                ],
                "after": [
                    "    return x, m, vs, x_shape\n"
                ],
                "parent_version_range": {
                    "start": 491,
                    "end": 492
                },
                "child_version_range": {
                    "start": 493,
                    "end": 494
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "sm3",
                        "signature": "def sm3(step_size, momentum=0.9):",
                        "at_line": 454
                    },
                    {
                        "type": "function",
                        "name": "update",
                        "signature": "def update(i, g, state):",
                        "at_line": 483
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: jax/experimental/optimizers.py\nCode:\n           def sm3(step_size, momentum=0.9):\n               ...\n               def update(i, g, state):\n                   ...\n488 490        m = (1. - momentum) * (g * accum_inv_sqrt) + momentum * m\n489 491        x = x - step_size(i) * m\n490 492        vs = [accum.max(splice(range(x.ndim), j, [])) for j in range(x.ndim)]\n491      -     return x, m, vs\n    493  +     return x, m, vs, x_shape\n492 494    \n493 495      def get_params(state):\n         ...\n",
                "file_path": "jax/experimental/optimizers.py",
                "identifiers_before": [
                    "m",
                    "vs",
                    "x"
                ],
                "identifiers_after": [
                    "m",
                    "vs",
                    "x",
                    "x_shape"
                ],
                "prefix": [
                    "    m = (1. - momentum) * (g * accum_inv_sqrt) + momentum * m\n",
                    "    x = x - step_size(i) * m\n",
                    "    vs = [accum.max(splice(range(x.ndim), j, [])) for j in range(x.ndim)]\n"
                ],
                "suffix": [
                    "\n",
                    "  def get_params(state):\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "x_shape",
                            "position": {
                                "start": {
                                    "line": 493,
                                    "column": 21
                                },
                                "end": {
                                    "line": 493,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/jax/jax/experimental/optimizers.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    1,
                    2
                ]
            },
            [
                "\n",
                "  def get_params(state):\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    x, _, _ = state\n",
                    "    return x\n"
                ],
                "after": [
                    "    x, _, _, x_shape = state\n",
                    "    return x.reshape(x_shape)\n"
                ],
                "parent_version_range": {
                    "start": 494,
                    "end": 496
                },
                "child_version_range": {
                    "start": 496,
                    "end": 498
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "sm3",
                        "signature": "def sm3(step_size, momentum=0.9):",
                        "at_line": 454
                    },
                    {
                        "type": "function",
                        "name": "get_params",
                        "signature": "def get_params(state):",
                        "at_line": 493
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: jax/experimental/optimizers.py\nCode:\n           def sm3(step_size, momentum=0.9):\n               ...\n492 494    \n493 495      def get_params(state):\n494      -     x, _, _ = state\n495      -     return x\n    496  +     x, _, _, x_shape = state\n    497  +     return x.reshape(x_shape)\n496 498    \n497 499      return init, update, get_params\n498 500    \n         ...\n",
                "file_path": "jax/experimental/optimizers.py",
                "identifiers_before": [
                    "_",
                    "state",
                    "x"
                ],
                "identifiers_after": [
                    "_",
                    "reshape",
                    "state",
                    "x",
                    "x_shape"
                ],
                "prefix": [
                    "\n",
                    "  def get_params(state):\n"
                ],
                "suffix": [
                    "\n",
                    "  return init, update, get_params\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "\n",
                "  return init, update, get_params\n",
                "\n",
                "\n",
                "### learning rate schedules\n",
                "\n",
                "def constant(step_size) -> Schedule:\n",
                "  def schedule(i):\n",
                "    return step_size\n",
                "  return schedule\n",
                "\n",
                "def exponential_decay(step_size, decay_steps, decay_rate):\n",
                "  def schedule(i):\n",
                "    return step_size * decay_rate ** (i / decay_steps)\n",
                "  return schedule\n",
                "\n",
                "def inverse_time_decay(step_size, decay_steps, decay_rate, staircase=False):\n",
                "  if staircase:\n",
                "    def schedule(i):\n",
                "      return step_size / (1 + decay_rate * jnp.floor(i / decay_steps))\n",
                "  else:\n",
                "    def schedule(i):\n",
                "      return step_size / (1 + decay_rate * i / decay_steps)\n",
                "  return schedule\n",
                "\n",
                "def polynomial_decay(step_size, decay_steps, final_step_size, power=1.0):\n",
                "  def schedule(step_num):\n",
                "    step_num = jnp.minimum(step_num, decay_steps)\n",
                "    step_mult = (1 - step_num / decay_steps) ** power\n",
                "    return step_mult * (step_size - final_step_size) + final_step_size\n",
                "\n",
                "  return schedule\n",
                "\n",
                "def piecewise_constant(boundaries: Any, values: Any):\n",
                "  boundaries = jnp.array(boundaries)\n",
                "  values = jnp.array(values)\n",
                "  if not boundaries.ndim == values.ndim == 1:\n",
                "    raise ValueError(\"boundaries and values must be sequences\")\n",
                "  if not boundaries.shape[0] == values.shape[0] - 1:\n",
                "    raise ValueError(\"boundaries length must be one shorter than values length\")\n",
                "\n",
                "  def schedule(i):\n",
                "    return values[jnp.sum(i > boundaries)]\n",
                "  return schedule\n",
                "\n",
                "def make_schedule(scalar_or_schedule: Union[float, Schedule]) -> Schedule:\n",
                "  if callable(scalar_or_schedule):\n",
                "    return scalar_or_schedule\n",
                "  elif jnp.ndim(scalar_or_schedule) == 0:\n",
                "    return constant(scalar_or_schedule)\n",
                "  else:\n",
                "    raise TypeError(type(scalar_or_schedule))\n",
                "\n",
                "\n",
                "### utilities\n",
                "\n",
                "def l2_norm(tree):\n",
                "  \"\"\"Compute the l2 norm of a pytree of arrays. Useful for weight decay.\"\"\"\n",
                "  leaves, _ = tree_flatten(tree)\n",
                "  return jnp.sqrt(sum(jnp.vdot(x, x) for x in leaves))\n",
                "\n",
                "def clip_grads(grad_tree, max_norm):\n",
                "  \"\"\"Clip gradients stored as a pytree of arrays to maximum norm `max_norm`.\"\"\"\n",
                "  norm = l2_norm(grad_tree)\n",
                "  normalize = lambda g: jnp.where(norm < max_norm, g, g * (max_norm / norm))\n",
                "  return tree_map(normalize, grad_tree)\n",
                "\n",
                "\n",
                "### serialization utilities\n",
                "\n",
                "class JoinPoint(object):\n",
                "  \"\"\"Marks the boundary between two joined (nested) pytrees.\"\"\"\n",
                "  def __init__(self, subtree):\n",
                "    self.subtree = subtree\n",
                "\n",
                "  # Since pytrees are containers of numpy arrays, look iterable.\n",
                "  def __iter__(self):\n",
                "    yield self.subtree\n",
                "\n",
                "def unpack_optimizer_state(opt_state):\n",
                "  \"\"\"Converts an OptimizerState to a marked pytree.\n",
                "\n",
                "  Converts an OptimizerState to a marked pytree with the leaves of the outer\n",
                "  pytree represented as JoinPoints to avoid losing information. This function is\n",
                "  intended to be useful when serializing optimizer states.\n",
                "\n",
                "  Args:\n",
                "    opt_state: An OptimizerState\n",
                "  Returns:\n",
                "    A pytree with JoinPoint leaves that contain a second level of pytrees.\n",
                "  \"\"\"\n",
                "  states_flat, tree_def, subtree_defs = opt_state\n",
                "  subtrees = map(tree_unflatten, subtree_defs, states_flat)\n",
                "  sentinels = [JoinPoint(subtree) for subtree in subtrees]\n",
                "  return tree_util.tree_unflatten(tree_def, sentinels)\n",
                "\n",
                "def pack_optimizer_state(marked_pytree):\n",
                "  \"\"\"Converts a marked pytree to an OptimizerState.\n",
                "\n",
                "  The inverse of unpack_optimizer_state. Converts a marked pytree with the\n",
                "  leaves of the outer pytree represented as JoinPoints back into an\n",
                "  OptimizerState. This function is intended to be useful when deserializing\n",
                "  optimizer states.\n",
                "\n",
                "  Args:\n",
                "    marked_pytree: A pytree containing JoinPoint leaves that hold more pytrees.\n",
                "  Returns:\n",
                "    An equivalent OptimizerState to the input argument.\n",
                "  \"\"\"\n",
                "  sentinels, tree_def = tree_flatten(marked_pytree)\n",
                "  assert all(isinstance(s, JoinPoint) for s in sentinels)\n",
                "  subtrees = [s.subtree for s in sentinels]\n",
                "  states_flat, subtree_defs = unzip2(map(tree_flatten, subtrees))\n",
                "  return OptimizerState(states_flat, tree_def, subtree_defs)"
            ]
        ],
        "tests/optimizers_test.py": [
            [
                "# Copyright 2018 Google LLC\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     https://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\n",
                "\"\"\"Tests for the optimizers module.\"\"\"\n",
                "\n",
                "import functools\n",
                "\n",
                "from absl.testing import absltest\n",
                "import numpy as np\n",
                "\n",
                "import jax.numpy as jnp\n",
                "import jax.test_util as jtu\n",
                "from jax import jit, grad, jacfwd, jacrev\n",
                "from jax import tree_util\n",
                "from jax import lax\n",
                "from jax.experimental import optimizers\n",
                "\n",
                "from jax.config import config\n",
                "config.parse_flags_with_absl()\n",
                "\n",
                "\n",
                "class OptimizerTests(jtu.JaxTestCase):\n",
                "\n",
                "  def _CheckOptimizer(self, optimizer, loss, x0, num_steps, *args, **kwargs):\n",
                "    self._CheckFuns(optimizer, loss, x0, *args)\n",
                "    self._CheckRun(optimizer, loss, x0, num_steps, *args, **kwargs)\n",
                "\n",
                "  def _CheckFuns(self, optimizer, loss, x0, *args):\n",
                "    init_fun, update_fun, get_params = optimizer(*args)\n",
                "    opt_state = init_fun(x0)\n",
                "    self.assertAllClose(x0, get_params(opt_state))\n",
                "    opt_state2 = update_fun(0, grad(loss)(x0), opt_state)  # doesn't crash\n",
                "    self.assertEqual(tree_util.tree_structure(opt_state),\n",
                "                     tree_util.tree_structure(opt_state2))\n",
                "\n",
                "  @jtu.skip_on_devices('gpu')\n",
                "  def _CheckRun(self, optimizer, loss, x0, num_steps, *args, **kwargs):\n",
                "    init_fun, update_fun, get_params = optimizer(*args)\n",
                "\n",
                "    opt_state = init_fun(x0)\n",
                "    for i in range(num_steps):\n",
                "      x = get_params(opt_state)\n",
                "      g = grad(loss)(x)\n",
                "      opt_state = update_fun(i, g, opt_state)\n",
                "    xstar = get_params(opt_state)\n",
                "    self.assertLess(loss(xstar), 1e-2)\n",
                "\n",
                "    update_fun_jitted = jit(update_fun)\n",
                "    opt_state = init_fun(x0)\n",
                "    for i in range(num_steps):\n",
                "      x = get_params(opt_state)\n",
                "      g = grad(loss)(x)\n",
                "      opt_state = update_fun_jitted(i, g, opt_state)\n",
                "    xstar = get_params(opt_state)\n",
                "    self.assertLess(loss(xstar), 1e-2)\n",
                "\n",
                "  def testSgdScalar(self):\n",
                "    def loss(x): return x**2\n",
                "    x0 = 1.\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    self._CheckOptimizer(optimizers.sgd, loss, x0, num_iters, step_size)\n",
                "\n",
                "  def testSgdVector(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    self._CheckOptimizer(optimizers.sgd, loss, x0, num_iters, step_size)\n",
                "\n",
                "  def testSgdNestedTuple(self):\n",
                "    def loss(xyz):\n",
                "      x, (y, z) = xyz\n",
                "      return sum(jnp.dot(a, a) for a in [x, y, z])\n",
                "    x0 = (jnp.ones(2), (jnp.ones(2), jnp.ones(2)))\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    self._CheckOptimizer(optimizers.sgd, loss, x0, num_iters, step_size)\n",
                "\n",
                "  def testMomentumVector(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    mass = 0.\n",
                "    self._CheckOptimizer(optimizers.momentum, loss, x0, num_iters, step_size, mass)\n",
                "\n",
                "  def testMomentumDict(self):\n",
                "    def loss(dct): return jnp.dot(dct['x'], dct['x'])\n",
                "    x0 = {'x': jnp.ones(2)}\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    mass = 0.\n",
                "    self._CheckOptimizer(optimizers.momentum, loss, x0, num_iters, step_size, mass)\n",
                "\n",
                "  def testRmspropVector(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    self._CheckOptimizer(optimizers.rmsprop, loss, x0, num_iters, step_size)\n",
                "\n",
                "  def testAdamVector(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    self._CheckOptimizer(optimizers.adam, loss, x0, num_iters, step_size)\n",
                "\n",
                "  def testSgdClosure(self):\n",
                "    def loss(y, x): return y**2 * x**2\n",
                "    x0 = 1.\n",
                "    y = 1.\n",
                "    num_iters = 20\n",
                "    step_size = 0.1\n",
                "    partial_loss = functools.partial(loss, y)\n",
                "    self._CheckRun(optimizers.sgd, partial_loss, x0, num_iters, step_size)\n",
                "\n",
                "  def testAdagrad(self):\n",
                "\n",
                "    def loss(xs):\n",
                "      x1, x2 = xs\n",
                "      return jnp.sum(x1**2) + jnp.sum(x2**2)\n",
                "\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    x0 = (jnp.ones(2), jnp.ones((2, 2)))\n",
                "    self._CheckOptimizer(optimizers.adagrad, loss, x0, num_iters, step_size)\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "  def testSM3(self):\n"
                ],
                "after": [
                    "  def testSM3Scalar(self):\n",
                    "    def loss(x): return x**2\n",
                    "    x0 = jnp.array(1.)\n",
                    "    num_iters = 100\n",
                    "    step_size = 0.1\n",
                    "    self._CheckOptimizer(optimizers.sm3, loss, x0, num_iters, step_size)\n",
                    "\n",
                    "  def testSM3Vector(self):\n"
                ],
                "parent_version_range": {
                    "start": 140,
                    "end": 141
                },
                "child_version_range": {
                    "start": 140,
                    "end": 148
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "OptimizerTests",
                        "signature": "class OptimizerTests(jtu.JaxTestCase):",
                        "at_line": 32
                    },
                    {
                        "type": "function",
                        "name": "testSM3",
                        "signature": "def testSM3(self):",
                        "at_line": 140
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: tests/optimizers_test.py\nCode:\n           class OptimizerTests(jtu.JaxTestCase):\n               ...\n137 137        x0 = (jnp.ones(2), jnp.ones((2, 2)))\n138 138        self._CheckOptimizer(optimizers.adagrad, loss, x0, num_iters, step_size)\n139 139    \n140      -   def testSM3(self):\n    140  +   def testSM3Scalar(self):\n    141  +     def loss(x): return x**2\n    142  +     x0 = jnp.array(1.)\n    143  +     num_iters = 100\n    144  +     step_size = 0.1\n    145  +     self._CheckOptimizer(optimizers.sm3, loss, x0, num_iters, step_size)\n    146  + \n    147  +   def testSM3Vector(self):\n141 148        def loss(xs):\n142 149          x1, x2 = xs\n143 150          return jnp.sum(x1 ** 2) + jnp.sum(x2 ** 2)\n         ...\n",
                "file_path": "tests/optimizers_test.py",
                "identifiers_before": [
                    "self",
                    "testSM3"
                ],
                "identifiers_after": [
                    "_CheckOptimizer",
                    "array",
                    "jnp",
                    "loss",
                    "num_iters",
                    "optimizers",
                    "self",
                    "sm3",
                    "step_size",
                    "testSM3Scalar",
                    "testSM3Vector",
                    "x",
                    "x0"
                ],
                "prefix": [
                    "    x0 = (jnp.ones(2), jnp.ones((2, 2)))\n",
                    "    self._CheckOptimizer(optimizers.adagrad, loss, x0, num_iters, step_size)\n",
                    "\n"
                ],
                "suffix": [
                    "    def loss(xs):\n",
                    "      x1, x2 = xs\n",
                    "      return jnp.sum(x1 ** 2) + jnp.sum(x2 ** 2)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    def loss(xs):\n",
                "      x1, x2 = xs\n",
                "      return jnp.sum(x1 ** 2) + jnp.sum(x2 ** 2)\n",
                "\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    x0 = (jnp.ones(2), jnp.ones((2, 2)))\n",
                "    self._CheckOptimizer(optimizers.sm3, loss, x0, num_iters, step_size)\n",
                "\n",
                "  def testAdaMaxVector(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    num_iters = 100\n",
                "    step_size = 0.1\n",
                "    self._CheckOptimizer(optimizers.adamax, loss, x0, num_iters, step_size)\n",
                "\n",
                "  def testSgdVectorExponentialDecaySchedule(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    step_schedule = optimizers.exponential_decay(0.1, 3, 2.)\n",
                "    self._CheckFuns(optimizers.sgd, loss, x0, step_schedule)\n",
                "\n",
                "  def testSgdVectorInverseTimeDecaySchedule(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    step_schedule = optimizers.inverse_time_decay(0.1, 3, 2.)\n",
                "    self._CheckFuns(optimizers.sgd, loss, x0, step_schedule)\n",
                "\n",
                "  def testAdamVectorInverseTimeDecaySchedule(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    step_schedule = optimizers.inverse_time_decay(0.1, 3, 2.)\n",
                "    self._CheckFuns(optimizers.adam, loss, x0, step_schedule)\n",
                "\n",
                "  def testMomentumVectorInverseTimeDecayStaircaseSchedule(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    step_sched = optimizers.inverse_time_decay(0.1, 3, 2., staircase=True)\n",
                "    mass = 0.9\n",
                "    self._CheckFuns(optimizers.momentum, loss, x0, step_sched, mass)\n",
                "\n",
                "  def testRmspropmomentumVectorPolynomialDecaySchedule(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    step_schedule = optimizers.polynomial_decay(1.0, 50, 0.1)\n",
                "    self._CheckFuns(optimizers.rmsprop_momentum, loss, x0, step_schedule)\n",
                "\n",
                "  def testRmspropVectorPiecewiseConstantSchedule(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    step_schedule = optimizers.piecewise_constant([25, 75], [1.0, 0.5, 0.1])\n",
                "    self._CheckFuns(optimizers.rmsprop, loss, x0, step_schedule)\n",
                "\n",
                "  def testTracedStepSize(self):\n",
                "    def loss(x): return jnp.dot(x, x)\n",
                "    x0 = jnp.ones(2)\n",
                "    step_size = 0.1\n",
                "\n",
                "    init_fun, _, _ = optimizers.sgd(step_size)\n",
                "    opt_state = init_fun(x0)\n",
                "\n",
                "    @jit\n",
                "    def update(opt_state, step_size):\n",
                "      _, update_fun, get_params = optimizers.sgd(step_size)\n",
                "      x = get_params(opt_state)\n",
                "      g = grad(loss)(x)\n",
                "      return update_fun(0, g, opt_state)\n",
                "\n",
                "    update(opt_state, 0.9)  # doesn't crash\n",
                "\n",
                "  # TODO(mattjj): re-enable\n",
                "  # def testDeviceTupleState(self):\n",
                "  #   init_fun, update_fun, _ = optimizers.sgd(0.1)\n",
                "  #   opt_state = init_fun(jnp.zeros(3))\n",
                "  #   self.assertIsInstance(opt_state, optimizers.OptimizerState)\n",
                "  #   self.assertIsInstance(opt_state.packed_state, core.JaxTuple)\n",
                "  #   opt_state = jit(update_fun)(0, jnp.zeros(3), opt_state)\n",
                "  #   self.assertIsInstance(opt_state, optimizers.OptimizerState)\n",
                "  #   self.assertIsInstance(opt_state.packed_state, xla.DeviceTuple)\n",
                "\n",
                "  def testUpdateFunStructureMismatchErrorMessage(self):\n",
                "    @optimizers.optimizer\n",
                "    def opt_maker():\n",
                "      def init_fun(x0):\n",
                "        return {'x': x0}\n",
                "      def update_fun(i, g, opt_state):\n",
                "        x = opt_state['x']\n",
                "        return {'x': x - 0.1 * g, 'v': g}  # bug!\n",
                "      def get_params(opt_state):\n",
                "        return opt_state['x']\n",
                "      return init_fun, update_fun, get_params\n",
                "\n",
                "    init_fun, update_fun, get_params = opt_maker()\n",
                "    opt_state = init_fun(jnp.zeros(3))\n",
                "    self.assertRaises(TypeError, lambda: update_fun(opt_state))\n",
                "\n",
                "  def testUtilityNorm(self):\n",
                "    x0 = (jnp.ones(2), (jnp.ones(3), jnp.ones(4)))\n",
                "    norm = optimizers.l2_norm(x0)\n",
                "    expected = np.sqrt(np.sum(np.ones(2+3+4)**2))\n",
                "    self.assertAllClose(norm, expected, check_dtypes=False)\n",
                "\n",
                "  def testUtilityClipGrads(self):\n",
                "    g = (jnp.ones(2), (jnp.ones(3), jnp.ones(4)))\n",
                "    norm = optimizers.l2_norm(g)\n",
                "\n",
                "    ans = optimizers.clip_grads(g, 1.1 * norm)\n",
                "    expected = g\n",
                "    self.assertAllClose(ans, expected, check_dtypes=False)\n",
                "\n",
                "    ans = optimizers.l2_norm(optimizers.clip_grads(g, 0.9 * norm))\n",
                "    expected = 0.9 * norm\n",
                "    self.assertAllClose(ans, expected, check_dtypes=False)\n",
                "\n",
                "  def testIssue758(self):\n",
                "    # code from https://github.com/google/jax/issues/758\n",
                "    # this is more of a scan + jacfwd/jacrev test, but it lives here to use the\n",
                "    # optimizers.py code\n",
                "\n",
                "    def harmonic_bond(conf, params):\n",
                "      return jnp.sum(conf * params)\n",
                "\n",
                "    opt_init, opt_update, get_params = optimizers.sgd(5e-2)\n",
                "\n",
                "    x0 = np.array([0.5], dtype=np.float64)\n",
                "\n",
                "    def minimize_structure(test_params):\n",
                "      energy_fn = functools.partial(harmonic_bond, params=test_params)\n",
                "      grad_fn = grad(energy_fn, argnums=(0,))\n",
                "      opt_state = opt_init(x0)\n",
                "\n",
                "      def apply_carry(carry, _):\n",
                "        i, x = carry\n",
                "        g = grad_fn(get_params(x))[0]\n",
                "        new_state = opt_update(i, g, x)\n",
                "        new_carry = (i+1, new_state)\n",
                "        return new_carry, _\n",
                "\n",
                "      carry_final, _ = lax.scan(apply_carry, (0, opt_state), jnp.zeros((75, 0)))\n",
                "      trip, opt_final = carry_final\n",
                "      assert trip == 75\n",
                "      return opt_final\n",
                "\n",
                "    initial_params = jnp.array(0.5)\n",
                "    minimize_structure(initial_params)\n",
                "\n",
                "    def loss(test_params):\n",
                "      opt_final = minimize_structure(test_params)\n",
                "      return 1.0 - get_params(opt_final)[0]\n",
                "\n",
                "    loss_opt_init, loss_opt_update, loss_get_params = optimizers.sgd(5e-2)\n",
                "\n",
                "    J1 = jacrev(loss, argnums=(0,))(initial_params)\n",
                "    J2 = jacfwd(loss, argnums=(0,))(initial_params)\n",
                "    self.assertAllClose(J1, J2, rtol=1e-6)\n",
                "\n",
                "  def testUnpackPackRoundTrip(self):\n",
                "    opt_init, _, _ = optimizers.momentum(0.1, mass=0.9)\n",
                "    params = [{'w': np.random.randn(1, 2), 'bias': np.random.randn(2)}]\n",
                "    expected = opt_init(params)\n",
                "    ans = optimizers.pack_optimizer_state(\n",
                "        optimizers.unpack_optimizer_state(expected))\n",
                "    self.assertEqual(ans, expected)\n",
                "\n",
                "if __name__ == '__main__':\n",
                "  absltest.main(testLoader=jtu.JaxTestLoader())"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "bi-directional",
            "reason": "data flow"
        },
        {
            "edit_hunk_pair": [
                1,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "hidden api upate"
        },
        {
            "edit_hunk_pair": [
                1,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "hidden api upate"
        },
        {
            "edit_hunk_pair": [
                1,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "hidden api upate"
        },
        {
            "edit_hunk_pair": [
                2,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "data flow"
        },
        {
            "edit_hunk_pair": [
                2,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "sync"
        },
        {
            "edit_hunk_pair": [
                3,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "sync"
        }
    ]
}