{
    "language": "python",
    "commit_url": "https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/11dd79e346bd780bc5c3119df962e7a9c20f2493",
    "commit_message": "Add an option for faster low quality previews",
    "commit_snapshots": {
        "modules/sd_samplers.py": [
            [
                "from collections import namedtuple, deque\n",
                "import numpy as np\n",
                "from math import floor\n",
                "import torch\n",
                "import tqdm\n",
                "from PIL import Image\n",
                "import inspect\n",
                "import k_diffusion.sampling\n",
                "import torchsde._brownian.brownian_interval\n",
                "import ldm.models.diffusion.ddim\n",
                "import ldm.models.diffusion.plms\n",
                "from modules import prompt_parser, devices, processing, images\n",
                "\n",
                "from modules.shared import opts, cmd_opts, state\n",
                "import modules.shared as shared\n",
                "from modules.script_callbacks import CFGDenoiserParams, cfg_denoiser_callback\n",
                "\n",
                "\n",
                "SamplerData = namedtuple('SamplerData', ['name', 'constructor', 'aliases', 'options'])\n",
                "\n",
                "samplers_k_diffusion = [\n",
                "    ('Euler a', 'sample_euler_ancestral', ['k_euler_a', 'k_euler_ancestral'], {}),\n",
                "    ('Euler', 'sample_euler', ['k_euler'], {}),\n",
                "    ('LMS', 'sample_lms', ['k_lms'], {}),\n",
                "    ('Heun', 'sample_heun', ['k_heun'], {}),\n",
                "    ('DPM2', 'sample_dpm_2', ['k_dpm_2'], {'discard_next_to_last_sigma': True}),\n",
                "    ('DPM2 a', 'sample_dpm_2_ancestral', ['k_dpm_2_a'], {'discard_next_to_last_sigma': True}),\n",
                "    ('DPM++ 2S a', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a'], {}),\n",
                "    ('DPM++ 2M', 'sample_dpmpp_2m', ['k_dpmpp_2m'], {}),\n",
                "    ('DPM++ SDE', 'sample_dpmpp_sde', ['k_dpmpp_sde'], {}),\n",
                "    ('DPM fast', 'sample_dpm_fast', ['k_dpm_fast'], {}),\n",
                "    ('DPM adaptive', 'sample_dpm_adaptive', ['k_dpm_ad'], {}),\n",
                "    ('LMS Karras', 'sample_lms', ['k_lms_ka'], {'scheduler': 'karras'}),\n",
                "    ('DPM2 Karras', 'sample_dpm_2', ['k_dpm_2_ka'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True}),\n",
                "    ('DPM2 a Karras', 'sample_dpm_2_ancestral', ['k_dpm_2_a_ka'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True}),\n",
                "    ('DPM++ 2S a Karras', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a_ka'], {'scheduler': 'karras'}),\n",
                "    ('DPM++ 2M Karras', 'sample_dpmpp_2m', ['k_dpmpp_2m_ka'], {'scheduler': 'karras'}),\n",
                "    ('DPM++ SDE Karras', 'sample_dpmpp_sde', ['k_dpmpp_sde_ka'], {'scheduler': 'karras'}),\n",
                "]\n",
                "\n",
                "samplers_data_k_diffusion = [\n",
                "    SamplerData(label, lambda model, funcname=funcname: KDiffusionSampler(funcname, model), aliases, options)\n",
                "    for label, funcname, aliases, options in samplers_k_diffusion\n",
                "    if hasattr(k_diffusion.sampling, funcname)\n",
                "]\n",
                "\n",
                "all_samplers = [\n",
                "    *samplers_data_k_diffusion,\n",
                "    SamplerData('DDIM', lambda model: VanillaStableDiffusionSampler(ldm.models.diffusion.ddim.DDIMSampler, model), [], {}),\n",
                "    SamplerData('PLMS', lambda model: VanillaStableDiffusionSampler(ldm.models.diffusion.plms.PLMSSampler, model), [], {}),\n",
                "]\n",
                "all_samplers_map = {x.name: x for x in all_samplers}\n",
                "\n",
                "samplers = []\n",
                "samplers_for_img2img = []\n",
                "samplers_map = {}\n",
                "\n",
                "\n",
                "def create_sampler(name, model):\n",
                "    if name is not None:\n",
                "        config = all_samplers_map.get(name, None)\n",
                "    else:\n",
                "        config = all_samplers[0]\n",
                "\n",
                "    assert config is not None, f'bad sampler name: {name}'\n",
                "\n",
                "    sampler = config.constructor(model)\n",
                "    sampler.config = config\n",
                "\n",
                "    return sampler\n",
                "\n",
                "\n",
                "def set_samplers():\n",
                "    global samplers, samplers_for_img2img\n",
                "\n",
                "    hidden = set(opts.hide_samplers)\n",
                "    hidden_img2img = set(opts.hide_samplers + ['PLMS'])\n",
                "\n",
                "    samplers = [x for x in all_samplers if x.name not in hidden]\n",
                "    samplers_for_img2img = [x for x in all_samplers if x.name not in hidden_img2img]\n",
                "\n",
                "    samplers_map.clear()\n",
                "    for sampler in all_samplers:\n",
                "        samplers_map[sampler.name.lower()] = sampler.name\n",
                "        for alias in sampler.aliases:\n",
                "            samplers_map[alias.lower()] = sampler.name\n",
                "\n",
                "\n",
                "set_samplers()\n",
                "\n",
                "sampler_extra_params = {\n",
                "    'sample_euler': ['s_churn', 's_tmin', 's_tmax', 's_noise'],\n",
                "    'sample_heun': ['s_churn', 's_tmin', 's_tmax', 's_noise'],\n",
                "    'sample_dpm_2': ['s_churn', 's_tmin', 's_tmax', 's_noise'],\n",
                "}\n",
                "\n",
                "\n",
                "def setup_img2img_steps(p, steps=None):\n",
                "    if opts.img2img_fix_steps or steps is not None:\n",
                "        steps = int((steps or p.steps) / min(p.denoising_strength, 0.999)) if p.denoising_strength > 0 else 0\n",
                "        t_enc = p.steps - 1\n",
                "    else:\n",
                "        steps = p.steps\n",
                "        t_enc = int(min(p.denoising_strength, 0.999) * steps)\n",
                "\n",
                "    return steps, t_enc\n",
                "\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "def single_sample_to_image(sample):\n",
                    "    x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\n"
                ],
                "after": [
                    "def single_sample_to_image(sample, approximation=False):\n",
                    "    if approximation:\n",
                    "        # https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204/2\n",
                    "        coefs = torch.tensor(\n",
                    "            [[ 0.298,  0.207,  0.208],\n",
                    "             [ 0.187,  0.286,  0.173],\n",
                    "             [-0.158,  0.189,  0.264],\n",
                    "             [-0.184, -0.271, -0.473]]).to(sample.device)\n",
                    "        x_sample = torch.einsum(\"lxy,lr -> rxy\", sample, coefs)\n",
                    "    else:\n",
                    "        x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\n"
                ],
                "parent_version_range": {
                    "start": 108,
                    "end": 110
                },
                "child_version_range": {
                    "start": 108,
                    "end": 119
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "single_sample_to_image",
                        "signature": "def single_sample_to_image(sample):",
                        "at_line": 108
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: modules/sd_samplers.py\nCode:\n105 105        return steps, t_enc\n106 106    \n107 107    \n108      - def single_sample_to_image(sample):\n109      -     x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\n    108  + def single_sample_to_image(sample, approximation=False):\n    109  +     if approximation:\n    110  +         # https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204/2\n    111  +         coefs = torch.tensor(\n    112  +             [[ 0.298,  0.207,  0.208],\n    113  +              [ 0.187,  0.286,  0.173],\n    114  +              [-0.158,  0.189,  0.264],\n    115  +              [-0.184, -0.271, -0.473]]).to(sample.device)\n    116  +         x_sample = torch.einsum(\"lxy,lr -> rxy\", sample, coefs)\n    117  +     else:\n    118  +         x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\n110 119        x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0)\n111 120        x_sample = 255. * np.moveaxis(x_sample.cpu().numpy(), 0, 2)\n112 121        x_sample = x_sample.astype(np.uint8)\n         ...\n",
                "file_path": "modules/sd_samplers.py",
                "identifiers_before": [
                    "decode_first_stage",
                    "processing",
                    "sample",
                    "sd_model",
                    "shared",
                    "single_sample_to_image",
                    "unsqueeze",
                    "x_sample"
                ],
                "identifiers_after": [
                    "approximation",
                    "coefs",
                    "decode_first_stage",
                    "device",
                    "einsum",
                    "processing",
                    "sample",
                    "sd_model",
                    "shared",
                    "single_sample_to_image",
                    "tensor",
                    "to",
                    "torch",
                    "unsqueeze",
                    "x_sample"
                ],
                "prefix": [
                    "    return steps, t_enc\n",
                    "\n",
                    "\n"
                ],
                "suffix": [
                    "    x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0)\n",
                    "    x_sample = 255. * np.moveaxis(x_sample.cpu().numpy(), 0, 2)\n",
                    "    x_sample = x_sample.astype(np.uint8)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "single_sample_to_image",
                            "position": {
                                "start": {
                                    "line": 108,
                                    "column": 4
                                },
                                "end": {
                                    "line": 108,
                                    "column": 26
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "single_sample_to_image",
                            "position": {
                                "start": {
                                    "line": 108,
                                    "column": 4
                                },
                                "end": {
                                    "line": 108,
                                    "column": 26
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "single_sample_to_image",
                            "position": {
                                "start": {
                                    "line": 108,
                                    "column": 4
                                },
                                "end": {
                                    "line": 108,
                                    "column": 26
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "single_sample_to_image",
                            "position": {
                                "start": {
                                    "line": 108,
                                    "column": 4
                                },
                                "end": {
                                    "line": 108,
                                    "column": 26
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "    x_sample = torch.clamp((x_sample + 1.0) / 2.0, min=0.0, max=1.0)\n",
                "    x_sample = 255. * np.moveaxis(x_sample.cpu().numpy(), 0, 2)\n",
                "    x_sample = x_sample.astype(np.uint8)\n",
                "    return Image.fromarray(x_sample)\n",
                "\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "def sample_to_image(samples, index=0):\n",
                    "    return single_sample_to_image(samples[index])\n"
                ],
                "after": [
                    "def sample_to_image(samples, index=0, approximation=False):\n",
                    "    return single_sample_to_image(samples[index], approximation)\n"
                ],
                "parent_version_range": {
                    "start": 116,
                    "end": 118
                },
                "child_version_range": {
                    "start": 125,
                    "end": 127
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "sample_to_image",
                        "signature": "def sample_to_image(samples, index=0):",
                        "at_line": 116
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: modules/sd_samplers.py\nCode:\n113 122        return Image.fromarray(x_sample)\n114 123    \n115 124    \n116      - def sample_to_image(samples, index=0):\n117      -     return single_sample_to_image(samples[index])\n    125  + def sample_to_image(samples, index=0, approximation=False):\n    126  +     return single_sample_to_image(samples[index], approximation)\n118 127    \n119 128    \n         ...\n",
                "file_path": "modules/sd_samplers.py",
                "identifiers_before": [
                    "index",
                    "sample_to_image",
                    "samples",
                    "single_sample_to_image"
                ],
                "identifiers_after": [
                    "approximation",
                    "index",
                    "sample_to_image",
                    "samples",
                    "single_sample_to_image"
                ],
                "prefix": [
                    "    return Image.fromarray(x_sample)\n",
                    "\n",
                    "\n"
                ],
                "suffix": [
                    "\n",
                    "\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "single_sample_to_image",
                            "position": {
                                "start": {
                                    "line": 117,
                                    "column": 11
                                },
                                "end": {
                                    "line": 117,
                                    "column": 33
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "sample_to_image",
                            "position": {
                                "start": {
                                    "line": 116,
                                    "column": 4
                                },
                                "end": {
                                    "line": 116,
                                    "column": 19
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "sample_to_image",
                            "position": {
                                "start": {
                                    "line": 116,
                                    "column": 4
                                },
                                "end": {
                                    "line": 116,
                                    "column": 19
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "single_sample_to_image",
                            "position": {
                                "start": {
                                    "line": 126,
                                    "column": 11
                                },
                                "end": {
                                    "line": 126,
                                    "column": 33
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "sample_to_image",
                            "position": {
                                "start": {
                                    "line": 125,
                                    "column": 4
                                },
                                "end": {
                                    "line": 125,
                                    "column": 19
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "sample_to_image",
                            "position": {
                                "start": {
                                    "line": 125,
                                    "column": 4
                                },
                                "end": {
                                    "line": 125,
                                    "column": 19
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "approximation",
                            "position": {
                                "start": {
                                    "line": 125,
                                    "column": 38
                                },
                                "end": {
                                    "line": 125,
                                    "column": 51
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "approximation",
                            "position": {
                                "start": {
                                    "line": 125,
                                    "column": 38
                                },
                                "end": {
                                    "line": 125,
                                    "column": 51
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    2
                ]
            },
            [
                "\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "def samples_to_image_grid(samples):\n",
                    "    return images.image_grid([single_sample_to_image(sample) for sample in samples])\n"
                ],
                "after": [
                    "def samples_to_image_grid(samples, approximation=False):\n",
                    "    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n"
                ],
                "parent_version_range": {
                    "start": 120,
                    "end": 122
                },
                "child_version_range": {
                    "start": 129,
                    "end": 131
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "samples_to_image_grid",
                        "signature": "def samples_to_image_grid(samples):",
                        "at_line": 120
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: modules/sd_samplers.py\nCode:\n118 127    \n119 128    \n120      - def samples_to_image_grid(samples):\n121      -     return images.image_grid([single_sample_to_image(sample) for sample in samples])\n    129  + def samples_to_image_grid(samples, approximation=False):\n    130  +     return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n122 131    \n123 132    \n124 133    def store_latent(decoded):\n         ...\n",
                "file_path": "modules/sd_samplers.py",
                "identifiers_before": [
                    "image_grid",
                    "images",
                    "sample",
                    "samples",
                    "samples_to_image_grid",
                    "single_sample_to_image"
                ],
                "identifiers_after": [
                    "approximation",
                    "image_grid",
                    "images",
                    "sample",
                    "samples",
                    "samples_to_image_grid",
                    "single_sample_to_image"
                ],
                "prefix": [
                    "\n",
                    "\n"
                ],
                "suffix": [
                    "\n",
                    "\n",
                    "def store_latent(decoded):\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "single_sample_to_image",
                            "position": {
                                "start": {
                                    "line": 121,
                                    "column": 30
                                },
                                "end": {
                                    "line": 121,
                                    "column": 52
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "samples_to_image_grid",
                            "position": {
                                "start": {
                                    "line": 120,
                                    "column": 4
                                },
                                "end": {
                                    "line": 120,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "single_sample_to_image",
                            "position": {
                                "start": {
                                    "line": 130,
                                    "column": 30
                                },
                                "end": {
                                    "line": 130,
                                    "column": 52
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "samples_to_image_grid",
                            "position": {
                                "start": {
                                    "line": 129,
                                    "column": 4
                                },
                                "end": {
                                    "line": 129,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "approximation",
                            "position": {
                                "start": {
                                    "line": 129,
                                    "column": 35
                                },
                                "end": {
                                    "line": 129,
                                    "column": 48
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    1
                ]
            },
            [
                "\n",
                "\n",
                "def store_latent(decoded):\n",
                "    state.current_latent = decoded\n",
                "\n",
                "    if opts.show_progress_every_n_steps > 0 and shared.state.sampling_step % opts.show_progress_every_n_steps == 0:\n",
                "        if not shared.parallel_processing_allowed:\n"
            ],
            {
                "type": "replace",
                "before": [
                    "            shared.state.current_image = sample_to_image(decoded)\n"
                ],
                "after": [
                    "            shared.state.current_image = sample_to_image(decoded, approximation=opts.show_progress_approximate)\n"
                ],
                "parent_version_range": {
                    "start": 129,
                    "end": 130
                },
                "child_version_range": {
                    "start": 138,
                    "end": 139
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if opts.show_progress_every_n_steps > 0 and shared.state.sampling_step % opts.show_progress_every_n_steps == 0:",
                        "start_line": 127,
                        "end_line": 129
                    },
                    {
                        "type": "if_statement",
                        "statement": "if not shared.parallel_processing_allowed:",
                        "start_line": 128,
                        "end_line": 129
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "store_latent",
                        "signature": "def store_latent(decoded):",
                        "at_line": 124
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: modules/sd_samplers.py\nCode:\n           def store_latent(decoded):\n               ...\n126 135    \n127 136        if opts.show_progress_every_n_steps > 0 and shared.state.sampling_step % opts.show_progress_every_n_steps == 0:\n128 137            if not shared.parallel_processing_allowed:\n129      -             shared.state.current_image = sample_to_image(decoded)\n    138  +             shared.state.current_image = sample_to_image(decoded, approximation=opts.show_progress_approximate)\n130 139    \n131 140    \n132 141    class InterruptedException(BaseException):\n         ...\n",
                "file_path": "modules/sd_samplers.py",
                "identifiers_before": [
                    "current_image",
                    "decoded",
                    "sample_to_image",
                    "shared",
                    "state"
                ],
                "identifiers_after": [
                    "approximation",
                    "current_image",
                    "decoded",
                    "opts",
                    "sample_to_image",
                    "shared",
                    "show_progress_approximate",
                    "state"
                ],
                "prefix": [
                    "\n",
                    "    if opts.show_progress_every_n_steps > 0 and shared.state.sampling_step % opts.show_progress_every_n_steps == 0:\n",
                    "        if not shared.parallel_processing_allowed:\n"
                ],
                "suffix": [
                    "\n",
                    "\n",
                    "class InterruptedException(BaseException):\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "sample_to_image",
                            "position": {
                                "start": {
                                    "line": 129,
                                    "column": 41
                                },
                                "end": {
                                    "line": 129,
                                    "column": 56
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "sample_to_image",
                            "position": {
                                "start": {
                                    "line": 138,
                                    "column": 41
                                },
                                "end": {
                                    "line": 138,
                                    "column": 56
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "approximation",
                            "position": {
                                "start": {
                                    "line": 138,
                                    "column": 66
                                },
                                "end": {
                                    "line": 138,
                                    "column": 79
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/sd_samplers.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    4,
                    5
                ]
            },
            [
                "\n",
                "\n",
                "class InterruptedException(BaseException):\n",
                "    pass\n",
                "\n",
                "\n",
                "class VanillaStableDiffusionSampler:\n",
                "    def __init__(self, constructor, sd_model):\n",
                "        self.sampler = constructor(sd_model)\n",
                "        self.is_plms = hasattr(self.sampler, 'p_sample_plms')\n",
                "        self.orig_p_sample_ddim = self.sampler.p_sample_plms if self.is_plms else self.sampler.p_sample_ddim\n",
                "        self.mask = None\n",
                "        self.nmask = None\n",
                "        self.init_latent = None\n",
                "        self.sampler_noises = None\n",
                "        self.step = 0\n",
                "        self.stop_at = None\n",
                "        self.eta = None\n",
                "        self.default_eta = 0.0\n",
                "        self.config = None\n",
                "        self.last_latent = None\n",
                "\n",
                "        self.conditioning_key = sd_model.model.conditioning_key\n",
                "\n",
                "    def number_of_needed_noises(self, p):\n",
                "        return 0\n",
                "\n",
                "    def launch_sampling(self, steps, func):\n",
                "        state.sampling_steps = steps\n",
                "        state.sampling_step = 0\n",
                "\n",
                "        try:\n",
                "            return func()\n",
                "        except InterruptedException:\n",
                "            return self.last_latent\n",
                "\n",
                "    def p_sample_ddim_hook(self, x_dec, cond, ts, unconditional_conditioning, *args, **kwargs):\n",
                "        if state.interrupted or state.skipped:\n",
                "            raise InterruptedException\n",
                "\n",
                "        if self.stop_at is not None and self.step > self.stop_at:\n",
                "            raise InterruptedException\n",
                "\n",
                "        # Have to unwrap the inpainting conditioning here to perform pre-processing\n",
                "        image_conditioning = None\n",
                "        if isinstance(cond, dict):\n",
                "            image_conditioning = cond[\"c_concat\"][0]\n",
                "            cond = cond[\"c_crossattn\"][0]\n",
                "            unconditional_conditioning = unconditional_conditioning[\"c_crossattn\"][0]\n",
                "\n",
                "        conds_list, tensor = prompt_parser.reconstruct_multicond_batch(cond, self.step)\n",
                "        unconditional_conditioning = prompt_parser.reconstruct_cond_batch(unconditional_conditioning, self.step)\n",
                "\n",
                "        assert all([len(conds) == 1 for conds in conds_list]), 'composition via AND is not supported for DDIM/PLMS samplers'\n",
                "        cond = tensor\n",
                "\n",
                "        # for DDIM, shapes must match, we can't just process cond and uncond independently;\n",
                "        # filling unconditional_conditioning with repeats of the last vector to match length is\n",
                "        # not 100% correct but should work well enough\n",
                "        if unconditional_conditioning.shape[1] < cond.shape[1]:\n",
                "            last_vector = unconditional_conditioning[:, -1:]\n",
                "            last_vector_repeated = last_vector.repeat([1, cond.shape[1] - unconditional_conditioning.shape[1], 1])\n",
                "            unconditional_conditioning = torch.hstack([unconditional_conditioning, last_vector_repeated])\n",
                "        elif unconditional_conditioning.shape[1] > cond.shape[1]:\n",
                "            unconditional_conditioning = unconditional_conditioning[:, :cond.shape[1]]\n",
                "\n",
                "        if self.mask is not None:\n",
                "            img_orig = self.sampler.model.q_sample(self.init_latent, ts)\n",
                "            x_dec = img_orig * self.mask + self.nmask * x_dec\n",
                "\n",
                "        # Wrap the image conditioning back up since the DDIM code can accept the dict directly.\n",
                "        # Note that they need to be lists because it just concatenates them later.\n",
                "        if image_conditioning is not None:\n",
                "            cond = {\"c_concat\": [image_conditioning], \"c_crossattn\": [cond]}\n",
                "            unconditional_conditioning = {\"c_concat\": [image_conditioning], \"c_crossattn\": [unconditional_conditioning]}\n",
                "\n",
                "        res = self.orig_p_sample_ddim(x_dec, cond, ts, unconditional_conditioning=unconditional_conditioning, *args, **kwargs)\n",
                "\n",
                "        if self.mask is not None:\n",
                "            self.last_latent = self.init_latent * self.mask + self.nmask * res[1]\n",
                "        else:\n",
                "            self.last_latent = res[1]\n",
                "\n",
                "        store_latent(self.last_latent)\n",
                "\n",
                "        self.step += 1\n",
                "        state.sampling_step = self.step\n",
                "        shared.total_tqdm.update()\n",
                "\n",
                "        return res\n",
                "\n",
                "    def initialize(self, p):\n",
                "        self.eta = p.eta if p.eta is not None else opts.eta_ddim\n",
                "\n",
                "        for fieldname in ['p_sample_ddim', 'p_sample_plms']:\n",
                "            if hasattr(self.sampler, fieldname):\n",
                "                setattr(self.sampler, fieldname, self.p_sample_ddim_hook)\n",
                "\n",
                "        self.mask = p.mask if hasattr(p, 'mask') else None\n",
                "        self.nmask = p.nmask if hasattr(p, 'nmask') else None\n",
                "\n",
                "    def adjust_steps_if_invalid(self, p, num_steps):\n",
                "        if  (self.config.name == 'DDIM' and p.ddim_discretize == 'uniform') or (self.config.name == 'PLMS'):\n",
                "            valid_step = 999 / (1000 // num_steps)\n",
                "            if valid_step == floor(valid_step):\n",
                "                return int(valid_step) + 1\n",
                "        \n",
                "        return num_steps\n",
                "\n",
                "    def sample_img2img(self, p, x, noise, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):\n",
                "        steps, t_enc = setup_img2img_steps(p, steps)\n",
                "        steps = self.adjust_steps_if_invalid(p, steps)\n",
                "        self.initialize(p)\n",
                "\n",
                "        self.sampler.make_schedule(ddim_num_steps=steps, ddim_eta=self.eta, ddim_discretize=p.ddim_discretize, verbose=False)\n",
                "        x1 = self.sampler.stochastic_encode(x, torch.tensor([t_enc] * int(x.shape[0])).to(shared.device), noise=noise)\n",
                "\n",
                "        self.init_latent = x\n",
                "        self.last_latent = x\n",
                "        self.step = 0\n",
                "\n",
                "        # Wrap the conditioning models with additional image conditioning for inpainting model\n",
                "        if image_conditioning is not None:\n",
                "            conditioning = {\"c_concat\": [image_conditioning], \"c_crossattn\": [conditioning]}\n",
                "            unconditional_conditioning = {\"c_concat\": [image_conditioning], \"c_crossattn\": [unconditional_conditioning]}\n",
                "            \n",
                "            \n",
                "        samples = self.launch_sampling(t_enc + 1, lambda: self.sampler.decode(x1, conditioning, t_enc, unconditional_guidance_scale=p.cfg_scale, unconditional_conditioning=unconditional_conditioning))\n",
                "\n",
                "        return samples\n",
                "\n",
                "    def sample(self, p, x, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):\n",
                "        self.initialize(p)\n",
                "\n",
                "        self.init_latent = None\n",
                "        self.last_latent = x\n",
                "        self.step = 0\n",
                "\n",
                "        steps = self.adjust_steps_if_invalid(p, steps or p.steps)\n",
                "\n",
                "        # Wrap the conditioning models with additional image conditioning for inpainting model\n",
                "        # dummy_for_plms is needed because PLMS code checks the first item in the dict to have the right shape\n",
                "        if image_conditioning is not None:\n",
                "            conditioning = {\"dummy_for_plms\": np.zeros((conditioning.shape[0],)), \"c_crossattn\": [conditioning], \"c_concat\": [image_conditioning]}\n",
                "            unconditional_conditioning = {\"c_crossattn\": [unconditional_conditioning], \"c_concat\": [image_conditioning]}\n",
                "\n",
                "        samples_ddim = self.launch_sampling(steps, lambda: self.sampler.sample(S=steps, conditioning=conditioning, batch_size=int(x.shape[0]), shape=x[0].shape, verbose=False, unconditional_guidance_scale=p.cfg_scale, unconditional_conditioning=unconditional_conditioning, x_T=x, eta=self.eta)[0])\n",
                "\n",
                "        return samples_ddim\n",
                "\n",
                "\n",
                "class CFGDenoiser(torch.nn.Module):\n",
                "    def __init__(self, model):\n",
                "        super().__init__()\n",
                "        self.inner_model = model\n",
                "        self.mask = None\n",
                "        self.nmask = None\n",
                "        self.init_latent = None\n",
                "        self.step = 0\n",
                "\n",
                "    def forward(self, x, sigma, uncond, cond, cond_scale, image_cond):\n",
                "        if state.interrupted or state.skipped:\n",
                "            raise InterruptedException\n",
                "\n",
                "        conds_list, tensor = prompt_parser.reconstruct_multicond_batch(cond, self.step)\n",
                "        uncond = prompt_parser.reconstruct_cond_batch(uncond, self.step)\n",
                "\n",
                "        batch_size = len(conds_list)\n",
                "        repeats = [len(conds_list[i]) for i in range(batch_size)]\n",
                "\n",
                "        x_in = torch.cat([torch.stack([x[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [x])\n",
                "        image_cond_in = torch.cat([torch.stack([image_cond[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [image_cond])\n",
                "        sigma_in = torch.cat([torch.stack([sigma[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [sigma])\n",
                "\n",
                "        denoiser_params = CFGDenoiserParams(x_in, image_cond_in, sigma_in, state.sampling_step, state.sampling_steps)\n",
                "        cfg_denoiser_callback(denoiser_params)\n",
                "        x_in = denoiser_params.x\n",
                "        image_cond_in = denoiser_params.image_cond\n",
                "        sigma_in = denoiser_params.sigma\n",
                "\n",
                "        if tensor.shape[1] == uncond.shape[1]:\n",
                "            cond_in = torch.cat([tensor, uncond])\n",
                "\n",
                "            if shared.batch_cond_uncond:\n",
                "                x_out = self.inner_model(x_in, sigma_in, cond={\"c_crossattn\": [cond_in], \"c_concat\": [image_cond_in]})\n",
                "            else:\n",
                "                x_out = torch.zeros_like(x_in)\n",
                "                for batch_offset in range(0, x_out.shape[0], batch_size):\n",
                "                    a = batch_offset\n",
                "                    b = a + batch_size\n",
                "                    x_out[a:b] = self.inner_model(x_in[a:b], sigma_in[a:b], cond={\"c_crossattn\": [cond_in[a:b]], \"c_concat\": [image_cond_in[a:b]]})\n",
                "        else:\n",
                "            x_out = torch.zeros_like(x_in)\n",
                "            batch_size = batch_size*2 if shared.batch_cond_uncond else batch_size\n",
                "            for batch_offset in range(0, tensor.shape[0], batch_size):\n",
                "                a = batch_offset\n",
                "                b = min(a + batch_size, tensor.shape[0])\n",
                "                x_out[a:b] = self.inner_model(x_in[a:b], sigma_in[a:b], cond={\"c_crossattn\": [tensor[a:b]], \"c_concat\": [image_cond_in[a:b]]})\n",
                "\n",
                "            x_out[-uncond.shape[0]:] = self.inner_model(x_in[-uncond.shape[0]:], sigma_in[-uncond.shape[0]:], cond={\"c_crossattn\": [uncond], \"c_concat\": [image_cond_in[-uncond.shape[0]:]]})\n",
                "\n",
                "        denoised_uncond = x_out[-uncond.shape[0]:]\n",
                "        denoised = torch.clone(denoised_uncond)\n",
                "\n",
                "        for i, conds in enumerate(conds_list):\n",
                "            for cond_index, weight in conds:\n",
                "                denoised[i] += (x_out[cond_index] - denoised_uncond[i]) * (weight * cond_scale)\n",
                "\n",
                "        if self.mask is not None:\n",
                "            denoised = self.init_latent * self.mask + self.nmask * denoised\n",
                "\n",
                "        self.step += 1\n",
                "\n",
                "        return denoised\n",
                "\n",
                "\n",
                "class TorchHijack:\n",
                "    def __init__(self, sampler_noises):\n",
                "        # Using a deque to efficiently receive the sampler_noises in the same order as the previous index-based\n",
                "        # implementation.\n",
                "        self.sampler_noises = deque(sampler_noises)\n",
                "\n",
                "    def __getattr__(self, item):\n",
                "        if item == 'randn_like':\n",
                "            return self.randn_like\n",
                "\n",
                "        if hasattr(torch, item):\n",
                "            return getattr(torch, item)\n",
                "\n",
                "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(type(self).__name__, item))\n",
                "\n",
                "    def randn_like(self, x):\n",
                "        if self.sampler_noises:\n",
                "            noise = self.sampler_noises.popleft()\n",
                "            if noise.shape == x.shape:\n",
                "                return noise\n",
                "\n",
                "        if x.device.type == 'mps':\n",
                "            return torch.randn_like(x, device=devices.cpu).to(x.device)\n",
                "        else:\n",
                "            return torch.randn_like(x)\n",
                "\n",
                "\n",
                "# MPS fix for randn in torchsde\n",
                "def torchsde_randn(size, dtype, device, seed):\n",
                "    if device.type == 'mps':\n",
                "        generator = torch.Generator(devices.cpu).manual_seed(int(seed))\n",
                "        return torch.randn(size, dtype=dtype, device=devices.cpu, generator=generator).to(device)\n",
                "    else:\n",
                "        generator = torch.Generator(device).manual_seed(int(seed))\n",
                "        return torch.randn(size, dtype=dtype, device=device, generator=generator)\n",
                "\n",
                "\n",
                "torchsde._brownian.brownian_interval._randn = torchsde_randn\n",
                "\n",
                "\n",
                "class KDiffusionSampler:\n",
                "    def __init__(self, funcname, sd_model):\n",
                "        denoiser = k_diffusion.external.CompVisVDenoiser if sd_model.parameterization == \"v\" else k_diffusion.external.CompVisDenoiser\n",
                "\n",
                "        self.model_wrap = denoiser(sd_model, quantize=shared.opts.enable_quantization)\n",
                "        self.funcname = funcname\n",
                "        self.func = getattr(k_diffusion.sampling, self.funcname)\n",
                "        self.extra_params = sampler_extra_params.get(funcname, [])\n",
                "        self.model_wrap_cfg = CFGDenoiser(self.model_wrap)\n",
                "        self.sampler_noises = None\n",
                "        self.stop_at = None\n",
                "        self.eta = None\n",
                "        self.default_eta = 1.0\n",
                "        self.config = None\n",
                "        self.last_latent = None\n",
                "\n",
                "        self.conditioning_key = sd_model.model.conditioning_key\n",
                "\n",
                "    def callback_state(self, d):\n",
                "        step = d['i']\n",
                "        latent = d[\"denoised\"]\n",
                "        store_latent(latent)\n",
                "        self.last_latent = latent\n",
                "\n",
                "        if self.stop_at is not None and step > self.stop_at:\n",
                "            raise InterruptedException\n",
                "\n",
                "        state.sampling_step = step\n",
                "        shared.total_tqdm.update()\n",
                "\n",
                "    def launch_sampling(self, steps, func):\n",
                "        state.sampling_steps = steps\n",
                "        state.sampling_step = 0\n",
                "\n",
                "        try:\n",
                "            return func()\n",
                "        except InterruptedException:\n",
                "            return self.last_latent\n",
                "\n",
                "    def number_of_needed_noises(self, p):\n",
                "        return p.steps\n",
                "\n",
                "    def initialize(self, p):\n",
                "        self.model_wrap_cfg.mask = p.mask if hasattr(p, 'mask') else None\n",
                "        self.model_wrap_cfg.nmask = p.nmask if hasattr(p, 'nmask') else None\n",
                "        self.model_wrap.step = 0\n",
                "        self.eta = p.eta or opts.eta_ancestral\n",
                "\n",
                "        k_diffusion.sampling.torch = TorchHijack(self.sampler_noises if self.sampler_noises is not None else [])\n",
                "\n",
                "        extra_params_kwargs = {}\n",
                "        for param_name in self.extra_params:\n",
                "            if hasattr(p, param_name) and param_name in inspect.signature(self.func).parameters:\n",
                "                extra_params_kwargs[param_name] = getattr(p, param_name)\n",
                "\n",
                "        if 'eta' in inspect.signature(self.func).parameters:\n",
                "            extra_params_kwargs['eta'] = self.eta\n",
                "\n",
                "        return extra_params_kwargs\n",
                "\n",
                "    def get_sigmas(self, p, steps):\n",
                "        if p.sampler_noise_scheduler_override:\n",
                "            sigmas = p.sampler_noise_scheduler_override(steps)\n",
                "        elif self.config is not None and self.config.options.get('scheduler', None) == 'karras':\n",
                "            sigmas = k_diffusion.sampling.get_sigmas_karras(n=steps, sigma_min=0.1, sigma_max=10, device=shared.device)\n",
                "        else:\n",
                "            sigmas = self.model_wrap.get_sigmas(steps)\n",
                "\n",
                "        if self.config is not None and self.config.options.get('discard_next_to_last_sigma', False):\n",
                "            sigmas = torch.cat([sigmas[:-2], sigmas[-1:]])\n",
                "\n",
                "        return sigmas\n",
                "\n",
                "    def sample_img2img(self, p, x, noise, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):\n",
                "        steps, t_enc = setup_img2img_steps(p, steps)\n",
                "\n",
                "        sigmas = self.get_sigmas(p, steps)\n",
                "\n",
                "        sigma_sched = sigmas[steps - t_enc - 1:]\n",
                "        xi = x + noise * sigma_sched[0]\n",
                "        \n",
                "        extra_params_kwargs = self.initialize(p)\n",
                "        if 'sigma_min' in inspect.signature(self.func).parameters:\n",
                "            ## last sigma is zero which isn't allowed by DPM Fast & Adaptive so taking value before last\n",
                "            extra_params_kwargs['sigma_min'] = sigma_sched[-2]\n",
                "        if 'sigma_max' in inspect.signature(self.func).parameters:\n",
                "            extra_params_kwargs['sigma_max'] = sigma_sched[0]\n",
                "        if 'n' in inspect.signature(self.func).parameters:\n",
                "            extra_params_kwargs['n'] = len(sigma_sched) - 1\n",
                "        if 'sigma_sched' in inspect.signature(self.func).parameters:\n",
                "            extra_params_kwargs['sigma_sched'] = sigma_sched\n",
                "        if 'sigmas' in inspect.signature(self.func).parameters:\n",
                "            extra_params_kwargs['sigmas'] = sigma_sched\n",
                "\n",
                "        self.model_wrap_cfg.init_latent = x\n",
                "        self.last_latent = x\n",
                "\n",
                "        samples = self.launch_sampling(t_enc + 1, lambda: self.func(self.model_wrap_cfg, xi, extra_args={\n",
                "            'cond': conditioning, \n",
                "            'image_cond': image_conditioning, \n",
                "            'uncond': unconditional_conditioning, \n",
                "            'cond_scale': p.cfg_scale\n",
                "        }, disable=False, callback=self.callback_state, **extra_params_kwargs))\n",
                "\n",
                "        return samples\n",
                "\n",
                "    def sample(self, p, x, conditioning, unconditional_conditioning, steps=None, image_conditioning = None):\n",
                "        steps = steps or p.steps\n",
                "\n",
                "        sigmas = self.get_sigmas(p, steps)\n",
                "\n",
                "        x = x * sigmas[0]\n",
                "\n",
                "        extra_params_kwargs = self.initialize(p)\n",
                "        if 'sigma_min' in inspect.signature(self.func).parameters:\n",
                "            extra_params_kwargs['sigma_min'] = self.model_wrap.sigmas[0].item()\n",
                "            extra_params_kwargs['sigma_max'] = self.model_wrap.sigmas[-1].item()\n",
                "            if 'n' in inspect.signature(self.func).parameters:\n",
                "                extra_params_kwargs['n'] = steps\n",
                "        else:\n",
                "            extra_params_kwargs['sigmas'] = sigmas\n",
                "\n",
                "        self.last_latent = x\n",
                "        samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\n",
                "            'cond': conditioning, \n",
                "            'image_cond': image_conditioning, \n",
                "            'uncond': unconditional_conditioning, \n",
                "            'cond_scale': p.cfg_scale\n",
                "        }, disable=False, callback=self.callback_state, **extra_params_kwargs))\n",
                "\n",
                "        return samples\n",
                ""
            ]
        ],
        "modules/shared.py": [
            [
                "import argparse\n",
                "import datetime\n",
                "import json\n",
                "import os\n",
                "import sys\n",
                "import time\n",
                "\n",
                "from PIL import Image\n",
                "import gradio as gr\n",
                "import tqdm\n",
                "\n",
                "import modules.artists\n",
                "import modules.interrogate\n",
                "import modules.memmon\n",
                "import modules.styles\n",
                "import modules.devices as devices\n",
                "from modules import localization, sd_vae, extensions, script_loading\n",
                "from modules.paths import models_path, script_path, sd_path\n",
                "\n",
                "\n",
                "demo = None\n",
                "\n",
                "sd_model_file = os.path.join(script_path, 'model.ckpt')\n",
                "default_sd_model_file = sd_model_file\n",
                "parser = argparse.ArgumentParser()\n",
                "parser.add_argument(\"--config\", type=str, default=os.path.join(script_path, \"v1-inference.yaml\"), help=\"path to config which constructs model\",)\n",
                "parser.add_argument(\"--ckpt\", type=str, default=sd_model_file, help=\"path to checkpoint of stable diffusion model; if specified, this checkpoint will be added to the list of checkpoints and loaded\",)\n",
                "parser.add_argument(\"--ckpt-dir\", type=str, default=None, help=\"Path to directory with stable diffusion checkpoints\")\n",
                "parser.add_argument(\"--gfpgan-dir\", type=str, help=\"GFPGAN directory\", default=('./src/gfpgan' if os.path.exists('./src/gfpgan') else './GFPGAN'))\n",
                "parser.add_argument(\"--gfpgan-model\", type=str, help=\"GFPGAN model file name\", default=None)\n",
                "parser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\n",
                "parser.add_argument(\"--no-half-vae\", action='store_true', help=\"do not switch the VAE model to 16-bit floats\")\n",
                "parser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware acceleration in browser)\")\n",
                "parser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\n",
                "parser.add_argument(\"--embeddings-dir\", type=str, default=os.path.join(script_path, 'embeddings'), help=\"embeddings directory for textual inversion (default: embeddings)\")\n",
                "parser.add_argument(\"--hypernetwork-dir\", type=str, default=os.path.join(models_path, 'hypernetworks'), help=\"hypernetwork directory\")\n",
                "parser.add_argument(\"--localizations-dir\", type=str, default=os.path.join(script_path, 'localizations'), help=\"localizations directory\")\n",
                "parser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\n",
                "parser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\n",
                "parser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\n",
                "parser.add_argument(\"--lowram\", action='store_true', help=\"load stable diffusion checkpoint weights to VRAM instead of RAM\")\n",
                "parser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"disables cond/uncond batching that is enabled to save memory with --medvram or --lowvram\")\n",
                "parser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"does not do anything.\")\n",
                "parser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\n",
                "parser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site\")\n",
                "parser.add_argument(\"--ngrok\", type=str, help=\"ngrok authtoken, alternative to gradio --share\", default=None)\n",
                "parser.add_argument(\"--ngrok-region\", type=str, help=\"The region in which ngrok should start.\", default=\"us\")\n",
                "parser.add_argument(\"--enable-insecure-extension-access\", action='store_true', help=\"enable extensions tab regardless of other options\")\n",
                "parser.add_argument(\"--codeformer-models-path\", type=str, help=\"Path to directory with codeformer model file(s).\", default=os.path.join(models_path, 'Codeformer'))\n",
                "parser.add_argument(\"--gfpgan-models-path\", type=str, help=\"Path to directory with GFPGAN model file(s).\", default=os.path.join(models_path, 'GFPGAN'))\n",
                "parser.add_argument(\"--esrgan-models-path\", type=str, help=\"Path to directory with ESRGAN model file(s).\", default=os.path.join(models_path, 'ESRGAN'))\n",
                "parser.add_argument(\"--bsrgan-models-path\", type=str, help=\"Path to directory with BSRGAN model file(s).\", default=os.path.join(models_path, 'BSRGAN'))\n",
                "parser.add_argument(\"--realesrgan-models-path\", type=str, help=\"Path to directory with RealESRGAN model file(s).\", default=os.path.join(models_path, 'RealESRGAN'))\n",
                "parser.add_argument(\"--clip-models-path\", type=str, help=\"Path to directory with CLIP model file(s).\", default=None)\n",
                "parser.add_argument(\"--xformers\", action='store_true', help=\"enable xformers for cross attention layers\")\n",
                "parser.add_argument(\"--force-enable-xformers\", action='store_true', help=\"enable xformers for cross attention layers regardless of whether the checking code thinks you can run it; do not make bug reports if this fails to work\")\n",
                "parser.add_argument(\"--deepdanbooru\", action='store_true', help=\"does not do anything\")\n",
                "parser.add_argument(\"--opt-split-attention\", action='store_true', help=\"force-enables Doggettx's cross-attention layer optimization. By default, it's on for torch cuda.\")\n",
                "parser.add_argument(\"--opt-split-attention-invokeai\", action='store_true', help=\"force-enables InvokeAI's cross-attention layer optimization. By default, it's on when cuda is unavailable.\")\n",
                "parser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of split attention optimization that does not consume all the VRAM it can find\")\n",
                "parser.add_argument(\"--disable-opt-split-attention\", action='store_true', help=\"force-disables cross-attention layer optimization\")\n",
                "parser.add_argument(\"--use-cpu\", nargs='+', help=\"use CPU as torch device for specified modules\", default=[], type=str.lower)\n",
                "parser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\n",
                "parser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\n",
                "parser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\n",
                "parser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(script_path, 'ui-config.json'))\n",
                "parser.add_argument(\"--hide-ui-dir-config\", action='store_true', help=\"hide directory configuration from webui\", default=False)\n",
                "parser.add_argument(\"--freeze-settings\", action='store_true', help=\"disable editing settings\", default=False)\n",
                "parser.add_argument(\"--ui-settings-file\", type=str, help=\"filename to use for ui settings\", default=os.path.join(script_path, 'config.json'))\n",
                "parser.add_argument(\"--gradio-debug\",  action='store_true', help=\"launch gradio with --debug option\")\n",
                "parser.add_argument(\"--gradio-auth\", type=str, help='set gradio authentication like \"username:password\"; or comma-delimit multiple like \"u1:p1,u2:p2,u3:p3\"', default=None)\n",
                "parser.add_argument(\"--gradio-img2img-tool\", type=str, help='gradio image uploader tool: can be either editor for ctopping, or color-sketch for drawing', choices=[\"color-sketch\", \"editor\"], default=\"editor\")\n",
                "parser.add_argument(\"--gradio-inpaint-tool\", type=str, choices=[\"sketch\", \"color-sketch\"], default=\"sketch\", help=\"gradio inpainting editor: can be either sketch to only blur/noise the input, or color-sketch to paint over it\")\n",
                "parser.add_argument(\"--opt-channelslast\", action='store_true', help=\"change memory type for stable diffusion to channels last\")\n",
                "parser.add_argument(\"--styles-file\", type=str, help=\"filename to use for styles\", default=os.path.join(script_path, 'styles.csv'))\n",
                "parser.add_argument(\"--autolaunch\", action='store_true', help=\"open the webui URL in the system's default browser upon launch\", default=False)\n",
                "parser.add_argument(\"--theme\", type=str, help=\"launches the UI with light or dark theme\", default=None)\n",
                "parser.add_argument(\"--use-textbox-seed\", action='store_true', help=\"use textbox for seeds in UI (no up/down, but possible to input long seeds)\", default=False)\n",
                "parser.add_argument(\"--disable-console-progressbars\", action='store_true', help=\"do not output progressbars to console\", default=False)\n",
                "parser.add_argument(\"--enable-console-prompts\", action='store_true', help=\"print prompts to console when generating with txt2img and img2img\", default=False)\n",
                "parser.add_argument('--vae-path', type=str, help='Path to Variational Autoencoders model', default=None)\n",
                "parser.add_argument(\"--disable-safe-unpickle\", action='store_true', help=\"disable checking pytorch models for malicious code\", default=False)\n",
                "parser.add_argument(\"--api\", action='store_true', help=\"use api=True to launch the API together with the webui (use --nowebui instead for only the API)\")\n",
                "parser.add_argument(\"--api-auth\", type=str, help='Set authentication for API like \"username:password\"; or comma-delimit multiple like \"u1:p1,u2:p2,u3:p3\"', default=None)\n",
                "parser.add_argument(\"--nowebui\", action='store_true', help=\"use api=True to launch the API instead of the webui\")\n",
                "parser.add_argument(\"--ui-debug-mode\", action='store_true', help=\"Don't load model to quickly launch UI\")\n",
                "parser.add_argument(\"--device-id\", type=str, help=\"Select the default CUDA device to use (export CUDA_VISIBLE_DEVICES=0,1,etc might be needed before)\", default=None)\n",
                "parser.add_argument(\"--administrator\", action='store_true', help=\"Administrator rights\", default=False)\n",
                "parser.add_argument(\"--cors-allow-origins\", type=str, help=\"Allowed CORS origin(s) in the form of a comma-separated list (no spaces)\", default=None)\n",
                "parser.add_argument(\"--cors-allow-origins-regex\", type=str, help=\"Allowed CORS origin(s) in the form of a single regular expression\", default=None)\n",
                "parser.add_argument(\"--tls-keyfile\", type=str, help=\"Partially enables TLS, requires --tls-certfile to fully function\", default=None)\n",
                "parser.add_argument(\"--tls-certfile\", type=str, help=\"Partially enables TLS, requires --tls-keyfile to fully function\", default=None)\n",
                "parser.add_argument(\"--server-name\", type=str, help=\"Sets hostname of server\", default=None)\n",
                "\n",
                "script_loading.preload_extensions(extensions.extensions_dir, parser)\n",
                "script_loading.preload_extensions(extensions.extensions_builtin_dir, parser)\n",
                "\n",
                "cmd_opts = parser.parse_args()\n",
                "\n",
                "restricted_opts = {\n",
                "    \"samples_filename_pattern\",\n",
                "    \"directories_filename_pattern\",\n",
                "    \"outdir_samples\",\n",
                "    \"outdir_txt2img_samples\",\n",
                "    \"outdir_img2img_samples\",\n",
                "    \"outdir_extras_samples\",\n",
                "    \"outdir_grids\",\n",
                "    \"outdir_txt2img_grids\",\n",
                "    \"outdir_save\",\n",
                "}\n",
                "\n",
                "cmd_opts.disable_extension_access = (cmd_opts.share or cmd_opts.listen or cmd_opts.server_name) and not cmd_opts.enable_insecure_extension_access\n",
                "\n",
                "devices.device, devices.device_interrogate, devices.device_gfpgan, devices.device_esrgan, devices.device_codeformer = \\\n",
                "    (devices.cpu if any(y in cmd_opts.use_cpu for y in [x, 'all']) else devices.get_optimal_device() for x in ['sd', 'interrogate', 'gfpgan', 'esrgan', 'codeformer'])\n",
                "\n",
                "device = devices.device\n",
                "weight_load_location = None if cmd_opts.lowram else \"cpu\"\n",
                "\n",
                "batch_cond_uncond = cmd_opts.always_batch_cond_uncond or not (cmd_opts.lowvram or cmd_opts.medvram)\n",
                "parallel_processing_allowed = not cmd_opts.lowvram and not cmd_opts.medvram\n",
                "xformers_available = False\n",
                "config_filename = cmd_opts.ui_settings_file\n",
                "\n",
                "os.makedirs(cmd_opts.hypernetwork_dir, exist_ok=True)\n",
                "hypernetworks = {}\n",
                "loaded_hypernetwork = None\n",
                "\n",
                "\n",
                "def reload_hypernetworks():\n",
                "    from modules.hypernetworks import hypernetwork\n",
                "    global hypernetworks\n",
                "\n",
                "    hypernetworks = hypernetwork.list_hypernetworks(cmd_opts.hypernetwork_dir)\n",
                "    hypernetwork.load_hypernetwork(opts.sd_hypernetwork)\n",
                "\n",
                "\n",
                "class State:\n",
                "    skipped = False\n",
                "    interrupted = False\n",
                "    job = \"\"\n",
                "    job_no = 0\n",
                "    job_count = 0\n",
                "    job_timestamp = '0'\n",
                "    sampling_step = 0\n",
                "    sampling_steps = 0\n",
                "    current_latent = None\n",
                "    current_image = None\n",
                "    current_image_sampling_step = 0\n",
                "    textinfo = None\n",
                "    time_start = None\n",
                "    need_restart = False\n",
                "\n",
                "    def skip(self):\n",
                "        self.skipped = True\n",
                "\n",
                "    def interrupt(self):\n",
                "        self.interrupted = True\n",
                "\n",
                "    def nextjob(self):\n",
                "        if opts.show_progress_every_n_steps == -1:\n",
                "            self.do_set_current_image()\n",
                "\n",
                "        self.job_no += 1\n",
                "        self.sampling_step = 0\n",
                "        self.current_image_sampling_step = 0\n",
                "\n",
                "    def dict(self):\n",
                "        obj = {\n",
                "            \"skipped\": self.skipped,\n",
                "            \"interrupted\": self.skipped,\n",
                "            \"job\": self.job,\n",
                "            \"job_count\": self.job_count,\n",
                "            \"job_no\": self.job_no,\n",
                "            \"sampling_step\": self.sampling_step,\n",
                "            \"sampling_steps\": self.sampling_steps,\n",
                "        }\n",
                "\n",
                "        return obj\n",
                "\n",
                "    def begin(self):\n",
                "        self.sampling_step = 0\n",
                "        self.job_count = -1\n",
                "        self.job_no = 0\n",
                "        self.job_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
                "        self.current_latent = None\n",
                "        self.current_image = None\n",
                "        self.current_image_sampling_step = 0\n",
                "        self.skipped = False\n",
                "        self.interrupted = False\n",
                "        self.textinfo = None\n",
                "        self.time_start = time.time()\n",
                "\n",
                "        devices.torch_gc()\n",
                "\n",
                "    def end(self):\n",
                "        self.job = \"\"\n",
                "        self.job_count = 0\n",
                "\n",
                "        devices.torch_gc()\n",
                "\n",
                "    \"\"\"sets self.current_image from self.current_latent if enough sampling steps have been made after the last call to this\"\"\"\n",
                "    def set_current_image(self):\n",
                "        if self.sampling_step - self.current_image_sampling_step >= opts.show_progress_every_n_steps and opts.show_progress_every_n_steps > 0:\n",
                "            self.do_set_current_image()\n",
                "\n",
                "    def do_set_current_image(self):\n",
                "        if not parallel_processing_allowed:\n",
                "            return\n",
                "        if self.current_latent is None:\n",
                "            return\n",
                "\n",
                "        import modules.sd_samplers\n",
                "        if opts.show_progress_grid:\n"
            ],
            {
                "type": "replace",
                "before": [
                    "            self.current_image = modules.sd_samplers.samples_to_image_grid(self.current_latent)\n"
                ],
                "after": [
                    "            self.current_image = modules.sd_samplers.samples_to_image_grid(self.current_latent, approximation=opts.show_progress_approximate)\n"
                ],
                "parent_version_range": {
                    "start": 214,
                    "end": 215
                },
                "child_version_range": {
                    "start": 214,
                    "end": 215
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if opts.show_progress_grid:",
                        "start_line": 213,
                        "end_line": 216
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "State",
                        "signature": "class State:",
                        "at_line": 137
                    },
                    {
                        "type": "function",
                        "name": "do_set_current_image",
                        "signature": "def do_set_current_image(self):",
                        "at_line": 206
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: modules/shared.py\nCode:\n           class State:\n               ...\n               def do_set_current_image(self):\n                   ...\n211 211    \n212 212            import modules.sd_samplers\n213 213            if opts.show_progress_grid:\n214      -             self.current_image = modules.sd_samplers.samples_to_image_grid(self.current_latent)\n    214  +             self.current_image = modules.sd_samplers.samples_to_image_grid(self.current_latent, approximation=opts.show_progress_approximate)\n215 215            else:\n         ...\n",
                "file_path": "modules/shared.py",
                "identifiers_before": [
                    "current_image",
                    "current_latent",
                    "modules",
                    "samples_to_image_grid",
                    "sd_samplers",
                    "self"
                ],
                "identifiers_after": [
                    "approximation",
                    "current_image",
                    "current_latent",
                    "modules",
                    "opts",
                    "samples_to_image_grid",
                    "sd_samplers",
                    "self",
                    "show_progress_approximate"
                ],
                "prefix": [
                    "\n",
                    "        import modules.sd_samplers\n",
                    "        if opts.show_progress_grid:\n"
                ],
                "suffix": [
                    "        else:\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "samples_to_image_grid",
                            "position": {
                                "start": {
                                    "line": 214,
                                    "column": 53
                                },
                                "end": {
                                    "line": 214,
                                    "column": 74
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/shared.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "samples_to_image_grid",
                            "position": {
                                "start": {
                                    "line": 214,
                                    "column": 53
                                },
                                "end": {
                                    "line": 214,
                                    "column": 74
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/shared.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "approximation",
                            "position": {
                                "start": {
                                    "line": 214,
                                    "column": 96
                                },
                                "end": {
                                    "line": 214,
                                    "column": 109
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/shared.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    3,
                    5
                ]
            },
            [
                "        else:\n"
            ],
            {
                "type": "replace",
                "before": [
                    "            self.current_image = modules.sd_samplers.sample_to_image(self.current_latent)\n"
                ],
                "after": [
                    "            self.current_image = modules.sd_samplers.sample_to_image(self.current_latent, approximation=opts.show_progress_approximate)\n"
                ],
                "parent_version_range": {
                    "start": 216,
                    "end": 217
                },
                "child_version_range": {
                    "start": 216,
                    "end": 217
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if opts.show_progress_grid:",
                        "start_line": 213,
                        "end_line": 216
                    },
                    {
                        "type": "else_clause",
                        "statement": "else:",
                        "start_line": 215,
                        "end_line": 216
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "State",
                        "signature": "class State:",
                        "at_line": 137
                    },
                    {
                        "type": "function",
                        "name": "do_set_current_image",
                        "signature": "def do_set_current_image(self):",
                        "at_line": 206
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: modules/shared.py\nCode:\n           class State:\n               ...\n               def do_set_current_image(self):\n                   ...\n215 215            else:\n216      -             self.current_image = modules.sd_samplers.sample_to_image(self.current_latent)\n    216  +             self.current_image = modules.sd_samplers.sample_to_image(self.current_latent, approximation=opts.show_progress_approximate)\n217 217    \n218 218            self.current_image_sampling_step = self.sampling_step\n219 219    \n         ...\n",
                "file_path": "modules/shared.py",
                "identifiers_before": [
                    "current_image",
                    "current_latent",
                    "modules",
                    "sample_to_image",
                    "sd_samplers",
                    "self"
                ],
                "identifiers_after": [
                    "approximation",
                    "current_image",
                    "current_latent",
                    "modules",
                    "opts",
                    "sample_to_image",
                    "sd_samplers",
                    "self",
                    "show_progress_approximate"
                ],
                "prefix": [
                    "        else:\n"
                ],
                "suffix": [
                    "\n",
                    "        self.current_image_sampling_step = self.sampling_step\n",
                    "\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "sample_to_image",
                            "position": {
                                "start": {
                                    "line": 216,
                                    "column": 53
                                },
                                "end": {
                                    "line": 216,
                                    "column": 68
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/shared.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "sample_to_image",
                            "position": {
                                "start": {
                                    "line": 216,
                                    "column": 53
                                },
                                "end": {
                                    "line": 216,
                                    "column": 68
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/shared.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "approximation",
                            "position": {
                                "start": {
                                    "line": 216,
                                    "column": 90
                                },
                                "end": {
                                    "line": 216,
                                    "column": 103
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/stable-diffusion-webui/modules/shared.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    3,
                    4
                ]
            },
            [
                "\n",
                "        self.current_image_sampling_step = self.sampling_step\n",
                "\n",
                "state = State()\n",
                "\n",
                "artist_db = modules.artists.ArtistsDatabase(os.path.join(script_path, 'artists.csv'))\n",
                "\n",
                "styles_filename = cmd_opts.styles_file\n",
                "prompt_styles = modules.styles.StyleDatabase(styles_filename)\n",
                "\n",
                "interrogator = modules.interrogate.InterrogateModels(\"interrogate\")\n",
                "\n",
                "face_restorers = []\n",
                "\n",
                "\n",
                "def realesrgan_models_names():\n",
                "    import modules.realesrgan_model\n",
                "    return [x.name for x in modules.realesrgan_model.get_realesrgan_models(None)]\n",
                "\n",
                "\n",
                "class OptionInfo:\n",
                "    def __init__(self, default=None, label=\"\", component=None, component_args=None, onchange=None, section=None, refresh=None):\n",
                "        self.default = default\n",
                "        self.label = label\n",
                "        self.component = component\n",
                "        self.component_args = component_args\n",
                "        self.onchange = onchange\n",
                "        self.section = section\n",
                "        self.refresh = refresh\n",
                "\n",
                "\n",
                "def options_section(section_identifier, options_dict):\n",
                "    for k, v in options_dict.items():\n",
                "        v.section = section_identifier\n",
                "\n",
                "    return options_dict\n",
                "\n",
                "\n",
                "def list_checkpoint_tiles():\n",
                "    import modules.sd_models\n",
                "    return modules.sd_models.checkpoint_tiles()\n",
                "\n",
                "\n",
                "def refresh_checkpoints():\n",
                "    import modules.sd_models\n",
                "    return modules.sd_models.list_models()\n",
                "\n",
                "\n",
                "def list_samplers():\n",
                "    import modules.sd_samplers\n",
                "    return modules.sd_samplers.all_samplers\n",
                "\n",
                "\n",
                "hide_dirs = {\"visible\": not cmd_opts.hide_ui_dir_config}\n",
                "\n",
                "options_templates = {}\n",
                "\n",
                "options_templates.update(options_section(('saving-images', \"Saving images/grids\"), {\n",
                "    \"samples_save\": OptionInfo(True, \"Always save all generated images\"),\n",
                "    \"samples_format\": OptionInfo('png', 'File format for images'),\n",
                "    \"samples_filename_pattern\": OptionInfo(\"\", \"Images filename pattern\", component_args=hide_dirs),\n",
                "    \"save_images_add_number\": OptionInfo(True, \"Add number to filename when saving\", component_args=hide_dirs),\n",
                "\n",
                "    \"grid_save\": OptionInfo(True, \"Always save all generated image grids\"),\n",
                "    \"grid_format\": OptionInfo('png', 'File format for grids'),\n",
                "    \"grid_extended_filename\": OptionInfo(False, \"Add extended info (seed, prompt) to filename when saving grid\"),\n",
                "    \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\n",
                "    \"grid_prevent_empty_spots\": OptionInfo(False, \"Prevent empty spots in grid (when set to autodetect)\"),\n",
                "    \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider, {\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\n",
                "\n",
                "    \"enable_pnginfo\": OptionInfo(True, \"Save text information about generation parameters as chunks to png files\"),\n",
                "    \"save_txt\": OptionInfo(False, \"Create a text file next to every image with generation parameters.\"),\n",
                "    \"save_images_before_face_restoration\": OptionInfo(False, \"Save a copy of image before doing face restoration.\"),\n",
                "    \"save_images_before_highres_fix\": OptionInfo(False, \"Save a copy of image before applying highres fix.\"),\n",
                "    \"save_images_before_color_correction\": OptionInfo(False, \"Save a copy of image before applying color correction to img2img results\"),\n",
                "    \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider, {\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\n",
                "    \"export_for_4chan\": OptionInfo(True, \"If PNG image is larger than 4MB or any dimension is larger than 4000, downscale and save copy as JPG\"),\n",
                "\n",
                "    \"use_original_name_batch\": OptionInfo(False, \"Use original name for output filename during batch process in extras tab\"),\n",
                "    \"use_upscaler_name_as_suffix\": OptionInfo(False, \"Use upscaler name as filename suffix in the extras tab\"),\n",
                "    \"save_selected_only\": OptionInfo(True, \"When using 'Save' button, only save a single selected image\"),\n",
                "    \"do_not_add_watermark\": OptionInfo(False, \"Do not add watermark to images\"),\n",
                "\n",
                "    \"temp_dir\":  OptionInfo(\"\", \"Directory for temporary images; leave empty for default\"),\n",
                "    \"clean_temp_dir_at_start\": OptionInfo(False, \"Cleanup non-default temporary directory when starting webui\"),\n",
                "\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('saving-paths', \"Paths for saving\"), {\n",
                "    \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to three directories below\", component_args=hide_dirs),\n",
                "    \"outdir_txt2img_samples\": OptionInfo(\"outputs/txt2img-images\", 'Output directory for txt2img images', component_args=hide_dirs),\n",
                "    \"outdir_img2img_samples\": OptionInfo(\"outputs/img2img-images\", 'Output directory for img2img images', component_args=hide_dirs),\n",
                "    \"outdir_extras_samples\": OptionInfo(\"outputs/extras-images\", 'Output directory for images from extras tab', component_args=hide_dirs),\n",
                "    \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\n",
                "    \"outdir_txt2img_grids\": OptionInfo(\"outputs/txt2img-grids\", 'Output directory for txt2img grids', component_args=hide_dirs),\n",
                "    \"outdir_img2img_grids\": OptionInfo(\"outputs/img2img-grids\", 'Output directory for img2img grids', component_args=hide_dirs),\n",
                "    \"outdir_save\": OptionInfo(\"log/images\", \"Directory for saving images using the Save button\", component_args=hide_dirs),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('saving-to-dirs', \"Saving to a directory\"), {\n",
                "    \"save_to_dirs\": OptionInfo(False, \"Save images to a subdirectory\"),\n",
                "    \"grid_save_to_dirs\": OptionInfo(False, \"Save grids to a subdirectory\"),\n",
                "    \"use_save_to_dirs_for_ui\": OptionInfo(False, \"When using \\\"Save\\\" button, save images to a subdirectory\"),\n",
                "    \"directories_filename_pattern\": OptionInfo(\"\", \"Directory name pattern\", component_args=hide_dirs),\n",
                "    \"directories_max_prompt_words\": OptionInfo(8, \"Max prompt words for [prompt_words] pattern\", gr.Slider, {\"minimum\": 1, \"maximum\": 20, \"step\": 1, **hide_dirs}),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('upscaling', \"Upscaling\"), {\n",
                "    \"ESRGAN_tile\": OptionInfo(192, \"Tile size for ESRGAN upscalers. 0 = no tiling.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}),\n",
                "    \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap, in pixels for ESRGAN upscalers. Low values = visible seam.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}),\n",
                "    \"realesrgan_enabled_models\": OptionInfo([\"R-ESRGAN 4x+\", \"R-ESRGAN 4x+ Anime6B\"], \"Select which Real-ESRGAN models to show in the web UI. (Requires restart)\", gr.CheckboxGroup, lambda: {\"choices\": realesrgan_models_names()}),\n",
                "    \"upscaler_for_img2img\": OptionInfo(None, \"Upscaler for img2img\", gr.Dropdown, lambda: {\"choices\": [x.name for x in sd_upscalers]}),\n",
                "    \"use_scale_latent_for_hires_fix\": OptionInfo(False, \"Upscale latent space image when doing hires. fix\"),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('face-restoration', \"Face restoration\"), {\n",
                "    \"face_restoration_model\": OptionInfo(None, \"Face restoration model\", gr.Radio, lambda: {\"choices\": [x.name() for x in face_restorers]}),\n",
                "    \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight parameter; 0 = maximum effect; 1 = minimum effect\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\n",
                "    \"face_restoration_unload\": OptionInfo(False, \"Move face restoration model from VRAM into RAM after processing\"),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('system', \"System\"), {\n",
                "    \"memmon_poll_rate\": OptionInfo(8, \"VRAM usage polls per second during generation. Set to 0 to disable.\", gr.Slider, {\"minimum\": 0, \"maximum\": 40, \"step\": 1}),\n",
                "    \"samples_log_stdout\": OptionInfo(False, \"Always print all generation info to standard output\"),\n",
                "    \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job.\"),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('training', \"Training\"), {\n",
                "    \"unload_models_when_training\": OptionInfo(False, \"Move VAE and CLIP to RAM when training if possible. Saves VRAM.\"),\n",
                "    \"pin_memory\": OptionInfo(False, \"Turn on pin_memory for DataLoader. Makes training slightly faster but can increase memory usage.\"),\n",
                "    \"save_optimizer_state\": OptionInfo(False, \"Saves Optimizer state as separate *.optim file. Training can be resumed with HN itself and matching optim file.\"),\n",
                "    \"dataset_filename_word_regex\": OptionInfo(\"\", \"Filename word regex\"),\n",
                "    \"dataset_filename_join_string\": OptionInfo(\" \", \"Filename join string\"),\n",
                "    \"training_image_repeats_per_epoch\": OptionInfo(1, \"Number of repeats for a single input image per epoch; used only for displaying epoch number\", gr.Number, {\"precision\": 0}),\n",
                "    \"training_write_csv_every\": OptionInfo(500, \"Save an csv containing the loss to log directory every N steps, 0 to disable\"),\n",
                "    \"training_xattention_optimizations\": OptionInfo(False, \"Use cross attention optimizations while training\"),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('sd', \"Stable Diffusion\"), {\n",
                "    \"sd_model_checkpoint\": OptionInfo(None, \"Stable Diffusion checkpoint\", gr.Dropdown, lambda: {\"choices\": list_checkpoint_tiles()}, refresh=refresh_checkpoints),\n",
                "    \"sd_checkpoint_cache\": OptionInfo(0, \"Checkpoints to cache in RAM\", gr.Slider, {\"minimum\": 0, \"maximum\": 10, \"step\": 1}),\n",
                "    \"sd_vae\": OptionInfo(\"auto\", \"SD VAE\", gr.Dropdown, lambda: {\"choices\": sd_vae.vae_list}, refresh=sd_vae.refresh_vae_list),\n",
                "    \"sd_vae_as_default\": OptionInfo(False, \"Ignore selected VAE for stable diffusion checkpoints that have their own .vae.pt next to them\"),\n",
                "    \"sd_hypernetwork\": OptionInfo(\"None\", \"Hypernetwork\", gr.Dropdown, lambda: {\"choices\": [\"None\"] + [x for x in hypernetworks.keys()]}, refresh=reload_hypernetworks),\n",
                "    \"sd_hypernetwork_strength\": OptionInfo(1.0, \"Hypernetwork strength\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.001}),\n",
                "    \"inpainting_mask_weight\": OptionInfo(1.0, \"Inpainting conditioning mask strength\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}),\n",
                "    \"initial_noise_multiplier\": OptionInfo(1.0, \"Noise multiplier for img2img\", gr.Slider, {\"minimum\": 0.5, \"maximum\": 1.5, \"step\": 0.01 }),\n",
                "    \"img2img_color_correction\": OptionInfo(False, \"Apply color correction to img2img results to match original colors.\"),\n",
                "    \"img2img_fix_steps\": OptionInfo(False, \"With img2img, do exactly the amount of steps the slider specifies (normally you'd do less with less denoising).\"),\n",
                "    \"img2img_background_color\": OptionInfo(\"#ffffff\", \"With img2img, fill image's transparent parts with this color.\", gr.ColorPicker, {}),\n",
                "    \"enable_quantization\": OptionInfo(False, \"Enable quantization in K samplers for sharper and cleaner results. This may change existing seeds. Requires restart to apply.\"),\n",
                "    \"enable_emphasis\": OptionInfo(True, \"Emphasis: use (text) to make model pay more attention to text and [text] to make it pay less attention\"),\n",
                "    \"use_old_emphasis_implementation\": OptionInfo(False, \"Use old emphasis implementation. Can be useful to reproduce old seeds.\"),\n",
                "    \"enable_batch_seeds\": OptionInfo(True, \"Make K-diffusion samplers produce same images in a batch as when making a single image\"),\n",
                "    \"comma_padding_backtrack\": OptionInfo(20, \"Increase coherency by padding from the last comma within n tokens when using more than 75 tokens\", gr.Slider, {\"minimum\": 0, \"maximum\": 74, \"step\": 1 }),\n",
                "    'CLIP_stop_at_last_layers': OptionInfo(1, \"Clip skip\", gr.Slider, {\"minimum\": 1, \"maximum\": 12, \"step\": 1}),\n",
                "    \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup, {\"choices\": artist_db.categories()}),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('interrogate', \"Interrogate Options\"), {\n",
                "    \"interrogate_keep_models_in_memory\": OptionInfo(False, \"Interrogate: keep models in VRAM\"),\n",
                "    \"interrogate_use_builtin_artists\": OptionInfo(True, \"Interrogate: use artists from artists.csv\"),\n",
                "    \"interrogate_return_ranks\": OptionInfo(False, \"Interrogate: include ranks of model tags matches in results (Has no effect on caption-based interrogators).\"),\n",
                "    \"interrogate_clip_num_beams\": OptionInfo(1, \"Interrogate: num_beams for BLIP\", gr.Slider, {\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\n",
                "    \"interrogate_clip_min_length\": OptionInfo(24, \"Interrogate: minimum description length (excluding artists, etc..)\", gr.Slider, {\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\n",
                "    \"interrogate_clip_max_length\": OptionInfo(48, \"Interrogate: maximum description length\", gr.Slider, {\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\n",
                "    \"interrogate_clip_dict_limit\": OptionInfo(1500, \"CLIP: maximum number of lines in text file (0 = No limit)\"),\n",
                "    \"interrogate_deepbooru_score_threshold\": OptionInfo(0.5, \"Interrogate: deepbooru score threshold\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\n",
                "    \"deepbooru_sort_alpha\": OptionInfo(True, \"Interrogate: deepbooru sort alphabetically\"),\n",
                "    \"deepbooru_use_spaces\": OptionInfo(False, \"use spaces for tags in deepbooru\"),\n",
                "    \"deepbooru_escape\": OptionInfo(True, \"escape (\\\\) brackets in deepbooru (so they are used as literal brackets and not for emphasis)\"),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('ui', \"User interface\"), {\n",
                "    \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\n",
                "    \"show_progress_every_n_steps\": OptionInfo(0, \"Show image creation progress every N sampling steps. Set to 0 to disable. Set to -1 to show after completion of batch.\", gr.Slider, {\"minimum\": -1, \"maximum\": 32, \"step\": 1}),\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "    \"show_progress_approximate\": OptionInfo(False, \"Calculate small previews using fast linear approximation instead of VAE\"),\n"
                ],
                "parent_version_range": {
                    "start": 393,
                    "end": 393
                },
                "child_version_range": {
                    "start": 393,
                    "end": 394
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "call",
                        "name": "options_templates.update",
                        "signature": "options_templates.update(options_section(('ui', \"User interface\"), {\n    \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\n    \"show_progress_every_n_steps\": OptionInfo(0, \"Show image creation progress every N sampling steps. Set to 0 to disable. Set to -1 to show after completion of batch.\", gr.Slider, {\"minimum\": -1, \"maximum\": 32, \"step\": 1}),\n    \"show_progress_grid\": OptionInfo(True, \"Show previews of all images generated in a batch as a grid\"),\n    \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\n    \"do_not_show_images\": OptionInfo(False, \"Do not show any images in results for web\"),\n    \"add_model_hash_to_info\": OptionInfo(True, \"Add model hash to generation information\"),\n    \"add_model_name_to_info\": OptionInfo(False, \"Add model name to generation information\"),\n    \"disable_weights_auto_swap\": OptionInfo(False, \"When reading generation parameters from text into UI (from PNG info or pasted text), do not change the selected model/checkpoint.\"),\n    \"send_seed\": OptionInfo(True, \"Send seed when sending prompt or image to other interface\"),\n    \"send_size\": OptionInfo(True, \"Send size when sending prompt or image to another interface\"),\n    \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\n    \"js_modal_lightbox\": OptionInfo(True, \"Enable full page image viewer\"),\n    \"js_modal_lightbox_initially_zoomed\": OptionInfo(True, \"Show images zoomed in by default in full page image viewer\"),\n    \"show_progress_in_title\": OptionInfo(True, \"Show generation progress in window title.\"),\n    'quicksettings': OptionInfo(\"sd_model_checkpoint\", \"Quicksettings list\"),\n    'localization': OptionInfo(\"None\", \"Localization (requires restart)\", gr.Dropdown, lambda: {\"choices\": [\"None\"] + list(localization.localizations.keys())}, refresh=lambda: localization.list_localizations(cmd_opts.localizations_dir)),\n}))",
                        "at_line": 390,
                        "argument": "options_section(('ui', \"User i..."
                    },
                    {
                        "type": "call",
                        "name": "options_section",
                        "signature": "options_section(('ui', \"User interface\"), {\n    \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\n    \"show_progress_every_n_steps\": OptionInfo(0, \"Show image creation progress every N sampling steps. Set to 0 to disable. Set to -1 to show after completion of batch.\", gr.Slider, {\"minimum\": -1, \"maximum\": 32, \"step\": 1}),\n    \"show_progress_grid\": OptionInfo(True, \"Show previews of all images generated in a batch as a grid\"),\n    \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\n    \"do_not_show_images\": OptionInfo(False, \"Do not show any images in results for web\"),\n    \"add_model_hash_to_info\": OptionInfo(True, \"Add model hash to generation information\"),\n    \"add_model_name_to_info\": OptionInfo(False, \"Add model name to generation information\"),\n    \"disable_weights_auto_swap\": OptionInfo(False, \"When reading generation parameters from text into UI (from PNG info or pasted text), do not change the selected model/checkpoint.\"),\n    \"send_seed\": OptionInfo(True, \"Send seed when sending prompt or image to other interface\"),\n    \"send_size\": OptionInfo(True, \"Send size when sending prompt or image to another interface\"),\n    \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\n    \"js_modal_lightbox\": OptionInfo(True, \"Enable full page image viewer\"),\n    \"js_modal_lightbox_initially_zoomed\": OptionInfo(True, \"Show images zoomed in by default in full page image viewer\"),\n    \"show_progress_in_title\": OptionInfo(True, \"Show generation progress in window title.\"),\n    'quicksettings': OptionInfo(\"sd_model_checkpoint\", \"Quicksettings list\"),\n    'localization': OptionInfo(\"None\", \"Localization (requires restart)\", gr.Dropdown, lambda: {\"choices\": [\"None\"] + list(localization.localizations.keys())}, refresh=lambda: localization.list_localizations(cmd_opts.localizations_dir)),\n})",
                        "at_line": 390,
                        "argument": "{\n    \"show_progressbar\": Opti..."
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: modules/shared.py\nCode:\n390 390    options_templates.update(options_section(('ui', \"User interface\"), {\n391 391        \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\n392 392        \"show_progress_every_n_steps\": OptionInfo(0, \"Show image creation progress every N sampling steps. Set to 0 to disable. Set to -1 to show after completion of batch.\", gr.Slider, {\"minimum\": -1, \"maximum\": 32, \"step\": 1}),\n    393  +     \"show_progress_approximate\": OptionInfo(False, \"Calculate small previews using fast linear approximation instead of VAE\"),\n393 394        \"show_progress_grid\": OptionInfo(True, \"Show previews of all images generated in a batch as a grid\"),\n394 395        \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\n395 396        \"do_not_show_images\": OptionInfo(False, \"Do not show any images in results for web\"),\n         ...\n",
                "file_path": "modules/shared.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "OptionInfo"
                ],
                "prefix": [
                    "options_templates.update(options_section(('ui', \"User interface\"), {\n",
                    "    \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\n",
                    "    \"show_progress_every_n_steps\": OptionInfo(0, \"Show image creation progress every N sampling steps. Set to 0 to disable. Set to -1 to show after completion of batch.\", gr.Slider, {\"minimum\": -1, \"maximum\": 32, \"step\": 1}),\n"
                ],
                "suffix": [
                    "    \"show_progress_grid\": OptionInfo(True, \"Show previews of all images generated in a batch as a grid\"),\n",
                    "    \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\n",
                    "    \"do_not_show_images\": OptionInfo(False, \"Do not show any images in results for web\"),\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    \"show_progress_grid\": OptionInfo(True, \"Show previews of all images generated in a batch as a grid\"),\n",
                "    \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\n",
                "    \"do_not_show_images\": OptionInfo(False, \"Do not show any images in results for web\"),\n",
                "    \"add_model_hash_to_info\": OptionInfo(True, \"Add model hash to generation information\"),\n",
                "    \"add_model_name_to_info\": OptionInfo(False, \"Add model name to generation information\"),\n",
                "    \"disable_weights_auto_swap\": OptionInfo(False, \"When reading generation parameters from text into UI (from PNG info or pasted text), do not change the selected model/checkpoint.\"),\n",
                "    \"send_seed\": OptionInfo(True, \"Send seed when sending prompt or image to other interface\"),\n",
                "    \"send_size\": OptionInfo(True, \"Send size when sending prompt or image to another interface\"),\n",
                "    \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\n",
                "    \"js_modal_lightbox\": OptionInfo(True, \"Enable full page image viewer\"),\n",
                "    \"js_modal_lightbox_initially_zoomed\": OptionInfo(True, \"Show images zoomed in by default in full page image viewer\"),\n",
                "    \"show_progress_in_title\": OptionInfo(True, \"Show generation progress in window title.\"),\n",
                "    'quicksettings': OptionInfo(\"sd_model_checkpoint\", \"Quicksettings list\"),\n",
                "    'localization': OptionInfo(\"None\", \"Localization (requires restart)\", gr.Dropdown, lambda: {\"choices\": [\"None\"] + list(localization.localizations.keys())}, refresh=lambda: localization.list_localizations(cmd_opts.localizations_dir)),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section(('sampler-params', \"Sampler parameters\"), {\n",
                "    \"hide_samplers\": OptionInfo([], \"Hide samplers in user interface (requires restart)\", gr.CheckboxGroup, lambda: {\"choices\": [x.name for x in list_samplers()]}),\n",
                "    \"eta_ddim\": OptionInfo(0.0, \"eta (noise multiplier) for DDIM\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}),\n",
                "    \"eta_ancestral\": OptionInfo(1.0, \"eta (noise multiplier) for ancestral samplers\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}),\n",
                "    \"ddim_discretize\": OptionInfo('uniform', \"img2img DDIM discretize\", gr.Radio, {\"choices\": ['uniform', 'quad']}),\n",
                "    's_churn': OptionInfo(0.0, \"sigma churn\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}),\n",
                "    's_tmin':  OptionInfo(0.0, \"sigma tmin\",  gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}),\n",
                "    's_noise': OptionInfo(1.0, \"sigma noise\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}),\n",
                "    'eta_noise_seed_delta': OptionInfo(0, \"Eta noise seed delta\", gr.Number, {\"precision\": 0}),\n",
                "}))\n",
                "\n",
                "options_templates.update(options_section((None, \"Hidden options\"), {\n",
                "    \"disabled_extensions\": OptionInfo([], \"Disable those extensions\"),\n",
                "}))\n",
                "\n",
                "options_templates.update()\n",
                "\n",
                "\n",
                "class Options:\n",
                "    data = None\n",
                "    data_labels = options_templates\n",
                "    typemap = {int: float}\n",
                "\n",
                "    def __init__(self):\n",
                "        self.data = {k: v.default for k, v in self.data_labels.items()}\n",
                "\n",
                "    def __setattr__(self, key, value):\n",
                "        if self.data is not None:\n",
                "            if key in self.data or key in self.data_labels:\n",
                "                assert not cmd_opts.freeze_settings, \"changing settings is disabled\"\n",
                "\n",
                "                info = opts.data_labels.get(key, None)\n",
                "                comp_args = info.component_args if info else None\n",
                "                if isinstance(comp_args, dict) and comp_args.get('visible', True) is False:\n",
                "                    raise RuntimeError(f\"not possible to set {key} because it is restricted\")\n",
                "\n",
                "                if cmd_opts.hide_ui_dir_config and key in restricted_opts:\n",
                "                    raise RuntimeError(f\"not possible to set {key} because it is restricted\")\n",
                "\n",
                "                self.data[key] = value\n",
                "                return\n",
                "\n",
                "        return super(Options, self).__setattr__(key, value)\n",
                "\n",
                "    def __getattr__(self, item):\n",
                "        if self.data is not None:\n",
                "            if item in self.data:\n",
                "                return self.data[item]\n",
                "\n",
                "        if item in self.data_labels:\n",
                "            return self.data_labels[item].default\n",
                "\n",
                "        return super(Options, self).__getattribute__(item)\n",
                "\n",
                "    def set(self, key, value):\n",
                "        \"\"\"sets an option and calls its onchange callback, returning True if the option changed and False otherwise\"\"\"\n",
                "\n",
                "        oldval = self.data.get(key, None)\n",
                "        if oldval == value:\n",
                "            return False\n",
                "\n",
                "        try:\n",
                "            setattr(self, key, value)\n",
                "        except RuntimeError:\n",
                "            return False\n",
                "\n",
                "        if self.data_labels[key].onchange is not None:\n",
                "            self.data_labels[key].onchange()\n",
                "\n",
                "        return True\n",
                "\n",
                "    def save(self, filename):\n",
                "        assert not cmd_opts.freeze_settings, \"saving settings is disabled\"\n",
                "\n",
                "        with open(filename, \"w\", encoding=\"utf8\") as file:\n",
                "            json.dump(self.data, file, indent=4)\n",
                "\n",
                "    def same_type(self, x, y):\n",
                "        if x is None or y is None:\n",
                "            return True\n",
                "\n",
                "        type_x = self.typemap.get(type(x), type(x))\n",
                "        type_y = self.typemap.get(type(y), type(y))\n",
                "\n",
                "        return type_x == type_y\n",
                "\n",
                "    def load(self, filename):\n",
                "        with open(filename, \"r\", encoding=\"utf8\") as file:\n",
                "            self.data = json.load(file)\n",
                "\n",
                "        bad_settings = 0\n",
                "        for k, v in self.data.items():\n",
                "            info = self.data_labels.get(k, None)\n",
                "            if info is not None and not self.same_type(info.default, v):\n",
                "                print(f\"Warning: bad setting value: {k}: {v} ({type(v).__name__}; expected {type(info.default).__name__})\", file=sys.stderr)\n",
                "                bad_settings += 1\n",
                "\n",
                "        if bad_settings > 0:\n",
                "            print(f\"The program is likely to not work with bad settings.\\nSettings file: {filename}\\nEither fix the file, or delete it and restart.\", file=sys.stderr)\n",
                "\n",
                "    def onchange(self, key, func, call=True):\n",
                "        item = self.data_labels.get(key)\n",
                "        item.onchange = func\n",
                "\n",
                "        if call:\n",
                "            func()\n",
                "\n",
                "    def dumpjson(self):\n",
                "        d = {k: self.data.get(k, self.data_labels.get(k).default) for k in self.data_labels.keys()}\n",
                "        return json.dumps(d)\n",
                "\n",
                "    def add_option(self, key, info):\n",
                "        self.data_labels[key] = info\n",
                "\n",
                "    def reorder(self):\n",
                "        \"\"\"reorder settings so that all items related to section always go together\"\"\"\n",
                "\n",
                "        section_ids = {}\n",
                "        settings_items = self.data_labels.items()\n",
                "        for k, item in settings_items:\n",
                "            if item.section not in section_ids:\n",
                "                section_ids[item.section] = len(section_ids)\n",
                "\n",
                "        self.data_labels = {k: v for k, v in sorted(settings_items, key=lambda x: section_ids[x[1].section])}\n",
                "\n",
                "\n",
                "opts = Options()\n",
                "if os.path.exists(config_filename):\n",
                "    opts.load(config_filename)\n",
                "\n",
                "sd_upscalers = []\n",
                "\n",
                "sd_model = None\n",
                "\n",
                "clip_model = None\n",
                "\n",
                "progress_print_out = sys.stdout\n",
                "\n",
                "\n",
                "class TotalTQDM:\n",
                "    def __init__(self):\n",
                "        self._tqdm = None\n",
                "\n",
                "    def reset(self):\n",
                "        self._tqdm = tqdm.tqdm(\n",
                "            desc=\"Total progress\",\n",
                "            total=state.job_count * state.sampling_steps,\n",
                "            position=1,\n",
                "            file=progress_print_out\n",
                "        )\n",
                "\n",
                "    def update(self):\n",
                "        if not opts.multiple_tqdm or cmd_opts.disable_console_progressbars:\n",
                "            return\n",
                "        if self._tqdm is None:\n",
                "            self.reset()\n",
                "        self._tqdm.update()\n",
                "\n",
                "    def updateTotal(self, new_total):\n",
                "        if not opts.multiple_tqdm or cmd_opts.disable_console_progressbars:\n",
                "            return\n",
                "        if self._tqdm is None:\n",
                "            self.reset()\n",
                "        self._tqdm.total=new_total\n",
                "\n",
                "    def clear(self):\n",
                "        if self._tqdm is not None:\n",
                "            self._tqdm.close()\n",
                "            self._tqdm = None\n",
                "\n",
                "\n",
                "total_tqdm = TotalTQDM()\n",
                "\n",
                "mem_mon = modules.memmon.MemUsageMonitor(\"MemMon\", device, opts)\n",
                "mem_mon.start()\n",
                "\n",
                "\n",
                "def listfiles(dirname):\n",
                "    filenames = [os.path.join(dirname, x) for x in sorted(os.listdir(dirname)) if not x.startswith(\".\")]\n",
                "    return [file for file in filenames if os.path.isfile(file)]"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "bi-directional",
            "reason": "func def and use"
        },
        {
            "edit_hunk_pair": [
                0,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "func def and use"
        },
        {
            "edit_hunk_pair": [
                1,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "func def and use"
        },
        {
            "edit_hunk_pair": [
                1,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "func def and use"
        },
        {
            "edit_hunk_pair": [
                2,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "func def and use"
        },
        {
            "edit_hunk_pair": [
                3,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "arg def and use"
        },
        {
            "edit_hunk_pair": [
                4,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "arg def and use"
        },
        {
            "edit_hunk_pair": [
                5,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "arg def and use"
        }
    ]
}