{
    "language": "python",
    "commit_url": "https://github.com/getredash/redash/commit/ab72531889f47f8e3d653849ebdf97cac1455a47",
    "commit_message": "Add settings to query results cleanup (closes #683)",
    "commit_snapshots": {
        "redash/models.py": [
            [
                "import json\n",
                "import hashlib\n",
                "import logging\n",
                "import os\n",
                "import threading\n",
                "import time\n",
                "import datetime\n",
                "import itertools\n",
                "\n",
                "import peewee\n",
                "from passlib.apps import custom_app_context as pwd_context\n",
                "from playhouse.postgres_ext import ArrayField, DateTimeTZField, PostgresqlExtDatabase\n",
                "from flask.ext.login import UserMixin, AnonymousUserMixin\n",
                "\n",
                "from redash import utils, settings, redis_connection\n",
                "from redash.query_runner import get_query_runner\n",
                "from utils import generate_token\n",
                "\n",
                "\n",
                "class Database(object):\n",
                "    def __init__(self):\n",
                "        self.database_config = dict(settings.DATABASE_CONFIG)\n",
                "        self.database_config['register_hstore'] = False\n",
                "        self.database_name = self.database_config.pop('name')\n",
                "        self.database = PostgresqlExtDatabase(self.database_name, **self.database_config)\n",
                "        self.app = None\n",
                "        self.pid = os.getpid()\n",
                "\n",
                "    def init_app(self, app):\n",
                "        self.app = app\n",
                "        self.register_handlers()\n",
                "\n",
                "    def connect_db(self):\n",
                "        self._check_pid()\n",
                "        self.database.connect()\n",
                "\n",
                "    def close_db(self, exc):\n",
                "        self._check_pid()\n",
                "        if not self.database.is_closed():\n",
                "            self.database.close()\n",
                "\n",
                "    def _check_pid(self):\n",
                "        current_pid = os.getpid()\n",
                "        if self.pid != current_pid:\n",
                "            logging.info(\"New pid detected (%d!=%d); resetting database lock.\", self.pid, current_pid)\n",
                "            self.pid = os.getpid()\n",
                "            self.database._conn_lock = threading.Lock()\n",
                "\n",
                "    def register_handlers(self):\n",
                "        self.app.before_request(self.connect_db)\n",
                "        self.app.teardown_request(self.close_db)\n",
                "\n",
                "\n",
                "db = Database()\n",
                "\n",
                "\n",
                "class BaseModel(peewee.Model):\n",
                "    class Meta:\n",
                "        database = db.database\n",
                "\n",
                "    @classmethod\n",
                "    def get_by_id(cls, model_id):\n",
                "        return cls.get(cls.id == model_id)\n",
                "\n",
                "    def pre_save(self, created):\n",
                "        pass\n",
                "\n",
                "    def post_save(self, created):\n",
                "        # Handler for post_save operations. Overriding if needed.\n",
                "        pass\n",
                "\n",
                "    def save(self, *args, **kwargs):\n",
                "        pk_value = self._get_pk_value()\n",
                "        created = kwargs.get('force_insert', False) or not bool(pk_value)\n",
                "        self.pre_save(created)\n",
                "        super(BaseModel, self).save(*args, **kwargs)\n",
                "        self.post_save(created)\n",
                "\n",
                "    def update_instance(self, **kwargs):\n",
                "        for k, v in kwargs.items():\n",
                "            # setattr(model_instance, field_name, field_obj.python_value(value))\n",
                "            setattr(self, k, v)\n",
                "\n",
                "        dirty_fields = self.dirty_fields\n",
                "        if hasattr(self, 'updated_at'):\n",
                "            dirty_fields = dirty_fields + [self.__class__.updated_at]\n",
                "\n",
                "        self.save(only=dirty_fields)\n",
                "\n",
                "\n",
                "class ModelTimestampsMixin(BaseModel):\n",
                "    updated_at = DateTimeTZField(default=datetime.datetime.now)\n",
                "    created_at = DateTimeTZField(default=datetime.datetime.now)\n",
                "\n",
                "    def pre_save(self, created):\n",
                "        super(ModelTimestampsMixin, self).pre_save(created)\n",
                "\n",
                "        self.updated_at = datetime.datetime.now()\n",
                "\n",
                "\n",
                "class PermissionsCheckMixin(object):\n",
                "    def has_permission(self, permission):\n",
                "        return self.has_permissions((permission,))\n",
                "\n",
                "    def has_permissions(self, permissions):\n",
                "        has_permissions = reduce(lambda a, b: a and b,\n",
                "                                 map(lambda permission: permission in self.permissions,\n",
                "                                     permissions),\n",
                "                                 True)\n",
                "\n",
                "        return has_permissions\n",
                "\n",
                "\n",
                "class AnonymousUser(AnonymousUserMixin, PermissionsCheckMixin):\n",
                "    @property\n",
                "    def permissions(self):\n",
                "        return []\n",
                "\n",
                "\n",
                "class ApiUser(UserMixin, PermissionsCheckMixin):\n",
                "    def __init__(self, api_key):\n",
                "        self.id = api_key\n",
                "\n",
                "    def __repr__(self):\n",
                "        return u\"<ApiUser: {}>\".format(self.id)\n",
                "\n",
                "    @property\n",
                "    def permissions(self):\n",
                "        return ['view_query']\n",
                "\n",
                "\n",
                "class Group(BaseModel):\n",
                "    DEFAULT_PERMISSIONS = ['create_dashboard', 'create_query', 'edit_dashboard', 'edit_query',\n",
                "                           'view_query', 'view_source', 'execute_query', 'list_users', 'schedule_query']\n",
                "\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    name = peewee.CharField(max_length=100)\n",
                "    permissions = ArrayField(peewee.CharField, default=DEFAULT_PERMISSIONS)\n",
                "    tables = ArrayField(peewee.CharField)\n",
                "    created_at = DateTimeTZField(default=datetime.datetime.now)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'groups'\n",
                "\n",
                "    def to_dict(self):\n",
                "        return {\n",
                "            'id': self.id,\n",
                "            'name': self.name,\n",
                "            'permissions': self.permissions,\n",
                "            'tables': self.tables,\n",
                "            'created_at': self.created_at\n",
                "        }\n",
                "\n",
                "    def __unicode__(self):\n",
                "        return unicode(self.id)\n",
                "\n",
                "\n",
                "class User(ModelTimestampsMixin, BaseModel, UserMixin, PermissionsCheckMixin):\n",
                "    DEFAULT_GROUPS = ['default']\n",
                "\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    name = peewee.CharField(max_length=320)\n",
                "    email = peewee.CharField(max_length=320, index=True, unique=True)\n",
                "    password_hash = peewee.CharField(max_length=128, null=True)\n",
                "    groups = ArrayField(peewee.CharField, default=DEFAULT_GROUPS)\n",
                "    api_key = peewee.CharField(max_length=40, unique=True)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'users'\n",
                "\n",
                "    def to_dict(self, with_api_key=False):\n",
                "        d = {\n",
                "            'id': self.id,\n",
                "            'name': self.name,\n",
                "            'email': self.email,\n",
                "            'gravatar_url': self.gravatar_url,\n",
                "            'groups': self.groups,\n",
                "            'updated_at': self.updated_at,\n",
                "            'created_at': self.created_at\n",
                "        }\n",
                "\n",
                "        if self.password_hash is None:\n",
                "            d['auth_type'] = 'external'\n",
                "        else:\n",
                "            d['auth_type'] = 'password'\n",
                "\n",
                "        if with_api_key:\n",
                "            d['api_key'] = self.api_key\n",
                "\n",
                "        return d\n",
                "\n",
                "    def __init__(self, *args, **kwargs):\n",
                "        super(User, self).__init__(*args, **kwargs)\n",
                "        self._allowed_tables = None\n",
                "\n",
                "    def pre_save(self, created):\n",
                "        super(User, self).pre_save(created)\n",
                "\n",
                "        if not self.api_key:\n",
                "            self.api_key = generate_token(40)\n",
                "\n",
                "    @property\n",
                "    def gravatar_url(self):\n",
                "        email_md5 = hashlib.md5(self.email.lower()).hexdigest()\n",
                "        return \"https://www.gravatar.com/avatar/%s?s=40\" % email_md5\n",
                "\n",
                "    @property\n",
                "    def permissions(self):\n",
                "        # TODO: this should be cached.\n",
                "        return list(itertools.chain(*[g.permissions for g in\n",
                "                                      Group.select().where(Group.name << self.groups)]))\n",
                "\n",
                "    @property\n",
                "    def allowed_tables(self):\n",
                "        # TODO: cache this as weel\n",
                "        if self._allowed_tables is None:\n",
                "            self._allowed_tables = set([t.lower() for t in itertools.chain(*[g.tables for g in\n",
                "                                        Group.select().where(Group.name << self.groups)])])\n",
                "\n",
                "        return self._allowed_tables\n",
                "\n",
                "    @classmethod\n",
                "    def get_by_email(cls, email):\n",
                "        return cls.get(cls.email == email)\n",
                "\n",
                "    @classmethod\n",
                "    def get_by_api_key(cls, api_key):\n",
                "        return cls.get(cls.api_key == api_key)\n",
                "\n",
                "    def __unicode__(self):\n",
                "        return u'%s (%s)' % (self.name, self.email)\n",
                "\n",
                "    def hash_password(self, password):\n",
                "        self.password_hash = pwd_context.encrypt(password)\n",
                "\n",
                "    def verify_password(self, password):\n",
                "        return self.password_hash and pwd_context.verify(password, self.password_hash)\n",
                "\n",
                "\n",
                "class ActivityLog(BaseModel):\n",
                "    QUERY_EXECUTION = 1\n",
                "\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    user = peewee.ForeignKeyField(User)\n",
                "    type = peewee.IntegerField()\n",
                "    activity = peewee.TextField()\n",
                "    created_at = DateTimeTZField(default=datetime.datetime.now)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'activity_log'\n",
                "\n",
                "    def to_dict(self):\n",
                "        return {\n",
                "            'id': self.id,\n",
                "            'user': self.user.to_dict(),\n",
                "            'type': self.type,\n",
                "            'activity': self.activity,\n",
                "            'created_at': self.created_at\n",
                "        }\n",
                "\n",
                "    def __unicode__(self):\n",
                "        return unicode(self.id)\n",
                "\n",
                "\n",
                "class DataSource(BaseModel):\n",
                "    SECRET_PLACEHOLDER = '--------'\n",
                "\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    name = peewee.CharField(unique=True)\n",
                "    type = peewee.CharField()\n",
                "    options = peewee.TextField()\n",
                "    queue_name = peewee.CharField(default=\"queries\")\n",
                "    scheduled_queue_name = peewee.CharField(default=\"scheduled_queries\")\n",
                "    created_at = DateTimeTZField(default=datetime.datetime.now)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'data_sources'\n",
                "\n",
                "    def to_dict(self, all=False):\n",
                "        d = {\n",
                "            'id': self.id,\n",
                "            'name': self.name,\n",
                "            'type': self.type,\n",
                "            'syntax': self.query_runner.syntax\n",
                "        }\n",
                "\n",
                "        if all:\n",
                "            d['options'] = self.configuration\n",
                "            d['queue_name'] = self.queue_name\n",
                "            d['scheduled_queue_name'] = self.scheduled_queue_name\n",
                "\n",
                "        return d\n",
                "\n",
                "    def __unicode__(self):\n",
                "        return self.name\n",
                "\n",
                "    @property\n",
                "    def configuration(self):\n",
                "        configuration = json.loads(self.options)\n",
                "        schema = self.query_runner.configuration_schema()\n",
                "        for prop in schema.get('secret', []):\n",
                "            if prop in configuration and configuration[prop]:\n",
                "                configuration[prop] = self.SECRET_PLACEHOLDER\n",
                "\n",
                "        return configuration\n",
                "\n",
                "    def replace_secret_placeholders(self, configuration):\n",
                "        current_configuration = json.loads(self.options)\n",
                "        schema = self.query_runner.configuration_schema()\n",
                "        for prop in schema.get('secret', []):\n",
                "            if prop in configuration and configuration[prop] == self.SECRET_PLACEHOLDER:\n",
                "                configuration[prop] = current_configuration[prop]\n",
                "\n",
                "    def get_schema(self, refresh=False):\n",
                "        key = \"data_source:schema:{}\".format(self.id)\n",
                "\n",
                "        cache = None\n",
                "        if not refresh:\n",
                "            cache = redis_connection.get(key)\n",
                "\n",
                "        if cache is None:\n",
                "            query_runner = self.query_runner\n",
                "            schema = sorted(query_runner.get_schema(), key=lambda t: t['name'])\n",
                "\n",
                "            redis_connection.set(key, json.dumps(schema))\n",
                "        else:\n",
                "            schema = json.loads(cache)\n",
                "\n",
                "        return schema\n",
                "\n",
                "    @property\n",
                "    def query_runner(self):\n",
                "        return get_query_runner(self.type, self.options)\n",
                "\n",
                "    @classmethod\n",
                "    def all(cls):\n",
                "        return cls.select().order_by(cls.id.asc())\n",
                "\n",
                "\n",
                "class JSONField(peewee.TextField):\n",
                "    def db_value(self, value):\n",
                "        return json.dumps(value)\n",
                "\n",
                "    def python_value(self, value):\n",
                "        return json.loads(value)\n",
                "\n",
                "\n",
                "class QueryResult(BaseModel):\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    data_source = peewee.ForeignKeyField(DataSource)\n",
                "    query_hash = peewee.CharField(max_length=32, index=True)\n",
                "    query = peewee.TextField()\n",
                "    data = peewee.TextField()\n",
                "    runtime = peewee.FloatField()\n",
                "    retrieved_at = DateTimeTZField()\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'query_results'\n",
                "\n",
                "    def to_dict(self):\n",
                "        return {\n",
                "            'id': self.id,\n",
                "            'query_hash': self.query_hash,\n",
                "            'query': self.query,\n",
                "            'data': json.loads(self.data),\n",
                "            'data_source_id': self._data.get('data_source', None),\n",
                "            'runtime': self.runtime,\n",
                "            'retrieved_at': self.retrieved_at\n",
                "        }\n",
                "\n",
                "    @classmethod\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    def unused(cls):\n",
                    "        week_ago = datetime.datetime.now() - datetime.timedelta(days=7)\n"
                ],
                "after": [
                    "    def unused(cls, days=7):\n",
                    "        age_threshold = datetime.datetime.now() - datetime.timedelta(days=days)\n"
                ],
                "parent_version_range": {
                    "start": 371,
                    "end": 373
                },
                "child_version_range": {
                    "start": 371,
                    "end": 373
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "QueryResult",
                        "signature": "class QueryResult(BaseModel):",
                        "at_line": 347
                    },
                    {
                        "type": "function",
                        "name": "unused",
                        "signature": "def unused(cls):",
                        "at_line": 371
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: redash/models.py\nCode:\n           class QueryResult(BaseModel):\n               ...\n368 368            }\n369 369    \n370 370        @classmethod\n371      -     def unused(cls):\n372      -         week_ago = datetime.datetime.now() - datetime.timedelta(days=7)\n    371  +     def unused(cls, days=7):\n    372  +         age_threshold = datetime.datetime.now() - datetime.timedelta(days=days)\n373 373    \n         ...\n",
                "file_path": "redash/models.py",
                "identifiers_before": [
                    "cls",
                    "datetime",
                    "days",
                    "now",
                    "timedelta",
                    "unused",
                    "week_ago"
                ],
                "identifiers_after": [
                    "age_threshold",
                    "cls",
                    "datetime",
                    "days",
                    "now",
                    "timedelta",
                    "unused"
                ],
                "prefix": [
                    "        }\n",
                    "\n",
                    "    @classmethod\n"
                ],
                "suffix": [
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "unused",
                            "position": {
                                "start": {
                                    "line": 371,
                                    "column": 8
                                },
                                "end": {
                                    "line": 371,
                                    "column": 14
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "cls",
                            "position": {
                                "start": {
                                    "line": 371,
                                    "column": 15
                                },
                                "end": {
                                    "line": 371,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "cls",
                            "position": {
                                "start": {
                                    "line": 371,
                                    "column": 15
                                },
                                "end": {
                                    "line": 371,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "week_ago",
                            "position": {
                                "start": {
                                    "line": 372,
                                    "column": 8
                                },
                                "end": {
                                    "line": 372,
                                    "column": 16
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "unused",
                            "position": {
                                "start": {
                                    "line": 371,
                                    "column": 8
                                },
                                "end": {
                                    "line": 371,
                                    "column": 14
                                }
                            },
                            "type": "identifier",
                            "kind": "function",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "cls",
                            "position": {
                                "start": {
                                    "line": 371,
                                    "column": 15
                                },
                                "end": {
                                    "line": 371,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "cls",
                            "position": {
                                "start": {
                                    "line": 371,
                                    "column": 15
                                },
                                "end": {
                                    "line": 371,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "age_threshold",
                            "position": {
                                "start": {
                                    "line": 372,
                                    "column": 8
                                },
                                "end": {
                                    "line": 372,
                                    "column": 21
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        unused_results = cls.select().where(Query.id == None, cls.retrieved_at < week_ago)\\\n"
                ],
                "after": [
                    "        unused_results = cls.select().where(Query.id == None, cls.retrieved_at < age_threshold)\\\n"
                ],
                "parent_version_range": {
                    "start": 374,
                    "end": 375
                },
                "child_version_range": {
                    "start": 374,
                    "end": 375
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "QueryResult",
                        "signature": "class QueryResult(BaseModel):",
                        "at_line": 347
                    },
                    {
                        "type": "function",
                        "name": "unused",
                        "signature": "def unused(cls):",
                        "at_line": 371
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: redash/models.py\nCode:\n           class QueryResult(BaseModel):\n               ...\n               def unused(cls):\n                   ...\n373 373    \n374      -         unused_results = cls.select().where(Query.id == None, cls.retrieved_at < week_ago)\\\n    374  +         unused_results = cls.select().where(Query.id == None, cls.retrieved_at < age_threshold)\\\n375 375                .join(Query, join_type=peewee.JOIN_LEFT_OUTER)\n376 376    \n377 377            return unused_results\n         ...\n",
                "file_path": "redash/models.py",
                "identifiers_before": [
                    "Query",
                    "cls",
                    "id",
                    "retrieved_at",
                    "select",
                    "unused_results",
                    "week_ago",
                    "where"
                ],
                "identifiers_after": [
                    "Query",
                    "age_threshold",
                    "cls",
                    "id",
                    "retrieved_at",
                    "select",
                    "unused_results",
                    "where"
                ],
                "prefix": [
                    "\n"
                ],
                "suffix": [
                    "            .join(Query, join_type=peewee.JOIN_LEFT_OUTER)\n",
                    "\n",
                    "        return unused_results\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "cls",
                            "position": {
                                "start": {
                                    "line": 374,
                                    "column": 25
                                },
                                "end": {
                                    "line": 374,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "cls",
                            "position": {
                                "start": {
                                    "line": 374,
                                    "column": 62
                                },
                                "end": {
                                    "line": 374,
                                    "column": 65
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "week_ago",
                            "position": {
                                "start": {
                                    "line": 374,
                                    "column": 81
                                },
                                "end": {
                                    "line": 374,
                                    "column": 89
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "cls",
                            "position": {
                                "start": {
                                    "line": 374,
                                    "column": 25
                                },
                                "end": {
                                    "line": 374,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "cls",
                            "position": {
                                "start": {
                                    "line": 374,
                                    "column": 62
                                },
                                "end": {
                                    "line": 374,
                                    "column": 65
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "age_threshold",
                            "position": {
                                "start": {
                                    "line": 374,
                                    "column": 81
                                },
                                "end": {
                                    "line": 374,
                                    "column": 94
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/models.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "            .join(Query, join_type=peewee.JOIN_LEFT_OUTER)\n",
                "\n",
                "        return unused_results\n",
                "\n",
                "    @classmethod\n",
                "    def get_latest(cls, data_source, query, max_age=0):\n",
                "        query_hash = utils.gen_query_hash(query)\n",
                "\n",
                "        if max_age == -1:\n",
                "            query = cls.select().where(cls.query_hash == query_hash,\n",
                "                                       cls.data_source == data_source).order_by(cls.retrieved_at.desc())\n",
                "        else:\n",
                "            query = cls.select().where(cls.query_hash == query_hash, cls.data_source == data_source,\n",
                "                                       peewee.SQL(\"retrieved_at + interval '%s second' >= now() at time zone 'utc'\",\n",
                "                                                  max_age)).order_by(cls.retrieved_at.desc())\n",
                "\n",
                "        return query.first()\n",
                "\n",
                "    @classmethod\n",
                "    def store_result(cls, data_source_id, query_hash, query, data, run_time, retrieved_at):\n",
                "        query_result = cls.create(query_hash=query_hash,\n",
                "                                  query=query,\n",
                "                                  runtime=run_time,\n",
                "                                  data_source=data_source_id,\n",
                "                                  retrieved_at=retrieved_at,\n",
                "                                  data=data)\n",
                "\n",
                "        logging.info(\"Inserted query (%s) data; id=%s\", query_hash, query_result.id)\n",
                "\n",
                "        sql = \"UPDATE queries SET latest_query_data_id = %s WHERE query_hash = %s AND data_source_id = %s RETURNING id\"\n",
                "        query_ids = [row[0] for row in db.database.execute_sql(sql, params=(query_result.id, query_hash, data_source_id))]\n",
                "\n",
                "        # TODO: when peewee with update & returning support is released, we can get back to using this code:\n",
                "        # updated_count = Query.update(latest_query_data=query_result).\\\n",
                "        #     where(Query.query_hash==query_hash, Query.data_source==data_source_id).\\\n",
                "        #     execute()\n",
                "\n",
                "        logging.info(\"Updated %s queries with result (%s).\", len(query_ids), query_hash)\n",
                "\n",
                "        return query_result, query_ids\n",
                "\n",
                "    def __unicode__(self):\n",
                "        return u\"%d | %s | %s\" % (self.id, self.query_hash, self.retrieved_at)\n",
                "\n",
                "\n",
                "def should_schedule_next(previous_iteration, now, schedule):\n",
                "    if schedule.isdigit():\n",
                "        ttl = int(schedule)\n",
                "        next_iteration = previous_iteration + datetime.timedelta(seconds=ttl)\n",
                "    else:\n",
                "        hour, minute = schedule.split(':')\n",
                "        hour, minute = int(hour), int(minute)\n",
                "\n",
                "        # The following logic is needed for cases like the following:\n",
                "        # - The query scheduled to run at 23:59.\n",
                "        # - The scheduler wakes up at 00:01.\n",
                "        # - Using naive implementation of comparing timestamps, it will skip the execution.\n",
                "        normalized_previous_iteration = previous_iteration.replace(hour=hour, minute=minute)\n",
                "        if normalized_previous_iteration > previous_iteration:\n",
                "            previous_iteration = normalized_previous_iteration - datetime.timedelta(days=1)\n",
                "\n",
                "        next_iteration = (previous_iteration + datetime.timedelta(days=1)).replace(hour=hour, minute=minute)\n",
                "\n",
                "    return now > next_iteration\n",
                "\n",
                "\n",
                "class Query(ModelTimestampsMixin, BaseModel):\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    data_source = peewee.ForeignKeyField(DataSource, null=True)\n",
                "    latest_query_data = peewee.ForeignKeyField(QueryResult, null=True)\n",
                "    name = peewee.CharField(max_length=255)\n",
                "    description = peewee.CharField(max_length=4096, null=True)\n",
                "    query = peewee.TextField()\n",
                "    query_hash = peewee.CharField(max_length=32)\n",
                "    api_key = peewee.CharField(max_length=40)\n",
                "    user_email = peewee.CharField(max_length=360, null=True)\n",
                "    user = peewee.ForeignKeyField(User)\n",
                "    last_modified_by = peewee.ForeignKeyField(User, null=True, related_name=\"modified_queries\")\n",
                "    is_archived = peewee.BooleanField(default=False, index=True)\n",
                "    schedule = peewee.CharField(max_length=10, null=True)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'queries'\n",
                "\n",
                "    def to_dict(self, with_stats=False, with_visualizations=False, with_user=True):\n",
                "        d = {\n",
                "            'id': self.id,\n",
                "            'latest_query_data_id': self._data.get('latest_query_data', None),\n",
                "            'name': self.name,\n",
                "            'description': self.description,\n",
                "            'query': self.query,\n",
                "            'query_hash': self.query_hash,\n",
                "            'schedule': self.schedule,\n",
                "            'api_key': self.api_key,\n",
                "            'is_archived': self.is_archived,\n",
                "            'updated_at': self.updated_at,\n",
                "            'created_at': self.created_at,\n",
                "            'data_source_id': self._data.get('data_source', None)\n",
                "        }\n",
                "\n",
                "        if with_user:\n",
                "            d['user'] = self.user.to_dict()\n",
                "            d['last_modified_by'] = self.last_modified_by.to_dict() if self.last_modified_by is not None else None\n",
                "        else:\n",
                "            d['user_id'] = self._data['user']\n",
                "\n",
                "        if with_stats:\n",
                "            d['retrieved_at'] = self.retrieved_at\n",
                "            d['runtime'] = self.runtime\n",
                "\n",
                "        if with_visualizations:\n",
                "            d['visualizations'] = [vis.to_dict(with_query=False)\n",
                "                                   for vis in self.visualizations]\n",
                "\n",
                "        return d\n",
                "\n",
                "    def archive(self):\n",
                "        self.is_archived = True\n",
                "        self.schedule = None\n",
                "\n",
                "        for vis in self.visualizations:\n",
                "            for w in vis.widgets:\n",
                "                w.delete_instance()\n",
                "\n",
                "        self.save()\n",
                "\n",
                "    @classmethod\n",
                "    def all_queries(cls):\n",
                "        q = Query.select(Query, User, QueryResult.retrieved_at, QueryResult.runtime)\\\n",
                "            .join(QueryResult, join_type=peewee.JOIN_LEFT_OUTER)\\\n",
                "            .switch(Query).join(User)\\\n",
                "            .where(Query.is_archived==False)\\\n",
                "            .group_by(Query.id, User.id, QueryResult.id, QueryResult.retrieved_at, QueryResult.runtime)\\\n",
                "            .order_by(cls.created_at.desc())\n",
                "\n",
                "        return q\n",
                "\n",
                "    @classmethod\n",
                "    def outdated_queries(cls):\n",
                "        queries = cls.select(cls, QueryResult.retrieved_at, DataSource)\\\n",
                "            .join(QueryResult)\\\n",
                "            .switch(Query).join(DataSource)\\\n",
                "            .where(cls.schedule != None)\n",
                "\n",
                "        now = utils.utcnow()\n",
                "        outdated_queries = {}\n",
                "        for query in queries:\n",
                "            if should_schedule_next(query.latest_query_data.retrieved_at, now, query.schedule):\n",
                "                key = \"{}:{}\".format(query.query_hash, query.data_source.id)\n",
                "                outdated_queries[key] = query\n",
                "\n",
                "        return outdated_queries.values()\n",
                "\n",
                "    @classmethod\n",
                "    def search(cls, term):\n",
                "        # This is very naive implementation of search, to be replaced with PostgreSQL full-text-search solution.\n",
                "\n",
                "        where = (cls.name**u\"%{}%\".format(term)) | (cls.description**u\"%{}%\".format(term))\n",
                "\n",
                "        if term.isdigit():\n",
                "            where |= cls.id == term\n",
                "\n",
                "        where &= cls.is_archived == False\n",
                "\n",
                "        return cls.select().where(where).order_by(cls.created_at.desc())\n",
                "\n",
                "    @classmethod\n",
                "    def recent(cls, user_id=None, limit=20):\n",
                "        # TODO: instead of t2 here, we should define table_alias for Query table\n",
                "        query = cls.select().where(Event.created_at > peewee.SQL(\"current_date - 7\")).\\\n",
                "            join(Event, on=(Query.id == peewee.SQL(\"t2.object_id::integer\"))).\\\n",
                "            where(Event.action << ('edit', 'execute', 'edit_name', 'edit_description', 'view_source')).\\\n",
                "            where(~(Event.object_id >> None)).\\\n",
                "            where(Event.object_type == 'query'). \\\n",
                "            where(cls.is_archived == False).\\\n",
                "            group_by(Event.object_id, Query.id).\\\n",
                "            order_by(peewee.SQL(\"count(0) desc\"))\n",
                "\n",
                "        if user_id:\n",
                "            query = query.where(Event.user == user_id)\n",
                "\n",
                "        query = query.limit(limit)\n",
                "\n",
                "        return query\n",
                "\n",
                "    @classmethod\n",
                "    def update_instance(cls, query_id, **kwargs):\n",
                "        if 'query' in kwargs:\n",
                "            kwargs['query_hash'] = utils.gen_query_hash(kwargs['query'])\n",
                "\n",
                "        update = cls.update(**kwargs).where(cls.id == query_id)\n",
                "        return update.execute()\n",
                "\n",
                "    def pre_save(self, created):\n",
                "        super(Query, self).pre_save(created)\n",
                "        self.query_hash = utils.gen_query_hash(self.query)\n",
                "        self._set_api_key()\n",
                "\n",
                "        if self.last_modified_by is None:\n",
                "            self.last_modified_by = self.user\n",
                "\n",
                "    def post_save(self, created):\n",
                "        if created:\n",
                "            self._create_default_visualizations()\n",
                "\n",
                "    def _create_default_visualizations(self):\n",
                "        table_visualization = Visualization(query=self, name=\"Table\",\n",
                "                                            description='',\n",
                "                                            type=\"TABLE\", options=\"{}\")\n",
                "        table_visualization.save()\n",
                "\n",
                "    def _set_api_key(self):\n",
                "        if not self.api_key:\n",
                "            self.api_key = hashlib.sha1(\n",
                "                u''.join((str(time.time()), self.query, str(self._data['user']), self.name)).encode('utf-8')).hexdigest()\n",
                "\n",
                "    @property\n",
                "    def runtime(self):\n",
                "        return self.latest_query_data.runtime\n",
                "\n",
                "    @property\n",
                "    def retrieved_at(self):\n",
                "        return self.latest_query_data.retrieved_at\n",
                "\n",
                "    def __unicode__(self):\n",
                "        return unicode(self.id)\n",
                "\n",
                "\n",
                "class Alert(ModelTimestampsMixin, BaseModel):\n",
                "    UNKNOWN_STATE = 'unknown'\n",
                "    OK_STATE = 'ok'\n",
                "    TRIGGERED_STATE = 'triggered'\n",
                "\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    name = peewee.CharField()\n",
                "    query = peewee.ForeignKeyField(Query, related_name='alerts')\n",
                "    user = peewee.ForeignKeyField(User, related_name='alerts')\n",
                "    options = JSONField()\n",
                "    state = peewee.CharField(default=UNKNOWN_STATE)\n",
                "    last_triggered_at = DateTimeTZField(null=True)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'alerts'\n",
                "\n",
                "    @classmethod\n",
                "    def all(cls):\n",
                "        return cls.select(Alert, User, Query).join(Query).switch(Alert).join(User)\n",
                "\n",
                "    def to_dict(self, full=True):\n",
                "        d = {\n",
                "                'id': self.id,\n",
                "                'name': self.name,\n",
                "                'options': self.options,\n",
                "                'state': self.state,\n",
                "                'last_triggered_at': self.last_triggered_at,\n",
                "                'updated_at': self.updated_at,\n",
                "                'created_at': self.created_at\n",
                "        }\n",
                "\n",
                "        if full:\n",
                "            d['query'] = self.query.to_dict()\n",
                "            d['user'] = self.user.to_dict()\n",
                "        else:\n",
                "            d['query_id'] = self._data['query']\n",
                "            d['user_id'] = self._data['user']\n",
                "\n",
                "        return d\n",
                "\n",
                "    def evaluate(self):\n",
                "        data = json.loads(self.query.latest_query_data.data)\n",
                "        # todo: safe guard for empty\n",
                "        value = data['rows'][0][self.options['column']]\n",
                "        op = self.options['op']\n",
                "\n",
                "        if op == 'greater than' and value > self.options['value']:\n",
                "            new_state = self.TRIGGERED_STATE\n",
                "        elif op == 'less than' and value < self.options['value']:\n",
                "            new_state = self.TRIGGERED_STATE\n",
                "        elif op == 'equals' and value == self.options['value']:\n",
                "            new_state = self.TRIGGERED_STATE\n",
                "        else:\n",
                "            new_state = self.OK_STATE\n",
                "\n",
                "        return new_state\n",
                "\n",
                "    def subscribers(self):\n",
                "        return User.select().join(AlertSubscription).where(AlertSubscription.alert==self)\n",
                "\n",
                "\n",
                "class AlertSubscription(ModelTimestampsMixin, BaseModel):\n",
                "    user = peewee.ForeignKeyField(User)\n",
                "    alert = peewee.ForeignKeyField(Alert)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'alert_subscriptions'\n",
                "\n",
                "    def to_dict(self):\n",
                "        return {\n",
                "            'user': self.user.to_dict(),\n",
                "            'alert_id': self._data['alert']\n",
                "        }\n",
                "\n",
                "    @classmethod\n",
                "    def all(cls, alert_id):\n",
                "        return AlertSubscription.select(AlertSubscription, User).join(User).where(AlertSubscription.alert==alert_id)\n",
                "\n",
                "    @classmethod\n",
                "    def unsubscribe(cls, alert_id, user_id):\n",
                "        query = AlertSubscription.delete().where(AlertSubscription.alert==alert_id).where(AlertSubscription.user==user_id)\n",
                "        return query.execute()\n",
                "\n",
                "\n",
                "class Dashboard(ModelTimestampsMixin, BaseModel):\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    slug = peewee.CharField(max_length=140, index=True)\n",
                "    name = peewee.CharField(max_length=100)\n",
                "    user_email = peewee.CharField(max_length=360, null=True)\n",
                "    user = peewee.ForeignKeyField(User)\n",
                "    layout = peewee.TextField()\n",
                "    dashboard_filters_enabled = peewee.BooleanField(default=False)\n",
                "    is_archived = peewee.BooleanField(default=False, index=True)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'dashboards'\n",
                "\n",
                "    def to_dict(self, with_widgets=False):\n",
                "        layout = json.loads(self.layout)\n",
                "\n",
                "        if with_widgets:\n",
                "            widgets = Widget.select(Widget, Visualization, Query, User)\\\n",
                "                .where(Widget.dashboard == self.id)\\\n",
                "                .join(Visualization, join_type=peewee.JOIN_LEFT_OUTER)\\\n",
                "                .join(Query, join_type=peewee.JOIN_LEFT_OUTER)\\\n",
                "                .join(User, join_type=peewee.JOIN_LEFT_OUTER)\n",
                "            widgets = {w.id: w.to_dict() for w in widgets}\n",
                "\n",
                "            # The following is a workaround for cases when the widget object gets deleted without the dashboard layout\n",
                "            # updated. This happens for users with old databases that didn't have a foreign key relationship between\n",
                "            # visualizations and widgets.\n",
                "            # It's temporary until better solution is implemented (we probably should move the position information\n",
                "            # to the widget).\n",
                "            widgets_layout = []\n",
                "            for row in layout:\n",
                "                new_row = []\n",
                "                for widget_id in row:\n",
                "                    widget = widgets.get(widget_id, None)\n",
                "                    if widget:\n",
                "                        new_row.append(widget)\n",
                "\n",
                "                widgets_layout.append(new_row)\n",
                "\n",
                "            # widgets_layout = map(lambda row: map(lambda widget_id: widgets.get(widget_id, None), row), layout)\n",
                "        else:\n",
                "            widgets_layout = None\n",
                "\n",
                "        return {\n",
                "            'id': self.id,\n",
                "            'slug': self.slug,\n",
                "            'name': self.name,\n",
                "            'user_id': self._data['user'],\n",
                "            'layout': layout,\n",
                "            'dashboard_filters_enabled': self.dashboard_filters_enabled,\n",
                "            'widgets': widgets_layout,\n",
                "            'is_archived': self.is_archived,\n",
                "            'updated_at': self.updated_at,\n",
                "            'created_at': self.created_at\n",
                "        }\n",
                "\n",
                "    @classmethod\n",
                "    def get_by_slug(cls, slug):\n",
                "        return cls.get(cls.slug == slug)\n",
                "\n",
                "    @classmethod\n",
                "    def recent(cls, user_id=None, limit=20):\n",
                "        query = cls.select().where(Event.created_at > peewee.SQL(\"current_date - 7\")). \\\n",
                "            join(Event, on=(Dashboard.id == peewee.SQL(\"t2.object_id::integer\"))). \\\n",
                "            where(Event.action << ('edit', 'view')).\\\n",
                "            where(~(Event.object_id >> None)). \\\n",
                "            where(Event.object_type == 'dashboard'). \\\n",
                "            where(Dashboard.is_archived == False). \\\n",
                "            group_by(Event.object_id, Dashboard.id). \\\n",
                "            order_by(peewee.SQL(\"count(0) desc\"))\n",
                "\n",
                "        if user_id:\n",
                "            query = query.where(Event.user == user_id)\n",
                "\n",
                "        query = query.limit(limit)\n",
                "\n",
                "        return query\n",
                "\n",
                "    def save(self, *args, **kwargs):\n",
                "        if not self.slug:\n",
                "            self.slug = utils.slugify(self.name)\n",
                "\n",
                "            tries = 1\n",
                "            while self.select().where(Dashboard.slug == self.slug).first() is not None:\n",
                "                self.slug = utils.slugify(self.name) + \"_{0}\".format(tries)\n",
                "                tries += 1\n",
                "\n",
                "        super(Dashboard, self).save(*args, **kwargs)\n",
                "\n",
                "    def __unicode__(self):\n",
                "        return u\"%s=%s\" % (self.id, self.name)\n",
                "\n",
                "\n",
                "class Visualization(ModelTimestampsMixin, BaseModel):\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    type = peewee.CharField(max_length=100)\n",
                "    query = peewee.ForeignKeyField(Query, related_name='visualizations')\n",
                "    name = peewee.CharField(max_length=255)\n",
                "    description = peewee.CharField(max_length=4096, null=True)\n",
                "    options = peewee.TextField()\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'visualizations'\n",
                "\n",
                "    def to_dict(self, with_query=True):\n",
                "        d = {\n",
                "            'id': self.id,\n",
                "            'type': self.type,\n",
                "            'name': self.name,\n",
                "            'description': self.description,\n",
                "            'options': json.loads(self.options),\n",
                "            'updated_at': self.updated_at,\n",
                "            'created_at': self.created_at\n",
                "        }\n",
                "\n",
                "        if with_query:\n",
                "            d['query'] = self.query.to_dict()\n",
                "\n",
                "        return d\n",
                "\n",
                "    def __unicode__(self):\n",
                "        return u\"%s %s\" % (self.id, self.type)\n",
                "\n",
                "\n",
                "class Widget(ModelTimestampsMixin, BaseModel):\n",
                "    id = peewee.PrimaryKeyField()\n",
                "    visualization = peewee.ForeignKeyField(Visualization, related_name='widgets', null=True)\n",
                "    text = peewee.TextField(null=True)\n",
                "    width = peewee.IntegerField()\n",
                "    options = peewee.TextField()\n",
                "    dashboard = peewee.ForeignKeyField(Dashboard, related_name='widgets', index=True)\n",
                "\n",
                "    # unused; kept for backward compatability:\n",
                "    type = peewee.CharField(max_length=100, null=True)\n",
                "    query_id = peewee.IntegerField(null=True)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'widgets'\n",
                "\n",
                "    def to_dict(self):\n",
                "        d = {\n",
                "            'id': self.id,\n",
                "            'width': self.width,\n",
                "            'options': json.loads(self.options),\n",
                "            'dashboard_id': self._data['dashboard'],\n",
                "            'text': self.text,\n",
                "            'updated_at': self.updated_at,\n",
                "            'created_at': self.created_at\n",
                "        }\n",
                "\n",
                "        if self.visualization and self.visualization.id:\n",
                "            d['visualization'] = self.visualization.to_dict()\n",
                "\n",
                "        return d\n",
                "    \n",
                "    def __unicode__(self):\n",
                "        return u\"%s\" % self.id\n",
                "\n",
                "    def delete_instance(self, *args, **kwargs):\n",
                "        layout = json.loads(self.dashboard.layout)\n",
                "        layout = map(lambda row: filter(lambda w: w != self.id, row), layout)\n",
                "        layout = filter(lambda row: len(row) > 0, layout)\n",
                "        self.dashboard.layout = json.dumps(layout)\n",
                "        self.dashboard.save()\n",
                "        super(Widget, self).delete_instance(*args, **kwargs)\n",
                "\n",
                "\n",
                "class Event(BaseModel):\n",
                "    user = peewee.ForeignKeyField(User, related_name=\"events\", null=True)\n",
                "    action = peewee.CharField()\n",
                "    object_type = peewee.CharField()\n",
                "    object_id = peewee.CharField(null=True)\n",
                "    additional_properties = peewee.TextField(null=True)\n",
                "    created_at = DateTimeTZField(default=datetime.datetime.now)\n",
                "\n",
                "    class Meta:\n",
                "        db_table = 'events'\n",
                "\n",
                "    def __unicode__(self):\n",
                "        return u\"%s,%s,%s,%s\" % (self._data['user'], self.action, self.object_type, self.object_id)\n",
                "\n",
                "    @classmethod\n",
                "    def record(cls, event):\n",
                "        user = event.pop('user_id')\n",
                "        action = event.pop('action')\n",
                "        object_type = event.pop('object_type')\n",
                "        object_id = event.pop('object_id', None)\n",
                "\n",
                "        created_at = datetime.datetime.utcfromtimestamp(event.pop('timestamp'))\n",
                "        additional_properties = json.dumps(event)\n",
                "\n",
                "        event = cls.create(user=user, action=action, object_type=object_type, object_id=object_id,\n",
                "                           additional_properties=additional_properties, created_at=created_at)\n",
                "\n",
                "        return event\n",
                "\n",
                "\n",
                "all_models = (DataSource, User, QueryResult, Query, Alert, AlertSubscription, Dashboard, Visualization, Widget, ActivityLog, Group, Event)\n",
                "\n",
                "\n",
                "def init_db():\n",
                "    Group.insert(name='admin', permissions=['admin'], tables=['*']).execute()\n",
                "    Group.insert(name='default', permissions=Group.DEFAULT_PERMISSIONS, tables=['*']).execute()\n",
                "\n",
                "\n",
                "def create_db(create_tables, drop_tables):\n",
                "    db.connect_db()\n",
                "\n",
                "    for model in all_models:\n",
                "        if drop_tables and model.table_exists():\n",
                "            # TODO: submit PR to peewee to allow passing cascade option to drop_table.\n",
                "            db.database.execute_sql('DROP TABLE %s CASCADE' % model._meta.db_table)\n",
                "\n",
                "        if create_tables and not model.table_exists():\n",
                "            model.create_table()\n",
                "\n",
                "    db.close_db(None)"
            ]
        ],
        "redash/settings.py": [
            [
                "import json\n",
                "import os\n",
                "import urlparse\n",
                "from funcy import distinct\n",
                "\n",
                "\n",
                "def parse_db_url(url):\n",
                "    url_parts = urlparse.urlparse(url)\n",
                "    connection = {'threadlocals': True}\n",
                "\n",
                "    if url_parts.hostname and not url_parts.path:\n",
                "        connection['name'] = url_parts.hostname\n",
                "    else:\n",
                "        connection['name'] = url_parts.path[1:]\n",
                "        connection['host'] = url_parts.hostname\n",
                "        connection['port'] = url_parts.port\n",
                "        connection['user'] = url_parts.username\n",
                "        connection['password'] = url_parts.password\n",
                "\n",
                "    return connection\n",
                "\n",
                "\n",
                "def fix_assets_path(path):\n",
                "    fullpath = os.path.join(os.path.dirname(__file__), path)\n",
                "    return fullpath\n",
                "\n",
                "\n",
                "def array_from_string(str):\n",
                "    array = str.split(',')\n",
                "    if \"\" in array:\n",
                "        array.remove(\"\")\n",
                "\n",
                "    return array\n",
                "\n",
                "\n",
                "def set_from_string(str):\n",
                "    return set(array_from_string(str))\n",
                "\n",
                "\n",
                "def parse_boolean(str):\n",
                "    return json.loads(str.lower())\n",
                "\n",
                "\n",
                "def all_settings():\n",
                "    from types import ModuleType\n",
                "\n",
                "    settings = {}\n",
                "    for name, item in globals().iteritems():\n",
                "        if not callable(item) and not name.startswith(\"__\") and not isinstance(item, ModuleType):\n",
                "            settings[name] = item\n",
                "\n",
                "    return settings\n",
                "\n",
                "\n",
                "NAME = os.environ.get('REDASH_NAME', 're:dash')\n",
                "\n",
                "REDIS_URL = os.environ.get('REDASH_REDIS_URL', \"redis://localhost:6379/0\")\n",
                "\n",
                "STATSD_HOST = os.environ.get('REDASH_STATSD_HOST', \"127.0.0.1\")\n",
                "STATSD_PORT = int(os.environ.get('REDASH_STATSD_PORT', \"8125\"))\n",
                "STATSD_PREFIX = os.environ.get('REDASH_STATSD_PREFIX', \"redash\")\n",
                "\n",
                "# Connection settings for re:dash's own database (where we store the queries, results, etc)\n",
                "DATABASE_CONFIG = parse_db_url(os.environ.get(\"REDASH_DATABASE_URL\", \"postgresql://postgres\"))\n",
                "\n",
                "# Celery related settings\n",
                "CELERY_BROKER = os.environ.get(\"REDASH_CELERY_BROKER\", REDIS_URL)\n",
                "CELERY_BACKEND = os.environ.get(\"REDASH_CELERY_BACKEND\", CELERY_BROKER)\n",
                "\n",
                "# The following enables periodic job (every 5 minutes) of removing unused query results.\n",
                "QUERY_RESULTS_CLEANUP_ENABLED = parse_boolean(os.environ.get(\"REDASH_QUERY_RESULTS_CLEANUP_ENABLED\", \"true\"))\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "QUERY_RESULTS_CLEANUP_COUNT = int(os.environ.get(\"REDASH_QUERY_RESULTS_CLEANUP_COUNT\", \"100\"))\n",
                    "QUERY_RESULTS_CLEANUP_MAX_AGE = int(os.environ.get(\"REDASH_QUERY_RESULTS_CLEANUP_MAX_AGE\", \"7\"))\n"
                ],
                "parent_version_range": {
                    "start": 71,
                    "end": 71
                },
                "child_version_range": {
                    "start": 71,
                    "end": 73
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "call",
                        "name": "parse_boolean",
                        "signature": "parse_boolean(os.environ.get(\"REDASH_QUERY_RESULTS_CLEANUP_ENABLED\", \"true\"))",
                        "at_line": 70,
                        "argument": "os.environ.get(\"REDASH_QUERY_R..."
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: redash/settings.py\nCode:\n68 68    \n69 69    # The following enables periodic job (every 5 minutes) of removing unused query results.\n70 70    QUERY_RESULTS_CLEANUP_ENABLED = parse_boolean(os.environ.get(\"REDASH_QUERY_RESULTS_CLEANUP_ENABLED\", \"true\"))\n   71  + QUERY_RESULTS_CLEANUP_COUNT = int(os.environ.get(\"REDASH_QUERY_RESULTS_CLEANUP_COUNT\", \"100\"))\n   72  + QUERY_RESULTS_CLEANUP_MAX_AGE = int(os.environ.get(\"REDASH_QUERY_RESULTS_CLEANUP_MAX_AGE\", \"7\"))\n71 73    \n72 74    AUTH_TYPE = os.environ.get(\"REDASH_AUTH_TYPE\", \"api_key\")\n73 75    PASSWORD_LOGIN_ENABLED = parse_boolean(os.environ.get(\"REDASH_PASSWORD_LOGIN_ENABLED\", \"true\"))\n       ...\n",
                "file_path": "redash/settings.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "QUERY_RESULTS_CLEANUP_COUNT",
                    "QUERY_RESULTS_CLEANUP_MAX_AGE",
                    "environ",
                    "get",
                    "int",
                    "os"
                ],
                "prefix": [
                    "\n",
                    "# The following enables periodic job (every 5 minutes) of removing unused query results.\n",
                    "QUERY_RESULTS_CLEANUP_ENABLED = parse_boolean(os.environ.get(\"REDASH_QUERY_RESULTS_CLEANUP_ENABLED\", \"true\"))\n"
                ],
                "suffix": [
                    "\n",
                    "AUTH_TYPE = os.environ.get(\"REDASH_AUTH_TYPE\", \"api_key\")\n",
                    "PASSWORD_LOGIN_ENABLED = parse_boolean(os.environ.get(\"REDASH_PASSWORD_LOGIN_ENABLED\", \"true\"))\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "QUERY_RESULTS_CLEANUP_COUNT",
                            "position": {
                                "start": {
                                    "line": 71,
                                    "column": 0
                                },
                                "end": {
                                    "line": 71,
                                    "column": 27
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/settings.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "QUERY_RESULTS_CLEANUP_COUNT",
                            "position": {
                                "start": {
                                    "line": 71,
                                    "column": 0
                                },
                                "end": {
                                    "line": 71,
                                    "column": 27
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/settings.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "QUERY_RESULTS_CLEANUP_MAX_AGE",
                            "position": {
                                "start": {
                                    "line": 72,
                                    "column": 0
                                },
                                "end": {
                                    "line": 72,
                                    "column": 29
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/settings.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "QUERY_RESULTS_CLEANUP_MAX_AGE",
                            "position": {
                                "start": {
                                    "line": 72,
                                    "column": 0
                                },
                                "end": {
                                    "line": 72,
                                    "column": 29
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/settings.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "\n",
                "AUTH_TYPE = os.environ.get(\"REDASH_AUTH_TYPE\", \"api_key\")\n",
                "PASSWORD_LOGIN_ENABLED = parse_boolean(os.environ.get(\"REDASH_PASSWORD_LOGIN_ENABLED\", \"true\"))\n",
                "\n",
                "# Google Apps domain to allow access from; any user with email in this Google Apps will be allowed\n",
                "# access\n",
                "GOOGLE_APPS_DOMAIN = set_from_string(os.environ.get(\"REDASH_GOOGLE_APPS_DOMAIN\", \"\"))\n",
                "\n",
                "GOOGLE_CLIENT_ID = os.environ.get(\"REDASH_GOOGLE_CLIENT_ID\", \"\")\n",
                "GOOGLE_CLIENT_SECRET = os.environ.get(\"REDASH_GOOGLE_CLIENT_SECRET\", \"\")\n",
                "GOOGLE_OAUTH_ENABLED = GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET\n",
                "\n",
                "SAML_METADATA_URL = os.environ.get(\"REDASH_SAML_METADATA_URL\", \"\")\n",
                "SAML_LOGIN_ENABLED = SAML_METADATA_URL != \"\"\n",
                "SAML_CALLBACK_SERVER_NAME = os.environ.get(\"REDASH_SAML_CALLBACK_SERVER_NAME\", \"\")\n",
                "\n",
                "STATIC_ASSETS_PATH = fix_assets_path(os.environ.get(\"REDASH_STATIC_ASSETS_PATH\", \"../rd_ui/app/\"))\n",
                "JOB_EXPIRY_TIME = int(os.environ.get(\"REDASH_JOB_EXPIRY_TIME\", 3600 * 6))\n",
                "COOKIE_SECRET = os.environ.get(\"REDASH_COOKIE_SECRET\", \"c292a0a3aa32397cdb050e233733900f\")\n",
                "LOG_LEVEL = os.environ.get(\"REDASH_LOG_LEVEL\", \"INFO\")\n",
                "ANALYTICS = os.environ.get(\"REDASH_ANALYTICS\", \"\")\n",
                "\n",
                "# Mail settings:\n",
                "MAIL_SERVER = os.environ.get('REDASH_MAIL_SERVER', 'localhost')\n",
                "MAIL_PORT = int(os.environ.get('REDASH_MAIL_PORT', 25))\n",
                "MAIL_USE_TLS = parse_boolean(os.environ.get('REDASH_MAIL_USE_TLS', 'false'))\n",
                "MAIL_USE_SSL = parse_boolean(os.environ.get('REDASH_MAIL_USE_SSL', 'false'))\n",
                "MAIL_USERNAME = os.environ.get('REDASH_MAIL_USERNAME', None)\n",
                "MAIL_PASSWORD = os.environ.get('REDASH_MAIL_PASSWORD', None)\n",
                "MAIL_DEFAULT_SENDER = os.environ.get('REDASH_MAIL_DEFAULT_SENDER', None)\n",
                "MAIL_MAX_EMAILS = os.environ.get('REDASH_MAIL_MAX_EMAILS', None)\n",
                "MAIL_ASCII_ATTACHMENTS = parse_boolean(os.environ.get('REDASH_MAIL_ASCII_ATTACHMENTS', 'false'))\n",
                "\n",
                "HOST = os.environ.get('REDASH_HOST', '')\n",
                "\n",
                "HIPCHAT_API_TOKEN = os.environ.get('REDASH_HIPCHAT_API_TOKEN', None)\n",
                "HIPCHAT_ROOM_ID = os.environ.get('REDASH_HIPCHAT_ROOM_ID', None)\n",
                "\n",
                "WEBHOOK_ENDPOINT = os.environ.get('REDASH_WEBHOOK_ENDPOINT', None)\n",
                "WEBHOOK_USERNAME = os.environ.get('REDASH_WEBHOOK_USERNAME', None)\n",
                "WEBHOOK_PASSWORD = os.environ.get('REDASH_WEBHOOK_PASSWORD', None)\n",
                "\n",
                "# CORS settings for the Query Result API (and possbily future external APIs).\n",
                "# In most cases all you need to do is set REDASH_CORS_ACCESS_CONTROL_ALLOW_ORIGIN\n",
                "# to the calling domain (or domains in a comma separated list).\n",
                "ACCESS_CONTROL_ALLOW_ORIGIN = set_from_string(os.environ.get(\"REDASH_CORS_ACCESS_CONTROL_ALLOW_ORIGIN\", \"\"))\n",
                "ACCESS_CONTROL_ALLOW_CREDENTIALS = parse_boolean(os.environ.get(\"REDASH_CORS_ACCESS_CONTROL_ALLOW_CREDENTIALS\", \"false\"))\n",
                "ACCESS_CONTROL_REQUEST_METHOD = os.environ.get(\"REDASH_CORS_ACCESS_CONTROL_REQUEST_METHOD\", \"GET, POST, PUT\")\n",
                "ACCESS_CONTROL_ALLOW_HEADERS = os.environ.get(\"REDASH_CORS_ACCESS_CONTROL_ALLOW_HEADERS\", \"Content-Type\")\n",
                "\n",
                "# Query Runners\n",
                "default_query_runners = [\n",
                "    'redash.query_runner.big_query',\n",
                "    'redash.query_runner.google_spreadsheets',\n",
                "    'redash.query_runner.graphite',\n",
                "    'redash.query_runner.mongodb',\n",
                "    'redash.query_runner.mql',\n",
                "    'redash.query_runner.mysql',\n",
                "    'redash.query_runner.pg',\n",
                "    'redash.query_runner.url',\n",
                "    'redash.query_runner.influx_db',\n",
                "    'redash.query_runner.elasticsearch',\n",
                "    'redash.query_runner.presto',\n",
                "    'redash.query_runner.hive_ds',\n",
                "    'redash.query_runner.impala_ds',\n",
                "    'redash.query_runner.vertica',\n",
                "    'redash.query_runner.treasuredata',\n",
                "    'redash.query_runner.oracle',\n",
                "    'redash.query_runner.sqlite',\n",
                "]\n",
                "\n",
                "enabled_query_runners = array_from_string(os.environ.get(\"REDASH_ENABLED_QUERY_RUNNERS\", \",\".join(default_query_runners)))\n",
                "additional_query_runners = array_from_string(os.environ.get(\"REDASH_ADDITIONAL_QUERY_RUNNERS\", \"\"))\n",
                "\n",
                "QUERY_RUNNERS = distinct(enabled_query_runners + additional_query_runners)\n",
                "\n",
                "# Support for Sentry (http://getsentry.com/). Just set your Sentry DSN to enable it:\n",
                "SENTRY_DSN = os.environ.get(\"REDASH_SENTRY_DSN\", \"\")\n",
                "\n",
                "# Client side toggles:\n",
                "ALLOW_SCRIPTS_IN_USER_INPUT = parse_boolean(os.environ.get(\"REDASH_ALLOW_SCRIPTS_IN_USER_INPUT\", \"false\"))\n",
                "CLIENT_SIDE_METRICS = parse_boolean(os.environ.get(\"REDASH_CLIENT_SIDE_METRICS\", \"false\"))\n",
                "# http://api.highcharts.com/highcharts#plotOptions.series.turboThreshold\n",
                "HIGHCHARTS_TURBO_THRESHOLD = int(os.environ.get(\"REDASH_HIGHCHARTS_TURBO_THRESHOLD\", \"1000\"))\n",
                "DATE_FORMAT = os.environ.get(\"REDASH_DATE_FORMAT\", \"DD/MM/YY\")\n",
                "\n",
                "# Features:\n",
                "FEATURE_ALLOW_ALL_TO_EDIT_QUERIES = parse_boolean(os.environ.get(\"REDASH_FEATURE_ALLOW_ALL_TO_EDIT\", \"true\"))\n",
                "FEATURE_TABLES_PERMISSIONS = parse_boolean(os.environ.get(\"REDASH_FEATURE_TABLES_PERMISSIONS\", \"false\"))\n",
                "VERSION_CHECK = parse_boolean(os.environ.get(\"REDASH_VERSION_CEHCK\", \"true\"))\n",
                "\n",
                "# BigQuery\n",
                "BIGQUERY_HTTP_TIMEOUT = int(os.environ.get(\"REDASH_BIGQUERY_HTTP_TIMEOUT\", \"600\"))"
            ]
        ],
        "redash/tasks.py": [
            [
                "import time\n",
                "import logging\n",
                "import signal\n",
                "import traceback\n",
                "from flask.ext.mail import Message\n",
                "import redis\n",
                "import hipchat\n",
                "import requests\n",
                "import json\n",
                "from redash.utils import json_dumps\n",
                "from requests.auth import HTTPBasicAuth\n",
                "from celery import Task\n",
                "from celery.result import AsyncResult\n",
                "from celery.utils.log import get_task_logger\n",
                "from redash import redis_connection, models, statsd_client, settings, utils, mail\n",
                "from redash.utils import gen_query_hash\n",
                "from redash.worker import celery\n",
                "from redash.query_runner import get_query_runner, InterruptException\n",
                "from version_check import run_version_check\n",
                "\n",
                "logger = get_task_logger(__name__)\n",
                "\n",
                "\n",
                "class BaseTask(Task):\n",
                "    abstract = True\n",
                "\n",
                "    def after_return(self, *args, **kwargs):\n",
                "        models.db.close_db(None)\n",
                "\n",
                "    def __call__(self, *args, **kwargs):\n",
                "        models.db.connect_db()\n",
                "        return super(BaseTask, self).__call__(*args, **kwargs)\n",
                "\n",
                "\n",
                "class QueryTask(object):\n",
                "    MAX_RETRIES = 5\n",
                "\n",
                "    # TODO: this is mapping to the old Job class statuses. Need to update the client side and remove this\n",
                "    STATUSES = {\n",
                "        'PENDING': 1,\n",
                "        'STARTED': 2,\n",
                "        'SUCCESS': 3,\n",
                "        'FAILURE': 4,\n",
                "        'REVOKED': 4\n",
                "    }\n",
                "\n",
                "    def __init__(self, job_id=None, async_result=None):\n",
                "        if async_result:\n",
                "            self._async_result = async_result\n",
                "        else:\n",
                "            self._async_result = AsyncResult(job_id, app=celery)\n",
                "\n",
                "    @property\n",
                "    def id(self):\n",
                "        return self._async_result.id\n",
                "\n",
                "    @classmethod\n",
                "    def add_task(cls, query, data_source, scheduled=False, metadata={}):\n",
                "        query_hash = gen_query_hash(query)\n",
                "        logging.info(\"[Manager][%s] Inserting job\", query_hash)\n",
                "        logging.info(\"[Manager] Metadata: [%s]\", metadata)\n",
                "        try_count = 0\n",
                "        job = None\n",
                "        \n",
                "        while try_count < cls.MAX_RETRIES:\n",
                "            try_count += 1\n",
                "\n",
                "            pipe = redis_connection.pipeline()\n",
                "            try:\n",
                "                pipe.watch(cls._job_lock_id(query_hash, data_source.id))\n",
                "                job_id = pipe.get(cls._job_lock_id(query_hash, data_source.id))\n",
                "                if job_id:\n",
                "                    logging.info(\"[Manager][%s] Found existing job: %s\", query_hash, job_id)\n",
                "\n",
                "                    job = cls(job_id=job_id)\n",
                "                    if job.ready():\n",
                "                        logging.info(\"[%s] job found is ready (%s), removing lock\", query_hash, job.celery_status)\n",
                "                        redis_connection.delete(QueryTask._job_lock_id(query_hash, data_source.id))\n",
                "                        job = None\n",
                "\n",
                "                if not job:\n",
                "                    pipe.multi()\n",
                "\n",
                "                    if scheduled:\n",
                "                        queue_name = data_source.scheduled_queue_name\n",
                "                    else:\n",
                "                        queue_name = data_source.queue_name\n",
                "\n",
                "                    result = execute_query.apply_async(args=(query, data_source.id, metadata), queue=queue_name)\n",
                "                    job = cls(async_result=result)\n",
                "                    \n",
                "                    logging.info(\"[Manager][%s] Created new job: %s\", query_hash, job.id)\n",
                "                    pipe.set(cls._job_lock_id(query_hash, data_source.id), job.id, settings.JOB_EXPIRY_TIME)\n",
                "                    pipe.execute()\n",
                "                break\n",
                "\n",
                "            except redis.WatchError:\n",
                "                continue\n",
                "\n",
                "        if not job:\n",
                "            logging.error(\"[Manager][%s] Failed adding job for query.\", query_hash)\n",
                "\n",
                "        return job\n",
                "\n",
                "    def to_dict(self):\n",
                "        if self._async_result.status == 'STARTED':\n",
                "            updated_at = self._async_result.result.get('start_time', 0)\n",
                "        else:\n",
                "            updated_at = 0\n",
                "\n",
                "        if self._async_result.failed() and isinstance(self._async_result.result, Exception):\n",
                "            error = self._async_result.result.message\n",
                "        elif self._async_result.status == 'REVOKED':\n",
                "            error = 'Query execution cancelled.'\n",
                "        else:\n",
                "            error = ''\n",
                "\n",
                "        if self._async_result.successful():\n",
                "            query_result_id = self._async_result.result\n",
                "        else:\n",
                "            query_result_id = None\n",
                "\n",
                "        return {\n",
                "            'id': self._async_result.id,\n",
                "            'updated_at': updated_at,\n",
                "            'status': self.STATUSES[self._async_result.status],\n",
                "            'error': error,\n",
                "            'query_result_id': query_result_id,\n",
                "        }\n",
                "\n",
                "    @property\n",
                "    def is_cancelled(self):\n",
                "        return self._async_result.status == 'REVOKED'\n",
                "\n",
                "    @property\n",
                "    def celery_status(self):\n",
                "        return self._async_result.status\n",
                "\n",
                "    def ready(self):\n",
                "        return self._async_result.ready()\n",
                "\n",
                "    def cancel(self):\n",
                "        return self._async_result.revoke(terminate=True, signal='SIGINT')\n",
                "\n",
                "    @staticmethod\n",
                "    def _job_lock_id(query_hash, data_source_id):\n",
                "        return \"query_hash_job:%s:%s\" % (data_source_id, query_hash)\n",
                "\n",
                "\n",
                "@celery.task(base=BaseTask)\n",
                "def refresh_queries():\n",
                "    # self.status['last_refresh_at'] = time.time()\n",
                "    # self._save_status()\n",
                "\n",
                "    logger.info(\"Refreshing queries...\")\n",
                "\n",
                "    outdated_queries_count = 0\n",
                "    for query in models.Query.outdated_queries():\n",
                "        QueryTask.add_task(query.query, query.data_source, scheduled=True,\n",
                "                           metadata={'Query ID': query.id, 'Username': 'Scheduled'})\n",
                "        outdated_queries_count += 1\n",
                "\n",
                "    statsd_client.gauge('manager.outdated_queries', outdated_queries_count)\n",
                "\n",
                "    logger.info(\"Done refreshing queries. Found %d outdated queries.\" % outdated_queries_count)\n",
                "\n",
                "    status = redis_connection.hgetall('redash:status')\n",
                "    now = time.time()\n",
                "\n",
                "    redis_connection.hmset('redash:status', {\n",
                "        'outdated_queries_count': outdated_queries_count,\n",
                "        'last_refresh_at': now\n",
                "    })\n",
                "\n",
                "    statsd_client.gauge('manager.seconds_since_refresh', now - float(status.get('last_refresh_at', now)))\n",
                "\n",
                "\n",
                "@celery.task(base=BaseTask)\n",
                "def cleanup_tasks():\n",
                "    # in case of cold restart of the workers, there might be jobs that still have their \"lock\" object, but aren't really\n",
                "    # going to run. this job removes them.\n",
                "    lock_keys = redis_connection.keys(\"query_hash_job:*\") # TODO: use set instead of keys command\n",
                "    if not lock_keys:\n",
                "        return\n",
                "    \n",
                "    query_tasks = [QueryTask(job_id=j) for j in redis_connection.mget(lock_keys)]\n",
                "\n",
                "    logger.info(\"Found %d locks\", len(query_tasks))\n",
                "\n",
                "    inspect = celery.control.inspect()\n",
                "    active_tasks = inspect.active()\n",
                "    if active_tasks is None:\n",
                "        active_tasks = []\n",
                "    else:\n",
                "        active_tasks = active_tasks.values()\n",
                "\n",
                "    all_tasks = set()\n",
                "    for task_list in active_tasks:\n",
                "        for task in task_list:\n",
                "            all_tasks.add(task['id'])\n",
                "\n",
                "    logger.info(\"Active jobs count: %d\", len(all_tasks))\n",
                "\n",
                "    for i, t in enumerate(query_tasks):\n",
                "        if t.ready():\n",
                "            # if locked task is ready already (failed, finished, revoked), we don't need the lock anymore\n",
                "            logger.warning(\"%s is ready (%s), removing lock.\", lock_keys[i], t.celery_status)\n",
                "            redis_connection.delete(lock_keys[i])\n",
                "\n",
                "        # if t.celery_status == 'STARTED' and t.id not in all_tasks:\n",
                "        #     logger.warning(\"Couldn't find active job for: %s, removing lock.\", lock_keys[i])\n",
                "        #     redis_connection.delete(lock_keys[i])\n",
                "\n",
                "\n",
                "@celery.task(base=BaseTask)\n",
                "def cleanup_query_results():\n",
                "    \"\"\"\n",
                "    Job to cleanup unused query results -- such that no query links to them anymore, and older than a week (so it's less\n",
                "    likely to be open in someone's browser and be used).\n",
                "\n",
                "    Each time the job deletes only 100 query results so it won't choke the database in case of many such results.\n",
                "    \"\"\"\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    unused_query_results = models.QueryResult.unused().limit(100)\n"
                ],
                "after": [
                    "    logging.info(\"Running query results clean up (removing maximum of %d unused results, that are %d days old or more)\",\n",
                    "                 settings.QUERY_RESULTS_CLEANUP_COUNT, settings.QUERY_RESULTS_CLEANUP_MAX_AGE)\n",
                    "\n",
                    "    unused_query_results = models.QueryResult.unused(settings.QUERY_RESULTS_CLEANUP_MAX_AGE).limit(settings.QUERY_RESULTS_CLEANUP_COUNT)\n"
                ],
                "parent_version_range": {
                    "start": 223,
                    "end": 224
                },
                "child_version_range": {
                    "start": 223,
                    "end": 227
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "cleanup_query_results",
                        "signature": "def cleanup_query_results():",
                        "at_line": 215
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: redash/tasks.py\nCode:\n           def cleanup_query_results():\n               ...\n220 220        Each time the job deletes only 100 query results so it won't choke the database in case of many such results.\n221 221        \"\"\"\n222 222    \n223      -     unused_query_results = models.QueryResult.unused().limit(100)\n    223  +     logging.info(\"Running query results clean up (removing maximum of %d unused results, that are %d days old or more)\",\n    224  +                  settings.QUERY_RESULTS_CLEANUP_COUNT, settings.QUERY_RESULTS_CLEANUP_MAX_AGE)\n    225  + \n    226  +     unused_query_results = models.QueryResult.unused(settings.QUERY_RESULTS_CLEANUP_MAX_AGE).limit(settings.QUERY_RESULTS_CLEANUP_COUNT)\n224 227        total_unused_query_results = models.QueryResult.unused().count()\n225 228        deleted_count = models.QueryResult.delete().where(models.QueryResult.id << unused_query_results).execute()\n226 229    \n         ...\n",
                "file_path": "redash/tasks.py",
                "identifiers_before": [
                    "QueryResult",
                    "limit",
                    "models",
                    "unused",
                    "unused_query_results"
                ],
                "identifiers_after": [
                    "QUERY_RESULTS_CLEANUP_COUNT",
                    "QUERY_RESULTS_CLEANUP_MAX_AGE",
                    "QueryResult",
                    "info",
                    "limit",
                    "logging",
                    "models",
                    "settings",
                    "unused",
                    "unused_query_results"
                ],
                "prefix": [
                    "    Each time the job deletes only 100 query results so it won't choke the database in case of many such results.\n",
                    "    \"\"\"\n",
                    "\n"
                ],
                "suffix": [
                    "    total_unused_query_results = models.QueryResult.unused().count()\n",
                    "    deleted_count = models.QueryResult.delete().where(models.QueryResult.id << unused_query_results).execute()\n",
                    "\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "unused",
                            "position": {
                                "start": {
                                    "line": 223,
                                    "column": 46
                                },
                                "end": {
                                    "line": 223,
                                    "column": 52
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/tasks.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "unused",
                            "position": {
                                "start": {
                                    "line": 226,
                                    "column": 46
                                },
                                "end": {
                                    "line": 226,
                                    "column": 52
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/tasks.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "QUERY_RESULTS_CLEANUP_COUNT",
                            "position": {
                                "start": {
                                    "line": 224,
                                    "column": 26
                                },
                                "end": {
                                    "line": 224,
                                    "column": 53
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/tasks.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "QUERY_RESULTS_CLEANUP_COUNT",
                            "position": {
                                "start": {
                                    "line": 226,
                                    "column": 108
                                },
                                "end": {
                                    "line": 226,
                                    "column": 135
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/tasks.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "QUERY_RESULTS_CLEANUP_MAX_AGE",
                            "position": {
                                "start": {
                                    "line": 224,
                                    "column": 64
                                },
                                "end": {
                                    "line": 224,
                                    "column": 93
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/tasks.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "QUERY_RESULTS_CLEANUP_MAX_AGE",
                            "position": {
                                "start": {
                                    "line": 226,
                                    "column": 62
                                },
                                "end": {
                                    "line": 226,
                                    "column": 91
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/redash/redash/tasks.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    total_unused_query_results = models.QueryResult.unused().count()\n",
                "    deleted_count = models.QueryResult.delete().where(models.QueryResult.id << unused_query_results).execute()\n",
                "\n",
                "    logger.info(\"Deleted %d unused query results out of total of %d.\" % (deleted_count, total_unused_query_results))\n",
                "\n",
                "\n",
                "@celery.task(base=BaseTask)\n",
                "def refresh_schemas():\n",
                "    \"\"\"\n",
                "    Refreshs the datasources schema.\n",
                "    \"\"\"\n",
                "\n",
                "    for ds in models.DataSource.all():\n",
                "        logger.info(\"Refreshing schema for: {}\".format(ds.name))\n",
                "        ds.get_schema(refresh=True)\n",
                "\n",
                "\n",
                "@celery.task(bind=True, base=BaseTask)\n",
                "def check_alerts_for_query(self, query_id):\n",
                "    from redash.wsgi import app\n",
                "\n",
                "    logger.debug(\"Checking query %d for alerts\", query_id)\n",
                "    query = models.Query.get_by_id(query_id)\n",
                "    for alert in query.alerts:\n",
                "        alert.query = query\n",
                "        new_state = alert.evaluate()\n",
                "        if new_state != alert.state:\n",
                "            logger.info(\"Alert %d new state: %s\", alert.id, new_state)\n",
                "            old_state = alert.state\n",
                "            alert.update_instance(state=new_state)\n",
                "\n",
                "            if old_state == models.Alert.UNKNOWN_STATE and new_state == models.Alert.OK_STATE:\n",
                "                logger.debug(\"Skipping notification (previous state was unknown and now it's ok).\")\n",
                "                continue\n",
                "\n",
                "            # message = Message\n",
                "            html = \"\"\"\n",
                "            Check <a href=\"{host}/alerts/{alert_id}\">alert</a> / check <a href=\"{host}/queries/{query_id}\">query</a>.\n",
                "            \"\"\".format(host=settings.HOST, alert_id=alert.id, query_id=query.id)\n",
                "\n",
                "            notify_mail(alert, html, new_state, app)\n",
                "\n",
                "            if settings.HIPCHAT_API_TOKEN:\n",
                "                notify_hipchat(alert, html, new_state)\n",
                "\n",
                "            if settings.WEBHOOK_ENDPOINT:\n",
                "                notify_webhook(alert, query, html, new_state)\n",
                "\n",
                "def signal_handler(*args):\n",
                "    raise InterruptException\n",
                "\n",
                "@celery.task(bind=True, base=BaseTask, track_started=True)\n",
                "def execute_query(self, query, data_source_id, metadata):\n",
                "    signal.signal(signal.SIGINT, signal_handler)\n",
                "    start_time = time.time()\n",
                "\n",
                "    logger.info(\"Loading data source (%d)...\", data_source_id)\n",
                "\n",
                "    # TODO: we should probably cache data sources in Redis\n",
                "    data_source = models.DataSource.get_by_id(data_source_id)\n",
                "\n",
                "    self.update_state(state='STARTED', meta={'start_time': start_time, 'custom_message': ''})\n",
                "\n",
                "    logger.info(\"Executing query:\\n%s\", query)\n",
                "\n",
                "    query_hash = gen_query_hash(query)\n",
                "    query_runner = get_query_runner(data_source.type, data_source.options)\n",
                "\n",
                "    if query_runner.annotate_query():\n",
                "        metadata['Task ID'] = self.request.id\n",
                "        metadata['Query Hash'] = query_hash\n",
                "        metadata['Queue'] = self.request.delivery_info['routing_key']\n",
                "\n",
                "        annotation = u\", \".join([u\"{}: {}\".format(k, v) for k, v in metadata.iteritems()])\n",
                "\n",
                "        logging.debug(u\"Annotation: %s\", annotation)\n",
                "\n",
                "        annotated_query = u\"/* {} */ {}\".format(annotation, query)\n",
                "    else:\n",
                "        annotated_query = query\n",
                "\n",
                "    with statsd_client.timer('query_runner.{}.{}.run_time'.format(data_source.type, data_source.name)):\n",
                "        data, error = query_runner.run_query(annotated_query)\n",
                "\n",
                "    run_time = time.time() - start_time\n",
                "    logger.info(\"Query finished... data length=%s, error=%s\", data and len(data), error)\n",
                "\n",
                "    self.update_state(state='STARTED', meta={'start_time': start_time, 'error': error, 'custom_message': ''})\n",
                "\n",
                "    # Delete query_hash\n",
                "    redis_connection.delete(QueryTask._job_lock_id(query_hash, data_source.id))\n",
                "\n",
                "    if not error:\n",
                "        query_result, updated_query_ids = models.QueryResult.store_result(data_source.id, query_hash, query, data, run_time, utils.utcnow())\n",
                "        for query_id in updated_query_ids:\n",
                "            check_alerts_for_query.delay(query_id)\n",
                "    else:\n",
                "        raise Exception(error)\n",
                "\n",
                "    return query_result.id\n",
                "\n",
                "\n",
                "@celery.task(base=BaseTask)\n",
                "def record_event(event):\n",
                "    models.Event.record(event)\n",
                "\n",
                "@celery.task(base=BaseTask)\n",
                "def version_check():\n",
                "    run_version_check()\n",
                "\n",
                "def notify_hipchat(alert, html, new_state):\n",
                "    try:\n",
                "        hipchat_client = hipchat.HipChat(token=settings.HIPCHAT_API_TOKEN)\n",
                "        message = '[' + new_state.upper() + '] ' + alert.name + '<br />' + html\n",
                "        hipchat_client.message_room(settings.HIPCHAT_ROOM_ID, settings.NAME, message, message_format='html')\n",
                "    except:\n",
                "        logger.exception(\"hipchat send ERROR.\")\n",
                "\n",
                "def notify_mail(alert, html, new_state, app):\n",
                "    recipients = [s.email for s in alert.subscribers()]\n",
                "    logger.debug(\"Notifying: %s\", recipients)\n",
                "    try:\n",
                "        with app.app_context():\n",
                "            message = Message(recipients=recipients,\n",
                "                              subject=\"[{1}] {0}\".format(alert.name, new_state.upper()),\n",
                "                              html=html)\n",
                "            mail.send(message)\n",
                "    except:\n",
                "        logger.exception(\"mail send ERROR.\")\n",
                "\n",
                "def notify_webhook(alert, query, html, new_state):\n",
                "    try:\n",
                "        data = {\n",
                "            'event': 'alert_state_change',\n",
                "            'alert': alert.to_dict(full=False),\n",
                "            'url_base': settings.HOST\n",
                "        }\n",
                "        headers = {'Content-Type': 'application/json'}\n",
                "        auth = HTTPBasicAuth(settings.WEBHOOK_USERNAME, settings.WEBHOOK_PASSWORD) if settings.WEBHOOK_USERNAME else None\n",
                "        resp = requests.post(settings.WEBHOOK_ENDPOINT, data=json_dumps(data), auth=auth, headers=headers)\n",
                "        if resp.status_code != 200:\n",
                "            logger.error(\"webhook send ERROR. status_code => {status}\".format(status=resp.status_code))\n",
                "    except:\n",
                "        logger.exception(\"webhook send ERROR.\")"
            ]
        ]
    },
    "edit_order": [
        [
            0,
            1,
            2,
            3
        ],
        [
            2,
            3,
            0,
            1
        ]
    ],
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "bi-directional",
            "reason": "variable dependency",
            "scenario of 0 -> 1": "user may rename edit 0 first, then edit 1",
            "scenario of 1 -> 0": "user may rename edit 1 first, then edit 0"
        },
        {
            "edit_hunk_pair": [
                0,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "def use",
            "scenario of 0 -> 1": "def before use",
            "scenario of 1 -> 0": "use before def"
        },
        {
            "edit_hunk_pair": [
                2,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "variable dependency",
            "scenario of 0 -> 1": "user may first define the global variable in edit 0 and then use it in edit 1",
            "scenario of 1 -> 0": "user may first use the undefined global variable in edit 1 and then define it in edit 0"
        }
    ]
}