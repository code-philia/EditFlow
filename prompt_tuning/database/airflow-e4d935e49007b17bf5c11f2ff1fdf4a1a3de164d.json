{
    "language": "python",
    "commit_url": "https://github.com/apache/airflow/commit/e4d935e49007b17bf5c11f2ff1fdf4a1a3de164d",
    "commit_message": "Add tags param in RedshiftCreateClusterSnapshotOperator (#31006)\n\nAdd tags param in RedshiftCreateClusterSnapshotOperator.\nThis will help to group cluster snapshot based on tags.",
    "commit_snapshots": {
        "airflow/providers/amazon/aws/hooks/redshift_cluster.py": [
            [
                "# Licensed to the Apache Software Foundation (ASF) under one\n",
                "# or more contributor license agreements.  See the NOTICE file\n",
                "# distributed with this work for additional information\n",
                "# regarding copyright ownership.  The ASF licenses this file\n",
                "# to you under the Apache License, Version 2.0 (the\n",
                "# \"License\"); you may not use this file except in compliance\n",
                "# with the License.  You may obtain a copy of the License at\n",
                "#\n",
                "#   http://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing,\n",
                "# software distributed under the License is distributed on an\n",
                "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
                "# KIND, either express or implied.  See the License for the\n",
                "# specific language governing permissions and limitations\n",
                "# under the License.\n",
                "from __future__ import annotations\n",
                "\n",
                "import asyncio\n",
                "from typing import Any, Sequence\n",
                "\n",
                "import botocore.exceptions\n",
                "from botocore.exceptions import ClientError\n",
                "\n",
                "from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseAsyncHook, AwsBaseHook\n",
                "\n",
                "\n",
                "class RedshiftHook(AwsBaseHook):\n",
                "    \"\"\"\n",
                "    Interact with Amazon Redshift.\n",
                "    Provide thin wrapper around :external+boto3:py:class:`boto3.client(\"redshift\") <Redshift.Client>`.\n",
                "\n",
                "    Additional arguments (such as ``aws_conn_id``) may be specified and\n",
                "    are passed down to the underlying AwsBaseHook.\n",
                "\n",
                "    .. seealso::\n",
                "        - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n",
                "    \"\"\"\n",
                "\n",
                "    template_fields: Sequence[str] = (\"cluster_identifier\",)\n",
                "\n",
                "    def __init__(self, *args, **kwargs) -> None:\n",
                "        kwargs[\"client_type\"] = \"redshift\"\n",
                "        super().__init__(*args, **kwargs)\n",
                "\n",
                "    def create_cluster(\n",
                "        self,\n",
                "        cluster_identifier: str,\n",
                "        node_type: str,\n",
                "        master_username: str,\n",
                "        master_user_password: str,\n",
                "        params: dict[str, Any],\n",
                "    ) -> dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Creates a new cluster with the specified parameters\n",
                "\n",
                "        .. seealso::\n",
                "            - :external+boto3:py:meth:`Redshift.Client.create_cluster`\n",
                "\n",
                "        :param cluster_identifier: A unique identifier for the cluster.\n",
                "        :param node_type: The node type to be provisioned for the cluster.\n",
                "            Valid Values: ``ds2.xlarge``, ``ds2.8xlarge``, ``dc1.large``,\n",
                "            ``dc1.8xlarge``, ``dc2.large``, ``dc2.8xlarge``, ``ra3.xlplus``,\n",
                "            ``ra3.4xlarge``, and ``ra3.16xlarge``.\n",
                "        :param master_username: The username associated with the admin user account\n",
                "            for the cluster that is being created.\n",
                "        :param master_user_password: password associated with the admin user account\n",
                "            for the cluster that is being created.\n",
                "        :param params: Remaining AWS Create cluster API params.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.get_conn().create_cluster(\n",
                "                ClusterIdentifier=cluster_identifier,\n",
                "                NodeType=node_type,\n",
                "                MasterUsername=master_username,\n",
                "                MasterUserPassword=master_user_password,\n",
                "                **params,\n",
                "            )\n",
                "            return response\n",
                "        except ClientError as e:\n",
                "            raise e\n",
                "\n",
                "    # TODO: Wrap create_cluster_snapshot\n",
                "    def cluster_status(self, cluster_identifier: str) -> str:\n",
                "        \"\"\"\n",
                "        Return status of a cluster\n",
                "\n",
                "        .. seealso::\n",
                "            - :external+boto3:py:meth:`Redshift.Client.describe_clusters`\n",
                "\n",
                "        :param cluster_identifier: unique identifier of a cluster\n",
                "        :param skip_final_cluster_snapshot: determines cluster snapshot creation\n",
                "        :param final_cluster_snapshot_identifier: Optional[str]\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.get_conn().describe_clusters(ClusterIdentifier=cluster_identifier)[\"Clusters\"]\n",
                "            return response[0][\"ClusterStatus\"] if response else None\n",
                "        except self.get_conn().exceptions.ClusterNotFoundFault:\n",
                "            return \"cluster_not_found\"\n",
                "\n",
                "    def delete_cluster(\n",
                "        self,\n",
                "        cluster_identifier: str,\n",
                "        skip_final_cluster_snapshot: bool = True,\n",
                "        final_cluster_snapshot_identifier: str | None = None,\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Delete a cluster and optionally create a snapshot\n",
                "\n",
                "        .. seealso::\n",
                "            - :external+boto3:py:meth:`Redshift.Client.delete_cluster`\n",
                "\n",
                "        :param cluster_identifier: unique identifier of a cluster\n",
                "        :param skip_final_cluster_snapshot: determines cluster snapshot creation\n",
                "        :param final_cluster_snapshot_identifier: name of final cluster snapshot\n",
                "        \"\"\"\n",
                "        final_cluster_snapshot_identifier = final_cluster_snapshot_identifier or \"\"\n",
                "\n",
                "        response = self.get_conn().delete_cluster(\n",
                "            ClusterIdentifier=cluster_identifier,\n",
                "            SkipFinalClusterSnapshot=skip_final_cluster_snapshot,\n",
                "            FinalClusterSnapshotIdentifier=final_cluster_snapshot_identifier,\n",
                "        )\n",
                "        return response[\"Cluster\"] if response[\"Cluster\"] else None\n",
                "\n",
                "    def describe_cluster_snapshots(self, cluster_identifier: str) -> list[str] | None:\n",
                "        \"\"\"\n",
                "        Gets a list of snapshots for a cluster\n",
                "\n",
                "        .. seealso::\n",
                "            - :external+boto3:py:meth:`Redshift.Client.describe_cluster_snapshots`\n",
                "\n",
                "        :param cluster_identifier: unique identifier of a cluster\n",
                "        \"\"\"\n",
                "        response = self.get_conn().describe_cluster_snapshots(ClusterIdentifier=cluster_identifier)\n",
                "        if \"Snapshots\" not in response:\n",
                "            return None\n",
                "        snapshots = response[\"Snapshots\"]\n",
                "        snapshots = [snapshot for snapshot in snapshots if snapshot[\"Status\"]]\n",
                "        snapshots.sort(key=lambda x: x[\"SnapshotCreateTime\"], reverse=True)\n",
                "        return snapshots\n",
                "\n",
                "    def restore_from_cluster_snapshot(self, cluster_identifier: str, snapshot_identifier: str) -> str:\n",
                "        \"\"\"\n",
                "        Restores a cluster from its snapshot\n",
                "\n",
                "        .. seealso::\n",
                "            - :external+boto3:py:meth:`Redshift.Client.restore_from_cluster_snapshot`\n",
                "\n",
                "        :param cluster_identifier: unique identifier of a cluster\n",
                "        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n",
                "        \"\"\"\n",
                "        response = self.get_conn().restore_from_cluster_snapshot(\n",
                "            ClusterIdentifier=cluster_identifier, SnapshotIdentifier=snapshot_identifier\n",
                "        )\n",
                "        return response[\"Cluster\"] if response[\"Cluster\"] else None\n",
                "\n",
                "    def create_cluster_snapshot(\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        self, snapshot_identifier: str, cluster_identifier: str, retention_period: int = -1\n"
                ],
                "after": [
                    "        self,\n",
                    "        snapshot_identifier: str,\n",
                    "        cluster_identifier: str,\n",
                    "        retention_period: int = -1,\n",
                    "        tags: list[Any] | None = None,\n"
                ],
                "parent_version_range": {
                    "start": 158,
                    "end": 159
                },
                "child_version_range": {
                    "start": 158,
                    "end": 163
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RedshiftHook",
                        "signature": "class RedshiftHook(AwsBaseHook):",
                        "at_line": 27
                    },
                    {
                        "type": "function",
                        "name": "create_cluster_snapshot",
                        "signature": "def create_cluster_snapshot(\n        self, snapshot_identifier: str, cluster_identifier: str, retention_period: int = -1\n    )->str:",
                        "at_line": 157
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: airflow/providers/amazon/aws/hooks/redshift_cluster.py\nCode:\n           class RedshiftHook(AwsBaseHook):\n               ...\n155 155            return response[\"Cluster\"] if response[\"Cluster\"] else None\n156 156    \n157 157        def create_cluster_snapshot(\n158      -         self, snapshot_identifier: str, cluster_identifier: str, retention_period: int = -1\n    158  +         self,\n    159  +         snapshot_identifier: str,\n    160  +         cluster_identifier: str,\n    161  +         retention_period: int = -1,\n    162  +         tags: list[Any] | None = None,\n159 163        ) -> str:\n160 164            \"\"\"\n161 165            Creates a snapshot of a cluster\n         ...\n",
                "file_path": "airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                "identifiers_before": [
                    "cluster_identifier",
                    "int",
                    "retention_period",
                    "self",
                    "snapshot_identifier",
                    "str"
                ],
                "identifiers_after": [
                    "Any",
                    "cluster_identifier",
                    "int",
                    "list",
                    "retention_period",
                    "self",
                    "snapshot_identifier",
                    "str",
                    "tags"
                ],
                "prefix": [
                    "        return response[\"Cluster\"] if response[\"Cluster\"] else None\n",
                    "\n",
                    "    def create_cluster_snapshot(\n"
                ],
                "suffix": [
                    "    ) -> str:\n",
                    "        \"\"\"\n",
                    "        Creates a snapshot of a cluster\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 162,
                                    "column": 8
                                },
                                "end": {
                                    "line": 162,
                                    "column": 12
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 162,
                                    "column": 8
                                },
                                "end": {
                                    "line": 162,
                                    "column": 12
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 162,
                                    "column": 8
                                },
                                "end": {
                                    "line": 162,
                                    "column": 12
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 162,
                                    "column": 8
                                },
                                "end": {
                                    "line": 162,
                                    "column": 12
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "    ) -> str:\n",
                "        \"\"\"\n",
                "        Creates a snapshot of a cluster\n",
                "\n",
                "        .. seealso::\n",
                "            - :external+boto3:py:meth:`Redshift.Client.create_cluster_snapshot`\n",
                "\n",
                "        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n",
                "        :param cluster_identifier: unique identifier of a cluster\n",
                "        :param retention_period: The number of days that a manual snapshot is retained.\n",
                "            If the value is -1, the manual snapshot is retained indefinitely.\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        :param tags: A list of tag instances\n"
                ],
                "parent_version_range": {
                    "start": 170,
                    "end": 170
                },
                "child_version_range": {
                    "start": 174,
                    "end": 175
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RedshiftHook",
                        "signature": "class RedshiftHook(AwsBaseHook):",
                        "at_line": 27
                    },
                    {
                        "type": "function",
                        "name": "create_cluster_snapshot",
                        "signature": "def create_cluster_snapshot(\n        self, snapshot_identifier: str, cluster_identifier: str, retention_period: int = -1\n    )->str:",
                        "at_line": 157
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: airflow/providers/amazon/aws/hooks/redshift_cluster.py\nCode:\n           class RedshiftHook(AwsBaseHook):\n               ...\n               def create_cluster_snapshot(\n        self, snapshot_identifier: str, cluster_identifier: str, retention_period: int = -1\n    )->str:\n                   ...\n167 171            :param cluster_identifier: unique identifier of a cluster\n168 172            :param retention_period: The number of days that a manual snapshot is retained.\n169 173                If the value is -1, the manual snapshot is retained indefinitely.\n    174  +         :param tags: A list of tag instances\n170 175            \"\"\"\n         ...\n",
                "file_path": "airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "A",
                    "instances",
                    "list",
                    "of",
                    "param",
                    "tag",
                    "tags"
                ],
                "prefix": [
                    "        :param cluster_identifier: unique identifier of a cluster\n",
                    "        :param retention_period: The number of days that a manual snapshot is retained.\n",
                    "            If the value is -1, the manual snapshot is retained indefinitely.\n"
                ],
                "suffix": [
                    "        \"\"\"\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    4
                ]
            },
            [
                "        \"\"\"\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        if tags is None:\n",
                    "            tags = []\n"
                ],
                "parent_version_range": {
                    "start": 171,
                    "end": 171
                },
                "child_version_range": {
                    "start": 176,
                    "end": 178
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RedshiftHook",
                        "signature": "class RedshiftHook(AwsBaseHook):",
                        "at_line": 27
                    },
                    {
                        "type": "function",
                        "name": "create_cluster_snapshot",
                        "signature": "def create_cluster_snapshot(\n        self, snapshot_identifier: str, cluster_identifier: str, retention_period: int = -1\n    )->str:",
                        "at_line": 157
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: airflow/providers/amazon/aws/hooks/redshift_cluster.py\nCode:\n           class RedshiftHook(AwsBaseHook):\n               ...\n               def create_cluster_snapshot(\n        self, snapshot_identifier: str, cluster_identifier: str, retention_period: int = -1\n    )->str:\n                   ...\n170 175            \"\"\"\n    176  +         if tags is None:\n    177  +             tags = []\n171 178            response = self.get_conn().create_cluster_snapshot(\n172 179                SnapshotIdentifier=snapshot_identifier,\n173 180                ClusterIdentifier=cluster_identifier,\n         ...\n",
                "file_path": "airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "tags"
                ],
                "prefix": [
                    "        \"\"\"\n"
                ],
                "suffix": [
                    "        response = self.get_conn().create_cluster_snapshot(\n",
                    "            SnapshotIdentifier=snapshot_identifier,\n",
                    "            ClusterIdentifier=cluster_identifier,\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 176,
                                    "column": 11
                                },
                                "end": {
                                    "line": 176,
                                    "column": 15
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 177,
                                    "column": 12
                                },
                                "end": {
                                    "line": 177,
                                    "column": 16
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        response = self.get_conn().create_cluster_snapshot(\n",
                "            SnapshotIdentifier=snapshot_identifier,\n",
                "            ClusterIdentifier=cluster_identifier,\n",
                "            ManualSnapshotRetentionPeriod=retention_period,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "            Tags=tags,\n"
                ],
                "parent_version_range": {
                    "start": 175,
                    "end": 175
                },
                "child_version_range": {
                    "start": 182,
                    "end": 183
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RedshiftHook",
                        "signature": "class RedshiftHook(AwsBaseHook):",
                        "at_line": 27
                    },
                    {
                        "type": "function",
                        "name": "create_cluster_snapshot",
                        "signature": "def create_cluster_snapshot(\n        self, snapshot_identifier: str, cluster_identifier: str, retention_period: int = -1\n    )->str:",
                        "at_line": 157
                    },
                    {
                        "type": "call",
                        "name": "self.get_conn().create_cluster_snapshot",
                        "signature": "self.get_conn().create_cluster_snapshot(\n            SnapshotIdentifier=snapshot_identifier,\n            ClusterIdentifier=cluster_identifier,\n            ManualSnapshotRetentionPeriod=retention_period,\n        )",
                        "at_line": 171,
                        "argument": "ManualSnapshotRetentionPeriod=..."
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: airflow/providers/amazon/aws/hooks/redshift_cluster.py\nCode:\n           class RedshiftHook(AwsBaseHook):\n               ...\n               def create_cluster_snapshot(\n        self, snapshot_identifier: str, cluster_identifier: str, retention_period: int = -1\n    )->str:\n                   ...\n                   self.get_conn().create_cluster_snapshot(\n            SnapshotIdentifier=snapshot_identifier,\n            ClusterIdentifier=cluster_identifier,\n            ManualSnapshotRetentionPeriod=retention_period,\n        )\n                       ...\n172 179                SnapshotIdentifier=snapshot_identifier,\n173 180                ClusterIdentifier=cluster_identifier,\n174 181                ManualSnapshotRetentionPeriod=retention_period,\n    182  +             Tags=tags,\n175 183            )\n176 184            return response[\"Snapshot\"] if response[\"Snapshot\"] else None\n177 185    \n         ...\n",
                "file_path": "airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "Tags",
                    "tags"
                ],
                "prefix": [
                    "            SnapshotIdentifier=snapshot_identifier,\n",
                    "            ClusterIdentifier=cluster_identifier,\n",
                    "            ManualSnapshotRetentionPeriod=retention_period,\n"
                ],
                "suffix": [
                    "        )\n",
                    "        return response[\"Snapshot\"] if response[\"Snapshot\"] else None\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 182,
                                    "column": 17
                                },
                                "end": {
                                    "line": 182,
                                    "column": 21
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/hooks/redshift_cluster.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        )\n",
                "        return response[\"Snapshot\"] if response[\"Snapshot\"] else None\n",
                "\n",
                "    def get_cluster_snapshot_status(self, snapshot_identifier: str):\n",
                "        \"\"\"\n",
                "        Return Redshift cluster snapshot status. If cluster snapshot not found return ``None``\n",
                "\n",
                "        :param snapshot_identifier: A unique identifier for the snapshot that you are requesting\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = self.get_conn().describe_cluster_snapshots(\n",
                "                SnapshotIdentifier=snapshot_identifier,\n",
                "            )\n",
                "            snapshot = response.get(\"Snapshots\")[0]\n",
                "            snapshot_status: str = snapshot.get(\"Status\")\n",
                "            return snapshot_status\n",
                "        except self.get_conn().exceptions.ClusterSnapshotNotFoundFault:\n",
                "            return None\n",
                "\n",
                "\n",
                "class RedshiftAsyncHook(AwsBaseAsyncHook):\n",
                "    \"\"\"Interact with AWS Redshift using aiobotocore library\"\"\"\n",
                "\n",
                "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
                "        kwargs[\"client_type\"] = \"redshift\"\n",
                "        super().__init__(*args, **kwargs)\n",
                "\n",
                "    async def cluster_status(self, cluster_identifier: str, delete_operation: bool = False) -> dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Connects to the AWS redshift cluster via aiobotocore and get the status\n",
                "        and returns the status of the cluster based on the cluster_identifier passed\n",
                "\n",
                "        :param cluster_identifier: unique identifier of a cluster\n",
                "        :param delete_operation: whether the method has been called as part of delete cluster operation\n",
                "        \"\"\"\n",
                "        async with await self.get_client_async() as client:\n",
                "            try:\n",
                "                response = await client.describe_clusters(ClusterIdentifier=cluster_identifier)\n",
                "                cluster_state = (\n",
                "                    response[\"Clusters\"][0][\"ClusterStatus\"] if response and response[\"Clusters\"] else None\n",
                "                )\n",
                "                return {\"status\": \"success\", \"cluster_state\": cluster_state}\n",
                "            except botocore.exceptions.ClientError as error:\n",
                "                if delete_operation and error.response.get(\"Error\", {}).get(\"Code\", \"\") == \"ClusterNotFound\":\n",
                "                    return {\"status\": \"success\", \"cluster_state\": \"cluster_not_found\"}\n",
                "                return {\"status\": \"error\", \"message\": str(error)}\n",
                "\n",
                "    async def pause_cluster(self, cluster_identifier: str, poll_interval: float = 5.0) -> dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Connects to the AWS redshift cluster via aiobotocore and\n",
                "        pause the cluster based on the cluster_identifier passed\n",
                "\n",
                "        :param cluster_identifier: unique identifier of a cluster\n",
                "        :param poll_interval: polling period in seconds to check for the status\n",
                "        \"\"\"\n",
                "        try:\n",
                "            async with await self.get_client_async() as client:\n",
                "                response = await client.pause_cluster(ClusterIdentifier=cluster_identifier)\n",
                "                status = response[\"Cluster\"][\"ClusterStatus\"] if response and response[\"Cluster\"] else None\n",
                "                if status == \"pausing\":\n",
                "                    flag = asyncio.Event()\n",
                "                    while True:\n",
                "                        expected_response = await asyncio.create_task(\n",
                "                            self.get_cluster_status(cluster_identifier, \"paused\", flag)\n",
                "                        )\n",
                "                        await asyncio.sleep(poll_interval)\n",
                "                        if flag.is_set():\n",
                "                            return expected_response\n",
                "                return {\"status\": \"error\", \"cluster_state\": status}\n",
                "        except botocore.exceptions.ClientError as error:\n",
                "            return {\"status\": \"error\", \"message\": str(error)}\n",
                "\n",
                "    async def resume_cluster(\n",
                "        self,\n",
                "        cluster_identifier: str,\n",
                "        polling_period_seconds: float = 5.0,\n",
                "    ) -> dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Connects to the AWS redshift cluster via aiobotocore and\n",
                "        resume the cluster for the cluster_identifier passed\n",
                "\n",
                "        :param cluster_identifier: unique identifier of a cluster\n",
                "        :param polling_period_seconds: polling period in seconds to check for the status\n",
                "        \"\"\"\n",
                "        async with await self.get_client_async() as client:\n",
                "            try:\n",
                "                response = await client.resume_cluster(ClusterIdentifier=cluster_identifier)\n",
                "                status = response[\"Cluster\"][\"ClusterStatus\"] if response and response[\"Cluster\"] else None\n",
                "                if status == \"resuming\":\n",
                "                    flag = asyncio.Event()\n",
                "                    while True:\n",
                "                        expected_response = await asyncio.create_task(\n",
                "                            self.get_cluster_status(cluster_identifier, \"available\", flag)\n",
                "                        )\n",
                "                        await asyncio.sleep(polling_period_seconds)\n",
                "                        if flag.is_set():\n",
                "                            return expected_response\n",
                "                return {\"status\": \"error\", \"cluster_state\": status}\n",
                "            except botocore.exceptions.ClientError as error:\n",
                "                return {\"status\": \"error\", \"message\": str(error)}\n",
                "\n",
                "    async def get_cluster_status(\n",
                "        self,\n",
                "        cluster_identifier: str,\n",
                "        expected_state: str,\n",
                "        flag: asyncio.Event,\n",
                "        delete_operation: bool = False,\n",
                "    ) -> dict[str, Any]:\n",
                "        \"\"\"\n",
                "        check for expected Redshift cluster state\n",
                "\n",
                "        :param cluster_identifier: unique identifier of a cluster\n",
                "        :param expected_state: expected_state example(\"available\", \"pausing\", \"paused\"\")\n",
                "        :param flag: asyncio even flag set true if success and if any error\n",
                "        :param delete_operation: whether the method has been called as part of delete cluster operation\n",
                "        \"\"\"\n",
                "        try:\n",
                "            response = await self.cluster_status(cluster_identifier, delete_operation=delete_operation)\n",
                "            if (\"cluster_state\" in response and response[\"cluster_state\"] == expected_state) or response[\n",
                "                \"status\"\n",
                "            ] == \"error\":\n",
                "                flag.set()\n",
                "            return response\n",
                "        except botocore.exceptions.ClientError as error:\n",
                "            flag.set()\n",
                "            return {\"status\": \"error\", \"message\": str(error)}"
            ]
        ],
        "airflow/providers/amazon/aws/operators/redshift_cluster.py": [
            [
                "# Licensed to the Apache Software Foundation (ASF) under one\n",
                "# or more contributor license agreements.  See the NOTICE file\n",
                "# distributed with this work for additional information\n",
                "# regarding copyright ownership.  The ASF licenses this file\n",
                "# to you under the Apache License, Version 2.0 (the\n",
                "# \"License\"); you may not use this file except in compliance\n",
                "# with the License.  You may obtain a copy of the License at\n",
                "#\n",
                "#   http://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing,\n",
                "# software distributed under the License is distributed on an\n",
                "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
                "# KIND, either express or implied.  See the License for the\n",
                "# specific language governing permissions and limitations\n",
                "# under the License.\n",
                "from __future__ import annotations\n",
                "\n",
                "import time\n",
                "from typing import TYPE_CHECKING, Any, Sequence\n",
                "\n",
                "from airflow.exceptions import AirflowException\n",
                "from airflow.models import BaseOperator\n",
                "from airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\n",
                "from airflow.providers.amazon.aws.triggers.redshift_cluster import (\n",
                "    RedshiftClusterTrigger,\n",
                "    RedshiftCreateClusterTrigger,\n",
                ")\n",
                "\n",
                "if TYPE_CHECKING:\n",
                "    from airflow.utils.context import Context\n",
                "\n",
                "\n",
                "class RedshiftCreateClusterOperator(BaseOperator):\n",
                "    \"\"\"Creates a new cluster with the specified parameters.\n",
                "\n",
                "    .. seealso::\n",
                "        For more information on how to use this operator, take a look at the guide:\n",
                "        :ref:`howto/operator:RedshiftCreateClusterOperator`\n",
                "\n",
                "    :param cluster_identifier:  A unique identifier for the cluster.\n",
                "    :param node_type: The node type to be provisioned for the cluster.\n",
                "            Valid Values: ``ds2.xlarge``, ``ds2.8xlarge``, ``dc1.large``,\n",
                "            ``dc1.8xlarge``, ``dc2.large``, ``dc2.8xlarge``, ``ra3.xlplus``,\n",
                "            ``ra3.4xlarge``, and ``ra3.16xlarge``.\n",
                "    :param master_username: The username associated with the admin user account for\n",
                "        the cluster that is being created.\n",
                "    :param master_user_password: The password associated with the admin user account for\n",
                "        the cluster that is being created.\n",
                "    :param cluster_type: The type of the cluster ``single-node`` or ``multi-node``.\n",
                "        The default value is ``multi-node``.\n",
                "    :param db_name: The name of the first database to be created when the cluster is created.\n",
                "    :param number_of_nodes: The number of compute nodes in the cluster.\n",
                "        This param require when ``cluster_type`` is ``multi-node``.\n",
                "    :param cluster_security_groups: A list of security groups to be associated with this cluster.\n",
                "    :param vpc_security_group_ids: A list of  VPC security groups to be associated with the cluster.\n",
                "    :param cluster_subnet_group_name: The name of a cluster subnet group to be associated with this cluster.\n",
                "    :param availability_zone: The EC2 Availability Zone (AZ).\n",
                "    :param preferred_maintenance_window: The time range (in UTC) during which automated cluster\n",
                "        maintenance can occur.\n",
                "    :param cluster_parameter_group_name: The name of the parameter group to be associated with this cluster.\n",
                "    :param automated_snapshot_retention_period: The number of days that automated snapshots are retained.\n",
                "        The default value is ``1``.\n",
                "    :param manual_snapshot_retention_period: The default number of days to retain a manual snapshot.\n",
                "    :param port: The port number on which the cluster accepts incoming connections.\n",
                "        The Default value is ``5439``.\n",
                "    :param cluster_version: The version of a Redshift engine software that you want to deploy on the cluster.\n",
                "    :param allow_version_upgrade: Whether major version upgrades can be applied during the maintenance window.\n",
                "        The Default value is ``True``.\n",
                "    :parma publicly_accessible: Whether cluster can be accessed from a public network.\n",
                "    :parma encrypted: Whether data in the cluster is encrypted at rest.\n",
                "        The default value is ``False``.\n",
                "    :parma hsm_client_certificate_identifier: Name of the HSM client certificate\n",
                "        the Amazon Redshift cluster uses to retrieve the data.\n",
                "    :parma hsm_configuration_identifier: Name of the HSM configuration\n",
                "    :parma elastic_ip: The Elastic IP (EIP) address for the cluster.\n",
                "    :parma tags: A list of tag instances\n",
                "    :parma kms_key_id: KMS key id of encryption key.\n",
                "    :param enhanced_vpc_routing: Whether to create the cluster with enhanced VPC routing enabled\n",
                "        Default value is ``False``.\n",
                "    :param additional_info: Reserved\n",
                "    :param iam_roles: A list of IAM roles that can be used by the cluster to access other AWS services.\n",
                "    :param maintenance_track_name: Name of the maintenance track for the cluster.\n",
                "    :param snapshot_schedule_identifier: A  unique identifier for the snapshot schedule.\n",
                "    :param availability_zone_relocation: Enable relocation for a Redshift cluster\n",
                "        between Availability Zones after the cluster is created.\n",
                "    :param aqua_configuration_status: The cluster is configured to use AQUA .\n",
                "    :param default_iam_role_arn: ARN for the IAM role.\n",
                "    :param aws_conn_id: str = The Airflow connection used for AWS credentials.\n",
                "        The default connection id is ``aws_default``.\n",
                "    :param wait_for_completion: Whether wait for the cluster to be in ``available`` state\n",
                "    :param max_attempt: The maximum number of attempts to be made. Default: 5\n",
                "    :param poll_interval: The amount of time in seconds to wait between attempts. Default: 60\n",
                "    :param deferrable: If True, the operator will run in deferrable mode\n",
                "    \"\"\"\n",
                "\n",
                "    template_fields: Sequence[str] = (\n",
                "        \"cluster_identifier\",\n",
                "        \"cluster_type\",\n",
                "        \"node_type\",\n",
                "        \"number_of_nodes\",\n",
                "        \"vpc_security_group_ids\",\n",
                "    )\n",
                "    ui_color = \"#eeaa11\"\n",
                "    ui_fgcolor = \"#ffffff\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        *,\n",
                "        cluster_identifier: str,\n",
                "        node_type: str,\n",
                "        master_username: str,\n",
                "        master_user_password: str,\n",
                "        cluster_type: str = \"multi-node\",\n",
                "        db_name: str = \"dev\",\n",
                "        number_of_nodes: int = 1,\n",
                "        cluster_security_groups: list[str] | None = None,\n",
                "        vpc_security_group_ids: list[str] | None = None,\n",
                "        cluster_subnet_group_name: str | None = None,\n",
                "        availability_zone: str | None = None,\n",
                "        preferred_maintenance_window: str | None = None,\n",
                "        cluster_parameter_group_name: str | None = None,\n",
                "        automated_snapshot_retention_period: int = 1,\n",
                "        manual_snapshot_retention_period: int | None = None,\n",
                "        port: int = 5439,\n",
                "        cluster_version: str = \"1.0\",\n",
                "        allow_version_upgrade: bool = True,\n",
                "        publicly_accessible: bool = True,\n",
                "        encrypted: bool = False,\n",
                "        hsm_client_certificate_identifier: str | None = None,\n",
                "        hsm_configuration_identifier: str | None = None,\n",
                "        elastic_ip: str | None = None,\n",
                "        tags: list[Any] | None = None,\n",
                "        kms_key_id: str | None = None,\n",
                "        enhanced_vpc_routing: bool = False,\n",
                "        additional_info: str | None = None,\n",
                "        iam_roles: list[str] | None = None,\n",
                "        maintenance_track_name: str | None = None,\n",
                "        snapshot_schedule_identifier: str | None = None,\n",
                "        availability_zone_relocation: bool | None = None,\n",
                "        aqua_configuration_status: str | None = None,\n",
                "        default_iam_role_arn: str | None = None,\n",
                "        aws_conn_id: str = \"aws_default\",\n",
                "        wait_for_completion: bool = False,\n",
                "        max_attempt: int = 5,\n",
                "        poll_interval: int = 60,\n",
                "        deferrable: bool = False,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.cluster_identifier = cluster_identifier\n",
                "        self.node_type = node_type\n",
                "        self.master_username = master_username\n",
                "        self.master_user_password = master_user_password\n",
                "        self.cluster_type = cluster_type\n",
                "        self.db_name = db_name\n",
                "        self.number_of_nodes = number_of_nodes\n",
                "        self.cluster_security_groups = cluster_security_groups\n",
                "        self.vpc_security_group_ids = vpc_security_group_ids\n",
                "        self.cluster_subnet_group_name = cluster_subnet_group_name\n",
                "        self.availability_zone = availability_zone\n",
                "        self.preferred_maintenance_window = preferred_maintenance_window\n",
                "        self.cluster_parameter_group_name = cluster_parameter_group_name\n",
                "        self.automated_snapshot_retention_period = automated_snapshot_retention_period\n",
                "        self.manual_snapshot_retention_period = manual_snapshot_retention_period\n",
                "        self.port = port\n",
                "        self.cluster_version = cluster_version\n",
                "        self.allow_version_upgrade = allow_version_upgrade\n",
                "        self.publicly_accessible = publicly_accessible\n",
                "        self.encrypted = encrypted\n",
                "        self.hsm_client_certificate_identifier = hsm_client_certificate_identifier\n",
                "        self.hsm_configuration_identifier = hsm_configuration_identifier\n",
                "        self.elastic_ip = elastic_ip\n",
                "        self.tags = tags\n",
                "        self.kms_key_id = kms_key_id\n",
                "        self.enhanced_vpc_routing = enhanced_vpc_routing\n",
                "        self.additional_info = additional_info\n",
                "        self.iam_roles = iam_roles\n",
                "        self.maintenance_track_name = maintenance_track_name\n",
                "        self.snapshot_schedule_identifier = snapshot_schedule_identifier\n",
                "        self.availability_zone_relocation = availability_zone_relocation\n",
                "        self.aqua_configuration_status = aqua_configuration_status\n",
                "        self.default_iam_role_arn = default_iam_role_arn\n",
                "        self.aws_conn_id = aws_conn_id\n",
                "        self.wait_for_completion = wait_for_completion\n",
                "        self.max_attempt = max_attempt\n",
                "        self.poll_interval = poll_interval\n",
                "        self.deferrable = deferrable\n",
                "        self.kwargs = kwargs\n",
                "\n",
                "    def execute(self, context: Context):\n",
                "        redshift_hook = RedshiftHook(aws_conn_id=self.aws_conn_id)\n",
                "        self.log.info(\"Creating Redshift cluster %s\", self.cluster_identifier)\n",
                "        params: dict[str, Any] = {}\n",
                "        if self.db_name:\n",
                "            params[\"DBName\"] = self.db_name\n",
                "        if self.cluster_type:\n",
                "            params[\"ClusterType\"] = self.cluster_type\n",
                "            if self.cluster_type == \"multi-node\":\n",
                "                params[\"NumberOfNodes\"] = self.number_of_nodes\n",
                "        if self.cluster_security_groups:\n",
                "            params[\"ClusterSecurityGroups\"] = self.cluster_security_groups\n",
                "        if self.vpc_security_group_ids:\n",
                "            params[\"VpcSecurityGroupIds\"] = self.vpc_security_group_ids\n",
                "        if self.cluster_subnet_group_name:\n",
                "            params[\"ClusterSubnetGroupName\"] = self.cluster_subnet_group_name\n",
                "        if self.availability_zone:\n",
                "            params[\"AvailabilityZone\"] = self.availability_zone\n",
                "        if self.preferred_maintenance_window:\n",
                "            params[\"PreferredMaintenanceWindow\"] = self.preferred_maintenance_window\n",
                "        if self.cluster_parameter_group_name:\n",
                "            params[\"ClusterParameterGroupName\"] = self.cluster_parameter_group_name\n",
                "        if self.automated_snapshot_retention_period:\n",
                "            params[\"AutomatedSnapshotRetentionPeriod\"] = self.automated_snapshot_retention_period\n",
                "        if self.manual_snapshot_retention_period:\n",
                "            params[\"ManualSnapshotRetentionPeriod\"] = self.manual_snapshot_retention_period\n",
                "        if self.port:\n",
                "            params[\"Port\"] = self.port\n",
                "        if self.cluster_version:\n",
                "            params[\"ClusterVersion\"] = self.cluster_version\n",
                "        if self.allow_version_upgrade:\n",
                "            params[\"AllowVersionUpgrade\"] = self.allow_version_upgrade\n",
                "        if self.publicly_accessible:\n",
                "            params[\"PubliclyAccessible\"] = self.publicly_accessible\n",
                "        if self.encrypted:\n",
                "            params[\"Encrypted\"] = self.encrypted\n",
                "        if self.hsm_client_certificate_identifier:\n",
                "            params[\"HsmClientCertificateIdentifier\"] = self.hsm_client_certificate_identifier\n",
                "        if self.hsm_configuration_identifier:\n",
                "            params[\"HsmConfigurationIdentifier\"] = self.hsm_configuration_identifier\n",
                "        if self.elastic_ip:\n",
                "            params[\"ElasticIp\"] = self.elastic_ip\n",
                "        if self.tags:\n",
                "            params[\"Tags\"] = self.tags\n",
                "        if self.kms_key_id:\n",
                "            params[\"KmsKeyId\"] = self.kms_key_id\n",
                "        if self.enhanced_vpc_routing:\n",
                "            params[\"EnhancedVpcRouting\"] = self.enhanced_vpc_routing\n",
                "        if self.additional_info:\n",
                "            params[\"AdditionalInfo\"] = self.additional_info\n",
                "        if self.iam_roles:\n",
                "            params[\"IamRoles\"] = self.iam_roles\n",
                "        if self.maintenance_track_name:\n",
                "            params[\"MaintenanceTrackName\"] = self.maintenance_track_name\n",
                "        if self.snapshot_schedule_identifier:\n",
                "            params[\"SnapshotScheduleIdentifier\"] = self.snapshot_schedule_identifier\n",
                "        if self.availability_zone_relocation:\n",
                "            params[\"AvailabilityZoneRelocation\"] = self.availability_zone_relocation\n",
                "        if self.aqua_configuration_status:\n",
                "            params[\"AquaConfigurationStatus\"] = self.aqua_configuration_status\n",
                "        if self.default_iam_role_arn:\n",
                "            params[\"DefaultIamRoleArn\"] = self.default_iam_role_arn\n",
                "\n",
                "        cluster = redshift_hook.create_cluster(\n",
                "            self.cluster_identifier,\n",
                "            self.node_type,\n",
                "            self.master_username,\n",
                "            self.master_user_password,\n",
                "            params,\n",
                "        )\n",
                "        if self.deferrable:\n",
                "            self.defer(\n",
                "                trigger=RedshiftCreateClusterTrigger(\n",
                "                    cluster_identifier=self.cluster_identifier,\n",
                "                    poll_interval=self.poll_interval,\n",
                "                    max_attempt=self.max_attempt,\n",
                "                    aws_conn_id=self.aws_conn_id,\n",
                "                ),\n",
                "                method_name=\"execute_complete\",\n",
                "            )\n",
                "        if self.wait_for_completion:\n",
                "            redshift_hook.get_conn().get_waiter(\"cluster_available\").wait(\n",
                "                ClusterIdentifier=self.cluster_identifier,\n",
                "                WaiterConfig={\n",
                "                    \"Delay\": self.poll_interval,\n",
                "                    \"MaxAttempts\": self.max_attempt,\n",
                "                },\n",
                "            )\n",
                "\n",
                "        self.log.info(\"Created Redshift cluster %s\", self.cluster_identifier)\n",
                "        self.log.info(cluster)\n",
                "\n",
                "    def execute_complete(self, context, event=None):\n",
                "        if event[\"status\"] != \"success\":\n",
                "            raise AirflowException(f\"Error creating cluster: {event}\")\n",
                "        return\n",
                "\n",
                "\n",
                "class RedshiftCreateClusterSnapshotOperator(BaseOperator):\n",
                "    \"\"\"\n",
                "    Creates a manual snapshot of the specified cluster. The cluster must be in the available state\n",
                "\n",
                "    .. seealso::\n",
                "        For more information on how to use this operator, take a look at the guide:\n",
                "        :ref:`howto/operator:RedshiftCreateClusterSnapshotOperator`\n",
                "\n",
                "    :param snapshot_identifier: A unique identifier for the snapshot that you are requesting\n",
                "    :param cluster_identifier: The cluster identifier for which you want a snapshot\n",
                "    :param retention_period: The number of days that a manual snapshot is retained.\n",
                "        If the value is -1, the manual snapshot is retained indefinitely.\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "    :parma tags: A list of tag instances\n"
                ],
                "parent_version_range": {
                    "start": 300,
                    "end": 300
                },
                "child_version_range": {
                    "start": 300,
                    "end": 301
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RedshiftCreateClusterSnapshotOperator",
                        "signature": "class RedshiftCreateClusterSnapshotOperator(BaseOperator):",
                        "at_line": 288
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: airflow/providers/amazon/aws/operators/redshift_cluster.py\nCode:\n           class RedshiftCreateClusterSnapshotOperator(BaseOperator):\n               ...\n297 297        :param cluster_identifier: The cluster identifier for which you want a snapshot\n298 298        :param retention_period: The number of days that a manual snapshot is retained.\n299 299            If the value is -1, the manual snapshot is retained indefinitely.\n    300  +     :parma tags: A list of tag instances\n300 301        :param wait_for_completion: Whether wait for the cluster snapshot to be in ``available`` state\n301 302        :param poll_interval: Time (in seconds) to wait between two consecutive calls to check state\n302 303        :param max_attempt: The maximum number of attempts to be made to check the state\n         ...\n",
                "file_path": "airflow/providers/amazon/aws/operators/redshift_cluster.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "A",
                    "instances",
                    "list",
                    "of",
                    "parma",
                    "tag",
                    "tags"
                ],
                "prefix": [
                    "    :param cluster_identifier: The cluster identifier for which you want a snapshot\n",
                    "    :param retention_period: The number of days that a manual snapshot is retained.\n",
                    "        If the value is -1, the manual snapshot is retained indefinitely.\n"
                ],
                "suffix": [
                    "    :param wait_for_completion: Whether wait for the cluster snapshot to be in ``available`` state\n",
                    "    :param poll_interval: Time (in seconds) to wait between two consecutive calls to check state\n",
                    "    :param max_attempt: The maximum number of attempts to be made to check the state\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    1
                ]
            },
            [
                "    :param wait_for_completion: Whether wait for the cluster snapshot to be in ``available`` state\n",
                "    :param poll_interval: Time (in seconds) to wait between two consecutive calls to check state\n",
                "    :param max_attempt: The maximum number of attempts to be made to check the state\n",
                "    :param aws_conn_id: The Airflow connection used for AWS credentials.\n",
                "        The default connection id is ``aws_default``\n",
                "    \"\"\"\n",
                "\n",
                "    template_fields: Sequence[str] = (\n",
                "        \"cluster_identifier\",\n",
                "        \"snapshot_identifier\",\n",
                "    )\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        *,\n",
                "        snapshot_identifier: str,\n",
                "        cluster_identifier: str,\n",
                "        retention_period: int = -1,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        tags: list[Any] | None = None,\n"
                ],
                "parent_version_range": {
                    "start": 318,
                    "end": 318
                },
                "child_version_range": {
                    "start": 319,
                    "end": 320
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RedshiftCreateClusterSnapshotOperator",
                        "signature": "class RedshiftCreateClusterSnapshotOperator(BaseOperator):",
                        "at_line": 288
                    },
                    {
                        "type": "function",
                        "name": "__init__",
                        "signature": "def __init__(\n        self,\n        *,\n        snapshot_identifier: str,\n        cluster_identifier: str,\n        retention_period: int = -1,\n        wait_for_completion: bool = False,\n        poll_interval: int = 15,\n        max_attempt: int = 20,\n        aws_conn_id: str = \"aws_default\",\n        **kwargs,\n    ):",
                        "at_line": 312
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: airflow/providers/amazon/aws/operators/redshift_cluster.py\nCode:\n           class RedshiftCreateClusterSnapshotOperator(BaseOperator):\n               ...\n               def __init__(\n        self,\n        *,\n        snapshot_identifier: str,\n        cluster_identifier: str,\n        retention_period: int = -1,\n        wait_for_completion: bool = False,\n        poll_interval: int = 15,\n        max_attempt: int = 20,\n        aws_conn_id: str = \"aws_default\",\n        **kwargs,\n    ):\n                   ...\n315 316            snapshot_identifier: str,\n316 317            cluster_identifier: str,\n317 318            retention_period: int = -1,\n    319  +         tags: list[Any] | None = None,\n318 320            wait_for_completion: bool = False,\n319 321            poll_interval: int = 15,\n320 322            max_attempt: int = 20,\n         ...\n",
                "file_path": "airflow/providers/amazon/aws/operators/redshift_cluster.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "Any",
                    "list",
                    "tags"
                ],
                "prefix": [
                    "        snapshot_identifier: str,\n",
                    "        cluster_identifier: str,\n",
                    "        retention_period: int = -1,\n"
                ],
                "suffix": [
                    "        wait_for_completion: bool = False,\n",
                    "        poll_interval: int = 15,\n",
                    "        max_attempt: int = 20,\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 319,
                                    "column": 8
                                },
                                "end": {
                                    "line": 319,
                                    "column": 12
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/operators/redshift_cluster.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 8,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 319,
                                    "column": 8
                                },
                                "end": {
                                    "line": 319,
                                    "column": 12
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/operators/redshift_cluster.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "        wait_for_completion: bool = False,\n",
                "        poll_interval: int = 15,\n",
                "        max_attempt: int = 20,\n",
                "        aws_conn_id: str = \"aws_default\",\n",
                "        **kwargs,\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.snapshot_identifier = snapshot_identifier\n",
                "        self.cluster_identifier = cluster_identifier\n",
                "        self.retention_period = retention_period\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        self.tags = tags\n"
                ],
                "parent_version_range": {
                    "start": 328,
                    "end": 328
                },
                "child_version_range": {
                    "start": 330,
                    "end": 331
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RedshiftCreateClusterSnapshotOperator",
                        "signature": "class RedshiftCreateClusterSnapshotOperator(BaseOperator):",
                        "at_line": 288
                    },
                    {
                        "type": "function",
                        "name": "__init__",
                        "signature": "def __init__(\n        self,\n        *,\n        snapshot_identifier: str,\n        cluster_identifier: str,\n        retention_period: int = -1,\n        wait_for_completion: bool = False,\n        poll_interval: int = 15,\n        max_attempt: int = 20,\n        aws_conn_id: str = \"aws_default\",\n        **kwargs,\n    ):",
                        "at_line": 312
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: airflow/providers/amazon/aws/operators/redshift_cluster.py\nCode:\n           class RedshiftCreateClusterSnapshotOperator(BaseOperator):\n               ...\n               def __init__(\n        self,\n        *,\n        snapshot_identifier: str,\n        cluster_identifier: str,\n        retention_period: int = -1,\n        wait_for_completion: bool = False,\n        poll_interval: int = 15,\n        max_attempt: int = 20,\n        aws_conn_id: str = \"aws_default\",\n        **kwargs,\n    ):\n                   ...\n325 327            self.snapshot_identifier = snapshot_identifier\n326 328            self.cluster_identifier = cluster_identifier\n327 329            self.retention_period = retention_period\n    330  +         self.tags = tags\n328 331            self.wait_for_completion = wait_for_completion\n329 332            self.poll_interval = poll_interval\n330 333            self.max_attempt = max_attempt\n         ...\n",
                "file_path": "airflow/providers/amazon/aws/operators/redshift_cluster.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "self",
                    "tags"
                ],
                "prefix": [
                    "        self.snapshot_identifier = snapshot_identifier\n",
                    "        self.cluster_identifier = cluster_identifier\n",
                    "        self.retention_period = retention_period\n"
                ],
                "suffix": [
                    "        self.wait_for_completion = wait_for_completion\n",
                    "        self.poll_interval = poll_interval\n",
                    "        self.max_attempt = max_attempt\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 330,
                                    "column": 20
                                },
                                "end": {
                                    "line": 330,
                                    "column": 24
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/operators/redshift_cluster.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 330,
                                    "column": 13
                                },
                                "end": {
                                    "line": 330,
                                    "column": 17
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/operators/redshift_cluster.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "        self.wait_for_completion = wait_for_completion\n",
                "        self.poll_interval = poll_interval\n",
                "        self.max_attempt = max_attempt\n",
                "        self.redshift_hook = RedshiftHook(aws_conn_id=aws_conn_id)\n",
                "\n",
                "    def execute(self, context: Context) -> Any:\n",
                "        cluster_state = self.redshift_hook.cluster_status(cluster_identifier=self.cluster_identifier)\n",
                "        if cluster_state != \"available\":\n",
                "            raise AirflowException(\n",
                "                \"Redshift cluster must be in available state. \"\n",
                "                f\"Redshift cluster current state is {cluster_state}\"\n",
                "            )\n",
                "\n",
                "        self.redshift_hook.create_cluster_snapshot(\n",
                "            cluster_identifier=self.cluster_identifier,\n",
                "            snapshot_identifier=self.snapshot_identifier,\n",
                "            retention_period=self.retention_period,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "            tags=self.tags,\n"
                ],
                "parent_version_range": {
                    "start": 345,
                    "end": 345
                },
                "child_version_range": {
                    "start": 348,
                    "end": 349
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RedshiftCreateClusterSnapshotOperator",
                        "signature": "class RedshiftCreateClusterSnapshotOperator(BaseOperator):",
                        "at_line": 288
                    },
                    {
                        "type": "function",
                        "name": "execute",
                        "signature": "def execute(self, context: Context)->Any:",
                        "at_line": 333
                    },
                    {
                        "type": "call",
                        "name": "self.redshift_hook.create_cluster_snapshot",
                        "signature": "self.redshift_hook.create_cluster_snapshot(\n            cluster_identifier=self.cluster_identifier,\n            snapshot_identifier=self.snapshot_identifier,\n            retention_period=self.retention_period,\n        )",
                        "at_line": 341,
                        "argument": "retention_period=..."
                    }
                ],
                "idx": 7,
                "hunk_diff": "File: airflow/providers/amazon/aws/operators/redshift_cluster.py\nCode:\n           class RedshiftCreateClusterSnapshotOperator(BaseOperator):\n               ...\n               def execute(self, context: Context)->Any:\n                   ...\n                   self.redshift_hook.create_cluster_snapshot(\n            cluster_identifier=self.cluster_identifier,\n            snapshot_identifier=self.snapshot_identifier,\n            retention_period=self.retention_period,\n        )\n                       ...\n342 345                cluster_identifier=self.cluster_identifier,\n343 346                snapshot_identifier=self.snapshot_identifier,\n344 347                retention_period=self.retention_period,\n    348  +             tags=self.tags,\n345 349            )\n346 350    \n347 351            if self.wait_for_completion:\n         ...\n",
                "file_path": "airflow/providers/amazon/aws/operators/redshift_cluster.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "self",
                    "tags"
                ],
                "prefix": [
                    "            cluster_identifier=self.cluster_identifier,\n",
                    "            snapshot_identifier=self.snapshot_identifier,\n",
                    "            retention_period=self.retention_period,\n"
                ],
                "suffix": [
                    "        )\n",
                    "\n",
                    "        if self.wait_for_completion:\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 348,
                                    "column": 22
                                },
                                "end": {
                                    "line": 348,
                                    "column": 26
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/operators/redshift_cluster.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 348,
                                    "column": 12
                                },
                                "end": {
                                    "line": 348,
                                    "column": 16
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/airflow/airflow/providers/amazon/aws/operators/redshift_cluster.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        )\n",
                "\n",
                "        if self.wait_for_completion:\n",
                "            self.redshift_hook.get_conn().get_waiter(\"snapshot_available\").wait(\n",
                "                ClusterIdentifier=self.cluster_identifier,\n",
                "                WaiterConfig={\n",
                "                    \"Delay\": self.poll_interval,\n",
                "                    \"MaxAttempts\": self.max_attempt,\n",
                "                },\n",
                "            )\n",
                "\n",
                "\n",
                "class RedshiftDeleteClusterSnapshotOperator(BaseOperator):\n",
                "    \"\"\"\n",
                "    Deletes the specified manual snapshot\n",
                "\n",
                "    .. seealso::\n",
                "        For more information on how to use this operator, take a look at the guide:\n",
                "        :ref:`howto/operator:RedshiftDeleteClusterSnapshotOperator`\n",
                "\n",
                "    :param snapshot_identifier: A unique identifier for the snapshot that you are requesting\n",
                "    :param cluster_identifier: The unique identifier of the cluster the snapshot was created from\n",
                "    :param wait_for_completion: Whether wait for cluster deletion or not\n",
                "        The default value is ``True``\n",
                "    :param aws_conn_id: The Airflow connection used for AWS credentials.\n",
                "        The default connection id is ``aws_default``\n",
                "    :param poll_interval: Time (in seconds) to wait between two consecutive calls to check snapshot state\n",
                "    \"\"\"\n",
                "\n",
                "    template_fields: Sequence[str] = (\n",
                "        \"cluster_identifier\",\n",
                "        \"snapshot_identifier\",\n",
                "    )\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        *,\n",
                "        snapshot_identifier: str,\n",
                "        cluster_identifier: str,\n",
                "        wait_for_completion: bool = True,\n",
                "        aws_conn_id: str = \"aws_default\",\n",
                "        poll_interval: int = 10,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.snapshot_identifier = snapshot_identifier\n",
                "        self.cluster_identifier = cluster_identifier\n",
                "        self.wait_for_completion = wait_for_completion\n",
                "        self.poll_interval = poll_interval\n",
                "        self.redshift_hook = RedshiftHook(aws_conn_id=aws_conn_id)\n",
                "\n",
                "    def execute(self, context: Context) -> Any:\n",
                "        self.redshift_hook.get_conn().delete_cluster_snapshot(\n",
                "            SnapshotClusterIdentifier=self.cluster_identifier,\n",
                "            SnapshotIdentifier=self.snapshot_identifier,\n",
                "        )\n",
                "\n",
                "        if self.wait_for_completion:\n",
                "            while self.get_status() is not None:\n",
                "                time.sleep(self.poll_interval)\n",
                "\n",
                "    def get_status(self) -> str:\n",
                "        return self.redshift_hook.get_cluster_snapshot_status(\n",
                "            snapshot_identifier=self.snapshot_identifier,\n",
                "        )\n",
                "\n",
                "\n",
                "class RedshiftResumeClusterOperator(BaseOperator):\n",
                "    \"\"\"\n",
                "    Resume a paused AWS Redshift Cluster\n",
                "\n",
                "    .. seealso::\n",
                "        For more information on how to use this operator, take a look at the guide:\n",
                "        :ref:`howto/operator:RedshiftResumeClusterOperator`\n",
                "\n",
                "    :param cluster_identifier:  Unique identifier of the AWS Redshift cluster\n",
                "    :param aws_conn_id: The Airflow connection used for AWS credentials.\n",
                "        The default connection id is ``aws_default``\n",
                "    :param deferrable: Run operator in deferrable mode\n",
                "    :param poll_interval: Time (in seconds) to wait between two consecutive calls to check cluster state\n",
                "    \"\"\"\n",
                "\n",
                "    template_fields: Sequence[str] = (\"cluster_identifier\",)\n",
                "    ui_color = \"#eeaa11\"\n",
                "    ui_fgcolor = \"#ffffff\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        *,\n",
                "        cluster_identifier: str,\n",
                "        aws_conn_id: str = \"aws_default\",\n",
                "        deferrable: bool = False,\n",
                "        poll_interval: int = 10,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.cluster_identifier = cluster_identifier\n",
                "        self.aws_conn_id = aws_conn_id\n",
                "        self.deferrable = deferrable\n",
                "        self.poll_interval = poll_interval\n",
                "        # These parameters are added to address an issue with the boto3 API where the API\n",
                "        # prematurely reports the cluster as available to receive requests. This causes the cluster\n",
                "        # to reject initial attempts to resume the cluster despite reporting the correct state.\n",
                "        self._attempts = 10\n",
                "        self._attempt_interval = 15\n",
                "\n",
                "    def execute(self, context: Context):\n",
                "        redshift_hook = RedshiftHook(aws_conn_id=self.aws_conn_id)\n",
                "\n",
                "        if self.deferrable:\n",
                "            self.defer(\n",
                "                timeout=self.execution_timeout,\n",
                "                trigger=RedshiftClusterTrigger(\n",
                "                    task_id=self.task_id,\n",
                "                    poll_interval=self.poll_interval,\n",
                "                    aws_conn_id=self.aws_conn_id,\n",
                "                    cluster_identifier=self.cluster_identifier,\n",
                "                    attempts=self._attempts,\n",
                "                    operation_type=\"resume_cluster\",\n",
                "                ),\n",
                "                method_name=\"execute_complete\",\n",
                "            )\n",
                "        else:\n",
                "            while self._attempts >= 1:\n",
                "                try:\n",
                "                    redshift_hook.get_conn().resume_cluster(ClusterIdentifier=self.cluster_identifier)\n",
                "                    return\n",
                "                except redshift_hook.get_conn().exceptions.InvalidClusterStateFault as error:\n",
                "                    self._attempts = self._attempts - 1\n",
                "\n",
                "                    if self._attempts > 0:\n",
                "                        self.log.error(\"Unable to resume cluster. %d attempts remaining.\", self._attempts)\n",
                "                        time.sleep(self._attempt_interval)\n",
                "                    else:\n",
                "                        raise error\n",
                "\n",
                "    def execute_complete(self, context: Context, event: Any = None) -> None:\n",
                "        \"\"\"\n",
                "        Callback for when the trigger fires - returns immediately.\n",
                "        Relies on trigger to throw an exception, otherwise it assumes execution was\n",
                "        successful.\n",
                "        \"\"\"\n",
                "        if event:\n",
                "            if \"status\" in event and event[\"status\"] == \"error\":\n",
                "                msg = f\"{event['status']}: {event['message']}\"\n",
                "                raise AirflowException(msg)\n",
                "            elif \"status\" in event and event[\"status\"] == \"success\":\n",
                "                self.log.info(\"%s completed successfully.\", self.task_id)\n",
                "                self.log.info(\"Resumed cluster successfully\")\n",
                "        else:\n",
                "            raise AirflowException(\"No event received from trigger\")\n",
                "\n",
                "\n",
                "class RedshiftPauseClusterOperator(BaseOperator):\n",
                "    \"\"\"\n",
                "    Pause an AWS Redshift Cluster if it has status `available`.\n",
                "\n",
                "    .. seealso::\n",
                "        For more information on how to use this operator, take a look at the guide:\n",
                "        :ref:`howto/operator:RedshiftPauseClusterOperator`\n",
                "\n",
                "    :param cluster_identifier: id of the AWS Redshift Cluster\n",
                "    :param aws_conn_id: aws connection to use\n",
                "    :param deferrable: Run operator in the deferrable mode. This mode requires an additional aiobotocore>=\n",
                "    \"\"\"\n",
                "\n",
                "    template_fields: Sequence[str] = (\"cluster_identifier\",)\n",
                "    ui_color = \"#eeaa11\"\n",
                "    ui_fgcolor = \"#ffffff\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        *,\n",
                "        cluster_identifier: str,\n",
                "        aws_conn_id: str = \"aws_default\",\n",
                "        deferrable: bool = False,\n",
                "        poll_interval: int = 10,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.cluster_identifier = cluster_identifier\n",
                "        self.aws_conn_id = aws_conn_id\n",
                "        self.deferrable = deferrable\n",
                "        self.poll_interval = poll_interval\n",
                "        # These parameters are added to address an issue with the boto3 API where the API\n",
                "        # prematurely reports the cluster as available to receive requests. This causes the cluster\n",
                "        # to reject initial attempts to pause the cluster despite reporting the correct state.\n",
                "        self._attempts = 10\n",
                "        self._attempt_interval = 15\n",
                "\n",
                "    def execute(self, context: Context):\n",
                "        redshift_hook = RedshiftHook(aws_conn_id=self.aws_conn_id)\n",
                "\n",
                "        if self.deferrable:\n",
                "            self.defer(\n",
                "                timeout=self.execution_timeout,\n",
                "                trigger=RedshiftClusterTrigger(\n",
                "                    task_id=self.task_id,\n",
                "                    poll_interval=self.poll_interval,\n",
                "                    aws_conn_id=self.aws_conn_id,\n",
                "                    cluster_identifier=self.cluster_identifier,\n",
                "                    attempts=self._attempts,\n",
                "                    operation_type=\"pause_cluster\",\n",
                "                ),\n",
                "                method_name=\"execute_complete\",\n",
                "            )\n",
                "        else:\n",
                "            while self._attempts >= 1:\n",
                "                try:\n",
                "                    redshift_hook.get_conn().pause_cluster(ClusterIdentifier=self.cluster_identifier)\n",
                "                    return\n",
                "                except redshift_hook.get_conn().exceptions.InvalidClusterStateFault as error:\n",
                "                    self._attempts = self._attempts - 1\n",
                "\n",
                "                    if self._attempts > 0:\n",
                "                        self.log.error(\"Unable to pause cluster. %d attempts remaining.\", self._attempts)\n",
                "                        time.sleep(self._attempt_interval)\n",
                "                    else:\n",
                "                        raise error\n",
                "\n",
                "    def execute_complete(self, context: Context, event: Any = None) -> None:\n",
                "        \"\"\"\n",
                "        Callback for when the trigger fires - returns immediately.\n",
                "        Relies on trigger to throw an exception, otherwise it assumes execution was\n",
                "        successful.\n",
                "        \"\"\"\n",
                "        if event:\n",
                "            if \"status\" in event and event[\"status\"] == \"error\":\n",
                "                msg = f\"{event['status']}: {event['message']}\"\n",
                "                raise AirflowException(msg)\n",
                "            elif \"status\" in event and event[\"status\"] == \"success\":\n",
                "                self.log.info(\"%s completed successfully.\", self.task_id)\n",
                "                self.log.info(\"Paused cluster successfully\")\n",
                "        else:\n",
                "            raise AirflowException(\"No event received from trigger\")\n",
                "\n",
                "\n",
                "class RedshiftDeleteClusterOperator(BaseOperator):\n",
                "    \"\"\"\n",
                "    Delete an AWS Redshift cluster.\n",
                "\n",
                "    .. seealso::\n",
                "        For more information on how to use this operator, take a look at the guide:\n",
                "        :ref:`howto/operator:RedshiftDeleteClusterOperator`\n",
                "\n",
                "    :param cluster_identifier: unique identifier of a cluster\n",
                "    :param skip_final_cluster_snapshot: determines cluster snapshot creation\n",
                "    :param final_cluster_snapshot_identifier: name of final cluster snapshot\n",
                "    :param wait_for_completion: Whether wait for cluster deletion or not\n",
                "        The default value is ``True``\n",
                "    :param aws_conn_id: aws connection to use\n",
                "    :param poll_interval: Time (in seconds) to wait between two consecutive calls to check cluster state\n",
                "    \"\"\"\n",
                "\n",
                "    template_fields: Sequence[str] = (\"cluster_identifier\",)\n",
                "    ui_color = \"#eeaa11\"\n",
                "    ui_fgcolor = \"#ffffff\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        *,\n",
                "        cluster_identifier: str,\n",
                "        skip_final_cluster_snapshot: bool = True,\n",
                "        final_cluster_snapshot_identifier: str | None = None,\n",
                "        wait_for_completion: bool = True,\n",
                "        aws_conn_id: str = \"aws_default\",\n",
                "        poll_interval: float = 30.0,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.cluster_identifier = cluster_identifier\n",
                "        self.skip_final_cluster_snapshot = skip_final_cluster_snapshot\n",
                "        self.final_cluster_snapshot_identifier = final_cluster_snapshot_identifier\n",
                "        self.wait_for_completion = wait_for_completion\n",
                "        self.poll_interval = poll_interval\n",
                "        # These parameters are added to keep trying if there is a running operation in the cluster\n",
                "        # If there is a running operation in the cluster while trying to delete it, a InvalidClusterStateFault\n",
                "        # is thrown. In such case, retrying\n",
                "        self._attempts = 10\n",
                "        self._attempt_interval = 15\n",
                "        self.redshift_hook = RedshiftHook(aws_conn_id=aws_conn_id)\n",
                "\n",
                "    def execute(self, context: Context):\n",
                "        while self._attempts >= 1:\n",
                "            try:\n",
                "                self.redshift_hook.delete_cluster(\n",
                "                    cluster_identifier=self.cluster_identifier,\n",
                "                    skip_final_cluster_snapshot=self.skip_final_cluster_snapshot,\n",
                "                    final_cluster_snapshot_identifier=self.final_cluster_snapshot_identifier,\n",
                "                )\n",
                "                break\n",
                "            except self.redshift_hook.get_conn().exceptions.InvalidClusterStateFault:\n",
                "                self._attempts = self._attempts - 1\n",
                "\n",
                "                if self._attempts > 0:\n",
                "                    self.log.error(\"Unable to delete cluster. %d attempts remaining.\", self._attempts)\n",
                "                    time.sleep(self._attempt_interval)\n",
                "                else:\n",
                "                    raise\n",
                "\n",
                "        if self.wait_for_completion:\n",
                "            waiter = self.redshift_hook.get_conn().get_waiter(\"cluster_deleted\")\n",
                "            waiter.wait(\n",
                "                ClusterIdentifier=self.cluster_identifier,\n",
                "                WaiterConfig={\"Delay\": self.poll_interval, \"MaxAttempts\": 30},\n",
                "            )"
            ]
        ],
        "tests/providers/amazon/aws/operators/test_redshift_cluster.py": [
            [
                "# Licensed to the Apache Software Foundation (ASF) under one\n",
                "# or more contributor license agreements.  See the NOTICE file\n",
                "# distributed with this work for additional information\n",
                "# regarding copyright ownership.  The ASF licenses this file\n",
                "# to you under the Apache License, Version 2.0 (the\n",
                "# \"License\"); you may not use this file except in compliance\n",
                "# with the License.  You may obtain a copy of the License at\n",
                "#\n",
                "#   http://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing,\n",
                "# software distributed under the License is distributed on an\n",
                "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
                "# KIND, either express or implied.  See the License for the\n",
                "# specific language governing permissions and limitations\n",
                "# under the License.\n",
                "from __future__ import annotations\n",
                "\n",
                "from unittest import mock\n",
                "\n",
                "import boto3\n",
                "import pytest\n",
                "\n",
                "from airflow.exceptions import AirflowException, TaskDeferred\n",
                "from airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\n",
                "from airflow.providers.amazon.aws.operators.redshift_cluster import (\n",
                "    RedshiftCreateClusterOperator,\n",
                "    RedshiftCreateClusterSnapshotOperator,\n",
                "    RedshiftDeleteClusterOperator,\n",
                "    RedshiftDeleteClusterSnapshotOperator,\n",
                "    RedshiftPauseClusterOperator,\n",
                "    RedshiftResumeClusterOperator,\n",
                ")\n",
                "from airflow.providers.amazon.aws.triggers.redshift_cluster import RedshiftClusterTrigger\n",
                "\n",
                "\n",
                "class TestRedshiftCreateClusterOperator:\n",
                "    def test_init(self):\n",
                "        redshift_operator = RedshiftCreateClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            node_type=\"dc2.large\",\n",
                "            master_username=\"adminuser\",\n",
                "            master_user_password=\"Test123$\",\n",
                "        )\n",
                "        assert redshift_operator.task_id == \"task_test\"\n",
                "        assert redshift_operator.cluster_identifier == \"test_cluster\"\n",
                "        assert redshift_operator.node_type == \"dc2.large\"\n",
                "        assert redshift_operator.master_username == \"adminuser\"\n",
                "        assert redshift_operator.master_user_password == \"Test123$\"\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_create_single_node_cluster(self, mock_get_conn):\n",
                "        redshift_operator = RedshiftCreateClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test-cluster\",\n",
                "            node_type=\"dc2.large\",\n",
                "            master_username=\"adminuser\",\n",
                "            master_user_password=\"Test123$\",\n",
                "            cluster_type=\"single-node\",\n",
                "            wait_for_completion=True,\n",
                "        )\n",
                "        redshift_operator.execute(None)\n",
                "        params = {\n",
                "            \"DBName\": \"dev\",\n",
                "            \"ClusterType\": \"single-node\",\n",
                "            \"AutomatedSnapshotRetentionPeriod\": 1,\n",
                "            \"ClusterVersion\": \"1.0\",\n",
                "            \"AllowVersionUpgrade\": True,\n",
                "            \"PubliclyAccessible\": True,\n",
                "            \"Port\": 5439,\n",
                "        }\n",
                "        mock_get_conn.return_value.create_cluster.assert_called_once_with(\n",
                "            ClusterIdentifier=\"test-cluster\",\n",
                "            NodeType=\"dc2.large\",\n",
                "            MasterUsername=\"adminuser\",\n",
                "            MasterUserPassword=\"Test123$\",\n",
                "            **params,\n",
                "        )\n",
                "\n",
                "        # wait_for_completion is True so check waiter is called\n",
                "        mock_get_conn.return_value.get_waiter.return_value.wait.assert_called_once_with(\n",
                "            ClusterIdentifier=\"test-cluster\", WaiterConfig={\"Delay\": 60, \"MaxAttempts\": 5}\n",
                "        )\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_create_multi_node_cluster(self, mock_get_conn):\n",
                "        redshift_operator = RedshiftCreateClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test-cluster\",\n",
                "            node_type=\"dc2.large\",\n",
                "            number_of_nodes=3,\n",
                "            master_username=\"adminuser\",\n",
                "            master_user_password=\"Test123$\",\n",
                "            cluster_type=\"multi-node\",\n",
                "        )\n",
                "        redshift_operator.execute(None)\n",
                "        params = {\n",
                "            \"DBName\": \"dev\",\n",
                "            \"ClusterType\": \"multi-node\",\n",
                "            \"NumberOfNodes\": 3,\n",
                "            \"AutomatedSnapshotRetentionPeriod\": 1,\n",
                "            \"ClusterVersion\": \"1.0\",\n",
                "            \"AllowVersionUpgrade\": True,\n",
                "            \"PubliclyAccessible\": True,\n",
                "            \"Port\": 5439,\n",
                "        }\n",
                "        mock_get_conn.return_value.create_cluster.assert_called_once_with(\n",
                "            ClusterIdentifier=\"test-cluster\",\n",
                "            NodeType=\"dc2.large\",\n",
                "            MasterUsername=\"adminuser\",\n",
                "            MasterUserPassword=\"Test123$\",\n",
                "            **params,\n",
                "        )\n",
                "\n",
                "        # wait_for_completion is False so check waiter is not called\n",
                "        mock_get_conn.return_value.get_waiter.assert_not_called()\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_create_cluster_deferrable(self, mock_get_conn):\n",
                "        redshift_operator = RedshiftCreateClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test-cluster\",\n",
                "            node_type=\"dc2.large\",\n",
                "            master_username=\"adminuser\",\n",
                "            master_user_password=\"Test123$\",\n",
                "            cluster_type=\"single-node\",\n",
                "            deferrable=True,\n",
                "        )\n",
                "\n",
                "        with pytest.raises(TaskDeferred):\n",
                "            redshift_operator.execute(None)\n",
                "\n",
                "\n",
                "class TestRedshiftCreateClusterSnapshotOperator:\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.cluster_status\")\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_create_cluster_snapshot_is_called_when_cluster_is_available(\n",
                "        self, mock_get_conn, mock_cluster_status\n",
                "    ):\n",
                "        mock_cluster_status.return_value = \"available\"\n",
                "        create_snapshot = RedshiftCreateClusterSnapshotOperator(\n",
                "            task_id=\"test_snapshot\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            snapshot_identifier=\"test_snapshot\",\n",
                "            retention_period=1,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "            tags=[\n",
                    "                {\n",
                    "                    \"Key\": \"user\",\n",
                    "                    \"Value\": \"airflow\",\n",
                    "                }\n",
                    "            ],\n"
                ],
                "parent_version_range": {
                    "start": 146,
                    "end": 146
                },
                "child_version_range": {
                    "start": 146,
                    "end": 152
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "TestRedshiftCreateClusterSnapshotOperator",
                        "signature": "class TestRedshiftCreateClusterSnapshotOperator:",
                        "at_line": 134
                    },
                    {
                        "type": "function",
                        "name": "test_create_cluster_snapshot_is_called_when_cluster_is_available",
                        "signature": "def test_create_cluster_snapshot_is_called_when_cluster_is_available(\n        self, mock_get_conn, mock_cluster_status\n    ):",
                        "at_line": 137
                    },
                    {
                        "type": "call",
                        "name": "RedshiftCreateClusterSnapshotOperator",
                        "signature": "RedshiftCreateClusterSnapshotOperator(\n            task_id=\"test_snapshot\",\n            cluster_identifier=\"test_cluster\",\n            snapshot_identifier=\"test_snapshot\",\n            retention_period=1,\n        )",
                        "at_line": 141,
                        "argument": "retention_period=..."
                    }
                ],
                "idx": 8,
                "hunk_diff": "File: tests/providers/amazon/aws/operators/test_redshift_cluster.py\nCode:\n           class TestRedshiftCreateClusterSnapshotOperator:\n               ...\n               def test_create_cluster_snapshot_is_called_when_cluster_is_available(\n        self, mock_get_conn, mock_cluster_status\n    ):\n                   ...\n                   RedshiftCreateClusterSnapshotOperator(\n            task_id=\"test_snapshot\",\n            cluster_identifier=\"test_cluster\",\n            snapshot_identifier=\"test_snapshot\",\n            retention_period=1,\n        )\n                       ...\n143 143                cluster_identifier=\"test_cluster\",\n144 144                snapshot_identifier=\"test_snapshot\",\n145 145                retention_period=1,\n    146  +             tags=[\n    147  +                 {\n    148  +                     \"Key\": \"user\",\n    149  +                     \"Value\": \"airflow\",\n    150  +                 }\n    151  +             ],\n146 152            )\n147 153            create_snapshot.execute(None)\n148 154            mock_get_conn.return_value.create_cluster_snapshot.assert_called_once_with(\n         ...\n",
                "file_path": "tests/providers/amazon/aws/operators/test_redshift_cluster.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "tags"
                ],
                "prefix": [
                    "            cluster_identifier=\"test_cluster\",\n",
                    "            snapshot_identifier=\"test_snapshot\",\n",
                    "            retention_period=1,\n"
                ],
                "suffix": [
                    "        )\n",
                    "        create_snapshot.execute(None)\n",
                    "        mock_get_conn.return_value.create_cluster_snapshot.assert_called_once_with(\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "tags",
                            "position": {
                                "start": {
                                    "line": 146,
                                    "column": 12
                                },
                                "end": {
                                    "line": 146,
                                    "column": 16
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/airflow/tests/providers/amazon/aws/operators/test_redshift_cluster.py",
                            "hunk_idx": 8,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    9
                ]
            },
            [
                "        )\n",
                "        create_snapshot.execute(None)\n",
                "        mock_get_conn.return_value.create_cluster_snapshot.assert_called_once_with(\n",
                "            ClusterIdentifier=\"test_cluster\",\n",
                "            SnapshotIdentifier=\"test_snapshot\",\n",
                "            ManualSnapshotRetentionPeriod=1,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "            Tags=[\n",
                    "                {\n",
                    "                    \"Key\": \"user\",\n",
                    "                    \"Value\": \"airflow\",\n",
                    "                }\n",
                    "            ],\n"
                ],
                "parent_version_range": {
                    "start": 152,
                    "end": 152
                },
                "child_version_range": {
                    "start": 158,
                    "end": 164
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "TestRedshiftCreateClusterSnapshotOperator",
                        "signature": "class TestRedshiftCreateClusterSnapshotOperator:",
                        "at_line": 134
                    },
                    {
                        "type": "function",
                        "name": "test_create_cluster_snapshot_is_called_when_cluster_is_available",
                        "signature": "def test_create_cluster_snapshot_is_called_when_cluster_is_available(\n        self, mock_get_conn, mock_cluster_status\n    ):",
                        "at_line": 137
                    },
                    {
                        "type": "call",
                        "name": "mock_get_conn.return_value.create_cluster_snapshot.assert_called_once_with",
                        "signature": "mock_get_conn.return_value.create_cluster_snapshot.assert_called_once_with(\n            ClusterIdentifier=\"test_cluster\",\n            SnapshotIdentifier=\"test_snapshot\",\n            ManualSnapshotRetentionPeriod=1,\n        )",
                        "at_line": 148,
                        "argument": "ManualSnapshotRetentionPeriod=..."
                    }
                ],
                "idx": 9,
                "hunk_diff": "File: tests/providers/amazon/aws/operators/test_redshift_cluster.py\nCode:\n           class TestRedshiftCreateClusterSnapshotOperator:\n               ...\n               def test_create_cluster_snapshot_is_called_when_cluster_is_available(\n        self, mock_get_conn, mock_cluster_status\n    ):\n                   ...\n                   mock_get_conn.return_value.create_cluster_snapshot.assert_called_once_with(\n            ClusterIdentifier=\"test_cluster\",\n            SnapshotIdentifier=\"test_snapshot\",\n            ManualSnapshotRetentionPeriod=1,\n        )\n                       ...\n149 155                ClusterIdentifier=\"test_cluster\",\n150 156                SnapshotIdentifier=\"test_snapshot\",\n151 157                ManualSnapshotRetentionPeriod=1,\n    158  +             Tags=[\n    159  +                 {\n    160  +                     \"Key\": \"user\",\n    161  +                     \"Value\": \"airflow\",\n    162  +                 }\n    163  +             ],\n152 164            )\n153 165    \n154 166            mock_get_conn.return_value.get_waiter.assert_not_called()\n         ...\n",
                "file_path": "tests/providers/amazon/aws/operators/test_redshift_cluster.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "Tags"
                ],
                "prefix": [
                    "            ClusterIdentifier=\"test_cluster\",\n",
                    "            SnapshotIdentifier=\"test_snapshot\",\n",
                    "            ManualSnapshotRetentionPeriod=1,\n"
                ],
                "suffix": [
                    "        )\n",
                    "\n",
                    "        mock_get_conn.return_value.get_waiter.assert_not_called()\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    8
                ]
            },
            [
                "        )\n",
                "\n",
                "        mock_get_conn.return_value.get_waiter.assert_not_called()\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.cluster_status\")\n",
                "    def test_raise_exception_when_cluster_is_not_available(self, mock_cluster_status):\n",
                "        mock_cluster_status.return_value = \"paused\"\n",
                "        create_snapshot = RedshiftCreateClusterSnapshotOperator(\n",
                "            task_id=\"test_snapshot\", cluster_identifier=\"test_cluster\", snapshot_identifier=\"test_snapshot\"\n",
                "        )\n",
                "        with pytest.raises(AirflowException):\n",
                "            create_snapshot.execute(None)\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.cluster_status\")\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_create_cluster_snapshot_with_wait(self, mock_get_conn, mock_cluster_status):\n",
                "        mock_cluster_status.return_value = \"available\"\n",
                "        create_snapshot = RedshiftCreateClusterSnapshotOperator(\n",
                "            task_id=\"test_snapshot\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            snapshot_identifier=\"test_snapshot\",\n",
                "            wait_for_completion=True,\n",
                "        )\n",
                "        create_snapshot.execute(None)\n",
                "        mock_get_conn.return_value.get_waiter.return_value.wait.assert_called_once_with(\n",
                "            ClusterIdentifier=\"test_cluster\",\n",
                "            WaiterConfig={\"Delay\": 15, \"MaxAttempts\": 20},\n",
                "        )\n",
                "\n",
                "\n",
                "class TestRedshiftDeleteClusterSnapshotOperator:\n",
                "    @mock.patch(\n",
                "        \"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_cluster_snapshot_status\"\n",
                "    )\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_delete_cluster_snapshot_wait(self, mock_get_conn, mock_get_cluster_snapshot_status):\n",
                "        mock_get_cluster_snapshot_status.return_value = None\n",
                "        delete_snapshot = RedshiftDeleteClusterSnapshotOperator(\n",
                "            task_id=\"test_snapshot\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            snapshot_identifier=\"test_snapshot\",\n",
                "        )\n",
                "        delete_snapshot.execute(None)\n",
                "        mock_get_conn.return_value.delete_cluster_snapshot.assert_called_once_with(\n",
                "            SnapshotClusterIdentifier=\"test_cluster\",\n",
                "            SnapshotIdentifier=\"test_snapshot\",\n",
                "        )\n",
                "\n",
                "        mock_get_cluster_snapshot_status.assert_called_once_with(\n",
                "            snapshot_identifier=\"test_snapshot\",\n",
                "        )\n",
                "\n",
                "    @mock.patch(\n",
                "        \"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_cluster_snapshot_status\"\n",
                "    )\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_delete_cluster_snapshot(self, mock_get_conn, mock_get_cluster_snapshot_status):\n",
                "        delete_snapshot = RedshiftDeleteClusterSnapshotOperator(\n",
                "            task_id=\"test_snapshot\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            snapshot_identifier=\"test_snapshot\",\n",
                "            wait_for_completion=False,\n",
                "        )\n",
                "        delete_snapshot.execute(None)\n",
                "        mock_get_conn.return_value.delete_cluster_snapshot.assert_called_once_with(\n",
                "            SnapshotClusterIdentifier=\"test_cluster\",\n",
                "            SnapshotIdentifier=\"test_snapshot\",\n",
                "        )\n",
                "\n",
                "        mock_get_cluster_snapshot_status.assert_not_called()\n",
                "\n",
                "\n",
                "class TestResumeClusterOperator:\n",
                "    def test_init(self):\n",
                "        redshift_operator = RedshiftResumeClusterOperator(\n",
                "            task_id=\"task_test\", cluster_identifier=\"test_cluster\", aws_conn_id=\"aws_conn_test\"\n",
                "        )\n",
                "        assert redshift_operator.task_id == \"task_test\"\n",
                "        assert redshift_operator.cluster_identifier == \"test_cluster\"\n",
                "        assert redshift_operator.aws_conn_id == \"aws_conn_test\"\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_resume_cluster_is_called_when_cluster_is_paused(self, mock_get_conn):\n",
                "        redshift_operator = RedshiftResumeClusterOperator(\n",
                "            task_id=\"task_test\", cluster_identifier=\"test_cluster\", aws_conn_id=\"aws_conn_test\"\n",
                "        )\n",
                "        redshift_operator.execute(None)\n",
                "        mock_get_conn.return_value.resume_cluster.assert_called_once_with(ClusterIdentifier=\"test_cluster\")\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.conn\")\n",
                "    @mock.patch(\"time.sleep\", return_value=None)\n",
                "    def test_resume_cluster_multiple_attempts(self, mock_sleep, mock_conn):\n",
                "        exception = boto3.client(\"redshift\").exceptions.InvalidClusterStateFault({}, \"test\")\n",
                "        returned_exception = type(exception)\n",
                "\n",
                "        mock_conn.exceptions.InvalidClusterStateFault = returned_exception\n",
                "        mock_conn.resume_cluster.side_effect = [exception, exception, True]\n",
                "        redshift_operator = RedshiftResumeClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            aws_conn_id=\"aws_conn_test\",\n",
                "        )\n",
                "        redshift_operator.execute(None)\n",
                "        assert mock_conn.resume_cluster.call_count == 3\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.conn\")\n",
                "    @mock.patch(\"time.sleep\", return_value=None)\n",
                "    def test_resume_cluster_multiple_attempts_fail(self, mock_sleep, mock_conn):\n",
                "        exception = boto3.client(\"redshift\").exceptions.InvalidClusterStateFault({}, \"test\")\n",
                "        returned_exception = type(exception)\n",
                "\n",
                "        mock_conn.exceptions.InvalidClusterStateFault = returned_exception\n",
                "        mock_conn.resume_cluster.side_effect = exception\n",
                "\n",
                "        redshift_operator = RedshiftResumeClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            aws_conn_id=\"aws_conn_test\",\n",
                "        )\n",
                "        with pytest.raises(returned_exception):\n",
                "            redshift_operator.execute(None)\n",
                "        assert mock_conn.resume_cluster.call_count == 10\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.cluster_status\")\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftAsyncHook.resume_cluster\")\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftAsyncHook.get_client_async\")\n",
                "    def test_resume_cluster(self, mock_async_client, mock_async_resume_cluster, mock_sync_cluster_status):\n",
                "        \"\"\"Test Resume cluster operator run\"\"\"\n",
                "        mock_sync_cluster_status.return_value = \"paused\"\n",
                "        mock_async_client.return_value.resume_cluster.return_value = {\n",
                "            \"Cluster\": {\"ClusterIdentifier\": \"test_cluster\", \"ClusterStatus\": \"resuming\"}\n",
                "        }\n",
                "        mock_async_resume_cluster.return_value = {\"status\": \"success\", \"cluster_state\": \"available\"}\n",
                "\n",
                "        redshift_operator = RedshiftResumeClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            aws_conn_id=\"aws_conn_test\",\n",
                "            deferrable=True,\n",
                "        )\n",
                "\n",
                "        with pytest.raises(TaskDeferred) as exc:\n",
                "            redshift_operator.execute({})\n",
                "\n",
                "        assert isinstance(\n",
                "            exc.value.trigger, RedshiftClusterTrigger\n",
                "        ), \"Trigger is not a RedshiftClusterTrigger\"\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.cluster_status\")\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftAsyncHook.resume_cluster\")\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftAsyncHook.get_client_async\")\n",
                "    def test_resume_cluster_failure(\n",
                "        self, mock_async_client, mock_async_resume_cluster, mock_sync_cluster_statue\n",
                "    ):\n",
                "        \"\"\"Test Resume cluster operator Failure\"\"\"\n",
                "        mock_sync_cluster_statue.return_value = \"paused\"\n",
                "        mock_async_client.return_value.resume_cluster.return_value = {\n",
                "            \"Cluster\": {\"ClusterIdentifier\": \"test_cluster\", \"ClusterStatus\": \"resuming\"}\n",
                "        }\n",
                "        mock_async_resume_cluster.return_value = {\"status\": \"success\", \"cluster_state\": \"available\"}\n",
                "\n",
                "        redshift_operator = RedshiftResumeClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            aws_conn_id=\"aws_conn_test\",\n",
                "            deferrable=True,\n",
                "        )\n",
                "\n",
                "        with pytest.raises(AirflowException):\n",
                "            redshift_operator.execute_complete(\n",
                "                context=None, event={\"status\": \"error\", \"message\": \"test failure message\"}\n",
                "            )\n",
                "\n",
                "\n",
                "class TestPauseClusterOperator:\n",
                "    def test_init(self):\n",
                "        redshift_operator = RedshiftPauseClusterOperator(\n",
                "            task_id=\"task_test\", cluster_identifier=\"test_cluster\", aws_conn_id=\"aws_conn_test\"\n",
                "        )\n",
                "        assert redshift_operator.task_id == \"task_test\"\n",
                "        assert redshift_operator.cluster_identifier == \"test_cluster\"\n",
                "        assert redshift_operator.aws_conn_id == \"aws_conn_test\"\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_pause_cluster_is_called_when_cluster_is_available(self, mock_get_conn):\n",
                "        redshift_operator = RedshiftPauseClusterOperator(\n",
                "            task_id=\"task_test\", cluster_identifier=\"test_cluster\", aws_conn_id=\"aws_conn_test\"\n",
                "        )\n",
                "        redshift_operator.execute(None)\n",
                "        mock_get_conn.return_value.pause_cluster.assert_called_once_with(ClusterIdentifier=\"test_cluster\")\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.conn\")\n",
                "    @mock.patch(\"time.sleep\", return_value=None)\n",
                "    def test_pause_cluster_multiple_attempts(self, mock_sleep, mock_conn):\n",
                "        exception = boto3.client(\"redshift\").exceptions.InvalidClusterStateFault({}, \"test\")\n",
                "        returned_exception = type(exception)\n",
                "\n",
                "        mock_conn.exceptions.InvalidClusterStateFault = returned_exception\n",
                "        mock_conn.pause_cluster.side_effect = [exception, exception, True]\n",
                "\n",
                "        redshift_operator = RedshiftPauseClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            aws_conn_id=\"aws_conn_test\",\n",
                "        )\n",
                "\n",
                "        redshift_operator.execute(None)\n",
                "        assert mock_conn.pause_cluster.call_count == 3\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.conn\")\n",
                "    @mock.patch(\"time.sleep\", return_value=None)\n",
                "    def test_pause_cluster_multiple_attempts_fail(self, mock_sleep, mock_conn):\n",
                "        exception = boto3.client(\"redshift\").exceptions.InvalidClusterStateFault({}, \"test\")\n",
                "        returned_exception = type(exception)\n",
                "\n",
                "        mock_conn.exceptions.InvalidClusterStateFault = returned_exception\n",
                "        mock_conn.pause_cluster.side_effect = exception\n",
                "\n",
                "        redshift_operator = RedshiftPauseClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            aws_conn_id=\"aws_conn_test\",\n",
                "        )\n",
                "        with pytest.raises(returned_exception):\n",
                "            redshift_operator.execute(None)\n",
                "        assert mock_conn.pause_cluster.call_count == 10\n",
                "\n",
                "    def test_pause_cluster_deferrable_mode(self):\n",
                "        \"\"\"Test Pause cluster operator with defer when deferrable param is true\"\"\"\n",
                "\n",
                "        redshift_operator = RedshiftPauseClusterOperator(\n",
                "            task_id=\"task_test\", cluster_identifier=\"test_cluster\", deferrable=True\n",
                "        )\n",
                "\n",
                "        with pytest.raises(TaskDeferred) as exc:\n",
                "            redshift_operator.execute(context=None)\n",
                "\n",
                "        assert isinstance(\n",
                "            exc.value.trigger, RedshiftClusterTrigger\n",
                "        ), \"Trigger is not a RedshiftClusterTrigger\"\n",
                "\n",
                "    def test_pause_cluster_execute_complete_success(self):\n",
                "        \"\"\"Asserts that logging occurs as expected\"\"\"\n",
                "        task = RedshiftPauseClusterOperator(\n",
                "            task_id=\"task_test\", cluster_identifier=\"test_cluster\", deferrable=True\n",
                "        )\n",
                "        with mock.patch.object(task.log, \"info\") as mock_log_info:\n",
                "            task.execute_complete(context=None, event={\"status\": \"success\"})\n",
                "        mock_log_info.assert_called_with(\"Paused cluster successfully\")\n",
                "\n",
                "    def test_pause_cluster_execute_complete_fail(self):\n",
                "        redshift_operator = RedshiftPauseClusterOperator(\n",
                "            task_id=\"task_test\", cluster_identifier=\"test_cluster\", deferrable=True\n",
                "        )\n",
                "\n",
                "        with pytest.raises(AirflowException):\n",
                "            redshift_operator.execute_complete(\n",
                "                context=None, event={\"status\": \"error\", \"message\": \"test failure message\"}\n",
                "            )\n",
                "\n",
                "\n",
                "class TestDeleteClusterOperator:\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.cluster_status\")\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_delete_cluster_with_wait_for_completion(self, mock_get_conn, mock_cluster_status):\n",
                "        mock_cluster_status.return_value = \"cluster_not_found\"\n",
                "        redshift_operator = RedshiftDeleteClusterOperator(\n",
                "            task_id=\"task_test\", cluster_identifier=\"test_cluster\", aws_conn_id=\"aws_conn_test\"\n",
                "        )\n",
                "        redshift_operator.execute(None)\n",
                "        mock_get_conn.return_value.delete_cluster.assert_called_once_with(\n",
                "            ClusterIdentifier=\"test_cluster\",\n",
                "            SkipFinalClusterSnapshot=True,\n",
                "            FinalClusterSnapshotIdentifier=\"\",\n",
                "        )\n",
                "\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.get_conn\")\n",
                "    def test_delete_cluster_without_wait_for_completion(self, mock_get_conn):\n",
                "        redshift_operator = RedshiftDeleteClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            aws_conn_id=\"aws_conn_test\",\n",
                "            wait_for_completion=False,\n",
                "        )\n",
                "        redshift_operator.execute(None)\n",
                "        mock_get_conn.return_value.delete_cluster.assert_called_once_with(\n",
                "            ClusterIdentifier=\"test_cluster\",\n",
                "            SkipFinalClusterSnapshot=True,\n",
                "            FinalClusterSnapshotIdentifier=\"\",\n",
                "        )\n",
                "\n",
                "        mock_get_conn.return_value.cluster_status.assert_not_called()\n",
                "\n",
                "    @mock.patch.object(RedshiftHook, \"delete_cluster\")\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.conn\")\n",
                "    @mock.patch(\"time.sleep\", return_value=None)\n",
                "    def test_delete_cluster_multiple_attempts(self, _, mock_conn, mock_delete_cluster):\n",
                "        exception = boto3.client(\"redshift\").exceptions.InvalidClusterStateFault({}, \"test\")\n",
                "        returned_exception = type(exception)\n",
                "        mock_conn.exceptions.InvalidClusterStateFault = returned_exception\n",
                "        mock_delete_cluster.side_effect = [exception, exception, True]\n",
                "\n",
                "        redshift_operator = RedshiftDeleteClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            aws_conn_id=\"aws_conn_test\",\n",
                "            wait_for_completion=False,\n",
                "        )\n",
                "        redshift_operator.execute(None)\n",
                "\n",
                "        assert mock_delete_cluster.call_count == 3\n",
                "\n",
                "    @mock.patch.object(RedshiftHook, \"delete_cluster\")\n",
                "    @mock.patch(\"airflow.providers.amazon.aws.hooks.redshift_cluster.RedshiftHook.conn\")\n",
                "    @mock.patch(\"time.sleep\", return_value=None)\n",
                "    def test_delete_cluster_multiple_attempts_fail(self, _, mock_conn, mock_delete_cluster):\n",
                "        exception = boto3.client(\"redshift\").exceptions.InvalidClusterStateFault({}, \"test\")\n",
                "        returned_exception = type(exception)\n",
                "        mock_conn.exceptions.InvalidClusterStateFault = returned_exception\n",
                "        mock_delete_cluster.side_effect = exception\n",
                "\n",
                "        redshift_operator = RedshiftDeleteClusterOperator(\n",
                "            task_id=\"task_test\",\n",
                "            cluster_identifier=\"test_cluster\",\n",
                "            aws_conn_id=\"aws_conn_test\",\n",
                "            wait_for_completion=False,\n",
                "        )\n",
                "        with pytest.raises(returned_exception):\n",
                "            redshift_operator.execute(None)\n",
                "\n",
                "        assert mock_delete_cluster.call_count == 10"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "bi-directional",
            "reason": "semantic sync"
        },
        {
            "edit_hunk_pair": [
                0,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "def-use"
        },
        {
            "edit_hunk_pair": [
                0,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "def-use"
        },
        {
            "edit_hunk_pair": [
                0,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "def-use"
        },
        {
            "edit_hunk_pair": [
                0,
                8
            ],
            "edit_order": "bi-directional",
            "reason": "def and test param"
        },
        {
            "edit_hunk_pair": [
                0,
                9
            ],
            "edit_order": "bi-directional",
            "reason": "def and test assertion"
        },
        {
            "edit_hunk_pair": [
                1,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "semantic sync of new argument"
        },
        {
            "edit_hunk_pair": [
                1,
                3
            ],
            "edit_order": "no relation",
            "reason": "semantically less strong tied"
        },
        {
            "edit_hunk_pair": [
                2,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "data flow"
        },
        {
            "edit_hunk_pair": [
                1,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        },
        {
            "edit_hunk_pair": [
                4,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "semantic sync of new argument"
        },
        {
            "edit_hunk_pair": [
                4,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "semantic sync of new argument"
        },
        {
            "edit_hunk_pair": [
                4,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "semantic sync of new argument"
        },
        {
            "edit_hunk_pair": [
                5,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                5,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                6,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                8,
                9
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        }
    ]
}