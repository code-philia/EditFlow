{
    "language": "python",
    "commit_url": "https://github.com/jax-ml/jax/commit/b1b4915c1caaf39309b88bd3f8492778fb3aeadb",
    "commit_message": "remove opaque dtype aval translation to MLIR types\n\nWe already have a mapping from opaquely-dtyped avals to basic\n\"physical\" avals, and we can map the latter to MLIR types.",
    "commit_snapshots": {
        "jax/_src/lax/lax.py": [
            [
                "# Copyright 2018 The JAX Authors.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     https://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\n",
                "import builtins\n",
                "import enum\n",
                "import functools\n",
                "from functools import partial\n",
                "import itertools\n",
                "import operator\n",
                "from typing import (Any, Callable, Optional, Sequence, Tuple, List, Dict,\n",
                "                    TypeVar, Union, cast as type_cast, overload)\n",
                "import warnings\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "import jax\n",
                "from jax._src import core\n",
                "from jax._src import ad_util\n",
                "from jax._src import api\n",
                "from jax._src import api_util\n",
                "from jax._src import array\n",
                "from jax._src import device_array\n",
                "from jax._src import dispatch\n",
                "from jax._src import linear_util as lu\n",
                "from jax._src import dtypes\n",
                "from jax import tree_util\n",
                "from jax._src import source_info_util\n",
                "from jax._src.sharding import PmapSharding\n",
                "from jax._src.config import config\n",
                "from jax._src.core import (Primitive, UnshapedArray, ShapedArray, ConcreteArray,\n",
                "                           raise_to_shaped, abstract_token, canonicalize_shape)\n",
                "from jax._src.abstract_arrays import array_types\n",
                "from jax.interpreters import partial_eval as pe\n",
                "from jax.interpreters import mlir\n",
                "from jax.interpreters import xla\n",
                "from jax.interpreters import pxla\n",
                "from jax.interpreters import ad\n",
                "from jax.interpreters import batching\n",
                "from jax.interpreters.batching import ConcatAxis\n",
                "import jax._src.pretty_printer as pp\n",
                "from jax._src import util\n",
                "from jax._src.util import (cache, prod, safe_zip, safe_map, canonicalize_axis,\n",
                "                           split_list)\n",
                "from jax.tree_util import tree_map\n",
                "from jax._src.lib import pytree\n",
                "from jax._src.lib import xla_bridge\n",
                "from jax._src.lib import xla_client\n",
                "from jax._src.lib.mlir import ir\n",
                "from jax._src.lib.mlir.dialects import chlo\n",
                "from jax._src.lib.mlir.dialects import hlo\n",
                "from jax._src.lax.utils import (\n",
                "  _input_dtype,\n",
                "  standard_abstract_eval,\n",
                "  standard_multi_result_abstract_eval,\n",
                "  standard_named_shape_rule,\n",
                "  standard_primitive,\n",
                "  standard_translate,\n",
                ")\n",
                "from jax._src.lax import slicing\n",
                "from jax._src.typing import Array, ArrayLike, DTypeLike, Shape\n",
                "\n",
                "xb = xla_bridge\n",
                "xc = xla_client\n",
                "xops = xla_client.ops\n",
                "xe = xla_client._xla\n",
                "\n",
                "_max = builtins.max\n",
                "_min = builtins.min\n",
                "_reduce = functools.reduce\n",
                "\n",
                "T = TypeVar(\"T\")\n",
                "\n",
                "map, unsafe_map = safe_map, map\n",
                "zip, unsafe_zip = safe_zip, zip\n",
                "\n",
                "def _validate_shapes(shapes: Sequence[Shape]):\n",
                "  def _check_static_shape(shape: Shape):\n",
                "    checked = canonicalize_shape(shape)\n",
                "    if not all(idx >= 0 for idx in checked):\n",
                "      msg = f\"Only non-negative indices are allowed when broadcasting\" \\\n",
                "            f\" static shapes, but got shape {shape!r}.\"\n",
                "      raise TypeError(msg)\n",
                "\n",
                "  assert shapes\n",
                "  if config.jax_dynamic_shapes:\n",
                "    # pass dynamic shapes through unchecked\n",
                "    return\n",
                "  else:\n",
                "    map(_check_static_shape, shapes)\n",
                "\n",
                "def _try_broadcast_shapes(\n",
                "    shapes: Sequence[Tuple[int, ...]]) -> Optional[Tuple[int, ...]]:\n",
                "  if len(shapes) == 1: return shapes[0]\n",
                "  rank, *others = {len(shape) for shape in shapes}\n",
                "  if others: return None  # must have consistent rank\n",
                "  if not rank: return ()  # scalar case\n",
                "  result_shape = []\n",
                "  for ds in unsafe_zip(*shapes):\n",
                "    if all(core.same_referent(d, ds[0]) for d in ds[1:]):\n",
                "      # if all axes are identical objects, the resulting size is the object\n",
                "      result_shape.append(ds[0])\n",
                "    else:\n",
                "      # if all dims are equal (or 1), the result is the non-1 size (or 1)\n",
                "      non_1s = [d for d in ds if not core.symbolic_equal_dim(d, 1)]\n",
                "      if not non_1s:\n",
                "        result_shape.append(1)\n",
                "      elif all(core.symbolic_equal_dim(non_1s[0], d) for d in non_1s[1:]):\n",
                "        result_shape.append(non_1s[0])\n",
                "      else:\n",
                "        return None\n",
                "  return tuple(result_shape)\n",
                "\n",
                "@overload\n",
                "def broadcast_shapes(*shapes: Tuple[int, ...]) -> Tuple[int, ...]: ...\n",
                "\n",
                "@overload\n",
                "def broadcast_shapes(*shapes: Tuple[Union[int, core.Tracer], ...]\n",
                "                     ) -> Tuple[Union[int, core.Tracer], ...]: ...\n",
                "\n",
                "def broadcast_shapes(*shapes):\n",
                "  \"\"\"Returns the shape that results from NumPy broadcasting of `shapes`.\"\"\"\n",
                "  # NOTE: We have both cached and uncached versions to handle Tracers in shapes.\n",
                "  try:\n",
                "    return _broadcast_shapes_cached(*shapes)\n",
                "  except:\n",
                "    return _broadcast_shapes_uncached(*shapes)\n",
                "\n",
                "@cache()\n",
                "def _broadcast_shapes_cached(*shapes: Tuple[int, ...]) -> Tuple[int, ...]:\n",
                "  return _broadcast_shapes_uncached(*shapes)\n",
                "\n",
                "def _broadcast_shapes_uncached(*shapes):\n",
                "  _validate_shapes(shapes)\n",
                "  fst, *rst = shapes\n",
                "  if not rst: return fst\n",
                "\n",
                "  # First check if we need only rank promotion (and not singleton-broadcasting).\n",
                "  try: return _reduce(_broadcast_ranks, rst, fst)\n",
                "  except ValueError: pass\n",
                "\n",
                "  # Next try singleton-broadcasting, padding out ranks using singletons.\n",
                "  ndim = _max(len(shape) for shape in shapes)\n",
                "  shape_list = [(1,) * (ndim - len(shape)) + shape for shape in shapes]\n",
                "  result_shape = _try_broadcast_shapes(shape_list)\n",
                "  if result_shape is None:\n",
                "    raise ValueError(f\"Incompatible shapes for broadcasting: shapes={list(shapes)}\")\n",
                "  return result_shape\n",
                "\n",
                "def _broadcast_ranks(s1, s2):\n",
                "  if len(s1) > len(s2):\n",
                "    s1, s2 = s2, s1\n",
                "  assert len(s1) <= len(s2)\n",
                "  s1_ = s2[len(s2) - len(s1):]\n",
                "  if core.symbolic_equal_shape(s1_, s1): return s2\n",
                "  else: raise ValueError\n",
                "\n",
                "def _identity(x): return x\n",
                "\n",
                "def _extract_tracers_dyn_shape(\n",
                "    shape: Sequence[Union[int, core.Tracer]]\n",
                "  ) -> Tuple[List[core.Tracer], List[Optional[int]]]:\n",
                "  # Given a sequence representing a shape, pull out Tracers, replacing with None\n",
                "  if config.jax_dynamic_shapes:\n",
                "    # We must gate this behavior under a flag because otherwise the errors\n",
                "    # raised are different (and have worse source provenance information).\n",
                "    dyn_shape = [d for d in shape if isinstance(d, core.Tracer)]\n",
                "    static_shape = [None if isinstance(d, core.Tracer) else d for d in shape]\n",
                "    return dyn_shape, static_shape\n",
                "  else:\n",
                "    return [], list(shape)  # type: ignore\n",
                "\n",
                "def _merge_dyn_shape(\n",
                "    static_shape: Sequence[Optional[int]],\n",
                "    dyn_shape: Sequence[Any],\n",
                "  ) -> Tuple[Union[int, mlir.Value], ...]:\n",
                "  # Replace Nones in static_shape with elements of dyn_shape, in order\n",
                "  dyn_shape_it = iter(dyn_shape)\n",
                "  shape = tuple(next(dyn_shape_it) if d is None else d for d in static_shape)\n",
                "  assert next(dyn_shape_it, None) is None\n",
                "  return shape\n",
                "\n",
                "def _dyn_shape_staging_rule(trace, prim, out_aval, *args, **params):\n",
                "  source_info = source_info_util.current()\n",
                "  out_tracer = pe.DynamicJaxprTracer(trace, out_aval, source_info)\n",
                "  eqn = pe.new_jaxpr_eqn([trace.getvar(x) for x in args],\n",
                "                         [trace.makevar(out_tracer)],\n",
                "                         prim, params, core.no_effects, source_info)\n",
                "  trace.frame.add_eqn(eqn)\n",
                "  return out_tracer\n",
                "\n",
                "\n",
                "### traceables\n",
                "\n",
                "def neg(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise negation: :math:`-x`.\"\"\"\n",
                "  return neg_p.bind(x)\n",
                "\n",
                "def sign(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise sign.\n",
                "\n",
                "  For floating-point inputs, returns\n",
                "  :math:`\\mathrm{sign}(x) = \\begin{cases}\n",
                "  -1 & x < 0\\\\\n",
                "  -0 & x = -0\\\\\n",
                "  \\mathit{NaN} & x = \\mathit{NaN}\\\\\n",
                "  +0 & x = +0\\\\\n",
                "  1 & x > 0\n",
                "  \\end{cases}`\n",
                "\n",
                "  For signed integer inputs, returns\n",
                "  :math:`\\mathrm{sign}(x) = \\begin{cases}\n",
                "  -1 & x < 0\\\\\n",
                "  0 & x = 0\\\\\n",
                "  1 & x > 0\n",
                "  \\end{cases}`\n",
                "\n",
                "  For complex inputs, returns the complex phase, i.e.\n",
                "  :math:`\\mathrm{sign}(x) = \\frac{x}{|x|}`.\n",
                "  \"\"\"\n",
                "  return sign_p.bind(x)\n",
                "\n",
                "def nextafter(x1: ArrayLike, x2: ArrayLike) -> Array:\n",
                "  r\"\"\"Returns the next representable value after `x1` in the direction of `x2`.\n",
                "\n",
                "  Note that in some environments flush-denormal-to-zero semantics is used.\n",
                "  This means that, around zero, this function returns strictly non-zero\n",
                "  values which appear as zero in any operations. Consider this example::\n",
                "\n",
                "    >>> jnp.nextafter(0, 1)  # denormal numbers are representable\n",
                "    Array(1.e-45, dtype=float32, weak_type=True)\n",
                "    >>> jnp.nextafter(0, 1) * 1  # but are flushed to zero\n",
                "    Array(0., dtype=float32, weak_type=True)\n",
                "\n",
                "  For the smallest usable (i.e. normal) float, use ``tiny`` of ``jnp.finfo``.\n",
                "  \"\"\"\n",
                "  return nextafter_p.bind(x1, x2)\n",
                "\n",
                "def floor(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise floor: :math:`\\left\\lfloor x \\right\\rfloor`.\"\"\"\n",
                "  return floor_p.bind(x)\n",
                "\n",
                "def ceil(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise ceiling: :math:`\\left\\lceil x \\right\\rceil`.\"\"\"\n",
                "  return ceil_p.bind(x)\n",
                "\n",
                "class RoundingMethod(enum.IntEnum):\n",
                "  AWAY_FROM_ZERO = 0\n",
                "  TO_NEAREST_EVEN = 1\n",
                "\n",
                "def round(x: ArrayLike,\n",
                "          rounding_method: RoundingMethod = RoundingMethod.AWAY_FROM_ZERO\n",
                "          ) -> Array:\n",
                "  r\"\"\"Elementwise round.\n",
                "\n",
                "  Rounds values to the nearest integer.\n",
                "\n",
                "  Args:\n",
                "    x: an array or scalar value to round.\n",
                "    rounding_method: the method to use when rounding halfway values\n",
                "      (e.g., `0.5`). See ``lax.RoundingMethod`` for the list of possible\n",
                "      values.\n",
                "\n",
                "  Returns:\n",
                "    An array containing the elementwise rounding of x.\n",
                "  \"\"\"\n",
                "  rounding_method = RoundingMethod(rounding_method)\n",
                "  return round_p.bind(x, rounding_method=rounding_method)\n",
                "\n",
                "def is_finite(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise :math:`\\mathrm{isfinite}`.\n",
                "\n",
                "  For each element x returns `True` if and only if x is not :math:`\\pm\\infty` or\n",
                "  :math:`\\mathit{NaN}`.\n",
                "  \"\"\"\n",
                "  return is_finite_p.bind(x)\n",
                "\n",
                "def exp(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise exponential: :math:`e^x`.\"\"\"\n",
                "  return exp_p.bind(x)\n",
                "\n",
                "def expm1(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise :math:`e^{x} - 1`.\"\"\"\n",
                "  return expm1_p.bind(x)\n",
                "\n",
                "def log(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise natural logarithm: :math:`\\mathrm{log}(x)`.\"\"\"\n",
                "  return log_p.bind(x)\n",
                "\n",
                "def log1p(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise :math:`\\mathrm{log}(1 + x)`.\"\"\"\n",
                "  return log1p_p.bind(x)\n",
                "\n",
                "def tanh(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise hyperbolic tangent: :math:`\\mathrm{tanh}(x)`.\"\"\"\n",
                "  return tanh_p.bind(x)\n",
                "\n",
                "def logistic(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise logistic (sigmoid) function: :math:`\\frac{1}{1 + e^{-x}}`.\"\"\"\n",
                "  return logistic_p.bind(x)\n",
                "\n",
                "def sin(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise sine: :math:`\\mathrm{sin}(x)`.\"\"\"\n",
                "  return sin_p.bind(x)\n",
                "\n",
                "def cos(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise cosine: :math:`\\mathrm{cos}(x)`.\"\"\"\n",
                "  return cos_p.bind(x)\n",
                "\n",
                "def atan2(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise arc tangent of two variables:\n",
                "    :math:`\\mathrm{atan}({x \\over y})`.\"\"\"\n",
                "  return atan2_p.bind(x, y)\n",
                "\n",
                "def betainc(a: ArrayLike, b: ArrayLike, x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise regularized incomplete beta integral.\"\"\"\n",
                "  return regularized_incomplete_beta_p.bind(a, b, x)\n",
                "\n",
                "def lgamma(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise log gamma: :math:`\\mathrm{log}(\\Gamma(x))`.\"\"\"\n",
                "  return lgamma_p.bind(x)\n",
                "\n",
                "def digamma(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise digamma: :math:`\\psi(x)`.\"\"\"\n",
                "  return digamma_p.bind(x)\n",
                "\n",
                "def igamma(a: ArrayLike, x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise regularized incomplete gamma function.\"\"\"\n",
                "  return igamma_p.bind(a, x)\n",
                "\n",
                "def igammac(a: ArrayLike, x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise complementary regularized incomplete gamma function.\"\"\"\n",
                "  return igammac_p.bind(a, x)\n",
                "\n",
                "def igamma_grad_a(a: ArrayLike, x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise derivative of the regularized incomplete gamma function.\"\"\"\n",
                "  return igamma_grad_a_p.bind(a, x)\n",
                "\n",
                "def random_gamma_grad(a: ArrayLike, x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise derivative of samples from `Gamma(a, 1)`.\"\"\"\n",
                "  return random_gamma_grad_p.bind(a, x)\n",
                "\n",
                "def bessel_i0e(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Exponentially scaled modified Bessel function of order 0:\n",
                "  :math:`\\mathrm{i0e}(x) = e^{-|x|} \\mathrm{i0}(x)`\n",
                "  \"\"\"\n",
                "  return bessel_i0e_p.bind(x)\n",
                "\n",
                "def bessel_i1e(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Exponentially scaled modified Bessel function of order 1:\n",
                "  :math:`\\mathrm{i1e}(x) = e^{-|x|} \\mathrm{i1}(x)`\n",
                "  \"\"\"\n",
                "  return bessel_i1e_p.bind(x)\n",
                "\n",
                "def erf(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise error function: :math:`\\mathrm{erf}(x)`.\"\"\"\n",
                "  return erf_p.bind(x)\n",
                "\n",
                "def erfc(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise complementary error function:\n",
                "    :math:`\\mathrm{erfc}(x) = 1 - \\mathrm{erf}(x)`.\"\"\"\n",
                "  return erfc_p.bind(x)\n",
                "\n",
                "def erf_inv(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise inverse error function: :math:`\\mathrm{erf}^{-1}(x)`.\"\"\"\n",
                "  return erf_inv_p.bind(x)\n",
                "\n",
                "def real(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise extract real part: :math:`\\mathrm{Re}(x)`.\n",
                "\n",
                "  Returns the real part of a complex number.\n",
                "  \"\"\"\n",
                "  return real_p.bind(x)\n",
                "\n",
                "def imag(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise extract imaginary part: :math:`\\mathrm{Im}(x)`.\n",
                "\n",
                "  Returns the imaginary part of a complex number.\n",
                "  \"\"\"\n",
                "  return imag_p.bind(x)\n",
                "\n",
                "def complex(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise make complex number: :math:`x + jy`.\n",
                "\n",
                "  Builds a complex number from real and imaginary parts.\n",
                "  \"\"\"\n",
                "  return complex_p.bind(x, y)\n",
                "\n",
                "def conj(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise complex conjugate function: :math:`\\overline{x}`.\"\"\"\n",
                "  return conj_p.bind(x, input_dtype=_dtype(x))\n",
                "\n",
                "def abs(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise absolute value: :math:`|x|`.\"\"\"\n",
                "  return abs_p.bind(x)\n",
                "\n",
                "def pow(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise power: :math:`x^y`.\"\"\"\n",
                "  return pow_p.bind(x, y)\n",
                "\n",
                "def integer_pow(x: ArrayLike, y: int) -> Array:\n",
                "  r\"\"\"Elementwise power: :math:`x^y`, where :math:`y` is a fixed integer.\"\"\"\n",
                "  return integer_pow_p.bind(x, y=y)\n",
                "\n",
                "def sqrt(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise square root: :math:`\\sqrt{x}`.\"\"\"\n",
                "  return sqrt_p.bind(x)\n",
                "\n",
                "def rsqrt(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise reciprocal square root:  :math:`1 \\over \\sqrt{x}`.\"\"\"\n",
                "  return rsqrt_p.bind(x)\n",
                "\n",
                "def cbrt(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise cube root: :math:`\\sqrt[3]{x}`.\"\"\"\n",
                "  return cbrt_p.bind(x)\n",
                "\n",
                "def bitwise_not(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise NOT: :math:`\\neg x`.\"\"\"\n",
                "  return not_p.bind(x)\n",
                "\n",
                "def bitwise_and(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise AND: :math:`x \\wedge y`.\"\"\"\n",
                "  return and_p.bind(x, y)\n",
                "\n",
                "def bitwise_or(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise OR: :math:`x \\vee y`.\"\"\"\n",
                "  return or_p.bind(x, y)\n",
                "\n",
                "def bitwise_xor(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise exclusive OR: :math:`x \\oplus y`.\"\"\"\n",
                "  return xor_p.bind(x, y)\n",
                "\n",
                "def population_count(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise popcount, count the number of set bits in each element.\"\"\"\n",
                "  return population_count_p.bind(x)\n",
                "\n",
                "def clz(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise count-leading-zeros.\"\"\"\n",
                "  return clz_p.bind(x)\n",
                "\n",
                "def add(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise addition: :math:`x + y`.\"\"\"\n",
                "  return add_p.bind(x, y)\n",
                "\n",
                "def sub(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise subtraction: :math:`x - y`.\"\"\"\n",
                "  return sub_p.bind(x, y)\n",
                "\n",
                "def mul(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise multiplication: :math:`x \\times y`.\"\"\"\n",
                "  return mul_p.bind(x, y)\n",
                "\n",
                "def div(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise division: :math:`x \\over y`.\n",
                "\n",
                "  Integer division overflow\n",
                "  (division by zero or signed division of INT_SMIN with -1)\n",
                "  produces an implementation defined value.\n",
                "  \"\"\"\n",
                "  return div_p.bind(x, y)\n",
                "\n",
                "def rem(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise remainder: :math:`x \\bmod y`.\n",
                "\n",
                "  The sign of the result is taken from the dividend,\n",
                "  and the absolute value of the result is always\n",
                "  less than the divisor's absolute value.\n",
                "\n",
                "  Integer division overflow\n",
                "  (remainder by zero or remainder of INT_SMIN with -1)\n",
                "  produces an implementation defined value.\n",
                "  \"\"\"\n",
                "  return rem_p.bind(x, y)\n",
                "\n",
                "def max(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise maximum: :math:`\\mathrm{max}(x, y)`\n",
                "\n",
                "  For complex numbers, uses a lexicographic comparison on the\n",
                "  `(real, imaginary)` pairs.\"\"\"\n",
                "  return max_p.bind(x, y)\n",
                "\n",
                "def min(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise minimum:  :math:`\\mathrm{min}(x, y)`\n",
                "\n",
                "  For complex numbers, uses a lexicographic comparison on the\n",
                "  `(real, imaginary)` pairs.\"\"\"\n",
                "  return min_p.bind(x, y)\n",
                "\n",
                "def shift_left(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise left shift: :math:`x \\ll y`.\"\"\"\n",
                "  return shift_left_p.bind(x, y)\n",
                "\n",
                "def shift_right_arithmetic(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise arithmetic right shift: :math:`x \\gg y`.\"\"\"\n",
                "  return shift_right_arithmetic_p.bind(x, y)\n",
                "\n",
                "def shift_right_logical(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise logical right shift: :math:`x \\gg y`.\"\"\"\n",
                "  return shift_right_logical_p.bind(x, y)\n",
                "\n",
                "def eq(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise equals: :math:`x = y`.\"\"\"\n",
                "  return eq_p.bind(x, y)\n",
                "\n",
                "def ne(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise not-equals: :math:`x \\neq y`.\"\"\"\n",
                "  return ne_p.bind(x, y)\n",
                "\n",
                "def ge(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise greater-than-or-equals: :math:`x \\geq y`.\"\"\"\n",
                "  return ge_p.bind(x, y)\n",
                "\n",
                "def gt(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise greater-than: :math:`x > y`.\"\"\"\n",
                "  return gt_p.bind(x, y)\n",
                "\n",
                "def le(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise less-than-or-equals: :math:`x \\leq y`.\"\"\"\n",
                "  return le_p.bind(x, y)\n",
                "\n",
                "def lt(x: ArrayLike, y: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise less-than: :math:`x < y`.\"\"\"\n",
                "  return lt_p.bind(x, y)\n",
                "\n",
                "def convert_element_type(operand: ArrayLike, new_dtype: DTypeLike) -> Array:\n",
                "  \"\"\"Elementwise cast.\n",
                "\n",
                "  Wraps XLA's `ConvertElementType\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#convertelementtype>`_\n",
                "  operator, which performs an elementwise conversion from one type to another.\n",
                "  Similar to a C++ `static_cast`.\n",
                "\n",
                "  Args:\n",
                "    operand: an array or scalar value to be cast\n",
                "    new_dtype: a NumPy dtype representing the target type.\n",
                "\n",
                "  Returns:\n",
                "    An array with the same shape as `operand`, cast elementwise to `new_dtype`.\n",
                "  \"\"\"\n",
                "  if hasattr(operand, '__jax_array__'):\n",
                "    operand = operand.__jax_array__()  # type: ignore\n",
                "  return _convert_element_type(operand, new_dtype, weak_type=False)\n",
                "\n",
                "def _convert_element_type(operand: ArrayLike, new_dtype: Optional[DTypeLike] = None,\n",
                "                          weak_type: bool = False):\n",
                "  if (core.is_opaque_dtype(new_dtype) or\n",
                "      core.is_opaque_dtype(getattr(operand, 'dtype', None))):\n",
                "    return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n",
                "                                       weak_type=bool(weak_type))\n",
                "\n",
                "  # Don't canonicalize old_dtype because x64 context might cause\n",
                "  # un-canonicalized operands to be passed in.\n",
                "  old_dtype = dtypes.dtype(operand, canonicalize=False)\n",
                "  old_weak_type = dtypes.is_weakly_typed(operand)\n",
                "  if new_dtype is None:\n",
                "    new_dtype = old_dtype\n",
                "  else:\n",
                "    new_dtype = np.dtype(new_dtype)\n",
                "  new_dtype = dtypes.dtype(new_dtype, canonicalize=True)\n",
                "\n",
                "  if (dtypes.issubdtype(old_dtype, np.complexfloating) and\n",
                "      not dtypes.issubdtype(new_dtype, np.complexfloating)):\n",
                "    msg = \"Casting complex values to real discards the imaginary part\"\n",
                "    warnings.warn(msg, np.ComplexWarning, stacklevel=2)\n",
                "\n",
                "  # Python has big integers, but convert_element_type(2 ** 100, np.float32) need\n",
                "  # not be an error since the target dtype fits the value. Handle this case by\n",
                "  # converting to a NumPy array before calling bind. Without this step, we'd\n",
                "  # first canonicalize the input to a value of dtype int32 or int64, leading to\n",
                "  # an overflow error.\n",
                "  if type(operand) is int:\n",
                "    operand = np.asarray(operand, new_dtype)\n",
                "    old_weak_type = False\n",
                "\n",
                "  if (old_dtype, old_weak_type) == (new_dtype, weak_type) and isinstance(operand, Array):\n",
                "    return type_cast(Array, operand)\n",
                "  else:\n",
                "    return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n",
                "                                       weak_type=bool(weak_type))\n",
                "\n",
                "def bitcast_convert_type(operand: ArrayLike, new_dtype: DTypeLike) -> Array:\n",
                "  \"\"\"Elementwise bitcast.\n",
                "\n",
                "  Wraps XLA's `BitcastConvertType\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#bitcastconverttype>`_\n",
                "  operator, which performs a bit cast from one type to another. The bitwidth\n",
                "  of the source and destination types must match.\n",
                "\n",
                "  Args:\n",
                "    operand: an array or scalar value to be cast\n",
                "    new_dtype: the new type. Should be a NumPy type.\n",
                "\n",
                "  Returns:\n",
                "    An array with the same shape as `operand`, bitcast elementwise to\n",
                "    `new_dtype`.\n",
                "  \"\"\"\n",
                "  new_dtype = dtypes.canonicalize_dtype(new_dtype)\n",
                "  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)\n",
                "\n",
                "def clamp(min: ArrayLike, x: ArrayLike, max: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise clamp.\n",
                "\n",
                "  Returns :math:`\\mathrm{clamp}(x) = \\begin{cases}\n",
                "  \\mathit{min} & \\text{if } x < \\mathit{min},\\\\\n",
                "  \\mathit{max} & \\text{if } x > \\mathit{max},\\\\\n",
                "  x & \\text{otherwise}\n",
                "  \\end{cases}`.\n",
                "  \"\"\"\n",
                "  return clamp_p.bind(min, x, max)\n",
                "\n",
                "def concatenate(operands: Union[Array, Sequence[ArrayLike]], dimension: int) -> Array:\n",
                "  \"\"\"Concatenates a sequence of arrays along `dimension`.\n",
                "\n",
                "  Wraps XLA's `Concatenate\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#concatenate>`_\n",
                "  operator.\n",
                "\n",
                "  Args:\n",
                "    operands: a sequence of arrays to concatenate. The arrays must have equal\n",
                "      shapes, except in the `dimension` axis.\n",
                "    dimension: the dimension along which to concatenate the arrays.\n",
                "\n",
                "  Returns:\n",
                "    An array containing the concatenation.\n",
                "  \"\"\"\n",
                "  if len(operands) == 0:\n",
                "    raise ValueError(\"concatenate requires a non-empty sequences of arrays\")\n",
                "  if len(operands) == 1:\n",
                "    op, = operands\n",
                "    if isinstance(op, Array):\n",
                "      return type_cast(Array, op)\n",
                "  return concatenate_p.bind(*operands, dimension=dimension)\n",
                "\n",
                "\n",
                "class _enum_descriptor:\n",
                "  def __init__(self, val):\n",
                "    self.val = val\n",
                "  def __get__(self, _, owner):\n",
                "    return owner(self.val)\n",
                "\n",
                "\n",
                "class Precision(xla_client.PrecisionConfig.Precision):  # type: ignore\n",
                "  \"\"\"Precision enum for lax functions\n",
                "\n",
                "  The `precision` argument to JAX functions generally controls the tradeoff\n",
                "  between speed and accuracy for array computations on accelerator backends,\n",
                "  (i.e. TPU and GPU). Members are:\n",
                "\n",
                "  DEFAULT:\n",
                "    Fastest mode, but least accurate. Performs computations in bfloat16.\n",
                "    Aliases: ``'default'``, ``'fastest'``, ``'bfloat16'``.\n",
                "  HIGH:\n",
                "    Slower but more accurate. Performs float32 computations in 3 bfloat16\n",
                "    passes, or using tensorfloat32 where available. Aliases: ``'high'``,\n",
                "    ``'bfloat16_3x'``, ``'tensorfloat32'``.\n",
                "  HIGHEST:\n",
                "    Slowest but most accurate. Performs computations in float32 or float64\n",
                "    as applicable. Aliases: ``'highest'``, ``'float32'``.\n",
                "  \"\"\"\n",
                "  # Wrap enum values with this class.\n",
                "  DEFAULT = _enum_descriptor('default')\n",
                "  HIGH = _enum_descriptor('high')\n",
                "  HIGHEST = _enum_descriptor('highest')\n",
                "\n",
                "  _strings = {\n",
                "      'highest':       xla_client.PrecisionConfig.Precision.HIGHEST,\n",
                "      'float32':       xla_client.PrecisionConfig.Precision.HIGHEST,\n",
                "      'high':          xla_client.PrecisionConfig.Precision.HIGH,\n",
                "      'bfloat16_3x':   xla_client.PrecisionConfig.Precision.HIGH,\n",
                "      'tensorfloat32': xla_client.PrecisionConfig.Precision.HIGH,\n",
                "      'default':       xla_client.PrecisionConfig.Precision.DEFAULT,\n",
                "      'bfloat16':      xla_client.PrecisionConfig.Precision.DEFAULT,\n",
                "      'fastest':       xla_client.PrecisionConfig.Precision.DEFAULT,\n",
                "      None:            xla_client.PrecisionConfig.Precision.DEFAULT,\n",
                "  }\n",
                "  def __init__(self, arg0):\n",
                "    arg0 = self._strings.get(arg0, arg0)\n",
                "    super().__init__(arg0)\n",
                "\n",
                "  def __str__(self) -> str:\n",
                "    return self.name\n",
                "\n",
                "\n",
                "PrecisionType = Precision\n",
                "PrecisionLike = Union[None, str, PrecisionType, Tuple[str, str],\n",
                "                      Tuple[PrecisionType, PrecisionType]]\n",
                "\n",
                "def dot(lhs: Array, rhs: Array, precision: PrecisionLike = None,\n",
                "        preferred_element_type: Optional[DTypeLike] = None) -> Array:\n",
                "  \"\"\"Vector/vector, matrix/vector, and matrix/matrix multiplication.\n",
                "\n",
                "  Wraps XLA's `Dot\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#dot>`_\n",
                "  operator.\n",
                "\n",
                "  For more general contraction, see the `dot_general` operator.\n",
                "\n",
                "  Args:\n",
                "    lhs: an array of dimension 1 or 2.\n",
                "    rhs: an array of dimension 1 or 2.\n",
                "    precision: Optional. Either ``None``, which means the default precision for\n",
                "      the backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``,\n",
                "      ``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two\n",
                "      :class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``.\n",
                "    preferred_element_type: Optional. Either ``None``, which means the default\n",
                "      accumulation type for the input types, or a datatype, indicating to\n",
                "      accumulate results to and return a result with that datatype.\n",
                "\n",
                "  Returns:\n",
                "    An array containing the product.\n",
                "  \"\"\"\n",
                "  if 1 <= lhs.ndim <= 2 and 1 <= rhs.ndim <= 2 and core.symbolic_equal_dim(lhs.shape[-1], rhs.shape[0]):\n",
                "    return dot_general(lhs, rhs, (((lhs.ndim - 1,), (0,)), ((), ())),\n",
                "                       precision=precision,\n",
                "                       preferred_element_type=preferred_element_type)\n",
                "  else:\n",
                "    raise TypeError(\"Incompatible shapes for dot: got {} and {}.\".format(\n",
                "        lhs.shape, rhs.shape))\n",
                "\n",
                "\n",
                "DotDimensionNumbers = Tuple[Tuple[Sequence[int], Sequence[int]],\n",
                "                            Tuple[Sequence[int], Sequence[int]]]\n",
                "\n",
                "def dot_general(lhs: ArrayLike, rhs: ArrayLike, dimension_numbers: DotDimensionNumbers,\n",
                "                precision: PrecisionLike = None,\n",
                "                preferred_element_type: Optional[DTypeLike] = None) -> Array:\n",
                "  \"\"\"More general contraction operator.\n",
                "\n",
                "  Wraps XLA's `DotGeneral\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#dotgeneral>`_\n",
                "  operator.\n",
                "\n",
                "  Args:\n",
                "    lhs: an array\n",
                "    rhs: an array\n",
                "    dimension_numbers: a tuple of tuples of the form\n",
                "      `((lhs_contracting_dims, rhs_contracting_dims),\n",
                "      (lhs_batch_dims, rhs_batch_dims))`\n",
                "    precision: Optional. Either ``None``, which means the default precision for\n",
                "      the backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``,\n",
                "      ``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple of two\n",
                "      :class:`~jax.lax.Precision` enums indicating precision of ``lhs``` and ``rhs``.\n",
                "    preferred_element_type: Optional. Either ``None``, which means the default\n",
                "      accumulation type for the input types, or a datatype, indicating to\n",
                "      accumulate results to and return a result with that datatype.\n",
                "\n",
                "  Returns:\n",
                "    An array containing the result.\n",
                "  \"\"\"\n",
                "  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers\n",
                "  cdims = (api_util._ensure_index_tuple(lhs_contract),\n",
                "           api_util._ensure_index_tuple(rhs_contract))\n",
                "  bdims = (api_util._ensure_index_tuple(lhs_batch),\n",
                "           api_util._ensure_index_tuple(rhs_batch))\n",
                "  preferred_element_type = (\n",
                "      None if preferred_element_type is None else\n",
                "      dtypes.canonicalize_dtype(np.dtype(preferred_element_type)))\n",
                "  return dot_general_p.bind(lhs, rhs,\n",
                "                            dimension_numbers=(cdims, bdims),\n",
                "                            precision=canonicalize_precision(precision),\n",
                "                            preferred_element_type=preferred_element_type)\n",
                "\n",
                "def broadcast(operand: ArrayLike, sizes: Sequence[int]) -> Array:\n",
                "  \"\"\"Broadcasts an array, adding new leading dimensions\n",
                "\n",
                "  Args:\n",
                "    operand: an array\n",
                "    sizes: a sequence of integers, giving the sizes of new leading dimensions\n",
                "      to add to the front of the array.\n",
                "\n",
                "  Returns:\n",
                "    An array containing the result.\n",
                "\n",
                "  See Also:\n",
                "    jax.lax.broadcast_in_dim : add new dimensions at any location in the array shape.\n",
                "  \"\"\"\n",
                "  dims = tuple(range(len(sizes), len(sizes) + np.ndim(operand)))\n",
                "  return broadcast_in_dim(operand, tuple(sizes) + np.shape(operand), dims)\n",
                "\n",
                "def broadcast_in_dim(operand: ArrayLike, shape: Shape,\n",
                "                     broadcast_dimensions: Sequence[int]) -> Array:\n",
                "  \"\"\"Wraps XLA's `BroadcastInDim\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#broadcastindim>`_\n",
                "  operator.\n",
                "\n",
                "  Args:\n",
                "    operand: an array\n",
                "    shape: the shape of the target array\n",
                "    broadcast_dimensions: to which dimension in the target shape each dimension\n",
                "      of the operand shape corresponds to\n",
                "\n",
                "  Returns:\n",
                "    An array containing the result.\n",
                "\n",
                "  See Also:\n",
                "    jax.lax.broadcast : simpler interface to add new leading dimensions.\n",
                "  \"\"\"\n",
                "  if np.ndim(operand) == len(shape) and not len(broadcast_dimensions) and isinstance(operand, Array):\n",
                "    return type_cast(Array, operand)\n",
                "  if config.jax_dynamic_shapes:\n",
                "    # We must gate this behavior under a flag because otherwise the errors\n",
                "    # raised are different (and have worse source provenance information).\n",
                "    dyn_shape, static_shape = _extract_tracers_dyn_shape(shape)\n",
                "  else:\n",
                "    dyn_shape, static_shape = [], shape  # type: ignore\n",
                "  return broadcast_in_dim_p.bind(\n",
                "      operand, *dyn_shape, shape=tuple(static_shape),\n",
                "      broadcast_dimensions=tuple(broadcast_dimensions))\n",
                "\n",
                "def broadcast_to_rank(x: Array, rank: int) -> Array:\n",
                "  \"\"\"Adds leading dimensions of ``1`` to give ``x`` rank ``rank``.\"\"\"\n",
                "  return broadcast(x, (1,) * (rank - x.ndim))\n",
                "\n",
                "def reshape(operand: ArrayLike, new_sizes: Shape,\n",
                "            dimensions: Optional[Sequence[int]] = None) -> Array:\n",
                "  \"\"\"Wraps XLA's `Reshape\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#reshape>`_\n",
                "  operator.\n",
                "\n",
                "  For inserting/removing dimensions of size 1, prefer using ``lax.squeeze`` /\n",
                "  ``lax.expand_dims``. These preserve information about axis identity that may\n",
                "  be useful for advanced transformation rules.\n",
                "\n",
                "  Args:\n",
                "    operand: array to be reshaped.\n",
                "    new_sizes: sequence of integers specifying the resulting shape. The size\n",
                "      of the final array must match the size of the input.\n",
                "    dimensions: optional sequence of integers specifying the permutation order of\n",
                "      the input shape. If specified, the length must match ``operand.shape``.\n",
                "\n",
                "  Returns:\n",
                "    out: reshaped array.\n",
                "\n",
                "  Examples:\n",
                "    Simple reshaping from one to two dimensions:\n",
                "\n",
                "    >>> x = jnp.arange(6)\n",
                "    >>> y = reshape(x, (2, 3))\n",
                "    >>> y\n",
                "    Array([[0, 1, 2],\n",
                "                 [3, 4, 5]], dtype=int32)\n",
                "\n",
                "    Reshaping back to one dimension:\n",
                "\n",
                "    >>> reshape(y, (6,))\n",
                "    Array([0, 1, 2, 3, 4, 5], dtype=int32)\n",
                "\n",
                "    Reshaping to one dimension with permutation of dimensions:\n",
                "\n",
                "    >>> reshape(y, (6,), (1, 0))\n",
                "    Array([0, 3, 1, 4, 2, 5], dtype=int32)\n",
                "  \"\"\"\n",
                "  new_sizes = canonicalize_shape(new_sizes)  # TODO\n",
                "  new_sizes = tuple(new_sizes)\n",
                "  same_shape = core.symbolic_equal_shape(np.shape(operand), new_sizes)\n",
                "  if dimensions is None:\n",
                "    same_dims = True\n",
                "    dims = None\n",
                "  else:\n",
                "    dims = api_util._ensure_index_tuple(dimensions)\n",
                "    same_dims = tuple(dims) == tuple(range(np.ndim(operand)))\n",
                "  if np.shape(operand) and same_shape and same_dims and isinstance(operand, Array):\n",
                "    return type_cast(Array, operand)\n",
                "  else:\n",
                "    dyn_shape, static_new_sizes = _extract_tracers_dyn_shape(new_sizes)\n",
                "\n",
                "    return reshape_p.bind(\n",
                "      operand, *dyn_shape, new_sizes=tuple(static_new_sizes),\n",
                "      dimensions=None if dims is None or same_dims else dims)\n",
                "\n",
                "def pad(operand: ArrayLike, padding_value: ArrayLike,\n",
                "        padding_config: Sequence[Tuple[int, int, int]]) -> Array:\n",
                "  \"\"\"Applies low, high, and/or interior padding to an array.\n",
                "\n",
                "  Wraps XLA's `Pad\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#pad>`_\n",
                "  operator.\n",
                "\n",
                "  Args:\n",
                "    operand: an array to be padded.\n",
                "    padding_value: the value to be inserted as padding. Must have the same dtype\n",
                "      as ``operand``.\n",
                "    padding_config: a sequence of ``(low, high, interior)`` tuples of integers,\n",
                "      giving the amount of low, high, and interior (dilation) padding to insert\n",
                "      in each dimension.\n",
                "\n",
                "  Returns:\n",
                "    The ``operand`` array with padding value ``padding_value`` inserted in each\n",
                "    dimension according to the ``padding_config``.\n",
                "  \"\"\"\n",
                "  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))\n",
                "\n",
                "def rev(operand: ArrayLike, dimensions: Sequence[int]) -> Array:\n",
                "  \"\"\"Wraps XLA's `Rev\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#rev_reverse>`_\n",
                "  operator.\n",
                "  \"\"\"\n",
                "  return rev_p.bind(operand, dimensions=tuple(dimensions))\n",
                "\n",
                "def select(pred: ArrayLike, on_true: ArrayLike, on_false: ArrayLike) -> Array:\n",
                "  \"\"\"Selects between two branches based on a boolean predicate.\n",
                "\n",
                "  Wraps XLA's `Select\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#select>`_\n",
                "  operator.\n",
                "\n",
                "  In general :func:`~jax.lax.select` leads to evaluation of both branches, although\n",
                "  the compiler may elide computations if possible. For a similar function that\n",
                "  usually evaluates only a single branch, see :func:`~jax.lax.cond`.\n",
                "\n",
                "  Args:\n",
                "    pred: boolean array\n",
                "    on_true: array containing entries to return where ``pred`` is True. Must have\n",
                "      the same shape as ``pred``, and the same shape and dtype as ``on_false``.\n",
                "    on_false: array containing entries to return where ``pred`` is False. Must have\n",
                "      the same shape as ``pred``, and the same shape and dtype as ``on_true``.\n",
                "\n",
                "  Returns:\n",
                "    result: array with same shape and dtype as ``on_true`` and ``on_false``.\n",
                "  \"\"\"\n",
                "  # Caution! The select_n_p primitive has the *opposite* order of arguments to\n",
                "  # select(). This is because it implements `select_n`.\n",
                "  return select_n_p.bind(pred, on_false, on_true)\n",
                "\n",
                "def select_n(which: ArrayLike, *cases: ArrayLike) -> Array:\n",
                "  \"\"\"Selects array values from multiple cases.\n",
                "\n",
                "  Generalizes XLA's `Select\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#select>`_\n",
                "  operator. Unlike XLA's version, the operator is variadic and can select\n",
                "  from many cases using an integer `pred`.\n",
                "\n",
                "  Args:\n",
                "    which: determines which case should be returned. Must be an array containing\n",
                "      either a boolean or integer values. May either be a scalar or have\n",
                "      shape matching ``cases``. For each array element, the value of ``which``\n",
                "      determines which of ``cases`` is taken. ``which`` must be in the range\n",
                "      ``[0 .. len(cases))``; for values outside that range the behavior is\n",
                "      implementation-defined.\n",
                "    *cases: a non-empty list of array cases. All must have equal dtypes and\n",
                "      equal shapes.\n",
                "  Returns:\n",
                "    An array with shape and dtype equal to the cases, whose values are chosen\n",
                "    according to ``which``.\n",
                "  \"\"\"\n",
                "  if len(cases) == 0:\n",
                "    raise ValueError(\"select_n() must have at least one case\")\n",
                "  return select_n_p.bind(which, *cases)\n",
                "\n",
                "\n",
                "def transpose(operand: ArrayLike, permutation: Sequence[int]) -> Array:\n",
                "  \"\"\"Wraps XLA's `Transpose\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#transpose>`_\n",
                "  operator.\n",
                "  \"\"\"\n",
                "  permutation = tuple(operator.index(d) for d in permutation)\n",
                "  if permutation == tuple(range(np.ndim(operand))) and isinstance(operand, Array):\n",
                "    return type_cast(Array, operand)\n",
                "  else:\n",
                "    return transpose_p.bind(operand, permutation=permutation)\n",
                "\n",
                "def argmin(operand: ArrayLike, axis: int,\n",
                "           index_dtype: DTypeLike) -> Array:\n",
                "  \"\"\"Computes the index of the minimum element along ``axis``.\"\"\"\n",
                "  return argmin_p.bind(operand, axes=(axis,),\n",
                "                       index_dtype=dtypes.canonicalize_dtype(index_dtype))\n",
                "\n",
                "def argmax(operand: ArrayLike, axis: int,\n",
                "           index_dtype: DTypeLike) -> Array:\n",
                "  \"\"\"Computes the index of the maximum element along ``axis``.\"\"\"\n",
                "  return argmax_p.bind(operand, axes=(axis,),\n",
                "                       index_dtype=dtypes.canonicalize_dtype(index_dtype))\n",
                "\n",
                "def reduce(operands: Any,\n",
                "           init_values: Any,\n",
                "           computation: Callable[[Any, Any], Any],\n",
                "           dimensions: Sequence[int]) -> Any:\n",
                "  \"\"\"Wraps XLA's `Reduce\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#reduce>`_\n",
                "  operator.\n",
                "\n",
                "  ``init_values`` and ``computation`` together must form a `monoid\n",
                "  <https://en.wikipedia.org/wiki/Monoid>`_\n",
                "  for correctness. That is ``init_values`` must be an identity of\n",
                "  ``computation``, and ``computation`` must be associative. XLA may exploit both\n",
                "  of these properties during code generation; if either is violated the result\n",
                "  is undefined.\n",
                "  \"\"\"\n",
                "  flat_operands, operand_tree = tree_util.tree_flatten(operands)\n",
                "  flat_init_values, init_value_tree = tree_util.tree_flatten(init_values)\n",
                "  if operand_tree != init_value_tree:\n",
                "    raise ValueError('Operands must have the same tree structure as init_values:'\n",
                "                     f' {operand_tree} vs. {init_value_tree}')\n",
                "  if len(flat_operands) != len(flat_init_values):\n",
                "    raise ValueError('Must have same total number of operands as init_values: '\n",
                "                     f' {len(flat_operands)} vs. {len(flat_init_values)}')\n",
                "  monoid_reducer = _get_monoid_reducer(computation, flat_init_values)\n",
                "  if monoid_reducer:\n",
                "    # monoid reducers bypass the weak_type_rule, so we set it explicitly.\n",
                "    weak_type = dtypes.is_weakly_typed(*flat_operands) and dtypes.is_weakly_typed(*flat_init_values)\n",
                "    return _convert_element_type(monoid_reducer(*flat_operands, dimensions),\n",
                "                                 weak_type=weak_type)\n",
                "  else:\n",
                "    flat_init_avals = safe_map(_abstractify, flat_init_values)\n",
                "    jaxpr, consts, out_tree = _variadic_reduction_jaxpr(\n",
                "        computation, tuple(flat_init_avals), init_value_tree)\n",
                "    out = reduce_p.bind(*flat_operands, *flat_init_values, computation=computation,\n",
                "                        jaxpr=jaxpr, consts=consts, dimensions=tuple(dimensions))\n",
                "    return tree_util.tree_unflatten(out_tree, out)\n",
                "\n",
                "@cache()\n",
                "def _reduction_jaxpr(computation, aval):\n",
                "  @lu.wrap_init\n",
                "  def comp(x, y):\n",
                "    result = computation(x, y)\n",
                "    if not (isinstance(result, core.Tracer) or core.valid_jaxtype(result)):\n",
                "      raise ValueError(\n",
                "          f\"Invalid return type from reduction function: {type(result)}\\n\"\n",
                "          f\"Reduction functions should only return an array.\\n\"\n",
                "          f\"Full return value: {result}\")\n",
                "    return (result,)\n",
                "  jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(comp, (aval, aval))\n",
                "  if any(isinstance(c, core.Tracer) for c in consts):\n",
                "    raise NotImplementedError(\n",
                "        \"Reduction computations can't close over Tracers. Please open an issue \"\n",
                "        \"at https://github.com/google/jax.\")\n",
                "  return jaxpr, tuple(consts)\n",
                "\n",
                "@cache()\n",
                "def _variadic_reduction_jaxpr(computation, flat_avals, aval_tree):\n",
                "  avals = tree_util.tree_unflatten(aval_tree, flat_avals)\n",
                "  flat_in_avals, in_tree = tree_util.tree_flatten((avals, avals))\n",
                "  comp = lu.wrap_init(computation)\n",
                "  flat_comp, out_tree = api_util.flatten_fun_nokwargs(comp, in_tree)\n",
                "  jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(flat_comp, tuple(flat_in_avals))\n",
                "  if any(isinstance(c, core.Tracer) for c in consts):\n",
                "    raise NotImplementedError(\n",
                "        \"Reduction computations can't close over Tracers. Please open an issue \"\n",
                "        \"at https://github.com/google/jax.\")\n",
                "  return jaxpr, tuple(consts), out_tree()\n",
                "\n",
                "def _get_monoid_reducer(monoid_op: Callable,\n",
                "                        xs: Sequence[Array]) -> Optional[Callable]:\n",
                "  if len(xs) != 1:\n",
                "    return None\n",
                "  x, = xs\n",
                "  aval = core.get_aval(x)\n",
                "  dtype = _dtype(x)\n",
                "  if (type(aval) is ConcreteArray) and aval.shape == ():\n",
                "    # allow bitwise reductions for boolean and integer types\n",
                "    _is_intlike = dtype == np.bool_ or dtypes.issubdtype(dtype, np.integer)\n",
                "    if monoid_op is add:\n",
                "      return _reduce_sum if np.equal(aval.val, 0) else None\n",
                "    elif monoid_op is mul:\n",
                "      return _reduce_prod if np.equal(aval.val, 1) else None\n",
                "    elif monoid_op is bitwise_or and _is_intlike:\n",
                "      return _reduce_or if np.equal(aval.val, _get_bitwise_or_identity(dtype)) else None\n",
                "    elif monoid_op is bitwise_and and _is_intlike:\n",
                "      return _reduce_and if np.equal(aval.val, _get_bitwise_and_identity(dtype)) else None\n",
                "    elif monoid_op is bitwise_xor and _is_intlike:\n",
                "      return _reduce_xor if np.equal(aval.val, _get_bitwise_or_identity(dtype)) else None\n",
                "    elif monoid_op is max:\n",
                "      return _reduce_max if np.equal(aval.val, _get_max_identity(dtype)) else None\n",
                "    elif monoid_op is min:\n",
                "      return _reduce_min if np.equal(aval.val, _get_min_identity(dtype)) else None\n",
                "  return None\n",
                "\n",
                "def _get_bitwise_and_identity(dtype: DTypeLike) -> np.ndarray:\n",
                "  return np.array(-1).astype(dtype)\n",
                "\n",
                "def _get_bitwise_or_identity(dtype: DTypeLike) -> np.ndarray:\n",
                "  return np.array(0, dtype)\n",
                "\n",
                "def _get_sum_identity(dtype: DTypeLike) -> np.ndarray:\n",
                "  return np.array(0, dtype)\n",
                "\n",
                "def _get_prod_identity(dtype: DTypeLike) -> np.ndarray:\n",
                "  return np.array(1, dtype)\n",
                "\n",
                "def _get_max_identity(dtype: DTypeLike) -> np.ndarray:\n",
                "  if dtypes.issubdtype(dtype, np.inexact):\n",
                "    return np.array(-np.inf, dtype)\n",
                "  elif dtypes.issubdtype(dtype, np.integer):\n",
                "    return np.array(dtypes.iinfo(dtype).min, dtype)\n",
                "  elif dtypes.issubdtype(dtype, np.bool_):\n",
                "    return np.array(False, np.bool_)\n",
                "  else:\n",
                "    raise ValueError(f\"Unsupported dtype for max: {dtype}\")\n",
                "\n",
                "def _get_min_identity(dtype: DTypeLike) -> np.ndarray:\n",
                "  if dtypes.issubdtype(dtype, np.inexact):\n",
                "    return np.array(np.inf, dtype)\n",
                "  elif dtypes.issubdtype(dtype, np.integer):\n",
                "    return np.array(dtypes.iinfo(dtype).max, dtype)\n",
                "  elif dtypes.issubdtype(dtype, np.bool_):\n",
                "    return np.array(True, np.bool_)\n",
                "  else:\n",
                "    raise ValueError(f\"Unsupported dtype for min: {dtype}\")\n",
                "\n",
                "def _reduce_sum(operand: ArrayLike, axes: Sequence[int]) -> Array:\n",
                "  return reduce_sum_p.bind(operand, axes=tuple(axes))\n",
                "\n",
                "def _reduce_prod(operand: ArrayLike, axes: Sequence[int]) -> Array:\n",
                "  return reduce_prod_p.bind(operand, axes=tuple(axes))\n",
                "\n",
                "def _reduce_max(operand: ArrayLike, axes: Sequence[int]) -> Array:\n",
                "  return reduce_max_p.bind(operand, axes=tuple(axes))\n",
                "\n",
                "def _reduce_min(operand: ArrayLike, axes: Sequence[int]) -> Array:\n",
                "  return reduce_min_p.bind(operand, axes=tuple(axes))\n",
                "\n",
                "def _reduce_or(operand: ArrayLike, axes: Sequence[int]) -> Array:\n",
                "  return reduce_or_p.bind(operand, axes=tuple(axes))\n",
                "\n",
                "def _reduce_and(operand: ArrayLike, axes: Sequence[int]) -> Array:\n",
                "  return reduce_and_p.bind(operand, axes=tuple(axes))\n",
                "\n",
                "def _reduce_xor(operand: ArrayLike, axes: Sequence[int]) -> Array:\n",
                "  return reduce_xor_p.bind(operand, axes=tuple(axes))\n",
                "\n",
                "@overload\n",
                "def sort(operand: Sequence[Array], dimension: int = -1,\n",
                "         is_stable: bool = True, num_keys: int = 1) -> Tuple[Array, ...]: ...\n",
                "\n",
                "@overload\n",
                "def sort(operand: Array, dimension: int = -1,\n",
                "         is_stable: bool = True, num_keys: int = 1) -> Array: ...\n",
                "\n",
                "def sort(operand: Union[Array, Sequence[Array]], dimension: int = -1,\n",
                "         is_stable: bool = True, num_keys: int = 1) -> Union[Array, Tuple[Array, ...]]:\n",
                "  \"\"\"Wraps XLA's `Sort\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#sort>`_ operator.\n",
                "\n",
                "  For floating point inputs, -0.0 and 0.0 are treated as equivalent, and NaN values\n",
                "  are sorted to the end of the array. For complex inputs, the sort order is\n",
                "  lexicographic over the real and imaginary parts, with the real part primary.\n",
                "\n",
                "  Args:\n",
                "    operand : Array or sequence of arrays\n",
                "    dimension : integer dimension along which to sort. Default: -1.\n",
                "    is_stable : boolean specifying whether to use a stable sort. Default: True.\n",
                "    num_keys : number of operands to treat as sort keys. Default: 1.\n",
                "      For num_keys > 1, the sort order will be determined lexicographically using\n",
                "      the first `num_keys` arrays, with the first key being primary.\n",
                "      The remaining operands will be returned with the same permutation.\n",
                "\n",
                "  Returns:\n",
                "    operand : sorted version of the input or inputs.\n",
                "  \"\"\"\n",
                "  if isinstance(operand, Sequence):\n",
                "    if len(operand) == 0:\n",
                "      raise TypeError(\"Sort requires at least one operand\")\n",
                "    if not (1 <= num_keys <= len(operand)):\n",
                "      raise ValueError(f\"{num_keys=} must be between 1 and {len(operand)=}\")\n",
                "    dimension = canonicalize_axis(dimension, len(operand[0].shape))\n",
                "    return tuple(sort_p.bind(*operand, dimension=dimension,\n",
                "                             is_stable=is_stable,\n",
                "                             num_keys=num_keys))\n",
                "  else:\n",
                "    if num_keys != 1:\n",
                "      raise ValueError(f\"{num_keys=} must equal 1 for a single operand.\")\n",
                "    dimension = canonicalize_axis(dimension, len(operand.shape))\n",
                "    return sort_p.bind(operand, dimension=dimension, is_stable=is_stable, num_keys=1)[0]\n",
                "\n",
                "def sort_key_val(keys: Array, values: ArrayLike, dimension: int = -1,\n",
                "                 is_stable: bool = True) -> Tuple[Array, Array]:\n",
                "  \"\"\"Sorts ``keys`` along ``dimension`` and applies the same permutation to ``values``.\"\"\"\n",
                "  dimension = canonicalize_axis(dimension, len(keys.shape))\n",
                "  k, v = sort_p.bind(keys, values, dimension=dimension, is_stable=is_stable, num_keys=1)\n",
                "  return k, v\n",
                "\n",
                "def top_k(operand: ArrayLike, k: int) -> Tuple[Array, Array]:\n",
                "  \"\"\"Returns top ``k`` values and their indices along the last axis of ``operand``.\"\"\"\n",
                "  k = int(k)\n",
                "  if k < 0:\n",
                "    raise ValueError(f\"k argument to top_k must be nonnegative, got {k}\")\n",
                "  return top_k_p.bind(operand, k=k)\n",
                "\n",
                "def tie_in(x: Any, y: T) -> T:\n",
                "  \"\"\"Deprecated. Ignores ``x`` and returns ``y``.\"\"\"\n",
                "  return y\n",
                "\n",
                "def full(shape: Shape, fill_value: ArrayLike, dtype: Optional[DTypeLike] = None) -> Array:\n",
                "  \"\"\"Returns an array of `shape` filled with `fill_value`.\n",
                "\n",
                "  Args:\n",
                "    shape: sequence of integers, describing the shape of the output array.\n",
                "    fill_value: the value to fill the new array with.\n",
                "    dtype: the type of the output array, or `None`. If not `None`, `fill_value`\n",
                "      will be cast to `dtype`.\n",
                "  \"\"\"\n",
                "  shape = canonicalize_shape(shape)\n",
                "  if np.shape(fill_value):\n",
                "    msg = \"full must be called with scalar fill_value, got fill_value.shape {}.\"\n",
                "    raise TypeError(msg.format(np.shape(fill_value)))\n",
                "  weak_type = dtype is None and dtypes.is_weakly_typed(fill_value)\n",
                "  dtype = dtypes.canonicalize_dtype(dtype or _dtype(fill_value))\n",
                "  fill_value = _convert_element_type(fill_value, dtype, weak_type)\n",
                "  return broadcast(fill_value, shape)\n",
                "\n",
                "def zeros_like_shaped_array(aval: ShapedArray) -> Array:\n",
                "  assert isinstance(aval, ShapedArray)\n",
                "  if aval.dtype == dtypes.float0:\n",
                "    scalar_zero = np.zeros((), dtype=aval.dtype)\n",
                "  else:\n",
                "    scalar_zero = _convert_element_type(0, aval.dtype, aval.weak_type)\n",
                "  return broadcast(scalar_zero, aval.shape)\n",
                "\n",
                "ad_util.aval_zeros_likers[ShapedArray] = zeros_like_shaped_array\n",
                "\n",
                "def iota(dtype: DTypeLike, size: int) -> Array:\n",
                "  \"\"\"Wraps XLA's `Iota\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#iota>`_\n",
                "  operator.\n",
                "  \"\"\"\n",
                "  return broadcasted_iota(dtype, (size,), 0)\n",
                "\n",
                "def broadcasted_iota(dtype: DTypeLike, shape: Shape, dimension: int) -> Array:\n",
                "  \"\"\"Convenience wrapper around ``iota``.\"\"\"\n",
                "  dtype = dtypes.canonicalize_dtype(dtype)\n",
                "  shape = canonicalize_shape(shape)\n",
                "  dynamic_shape = [d for d in shape if isinstance(d, core.Tracer)]\n",
                "  static_shape = [None if isinstance(d, core.Tracer) else d for d in shape]\n",
                "  dimension = core.concrete_or_error(\n",
                "      int, dimension, \"dimension argument of lax.broadcasted_iota\")\n",
                "  return iota_p.bind(*dynamic_shape, dtype=dtype, shape=tuple(static_shape),\n",
                "                     dimension=dimension)\n",
                "\n",
                "def _eye(dtype: DTypeLike, shape: Shape, offset: int) -> Array:\n",
                "  \"\"\"Like numpy.eye, create a 2D array with ones on a diagonal.\"\"\"\n",
                "  offset = int(offset)\n",
                "  dtype = dtypes.canonicalize_dtype(dtype)\n",
                "  bool_eye = eq(add(broadcasted_iota(np.int32, shape, 0), np.int32(offset)),\n",
                "                broadcasted_iota(np.int32, shape, 1))\n",
                "  return convert_element_type_p.bind(bool_eye, new_dtype=dtype, weak_type=False)\n",
                "\n",
                "def _delta(dtype: DTypeLike, shape: Shape, axes: Sequence[int]) -> Array:\n",
                "  \"\"\"This utility function exists for creating Kronecker delta arrays.\"\"\"\n",
                "  axes = map(int, axes)\n",
                "  dtype = dtypes.canonicalize_dtype(dtype)\n",
                "  base_shape = tuple(np.take(shape, axes))  # type: ignore[arg-type]\n",
                "  iotas = [broadcasted_iota(np.uint32, base_shape, i)\n",
                "           for i in range(len(base_shape))]\n",
                "  eyes = [eq(i1, i2) for i1, i2 in zip(iotas[:-1], iotas[1:])]\n",
                "  result = convert_element_type_p.bind(_reduce(operator.and_, eyes),\n",
                "                                       new_dtype=dtype, weak_type=False)\n",
                "  return broadcast_in_dim(result, shape, axes)\n",
                "\n",
                "def _tri(dtype: DTypeLike, shape: Shape, offset: int) -> Array:\n",
                "  \"\"\"Like numpy.tri, create a 2D array with ones below a diagonal.\"\"\"\n",
                "  offset = int(offset)\n",
                "  dtype = dtypes.canonicalize_dtype(dtype)\n",
                "  bool_tri = ge(add(broadcasted_iota(np.int32, shape, 0), np.int32(offset)),\n",
                "                broadcasted_iota(np.int32, shape, 1))\n",
                "  return convert_element_type_p.bind(bool_tri, new_dtype=dtype, weak_type=False)\n",
                "\n",
                "def stop_gradient(x: T) -> T:\n",
                "  \"\"\"Stops gradient computation.\n",
                "\n",
                "  Operationally ``stop_gradient`` is the identity function, that is, it returns\n",
                "  argument `x` unchanged. However, ``stop_gradient`` prevents the flow of\n",
                "  gradients during forward or reverse-mode automatic differentiation. If there\n",
                "  are multiple nested gradient computations, ``stop_gradient`` stops gradients\n",
                "  for all of them.\n",
                "\n",
                "  For example:\n",
                "\n",
                "  >>> jax.grad(lambda x: x**2)(3.)\n",
                "  Array(6., dtype=float32, weak_type=True)\n",
                "  >>> jax.grad(lambda x: jax.lax.stop_gradient(x)**2)(3.)\n",
                "  Array(0., dtype=float32, weak_type=True)\n",
                "  >>> jax.grad(jax.grad(lambda x: x**2))(3.)\n",
                "  Array(2., dtype=float32, weak_type=True)\n",
                "  >>> jax.grad(jax.grad(lambda x: jax.lax.stop_gradient(x)**2))(3.)\n",
                "  Array(0., dtype=float32, weak_type=True)\n",
                "  \"\"\"\n",
                "  def stop(x):\n",
                "    # only bind primitive on inexact dtypes, to avoid some staging\n",
                "    if core.has_opaque_dtype(x):\n",
                "      return x\n",
                "    elif (dtypes.issubdtype(_dtype(x), np.floating) or\n",
                "        dtypes.issubdtype(_dtype(x), np.complexfloating)):\n",
                "      return ad_util.stop_gradient_p.bind(x)\n",
                "    else:\n",
                "      return x\n",
                "  return tree_map(stop, x)\n",
                "\n",
                "def reduce_precision(operand: Union[float, ArrayLike],\n",
                "                     exponent_bits: int,\n",
                "                     mantissa_bits: int) -> Array:\n",
                "  \"\"\"Wraps XLA's `ReducePrecision\n",
                "  <https://www.tensorflow.org/xla/operation_semantics#reduceprecision>`_\n",
                "  operator.\n",
                "  \"\"\"\n",
                "  exponent_bits = core.concrete_or_error(\n",
                "    operator.index, exponent_bits, \"exponent_bits argument of lax.reduce_precision\")\n",
                "  mantissa_bits = core.concrete_or_error(\n",
                "    operator.index, mantissa_bits, \"mantissa_bits argument of lax.reduce_precision\")\n",
                "  return reduce_precision_p.bind(operand, exponent_bits=exponent_bits, mantissa_bits=mantissa_bits)\n",
                "\n",
                "def squeeze(array: ArrayLike, dimensions: Sequence[int]) -> Array:\n",
                "  \"\"\"Squeeze any number of size 1 dimensions from an array.\"\"\"\n",
                "  ndim = np.ndim(array)\n",
                "  dimensions = tuple(sorted(canonicalize_axis(i, ndim) for i in dimensions))\n",
                "  if not dimensions and isinstance(array, Array):\n",
                "    return type_cast(Array, array)\n",
                "  return squeeze_p.bind(array, dimensions=dimensions)\n",
                "\n",
                "def expand_dims(array: ArrayLike, dimensions: Sequence[int]) -> Array:\n",
                "  \"\"\"Insert any number of size 1 dimensions into an array.\"\"\"\n",
                "  if len(set(dimensions)) != len(dimensions):\n",
                "    raise ValueError(f'repeated axis in lax.expand_dims: {dimensions}')\n",
                "  ndim_out = np.ndim(array) + len(dimensions)\n",
                "  dims = [canonicalize_axis(i, ndim_out) for i in dimensions]\n",
                "  if len(set(dims)) != len(dims):  # check again after canonicalizing\n",
                "    raise ValueError(f'repeated axis in lax.expand_dims: {dims}')\n",
                "  dims_set = frozenset(dims)\n",
                "  result_shape = list(np.shape(array))\n",
                "  for i in sorted(dims_set):\n",
                "    result_shape.insert(i, 1)\n",
                "  broadcast_dims = [i for i in range(ndim_out) if i not in dims_set]\n",
                "  return broadcast_in_dim(array, result_shape, broadcast_dims)\n",
                "\n",
                "\n",
                "### convenience wrappers around traceables\n",
                "\n",
                "def full_like(x: ArrayLike, fill_value: ArrayLike, dtype: Optional[DTypeLike] = None,\n",
                "              shape: Optional[Shape] = None) -> Array:\n",
                "  \"\"\"Create a full array like np.full based on the example array `x`.\n",
                "\n",
                "  Args:\n",
                "    x: example array-like, used for shape and dtype information.\n",
                "    fill_value: a scalar value to fill the entries of the output array.\n",
                "    dtype: optional, a dtype parameter for the output ndarray.\n",
                "    shape: optional, a shape parameter for the output ndarray.\n",
                "\n",
                "  Returns:\n",
                "    An ndarray with the same shape as `x` with its entries set equal to\n",
                "    `fill_value`, similar to the output of np.full.\n",
                "  \"\"\"\n",
                "  fill_shape = np.shape(x) if shape is None else canonicalize_shape(shape)\n",
                "  weak_type = dtype is None and dtypes.is_weakly_typed(x)\n",
                "  dtype = dtype or _dtype(x)\n",
                "  val = full(fill_shape, _convert_element_type(fill_value, dtype, weak_type))\n",
                "  # If the sharding is SingleDeviceSharding then don't take the `if` branch\n",
                "  # because `val` is already an array with SingleDeviceSharding making this an\n",
                "  # optimization.\n",
                "  # TODO(yashkatariya,mattjj): `x` and `val` should have the same sharding,\n",
                "  # probably in the form of a primitive like `val = match_sharding_p.bind(x, val)`\n",
                "  # (so it works in staged-out code as well as 'eager' code). Related to\n",
                "  # equi-sharding.\n",
                "  if config.jax_array and shape is None and isinstance(x, array.ArrayImpl):\n",
                "    sharding = x.sharding  # type: ignore[union-attr]\n",
                "    if (not dispatch.is_single_device_sharding(sharding) and\n",
                "        not isinstance(sharding, PmapSharding)):\n",
                "      return array.make_array_from_callback(\n",
                "          type_cast(array.Shape, fill_shape), sharding, lambda idx: val[idx])\n",
                "  return val\n",
                "\n",
                "\n",
                "def collapse(operand: Array, start_dimension: int,\n",
                "             stop_dimension: int) -> Array:\n",
                "  \"\"\"Collapses dimensions of an array into a single dimension.\n",
                "\n",
                "  For example, if ``operand`` is an array with shape ``[2, 3, 4]``,\n",
                "  ``collapse(operand, 0, 2).shape == [6, 4]``. The elements of the collapsed\n",
                "  dimension are laid out major-to-minor, i.e., with the lowest-numbered\n",
                "  dimension as the slowest varying dimension.\n",
                "\n",
                "  Args:\n",
                "    operand: an input array.\n",
                "    start_dimension: the start of the dimensions to collapse (inclusive).\n",
                "    stop_dimension: the end of the dimensions to collapse (exclusive).\n",
                "\n",
                "  Returns:\n",
                "    An array where dimensions ``[start_dimension, stop_dimension)`` have been\n",
                "    collapsed (raveled) into a single dimension.\n",
                "  \"\"\"\n",
                "  lo, hi = start_dimension, stop_dimension\n",
                "  size = prod(operand.shape[lo:hi])\n",
                "  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]\n",
                "  return reshape(operand, new_shape)\n",
                "\n",
                "\n",
                "def batch_matmul(lhs: Array, rhs: Array,\n",
                "                 precision: PrecisionLike = None) -> Array:\n",
                "  \"\"\"Batch matrix multiplication.\"\"\"\n",
                "  if _min(lhs.ndim, rhs.ndim) < 2:\n",
                "    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'\n",
                "                     .format(lhs.ndim, rhs.ndim))\n",
                "  if lhs.ndim != rhs.ndim:\n",
                "    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'\n",
                "                     .format(lhs.ndim, rhs.ndim))\n",
                "  lhs_contract = (lhs.ndim - 1,)\n",
                "  rhs_contract = (rhs.ndim - 2,)\n",
                "  batch = tuple(range(lhs.ndim - 2))\n",
                "  return dot_general(lhs, rhs, ((lhs_contract, rhs_contract), (batch, batch)),\n",
                "                     precision=precision)\n",
                "\n",
                "\n",
                "# These functions also exist in the XLA client library, but we treat them\n",
                "# as non-primitive to maintain a smaller set of autodiff primitives.\n",
                "\n",
                "def square(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise square: :math:`x^2`.\"\"\"\n",
                "  return integer_pow(x, 2)\n",
                "\n",
                "def reciprocal(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise reciprocal: :math:`1 \\over x`.\"\"\"\n",
                "  return integer_pow(x, -1)\n",
                "\n",
                "def _upcast_fp16_for_computation(f):\n",
                "  @functools.wraps(f)\n",
                "  def f_wrapped(x):\n",
                "    dtype = _dtype(x)\n",
                "    if dtype == np.float16 or dtype == dtypes.bfloat16:\n",
                "      return convert_element_type(\n",
                "        f(convert_element_type(x, np.float32)), dtype)\n",
                "    return f(x)\n",
                "\n",
                "  return f_wrapped\n",
                "\n",
                "def tan(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise tangent: :math:`\\mathrm{tan}(x)`.\"\"\"\n",
                "  return tan_p.bind(x)\n",
                "\n",
                "def asin(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise arc sine: :math:`\\mathrm{asin}(x)`.\"\"\"\n",
                "  return asin_p.bind(x)\n",
                "\n",
                "def acos(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise arc cosine: :math:`\\mathrm{acos}(x)`.\"\"\"\n",
                "  return acos_p.bind(x)\n",
                "\n",
                "def atan(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise arc tangent: :math:`\\mathrm{atan}(x)`.\"\"\"\n",
                "  return atan_p.bind(x)\n",
                "\n",
                "def sinh(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise hyperbolic sine: :math:`\\mathrm{sinh}(x)`.\"\"\"\n",
                "  return sinh_p.bind(x)\n",
                "\n",
                "def cosh(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise hyperbolic cosine: :math:`\\mathrm{cosh}(x)`.\"\"\"\n",
                "  return cosh_p.bind(x)\n",
                "\n",
                "def asinh(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise inverse hyperbolic sine: :math:`\\mathrm{asinh}(x)`.\"\"\"\n",
                "  return asinh_p.bind(x)\n",
                "\n",
                "def acosh(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise inverse hyperbolic cosine: :math:`\\mathrm{acosh}(x)`.\"\"\"\n",
                "  return acosh_p.bind(x)\n",
                "\n",
                "def atanh(x: ArrayLike) -> Array:\n",
                "  r\"\"\"Elementwise inverse hyperbolic tangent: :math:`\\mathrm{atanh}(x)`.\"\"\"\n",
                "  return atanh_p.bind(x)\n",
                "\n",
                "\n",
                "# Add some methods to ShapedArray that rely on lax primitives\n",
                "\n",
                "ShapedArray.broadcast = core.aval_method(broadcast)\n",
                "ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy\n",
                "ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy\n",
                "\n",
                "def _iter(tracer):\n",
                "  if tracer.ndim == 0:\n",
                "    raise TypeError(\"iteration over a 0-d array\")  # same as numpy error\n",
                "  else:\n",
                "    n = int(tracer.shape[0])\n",
                "    if any(isinstance(d, core.Tracer) for d in tracer.shape):\n",
                "      return (slicing.dynamic_index_in_dim(tracer, i, keepdims=False)\n",
                "              for i in range(n))\n",
                "    else:\n",
                "      return (slicing.index_in_dim(tracer, i, keepdims=False) for i in range(n))\n",
                "ShapedArray._iter = staticmethod(_iter)\n",
                "core.DShapedArray._iter = staticmethod(_iter)\n",
                "\n",
                "# Add some ad handlers that use (or could use) lax primitives\n",
                "\n",
                "def zeros_like_array(x: ArrayLike) -> Array:\n",
                "  return full_like(x, 0)\n",
                "\n",
                "for t in itertools.chain(\n",
                "    dtypes.python_scalar_dtypes.keys(), array_types,\n",
                "    device_array.device_array_types, [array.ArrayImpl],\n",
                "    [pxla.ShardedDeviceArray, pxla._ShardedDeviceArray,\n",
                "     pxla.pmap_lib.ShardedDeviceArray]):\n",
                "  ad_util.jaxval_adders[t] = add\n",
                "ad_util.jaxval_zeros_likers[device_array._DeviceArray] = zeros_like_array\n",
                "ad_util.jaxval_zeros_likers[device_array.Buffer] = zeros_like_array\n",
                "ad_util.jaxval_zeros_likers[pxla.ShardedDeviceArray] = zeros_like_array\n",
                "ad_util.jaxval_zeros_likers[pxla.pmap_lib.ShardedDeviceArray] = zeros_like_array\n",
                "ad_util.jaxval_zeros_likers[array.ArrayImpl] = zeros_like_array\n",
                "\n",
                "\n",
                "### primitives\n",
                "\n",
                "\n",
                "_fixed_dtype = lambda dtype: lambda *args, **kwargs: dtypes.canonicalize_dtype(dtype)\n",
                "_complex_basetype = lambda dtype: np.abs(np.zeros((), dtype)).dtype\n",
                "\n",
                "_strip_weak_type = lambda *args, **_: False\n",
                "\n",
                "\n",
                "def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval, **kwargs):\n",
                "  if not any(dtypes.issubdtype(aval.dtype, t) for t in accepted_dtypes):\n",
                "    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'\n",
                "    typename = str(np.dtype(aval.dtype).name)\n",
                "    accepted_typenames = (t.__name__ for t in accepted_dtypes)\n",
                "    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))\n",
                "  return result_dtype(aval.dtype)\n",
                "\n",
                "\n",
                "def unop(result_dtype, accepted_dtypes, name):\n",
                "  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)\n",
                "  weak_type_rule = partial(_naryop_weak_type_rule, name)\n",
                "  prim = standard_primitive(_attrgetter('shape'), dtype_rule, name,\n",
                "                            weak_type_rule=weak_type_rule)\n",
                "  batching.defvectorized(prim)\n",
                "  pe.def_trivial_padding(prim)\n",
                "  return prim\n",
                "standard_unop = partial(unop, _identity)\n",
                "_attrgetter = lambda name: lambda x, **kwargs: getattr(x, name)\n",
                "\n",
                "\n",
                "def naryop_dtype_rule(result_dtype, accepted_dtypes, name, *avals, **kwargs):\n",
                "  aval_dtypes = [aval.dtype for aval in avals]\n",
                "  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):\n",
                "    if not any(dtypes.issubdtype(aval_dtype, t) for t in types):\n",
                "      if aval_dtype == dtypes.float0:\n",
                "        raise TypeError(\n",
                "            f\"Called {name} with a float0 at position {i}. \"\n",
                "            \"float0s do not support any operations by design, because they \"\n",
                "            \"are not compatible with non-trivial vector spaces. No implicit dtype \"\n",
                "            \"conversion is done. You can use np.zeros_like(arr, dtype=np.float) \"\n",
                "            \"to cast a float0 array to a regular zeros array. \\n\"\n",
                "            \"If you didn't expect to get a float0 you might have accidentally \"\n",
                "            \"taken a gradient with respect to an integer argument.\")\n",
                "      else:\n",
                "        msg = ('{} does not accept dtype {} at position {}. '\n",
                "               'Accepted dtypes at position {} are subtypes of {}.')\n",
                "        typename = str(np.dtype(aval_dtype).name)\n",
                "        typenames = ', '.join(t.__name__ for t in types)\n",
                "        raise TypeError(msg.format(name, typename, i, i, typenames))\n",
                "  _check_same_dtypes(name, False, *aval_dtypes)\n",
                "  return result_dtype(*avals)\n",
                "\n",
                "\n",
                "def broadcasting_shape_rule(name, *avals):\n",
                "  shapes = [aval.shape for aval in avals if aval.shape]\n",
                "  if not shapes:\n",
                "    return ()\n",
                "  if len({len(shape) for shape in shapes}) != 1:\n",
                "    msg = '{}: arrays must have same number of dimensions, got {}.'\n",
                "    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))\n",
                "  # TODO(mattjj): de-duplicate with _try_broadcast_shapes\n",
                "  result_shape = []\n",
                "  for ds in zip(*shapes):\n",
                "    if all(core.same_referent(d, ds[0]) for d in ds[1:]):\n",
                "      # if all axes are identical objects, the resulting size is the object\n",
                "      result_shape.append(ds[0])\n",
                "    else:\n",
                "      # if all dims are equal (or 1), the result is the non-1 size\n",
                "      non_1s = [d for d in ds if not core.symbolic_equal_dim(d, 1)]\n",
                "      if not non_1s:\n",
                "        result_shape.append(1)\n",
                "      elif all(core.symbolic_equal_dim(non_1s[0], d) for d in non_1s[1:]):\n",
                "        result_shape.append(non_1s[0])\n",
                "      else:\n",
                "        raise TypeError(f'{name} got incompatible shapes for broadcasting: '\n",
                "                        f'{\", \".join(map(str, map(tuple, shapes)))}.')\n",
                "\n",
                "  return tuple(result_shape)\n",
                "\n",
                "def _naryop_weak_type_rule(name, *avals, **kwargs):\n",
                "  if any(aval.dtype == dtypes.float0 for aval in avals):\n",
                "    pos = next(i for i, aval in enumerate(avals) if aval.dtype == dtypes.float0)\n",
                "    raise TypeError(\n",
                "        f\"Called {name} with a float0 at position {pos}. \"\n",
                "        \"float0s do not support any operations by design, because they \"\n",
                "        \"are not compatible with non-trivial vector spaces. No implicit dtype \"\n",
                "        \"conversion is done. You can use np.zeros_like(arr, dtype=np.float) \"\n",
                "        \"to cast a float0 array to a regular zeros array. \\n\"\n",
                "        \"If you didn't expect to get a float0 you might have accidentally \"\n",
                "        \"taken a gradient with respect to an integer argument.\")\n",
                "  return all(aval.weak_type for aval in avals)\n",
                "\n",
                "def naryop(result_dtype, accepted_dtypes, name):\n",
                "  dtype_rule = partial(naryop_dtype_rule, result_dtype, accepted_dtypes, name)\n",
                "  shape_rule = partial(broadcasting_shape_rule, name)\n",
                "  weak_type_rule = partial(_naryop_weak_type_rule, name)\n",
                "  prim = standard_primitive(shape_rule, dtype_rule, name,\n",
                "                            weak_type_rule=weak_type_rule)\n",
                "  batching.defbroadcasting(prim)\n",
                "  pe.def_trivial_padding(prim)\n",
                "  return prim\n",
                "standard_naryop = partial(naryop, _input_dtype)\n",
                "\n",
                "\n",
                "def _broadcast_translate(op, ctx, avals_in, avals_out, *args):\n",
                "  \"\"\"Variant of _standard_translate that performs explicit broadcasting.\n",
                "\n",
                "  Not all XLA library functions perform their own broadcasting.\"\"\"\n",
                "  aval_out, = avals_out\n",
                "  broadcasted_args = []\n",
                "  for aval_in, arg in zip(avals_in, args):\n",
                "    if aval_out.shape != aval_in.shape:\n",
                "      bcast_dims = tuple(range(len(aval_out.shape) - len(aval_in.shape),\n",
                "                               len(aval_out.shape)))\n",
                "      arg = xops.BroadcastInDim(arg, aval_out.shape, bcast_dims)\n",
                "    broadcasted_args.append(arg)\n",
                "  return [op(*broadcasted_args)]\n",
                "\n",
                "\n",
                "# Like autograd.numpy.numpy_vjps.unbroadcast, this utility handles transposition\n",
                "# involving linear primitives with implicit broadcasting.\n",
                "def _unbroadcast(aval, x):\n",
                "  if not isinstance(aval, (core.DShapedArray, ShapedArray)):\n",
                "    raise TypeError(\"transpose with implicit broadcasting of unshaped values\")\n",
                "  x_shape = np.shape(x)\n",
                "  if core.symbolic_equal_shape(aval.shape, x_shape):\n",
                "    return x\n",
                "  assert not aval.shape or len(x_shape) == len(aval.shape)\n",
                "  if not aval.shape:\n",
                "    return _reduce_sum(x, list(range(len(x_shape))))\n",
                "  else:\n",
                "    dims = [i for i, (a, b) in enumerate(zip(x_shape, aval.shape)) if not core.symbolic_equal_dim(a, b)]\n",
                "    if config.jax_enable_checks: assert all(aval.shape[i] == 1 for i in dims)\n",
                "    return reshape(_reduce_sum(x, dims), aval.shape)\n",
                "\n",
                "def _maybe_broadcast(target_shape, x):\n",
                "  x_shape = np.shape(x)\n",
                "  if core.symbolic_equal_shape(x_shape, target_shape):\n",
                "    return x\n",
                "  elif not x_shape:\n",
                "    return broadcast_in_dim(x, target_shape, ())\n",
                "  else:\n",
                "    dims = [i for i, (a, b) in enumerate(zip(x_shape, target_shape))\n",
                "            if core.symbolic_equal_dim(a, b)]\n",
                "    squeeze_shape = [x_shape[i] for i in dims]\n",
                "    return broadcast_in_dim(reshape(x, squeeze_shape), target_shape, dims)\n",
                "\n",
                "def broadcast_hlo(\n",
                "    aval_out: core.ShapedArray, avals: Sequence[core.ShapedArray],\n",
                "    args: Sequence[ir.Value]) -> Sequence[ir.Value]:\n",
                "  \"\"\"Broadcasts HLO values with broadcast-compatible shapes to the same shape.\n",
                "  \"\"\"\n",
                "  out = []\n",
                "  for aval, arg in zip(avals, args):\n",
                "    if aval.shape != aval_out.shape:\n",
                "      assert len(aval.shape) <= len(aval_out.shape), (aval, aval_out)\n",
                "      dims = mlir.dense_int_elements(\n",
                "          range(len(aval_out.shape) - len(aval.shape), len(aval_out.shape)))\n",
                "      if any(isinstance(d, ir.Value) for d in aval_out.shape):\n",
                "        arg = hlo.DynamicBroadcastInDimOp(\n",
                "            mlir.aval_to_ir_type(aval_out), arg,\n",
                "            mlir.shape_tensor(aval_out.shape), dims).result\n",
                "      else:\n",
                "        arg = hlo.BroadcastInDimOp(\n",
                "            mlir.aval_to_ir_type(aval.update(shape=aval_out.shape)), arg,\n",
                "            dims).result\n",
                "    out.append(arg)\n",
                "  return out\n",
                "\n",
                "def _nary_lower_hlo(op: Callable, ctx,\n",
                "                    *args: Union[ir.Value, Sequence[ir.Value]],\n",
                "                    explicit_type=False, **params):\n",
                "  \"\"\"Lowers an elementwise operator to its MLIR equivalent.\n",
                "\n",
                "  Args:\n",
                "    explicit_type: does the MLIR op require its output type to be provided?\n",
                "  \"\"\"\n",
                "  del params\n",
                "  avals_in, (aval_out,) = ctx.avals_in, ctx.avals_out\n",
                "  broadcasted_args = mlir.multi_broadcast_in_dim(\n",
                "      ctx, args, avals_in, aval_out.shape)\n",
                "\n",
                "  if explicit_type:\n",
                "    return op(mlir.aval_to_ir_type(aval_out), *broadcasted_args).results\n",
                "  else:\n",
                "    return op(*broadcasted_args).results\n",
                "\n",
                "def _substitute_axis_sizes_in_aval(\n",
                "    env: Dict[core.Var, ir.Value], a: core.AbstractValue) -> core.AbstractValue:\n",
                "  if isinstance(a, core.DShapedArray):\n",
                "    return a.update(shape=tuple(env.get(d, d) for d in a.shape))  # type: ignore\n",
                "  return a\n",
                "\n",
                "\n",
                "_float = {np.floating}\n",
                "_complex = {np.complexfloating}\n",
                "_complex_elem_types = {np.float32, np.float64}\n",
                "_int = {np.integer}\n",
                "_bool = {np.bool_}\n",
                "\n",
                "_num = _int | _float | _complex\n",
                "_any = _int | _float | _complex | _bool\n",
                "_bool_or_int = _int | _bool\n",
                "_ordered = _int | _float | _bool\n",
                "\n",
                "neg_p = standard_unop(_num, 'neg')\n",
                "ad.deflinear2(neg_p, lambda t, operand: [neg(t)])\n",
                "mlir.register_lowering(neg_p, partial(_nary_lower_hlo, hlo.NegOp))\n",
                "\n",
                "sign_p = standard_unop(_num, 'sign')\n",
                "ad.defjvp_zero(sign_p)\n",
                "\n",
                "def _sign_lower_hlo(ctx, x):\n",
                "  x_aval, = ctx.avals_in\n",
                "  if dtypes.issubdtype(x_aval.dtype, np.unsignedinteger):\n",
                "    return hlo.SelectOp(\n",
                "        mlir.compare_hlo(x, mlir.full_like_aval(ctx, 0, x_aval), 'EQ',\n",
                "                         'UNSIGNED').result,\n",
                "        mlir.full_like_aval(ctx, 0, x_aval),\n",
                "        mlir.full_like_aval(ctx, 1, x_aval)).results\n",
                "  return hlo.SignOp(x).results\n",
                "\n",
                "mlir.register_lowering(sign_p, _sign_lower_hlo)\n",
                "\n",
                "nextafter_p = standard_naryop([_float, _float], 'nextafter')\n",
                "mlir.register_lowering(nextafter_p, partial(_nary_lower_hlo, chlo.NextAfterOp))\n",
                "\n",
                "floor_p = standard_unop(_float, 'floor')\n",
                "ad.defjvp_zero(floor_p)\n",
                "mlir.register_lowering(floor_p, partial(_nary_lower_hlo, hlo.FloorOp))\n",
                "\n",
                "ceil_p = standard_unop(_float, 'ceil')\n",
                "ad.defjvp_zero(ceil_p)\n",
                "mlir.register_lowering(ceil_p, partial(_nary_lower_hlo, hlo.CeilOp))\n",
                "\n",
                "round_p = standard_unop(_float, 'round')\n",
                "ad.defjvp_zero(round_p)\n",
                "\n",
                "def _round_lower(ctx, x, *, rounding_method):\n",
                "  if rounding_method is RoundingMethod.AWAY_FROM_ZERO:\n",
                "    return hlo.RoundOp(x).results\n",
                "  else:\n",
                "    assert rounding_method is RoundingMethod.TO_NEAREST_EVEN\n",
                "    return hlo.RoundNearestEvenOp(x).results\n",
                "mlir.register_lowering(round_p, _round_lower)\n",
                "\n",
                "is_finite_p = unop(_fixed_dtype(np.bool_), _float, 'is_finite')\n",
                "ad.defjvp_zero(is_finite_p)\n",
                "mlir.register_lowering(is_finite_p, partial(_nary_lower_hlo, hlo.IsFiniteOp))\n",
                "\n",
                "exp_p = standard_unop(_float | _complex, 'exp')\n",
                "ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))\n",
                "# For exp_p it is more efficient to use the reconstructed output for the vjp\n",
                "# rule instead of computing it again from the input.\n",
                "mlir.register_lowering(exp_p, partial(_nary_lower_hlo, hlo.ExpOp))\n",
                "\n",
                "log_p = standard_unop(_float | _complex, 'log')\n",
                "ad.defjvp(log_p, lambda g, x: div(g, x))\n",
                "mlir.register_lowering(log_p, partial(_nary_lower_hlo, hlo.LogOp))\n",
                "\n",
                "expm1_p = standard_unop(_float | _complex, 'expm1')\n",
                "ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))\n",
                "mlir.register_lowering(expm1_p, partial(_nary_lower_hlo, hlo.Expm1Op))\n",
                "\n",
                "log1p_p = standard_unop(_float | _complex, 'log1p')\n",
                "ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))\n",
                "mlir.register_lowering(log1p_p, partial(_nary_lower_hlo, hlo.Log1pOp))\n",
                "\n",
                "tanh_p = standard_unop(_float | _complex, 'tanh')\n",
                "ad.defjvp2(tanh_p, lambda g, ans, x: mul(add(g, mul(g, ans)),\n",
                "                                         sub(_one(x), ans)))\n",
                "mlir.register_lowering(tanh_p, partial(_nary_lower_hlo, hlo.TanhOp))\n",
                "\n",
                "logistic_p = standard_unop(_float | _complex, 'logistic')\n",
                "ad.defjvp2(logistic_p, lambda g, ans, x: mul(g, mul(ans, sub(_one(ans), ans))))\n",
                "# TODO(phawkins): switch to LogisticOp lowering; debug numerical problems.\n",
                "# mlir.register_lowering(logistic_p, partial(_nary_lower_hlo, hlo.LogisticOp))\n",
                "\n",
                "def logistic_impl(x):\n",
                "  one = _const(x, 1)\n",
                "  return div(one, add(one, exp(neg(x))))\n",
                "\n",
                "mlir.register_lowering(logistic_p,\n",
                "                       mlir.lower_fun(logistic_impl, multiple_results=False))\n",
                "\n",
                "sin_p = standard_unop(_float | _complex, 'sin')\n",
                "ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))\n",
                "mlir.register_lowering(sin_p, partial(_nary_lower_hlo, hlo.SineOp))\n",
                "\n",
                "cos_p = standard_unop(_float | _complex, 'cos')\n",
                "ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))\n",
                "mlir.register_lowering(cos_p, partial(_nary_lower_hlo, hlo.CosineOp))\n",
                "\n",
                "@_upcast_fp16_for_computation\n",
                "def _tan_impl(x):\n",
                "  return div(sin(x), cos(x))\n",
                "\n",
                "tan_p = standard_unop(_float | _complex, 'tan')\n",
                "ad.defjvp2(tan_p, lambda g, ans, x: mul(g, _const(x, 1) + square(ans)))\n",
                "mlir.register_lowering(tan_p, partial(_nary_lower_hlo, chlo.TanOp))\n",
                "\n",
                "def asin_impl(x):\n",
                "  if dtypes.issubdtype(_dtype(x), np.complexfloating):\n",
                "    return mul(_const(x, -1j), asinh(mul(_const(x, 1j), x)))\n",
                "  else:\n",
                "    return mul(_const(x, 2),\n",
                "               atan2(x, add(_const(x, 1), sqrt(sub(_const(x, 1), square(x))))))\n",
                "\n",
                "asin_p = standard_unop(_float | _complex, 'asin')\n",
                "ad.defjvp(asin_p, lambda g, x: mul(g, rsqrt(_const(x, 1) - square(x))))\n",
                "mlir.register_lowering(asin_p, partial(_nary_lower_hlo, chlo.AsinOp))\n",
                "\n",
                "def acos_impl(x):\n",
                "  if dtypes.issubdtype(_dtype(x), np.complexfloating):\n",
                "    result = mul(_const(x, 1j), acosh(x))\n",
                "    # By convention, numpy chooses the branch with positive real part.\n",
                "    rpart = real(result)\n",
                "    return select(\n",
                "      gt(rpart, _const(rpart, 0)),\n",
                "      result,\n",
                "      neg(result)\n",
                "    )\n",
                "  else:\n",
                "    return select(\n",
                "        ne(x, _const(x, -1.0)),\n",
                "        mul(_const(x, 2),\n",
                "            atan2(sqrt(sub(_const(x, 1), square(x))), add(_const(x, 1), x))),\n",
                "        full_like(x, np.pi))\n",
                "\n",
                "acos_p = standard_unop(_float | _complex, 'acos')\n",
                "ad.defjvp(acos_p, lambda g, x: mul(g, -rsqrt(_const(x, 1) - square(x))))\n",
                "mlir.register_lowering(acos_p,\n",
                "                       mlir.lower_fun(acos_impl, multiple_results=False))\n",
                "\n",
                "def atan_impl(x):\n",
                "  return atan2(x, _const(x, 1))\n",
                "\n",
                "atan_p = standard_unop(_float | _complex, 'atan')\n",
                "ad.defjvp(atan_p, lambda g, x: div(g, _const(x, 1) + square(x)))\n",
                "mlir.register_lowering(atan_p, partial(_nary_lower_hlo, chlo.AtanOp))\n",
                "\n",
                "atan2_p = standard_naryop([_float | _complex, _float | _complex], 'atan2')\n",
                "ad.defjvp(atan2_p,\n",
                "          lambda g, x, y: g * (y / (square(x) + square(y))),\n",
                "          lambda g, x, y: g * -x / (square(x) + square(y)))\n",
                "mlir.register_lowering(atan2_p, partial(_nary_lower_hlo, hlo.Atan2Op))\n",
                "\n",
                "sinh_p = standard_unop(_float | _complex, 'sinh')\n",
                "ad.defjvp(sinh_p, lambda g, x: mul(g, cosh(x)))\n",
                "mlir.register_lowering(sinh_p, partial(_nary_lower_hlo, chlo.SinhOp))\n",
                "\n",
                "cosh_p = standard_unop(_float | _complex, 'cosh')\n",
                "ad.defjvp(cosh_p, lambda g, x: mul(g, sinh(x)))\n",
                "mlir.register_lowering(cosh_p, partial(_nary_lower_hlo, chlo.CoshOp))\n",
                "\n",
                "asinh_p = standard_unop(_float | _complex, 'asinh')\n",
                "ad.defjvp(asinh_p, lambda g, x: mul(g, rsqrt(square(x) + _one(x))))\n",
                "mlir.register_lowering(asinh_p, partial(_nary_lower_hlo, chlo.AsinhOp))\n",
                "\n",
                "acosh_p = standard_unop(_float | _complex, 'acosh')\n",
                "ad.defjvp(acosh_p,\n",
                "          lambda g, x: mul(g, rsqrt((x - _one(x)) * (x + _one(x)))))\n",
                "mlir.register_lowering(acosh_p, partial(_nary_lower_hlo, chlo.AcoshOp))\n",
                "\n",
                "atanh_p = standard_unop(_float | _complex, 'atanh')\n",
                "ad.defjvp(atanh_p,\n",
                "          lambda g, x: mul(reciprocal(_one(x) + x), div(g, (_one(x) - x))))\n",
                "mlir.register_lowering(atanh_p, partial(_nary_lower_hlo, chlo.AtanhOp))\n",
                "\n",
                "regularized_incomplete_beta_p = standard_naryop(\n",
                "    [_float, _float, _float], 'regularized_incomplete_beta')\n",
                "xla.register_translation(\n",
                "    regularized_incomplete_beta_p,\n",
                "    partial(_broadcast_translate, xops.RegularizedIncompleteBeta))\n",
                "\n",
                "def betainc_gradx(g, a, b, x):\n",
                "  lbeta = lgamma(a) + lgamma(b) - lgamma(a + b)\n",
                "  partial_x = exp((b - 1) * log1p(-x) +\n",
                "                  (a - 1) * log(x) - lbeta)\n",
                "  return partial_x * g\n",
                "\n",
                "def betainc_grad_not_implemented(g, a, b, x):\n",
                "  raise ValueError(\"Betainc gradient with respect to a and b not supported.\")\n",
                "\n",
                "ad.defjvp(regularized_incomplete_beta_p,\n",
                "  betainc_grad_not_implemented,\n",
                "  betainc_grad_not_implemented,\n",
                "  betainc_gradx)\n",
                "\n",
                "lgamma_p = standard_unop(_float, 'lgamma')\n",
                "ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))\n",
                "mlir.register_lowering(lgamma_p, partial(_nary_lower_hlo, chlo.LgammaOp))\n",
                "\n",
                "digamma_p = standard_unop(_float, 'digamma')\n",
                "mlir.register_lowering(digamma_p, partial(_nary_lower_hlo, chlo.DigammaOp))\n",
                "\n",
                "igamma_p = standard_naryop([_float, _float], 'igamma')\n",
                "xla.register_translation(igamma_p, partial(_broadcast_translate, xops.Igamma))\n",
                "igamma_grad_a_p = standard_naryop([_float, _float], 'igamma_grad_a')\n",
                "xla.register_translation(igamma_grad_a_p,\n",
                "                         partial(_broadcast_translate, xops.IgammaGradA))\n",
                "\n",
                "def igamma_gradx(g, a, x):\n",
                "  return g * exp(-x + (a - _ones(a)) * log(x) - lgamma(a))\n",
                "\n",
                "def igamma_grada(g, a, x):\n",
                "  return g * igamma_grad_a(a, x)\n",
                "\n",
                "ad.defjvp(igamma_p, igamma_grada, igamma_gradx)\n",
                "\n",
                "igammac_p = standard_naryop([_float, _float], 'igammac')\n",
                "xla.register_translation(igammac_p, partial(_broadcast_translate, xops.Igammac))\n",
                "\n",
                "def igammac_gradx(g, a, x):\n",
                "  return -igamma_gradx(g, a, x)\n",
                "\n",
                "def igammac_grada(g, a, x):\n",
                "  return -igamma_grada(g, a, x)\n",
                "\n",
                "ad.defjvp(igammac_p, igammac_grada, igammac_gradx)\n",
                "\n",
                "random_gamma_grad_p = standard_naryop([_float, _float], 'random_gamma_grad')\n",
                "xla.register_translation(random_gamma_grad_p,\n",
                "                         partial(_broadcast_translate, xops.RandomGammaGrad))\n",
                "bessel_i0e_p = standard_unop(_float, 'bessel_i0e')\n",
                "xla.register_translation(bessel_i0e_p, standard_translate(bessel_i0e_p))\n",
                "ad.defjvp2(bessel_i0e_p, lambda g, y, x: g * (bessel_i1e(x) - sign(x) * y))\n",
                "\n",
                "bessel_i1e_p = standard_unop(_float, 'bessel_i1e')\n",
                "mlir.register_lowering(bessel_i1e_p,\n",
                "                        partial(_nary_lower_hlo, chlo.BesselI1eOp))\n",
                "\n",
                "def _bessel_i1e_jvp(g, y, x):\n",
                "  eps = dtypes.finfo(_dtype(x)).eps\n",
                "  x_is_not_tiny = abs(x) > eps\n",
                "  safe_x = select(x_is_not_tiny, x, full_like(x, eps))\n",
                "  dy_dx = bessel_i0e(safe_x) - y * (sign(safe_x) + reciprocal(safe_x))\n",
                "  dy_dx = select(x_is_not_tiny, dy_dx, full_like(x, 0.5))\n",
                "  return g * dy_dx\n",
                "ad.defjvp2(bessel_i1e_p, _bessel_i1e_jvp)\n",
                "\n",
                "erf_p = standard_unop(_float, 'erf')\n",
                "ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / np.sqrt(np.pi)),\n",
                "                                  mul(g, exp(neg(square(x))))))\n",
                "mlir.register_lowering(erf_p, partial(_nary_lower_hlo, chlo.ErfOp))\n",
                "\n",
                "erfc_p = standard_unop(_float, 'erfc')\n",
                "ad.defjvp(erfc_p, lambda g, x: mul(_const(x, -2. / np.sqrt(np.pi)),\n",
                "                                   mul(g, exp(neg(square(x))))))\n",
                "mlir.register_lowering(erfc_p, partial(_nary_lower_hlo, chlo.ErfcOp))\n",
                "\n",
                "erf_inv_p = standard_unop(_float, 'erf_inv')\n",
                "ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, np.sqrt(np.pi) / 2.),\n",
                "                                            mul(g, exp(square(ans)))))\n",
                "xla.register_translation(erf_inv_p, standard_translate(erf_inv_p))\n",
                "\n",
                "real_p = unop(_complex_basetype, _complex, 'real')\n",
                "ad.deflinear2(real_p, lambda t, _: [complex(t, np.zeros((), _dtype(t)))])\n",
                "mlir.register_lowering(real_p, partial(_nary_lower_hlo, hlo.RealOp))\n",
                "\n",
                "imag_p = unop(_complex_basetype, _complex, 'imag')\n",
                "ad.deflinear2(imag_p, lambda t, _: [complex(np.zeros((), _dtype(t)), neg(t))])\n",
                "mlir.register_lowering(imag_p, partial(_nary_lower_hlo, hlo.ImagOp))\n",
                "\n",
                "\n",
                "def _complex_transpose_rule(t, x, y):\n",
                "  assert ad.is_undefined_primal(x) or ad.is_undefined_primal(y)\n",
                "  if ad.is_undefined_primal(x) and ad.is_undefined_primal(y):\n",
                "    if type(t) is ad_util.Zero:\n",
                "      return [ad_util.Zero(x.aval), ad_util.Zero(y.aval)]\n",
                "    else:\n",
                "      return [_unbroadcast(x.aval, real(t)), _unbroadcast(y.aval, imag(neg(t)))]\n",
                "  elif ad.is_undefined_primal(x):\n",
                "    if type(t) is ad_util.Zero:\n",
                "      return [ad_util.Zero(x.aval), None]\n",
                "    else:\n",
                "      return [_unbroadcast(x.aval, real(t)), None]\n",
                "  else:\n",
                "    if type(t) is ad_util.Zero:\n",
                "      return [None, ad_util.Zero(y.aval)]\n",
                "    else:\n",
                "      return [None, _unbroadcast(y.aval, imag(neg(t)))]\n",
                "\n",
                "_complex_dtype = lambda dtype, *args: (np.zeros((), dtype) + np.zeros((), np.complex64)).dtype\n",
                "complex_p = naryop(_complex_dtype, [_complex_elem_types, _complex_elem_types],\n",
                "                  'complex')\n",
                "ad.deflinear2(complex_p, _complex_transpose_rule)\n",
                "mlir.register_lowering(complex_p, partial(_nary_lower_hlo, hlo.ComplexOp))\n",
                "\n",
                "conj_p = unop(_complex_dtype, _complex_elem_types | _complex, 'conj')\n",
                "\n",
                "def _conj_impl(x, **kw):\n",
                "  if dtypes.issubdtype(x.dtype, np.complexfloating):\n",
                "    return complex(real(x), -imag(x))\n",
                "  else:\n",
                "    return complex(x, _zeros(x))\n",
                "\n",
                "mlir.register_lowering(conj_p,\n",
                "                       mlir.lower_fun(_conj_impl, multiple_results=False))\n",
                "\n",
                "\n",
                "def _conj_transpose_rule(t, x, *, input_dtype):\n",
                "  assert ad.is_undefined_primal(x)\n",
                "  if dtypes.issubdtype(input_dtype, np.complexfloating):\n",
                "    return [conj(t)]\n",
                "  else:\n",
                "    return [real(t)]\n",
                "\n",
                "ad.primitive_jvps[conj_p] = partial(ad.linear_jvp, conj_p)\n",
                "ad.primitive_transposes[conj_p] = _conj_transpose_rule\n",
                "\n",
                "abs_p = unop(_complex_basetype, _num, 'abs')\n",
                "mlir.register_lowering(abs_p, partial(_nary_lower_hlo, hlo.AbsOp))\n",
                "\n",
                "def _abs_jvp_rule(g, ans, x):\n",
                "  if _iscomplex(x):\n",
                "    return _maybe_real(mul(g, div(_maybe_conj(x),\n",
                "           _replace_zero(convert_element_type(ans, _dtype(x))))))\n",
                "  else:\n",
                "    return select(ge(x, _zero(x)), g, neg(g))\n",
                "ad.defjvp2(abs_p, _abs_jvp_rule)\n",
                "_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x\n",
                "_maybe_real = lambda x: real(x) if _iscomplex(x) else x\n",
                "\n",
                "sqrt_p = standard_unop(_float | _complex, 'sqrt')\n",
                "ad.defjvp2(sqrt_p, lambda g, ans, x: mul(g, div(_const(x, 0.5), ans)))\n",
                "mlir.register_lowering(sqrt_p, partial(_nary_lower_hlo, hlo.SqrtOp))\n",
                "\n",
                "rsqrt_p = standard_unop(_float | _complex, 'rsqrt')\n",
                "ad.defjvp2(rsqrt_p,\n",
                "           lambda g, ans, x:\n",
                "           mul(g, mul(_const(x, -0.5), div(ans, x))))\n",
                "mlir.register_lowering(rsqrt_p, partial(_nary_lower_hlo, hlo.RsqrtOp))\n",
                "\n",
                "cbrt_p = standard_unop(_float, 'cbrt')\n",
                "ad.defjvp2(cbrt_p,\n",
                "           lambda g, ans, x: mul(g, mul(_const(x, 1/3), integer_pow(ans, -2))))\n",
                "mlir.register_lowering(cbrt_p, partial(_nary_lower_hlo, hlo.CbrtOp))\n",
                "\n",
                "pow_p = standard_naryop([_float | _complex, _float | _complex], 'pow')\n",
                "\n",
                "def _pow_jvp_lhs(g, ans, x, y):\n",
                "  return mul(g, mul(y, pow(x, sub(y, _ones(y)))))\n",
                "\n",
                "def _pow_jvp_rhs(g, ans, x, y):\n",
                "  return mul(g, mul(log(_replace_zero(x)), ans))\n",
                "\n",
                "ad.defjvp2(pow_p, _pow_jvp_lhs, _pow_jvp_rhs)\n",
                "mlir.register_lowering(pow_p, partial(_nary_lower_hlo, hlo.PowOp))\n",
                "\n",
                "\n",
                "def _integer_pow_dtype_rule(x, *, y):\n",
                "  dtype = unop_dtype_rule(_identity, _int | _float | _complex, 'integer_pow', x)\n",
                "  if y < 0 and dtypes.issubdtype(dtype, np.integer):\n",
                "    raise TypeError(\"Integers cannot be raised to negative powers, got \"\n",
                "                    f\"integer_pow({x}, {y})\")\n",
                "  return dtype\n",
                "\n",
                "def _integer_pow_jvp(g, x, *, y):\n",
                "  return _zeros(g) if y == 0 else mul(g, mul(_const(x, y), integer_pow(x, y - 1)))\n",
                "\n",
                "integer_pow_p = standard_primitive(\n",
                "  _attrgetter('shape'), _integer_pow_dtype_rule, 'integer_pow')\n",
                "batching.defvectorized(integer_pow_p)\n",
                "ad.defjvp(integer_pow_p, _integer_pow_jvp)\n",
                "pe.def_trivial_padding(integer_pow_p)\n",
                "\n",
                "def _integer_pow(x, *, y):\n",
                "  # This should be kept in sync with the jax2tf translation rule.\n",
                "  if y == 0:\n",
                "    return full_like(x, 1)\n",
                "  is_reciprocal = y < 0\n",
                "  if is_reciprocal:\n",
                "    y = -y\n",
                "  acc = None\n",
                "  while y > 0:\n",
                "    if y & 1:\n",
                "      acc = x if acc is None else mul(acc, x)\n",
                "    y >>= 1\n",
                "    if y > 0:\n",
                "      # We don't call square because it calls integer_pow.\n",
                "      x = mul(x, x)\n",
                "  return div(full_like(acc, 1), acc) if is_reciprocal else acc\n",
                "\n",
                "\n",
                "def _integer_pow_lowering(ctx, x, *, y):\n",
                "  lowering = mlir.lower_fun(_integer_pow, multiple_results=False)\n",
                "  # TODO(b/217551391): emitting an out-of-line call leads to a large\n",
                "  # expansion when the MLIR is lowered to HLO, because the HLO lowering\n",
                "  # clones the callee. Consider unconditionally caching when the MLIR->HLO\n",
                "  # lowering doesn't expand the program.\n",
                "  if y >= 4:\n",
                "    lowering = mlir.cache_lowering(lowering)\n",
                "  return lowering(ctx, x, y=y)\n",
                "\n",
                "mlir.register_lowering(integer_pow_p, _integer_pow_lowering)\n",
                "\n",
                "_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)\n",
                "\n",
                "not_p = standard_unop(_bool_or_int, 'not')\n",
                "ad.defjvp_zero(not_p)\n",
                "mlir.register_lowering(not_p, partial(_nary_lower_hlo, hlo.NotOp))\n",
                "\n",
                "and_p = standard_naryop([_bool_or_int, _bool_or_int], 'and')\n",
                "ad.defjvp_zero(and_p)\n",
                "mlir.register_lowering(and_p, partial(_nary_lower_hlo, hlo.AndOp))\n",
                "\n",
                "or_p = standard_naryop([_bool_or_int, _bool_or_int], 'or')\n",
                "ad.defjvp_zero(or_p)\n",
                "mlir.register_lowering(or_p, partial(_nary_lower_hlo, hlo.OrOp))\n",
                "\n",
                "xor_p = standard_naryop([_bool_or_int, _bool_or_int], 'xor')\n",
                "ad.defjvp_zero(xor_p)\n",
                "mlir.register_lowering(xor_p, partial(_nary_lower_hlo, hlo.XorOp))\n",
                "\n",
                "population_count_p = standard_unop(_int, 'population_count')\n",
                "mlir.register_lowering(population_count_p,\n",
                "                       partial(_nary_lower_hlo, hlo.PopulationCountOp))\n",
                "\n",
                "clz_p = standard_unop(_int, 'clz')\n",
                "mlir.register_lowering(clz_p, partial(_nary_lower_hlo, hlo.ClzOp))\n",
                "\n",
                "def _add_jvp(primals, tangents):\n",
                "  x, y = primals\n",
                "  xdot, ydot = tangents\n",
                "  primal_out = add(x, y)\n",
                "  if type(xdot) is type(ydot) is ad_util.Zero:\n",
                "    return primal_out, ad_util.Zero.from_value(primal_out)\n",
                "  if type(xdot) is ad_util.Zero:\n",
                "    return primal_out, _maybe_broadcast(primal_out.shape, ydot)\n",
                "  elif type(ydot) is ad_util.Zero:\n",
                "    return primal_out, _maybe_broadcast(primal_out.shape, xdot)\n",
                "  else:\n",
                "    return primal_out, add(xdot, ydot)\n",
                "\n",
                "def _add_transpose(t, x, y):\n",
                "  # Morally the following assertion is true, but because we instantiate zeros in\n",
                "  # some places (e.g. in custom_jvp) it may not always hold. For example, see\n",
                "  # api_test.py's CustomJVPTest.test_jaxpr_zeros.\n",
                "  # assert ad.is_undefined_primal(x) and ad.is_undefined_primal(y)\n",
                "  x_aval = x.aval if ad.is_undefined_primal(x) else _abstractify(x)\n",
                "  y_aval = y.aval if ad.is_undefined_primal(y) else _abstractify(y)\n",
                "  if type(t) is ad_util.Zero:\n",
                "    return [ad_util.Zero(x_aval), ad_util.Zero(y_aval)]\n",
                "  else:\n",
                "    return [_unbroadcast(x_aval, t), _unbroadcast(y_aval, t)]\n",
                "\n",
                "def _add_inverse(r, x, y):\n",
                "  xr = r - y\n",
                "  yr = r - x\n",
                "  return xr, yr\n",
                "\n",
                "# TODO(slebedev): Why does mypy fail to infer the type here?\n",
                "add_p: Primitive = standard_naryop([_num, _num], 'add')\n",
                "ad.primitive_jvps[add_p] = _add_jvp\n",
                "ad.primitive_transposes[add_p] = _add_transpose\n",
                "mlir.register_lowering(add_p, partial(_nary_lower_hlo, hlo.AddOp))\n",
                "\n",
                "def _sub_jvp(primals, tangents):\n",
                "  x, y = primals\n",
                "  xdot, ydot = tangents\n",
                "  primal_out = sub(x, y)\n",
                "  if type(xdot) is type(ydot) is ad_util.Zero:\n",
                "    return primal_out, ad_util.Zero.from_value(primal_out)\n",
                "  if type(xdot) is ad_util.Zero:\n",
                "    return primal_out, _maybe_broadcast(primal_out.shape, neg(ydot))\n",
                "  elif type(ydot) is ad_util.Zero:\n",
                "    return primal_out, _maybe_broadcast(primal_out.shape, xdot)\n",
                "  else:\n",
                "    return primal_out, sub(xdot, ydot)\n",
                "\n",
                "def _sub_transpose(t, x, y):\n",
                "  # Morally the following assertion is true, but see the comment in add_p's\n",
                "  # transpose rule.\n",
                "  # assert ad.is_undefined_primal(x) and ad.is_undefined_primal(y)\n",
                "  x_aval = x.aval if ad.is_undefined_primal(x) else _abstractify(x)\n",
                "  y_aval = y.aval if ad.is_undefined_primal(y) else _abstractify(y)\n",
                "  if type(t) is ad_util.Zero:\n",
                "    return [ad_util.Zero(x_aval), ad_util.Zero(y_aval)]\n",
                "  else:\n",
                "    return [_unbroadcast(x_aval, t), _unbroadcast(y_aval, neg(t))]\n",
                "\n",
                "sub_p = standard_naryop([_num, _num], 'sub')\n",
                "ad.primitive_jvps[sub_p] = _sub_jvp\n",
                "ad.primitive_transposes[sub_p] = _sub_transpose\n",
                "mlir.register_lowering(sub_p, partial(_nary_lower_hlo, hlo.SubtractOp))\n",
                "\n",
                "\n",
                "def _mul_transpose(ct, x, y):\n",
                "  assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n",
                "  if ad.is_undefined_primal(x):\n",
                "    if type(ct) is ad_util.Zero:\n",
                "      return [ad_util.Zero(x.aval), None]\n",
                "    else:\n",
                "      return [_unbroadcast(x.aval, mul(ct, y)), None]\n",
                "  else:\n",
                "    if type(ct) is ad_util.Zero:\n",
                "      return [None, ad_util.Zero(y.aval)]\n",
                "    else:\n",
                "      return [None, _unbroadcast(y.aval, mul(x, ct))]\n",
                "\n",
                "def _mul_inverse(r, x, y):\n",
                "  xr = r / y\n",
                "  yr = r / x\n",
                "  return xr, yr\n",
                "\n",
                "mul_p = standard_naryop([_num, _num], 'mul')\n",
                "ad.defjvp(mul_p,\n",
                "          lambda xdot, x, y: mul(xdot, y),\n",
                "          lambda ydot, x, y: mul(x, ydot))\n",
                "ad.primitive_transposes[mul_p] = _mul_transpose\n",
                "mlir.register_lowering(mul_p, partial(_nary_lower_hlo, hlo.MulOp))\n",
                "\n",
                "def _div_transpose_rule(cotangent, x, y):\n",
                "  assert ad.is_undefined_primal(x) and not ad.is_undefined_primal(y)\n",
                "  if type(cotangent) is ad_util.Zero:\n",
                "    return [ad_util.Zero(x.aval), None]\n",
                "  else:\n",
                "    return [_unbroadcast(x.aval, div(cotangent, y)), None]\n",
                "div_p = standard_naryop([_num, _num], 'div')\n",
                "ad.defjvp(div_p,\n",
                "          lambda g, x, y: div(g, y),\n",
                "          lambda g, x, y: mul(mul(neg(g), x), integer_pow(y, -2)))\n",
                "ad.primitive_transposes[div_p] = _div_transpose_rule\n",
                "mlir.register_lowering(div_p, partial(_nary_lower_hlo, hlo.DivOp))\n",
                "\n",
                "rem_p = standard_naryop([_int | _float, _int | _float], 'rem')\n",
                "ad.defjvp(\n",
                "    rem_p,\n",
                "    lambda g, x, y: _maybe_broadcast(broadcast_shapes(np.shape(x), np.shape(y)), g),\n",
                "    lambda g, x, y: mul(neg(g), mul(sign(div(x, y)), floor(abs(div(x, y))))))\n",
                "mlir.register_lowering(rem_p, partial(_nary_lower_hlo, hlo.RemOp))\n",
                "\n",
                "def _minmax_complex_lowering(x, y, *, lax_cmp_pick_x):\n",
                "  result_shape = broadcast_shapes(np.shape(x), np.shape(y))\n",
                "  x = _maybe_broadcast(result_shape, x)\n",
                "  y = _maybe_broadcast(result_shape, y)\n",
                "  rx = real(x)\n",
                "  ry = real(y)\n",
                "  pick_x = select(eq(rx, ry), lax_cmp_pick_x(imag(x), imag(y)),\n",
                "                  lax_cmp_pick_x(rx, ry))\n",
                "  return select(pick_x, x, y)\n",
                "\n",
                "max_p: core.Primitive = standard_naryop([_any, _any], 'max')\n",
                "ad.defjvp2(max_p,\n",
                "           lambda g, ans, x, y: mul(g, _balanced_eq(x, ans, y)),\n",
                "           lambda g, ans, x, y: mul(g, _balanced_eq(y, ans, x)))\n",
                "mlir.register_lowering(max_p, partial(_nary_lower_hlo, mlir.max_hlo))\n",
                "\n",
                "min_p: core.Primitive = standard_naryop([_any, _any], 'min')\n",
                "ad.defjvp2(min_p,\n",
                "           lambda g, ans, x, y: mul(g, _balanced_eq(x, ans, y)),\n",
                "           lambda g, ans, x, y: mul(g, _balanced_eq(y, ans, x)))\n",
                "mlir.register_lowering(min_p, partial(_nary_lower_hlo, mlir.min_hlo))\n",
                "\n",
                "shift_left_p = standard_naryop([_int, _int], 'shift_left')\n",
                "ad.defjvp_zero(shift_left_p)\n",
                "mlir.register_lowering(shift_left_p, partial(_nary_lower_hlo, hlo.ShiftLeftOp))\n",
                "\n",
                "shift_right_arithmetic_p = standard_naryop([_int, _int], 'shift_right_arithmetic')\n",
                "ad.defjvp_zero(shift_right_arithmetic_p)\n",
                "mlir.register_lowering(shift_right_arithmetic_p,\n",
                "                       partial(_nary_lower_hlo, hlo.ShiftRightArithmeticOp))\n",
                "\n",
                "shift_right_logical_p = standard_naryop([_int, _int], 'shift_right_logical')\n",
                "ad.defjvp_zero(shift_right_logical_p)\n",
                "mlir.register_lowering(shift_right_logical_p,\n",
                "                       partial(_nary_lower_hlo, hlo.ShiftRightLogicalOp))\n",
                "\n",
                "def _compare_lower_hlo(direction: str, ctx, x, y):\n",
                "  avals_in, (aval_out,) = ctx.avals_in, ctx.avals_out\n",
                "  x_dtype = avals_in[0].dtype\n",
                "  x, y = mlir.multi_broadcast_in_dim(ctx, (x, y), avals_in, aval_out.shape)\n",
                "\n",
                "  if dtypes.issubdtype(x_dtype, np.inexact):\n",
                "    compare_type = \"FLOAT\"\n",
                "  elif dtypes.issubdtype(x_dtype, np.signedinteger):\n",
                "    compare_type = \"SIGNED\"\n",
                "  else:\n",
                "    compare_type = \"UNSIGNED\"\n",
                "  return mlir.compare_hlo(x, y, direction, compare_type).results\n",
                "\n",
                "eq_p = naryop(_fixed_dtype(np.bool_), [_any, _any], 'eq')\n",
                "ad.defjvp_zero(eq_p)\n",
                "mlir.register_lowering(eq_p, partial(_compare_lower_hlo, \"EQ\"))\n",
                "\n",
                "ne_p = naryop(_fixed_dtype(np.bool_), [_any, _any], 'ne')\n",
                "ad.defjvp_zero(ne_p)\n",
                "mlir.register_lowering(ne_p, partial(_compare_lower_hlo, \"NE\"))\n",
                "\n",
                "ge_p = naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'ge')\n",
                "ad.defjvp_zero(ge_p)\n",
                "mlir.register_lowering(ge_p, partial(_compare_lower_hlo, \"GE\"))\n",
                "\n",
                "gt_p = naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'gt')\n",
                "ad.defjvp_zero(gt_p)\n",
                "mlir.register_lowering(gt_p, partial(_compare_lower_hlo, \"GT\"))\n",
                "\n",
                "le_p = naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'le')\n",
                "ad.defjvp_zero(le_p)\n",
                "mlir.register_lowering(le_p, partial(_compare_lower_hlo, \"LE\"))\n",
                "\n",
                "lt_p = naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'lt')\n",
                "ad.defjvp_zero(lt_p)\n",
                "mlir.register_lowering(lt_p, partial(_compare_lower_hlo, \"LT\"))\n",
                "\n",
                "\n",
                "def _convert_element_type_shape_rule(operand, *, new_dtype, weak_type):\n",
                "  return operand.shape\n",
                "\n",
                "def _convert_element_type_dtype_rule(operand, *, new_dtype, weak_type):\n",
                "  return new_dtype\n",
                "\n",
                "def _convert_element_type_weak_type_rule(operand, *, new_dtype, weak_type):\n",
                "  return weak_type\n",
                "\n",
                "def _convert_element_type_transpose_rule(ct, operand, *, new_dtype, weak_type):\n",
                "  assert ad.is_undefined_primal(operand)\n",
                "  old_dtype = operand.aval.dtype\n",
                "  old_weak_type = dtypes.is_weakly_typed(operand)\n",
                "  if type(ct) is ad_util.Zero:\n",
                "    return [ad_util.Zero(operand.aval)]\n",
                "  elif core.primal_dtype_to_tangent_dtype(old_dtype) == dtypes.float0:\n",
                "    return [ad_util.Zero(operand.aval.update(dtype=dtypes.float0, weak_type=False))]\n",
                "  else:\n",
                "    return [convert_element_type_p.bind(ct, new_dtype=old_dtype,\n",
                "                                        weak_type=old_weak_type)]\n",
                "\n",
                "def _convert_element_type_jvp_rule(tangent, operand , *, new_dtype, weak_type):\n",
                "  if core.primal_dtype_to_tangent_dtype(new_dtype) == dtypes.float0:\n",
                "    return ad_util.Zero(tangent.aval.update(dtype=dtypes.float0, weak_type=False))\n",
                "  else:\n",
                "    return convert_element_type_p.bind(tangent, new_dtype=new_dtype,\n",
                "                                       weak_type=weak_type)\n",
                "\n",
                "def _convert_elt_type_folding_rule(consts, eqn):\n",
                "  # We constant-fold convert_element_types applied to constants if those\n",
                "  # constants are Python builtin numeric types or numpy.ndarrays (so as not\n",
                "  # to perform any device operations when constant-folding) and if the output\n",
                "  # type can be faithfully represented by a Python builtin numeric type or\n",
                "  # numpy.ndarray. If those conditions are met, we output a numpy.ndarray\n",
                "  # constant if the output type is not weak, and if the output type is weak then\n",
                "  # we output a Python builtin numeric type.\n",
                "  # TODO(mattjj): allow constant-folding CPU-backed JAX arrays\n",
                "  c, = consts\n",
                "  o, = eqn.outvars\n",
                "  if (type(c) in {np.ndarray, *dtypes.python_scalar_dtypes} and\n",
                "      isinstance(o.aval, core.UnshapedArray) and not np.shape(c) and\n",
                "      not core.is_opaque_dtype(eqn.params['new_dtype'])):\n",
                "    out = np.array(c, eqn.params['new_dtype'])\n",
                "    if not o.aval.weak_type:\n",
                "      return [out], None\n",
                "    out = out.item()\n",
                "    if core.get_aval(out).dtype is o.aval.dtype:\n",
                "      return [out], None\n",
                "  return [None], eqn\n",
                "\n",
                "def _convert_elt_type_fwd_rule(eqn):\n",
                "  v, = eqn.invars\n",
                "  if (not core.is_opaque_dtype(eqn.params['new_dtype']) and\n",
                "      not core.is_opaque_dtype(v.aval.dtype) and\n",
                "      v.aval.dtype == eqn.params['new_dtype'] and\n",
                "      v.aval.weak_type == eqn.params['weak_type']):\n",
                "    return [v], None\n",
                "  else:\n",
                "    return [None], eqn\n",
                "\n",
                "def _convert_elt_type_pp_rule(eqn, context, settings):\n",
                "  # don't print new_dtype because the output binder shows it, don't print\n",
                "  # weak_type when false\n",
                "  printed_params = {}\n",
                "  if eqn.params['weak_type']:\n",
                "    printed_params['weak_type'] = True\n",
                "  lhs = core.pp_vars(eqn.outvars, context, print_shapes=settings.print_shapes)\n",
                "  rhs = [pp.text(eqn.primitive.name),\n",
                "         core.pp_kv_pairs(sorted(printed_params.items()), context, settings),\n",
                "         pp.text(\" \") + core.pp_vars(eqn.invars, context)]\n",
                "  annotation = (source_info_util.summarize(eqn.source_info)\n",
                "                if settings.source_info else None)\n",
                "  return [lhs, pp.text(\" = \", annotation=annotation), *rhs]\n",
                "\n",
                "convert_element_type_p = Primitive('convert_element_type')\n",
                "convert_element_type_p.def_impl(partial(xla.apply_primitive, convert_element_type_p))\n",
                "convert_element_type_p.def_abstract_eval(\n",
                "    partial(standard_abstract_eval, convert_element_type_p,\n",
                "            _convert_element_type_shape_rule, _convert_element_type_dtype_rule,\n",
                "            _convert_element_type_weak_type_rule, standard_named_shape_rule))\n",
                "ad.defjvp(convert_element_type_p, _convert_element_type_jvp_rule)\n",
                "ad.primitive_transposes[convert_element_type_p] = _convert_element_type_transpose_rule\n",
                "batching.defvectorized(convert_element_type_p)\n",
                "pe.const_fold_rules[convert_element_type_p] = _convert_elt_type_folding_rule\n",
                "pe.forwarding_rules[convert_element_type_p] = _convert_elt_type_fwd_rule\n",
                "pe.def_trivial_padding(convert_element_type_p)\n",
                "# TODO(mattjj): un-comment the next line (see #9456)\n",
                "# core.pp_eqn_rules[convert_element_type_p] = _convert_elt_type_pp_rule\n",
                "\n",
                "def _real_dtype(dtype): return np.finfo(dtype).dtype\n",
                "\n",
                "def _convert_element_type_lower(ctx, operand, *, new_dtype, weak_type):\n",
                "  aval_in, = ctx.avals_in\n",
                "  aval_out, = ctx.avals_out\n",
                "  if (dtypes.issubdtype(aval_in.dtype, np.complexfloating) and\n",
                "      not dtypes.issubdtype(new_dtype, np.complexfloating)):\n",
                "    operand = hlo.RealOp(operand).result\n",
                "    aval_in = aval_in.update(dtype=_real_dtype(aval_in.dtype))\n",
                "  return [mlir.convert_hlo(ctx, operand, aval_in, aval_out)]\n",
                "\n",
                "mlir.register_lowering(convert_element_type_p, _convert_element_type_lower)\n",
                "\n",
                "\n",
                "def _bitcast_convert_type_shape_rule(operand, *, new_dtype):\n",
                "  return operand.shape\n",
                "\n",
                "def _bitcast_convert_type_dtype_rule(operand, *, new_dtype):\n",
                "  old_dtype = dtypes.canonicalize_dtype(operand.dtype)\n",
                "  if dtypes.issubdtype(old_dtype, np.bool_) or dtypes.issubdtype(old_dtype, np.complexfloating):\n",
                "    if old_dtype != new_dtype:\n",
                "      raise TypeError(f\"`bitcast_convert_type` for operand type ({old_dtype}) cannot have different destination type ({new_dtype})\")\n",
                "  if np.dtype(old_dtype).itemsize != np.dtype(new_dtype).itemsize:\n",
                "    raise TypeError(f\"`bitcast_convert_type` for operand type ({old_dtype}) must have destination type ({new_dtype}) of same size.\")\n",
                "  return new_dtype\n",
                "\n",
                "bitcast_convert_type_p = standard_primitive(\n",
                "    _bitcast_convert_type_shape_rule, _bitcast_convert_type_dtype_rule,\n",
                "    'bitcast_convert_type', weak_type_rule=_strip_weak_type)\n",
                "ad.defjvp_zero(bitcast_convert_type_p)\n",
                "batching.defvectorized(bitcast_convert_type_p)\n",
                "\n",
                "def _bitcast_convert_type_lower(ctx, operand, *, new_dtype):\n",
                "  aval_out, = ctx.avals_out\n",
                "  return hlo.BitcastConvertOp(mlir.aval_to_ir_type(aval_out), operand).results\n",
                "\n",
                "mlir.register_lowering(bitcast_convert_type_p, _bitcast_convert_type_lower)\n",
                "\n",
                "\n",
                "def _validate_preferred_element_type(input_dtype, preferred_element_type):\n",
                "\n",
                "  if dtypes.issubdtype(input_dtype, np.integer) and dtypes.issubdtype(preferred_element_type, np.floating):\n",
                "    # Special-case integer->float multiply. This is allowed, and also allows\n",
                "    # different signedness between input and output.\n",
                "    pass\n",
                "  else:\n",
                "    allowed_types = (np.integer, np.floating, np.complexfloating)\n",
                "    if any(dtypes.issubdtype(input_dtype, t) and not dtypes.issubdtype(preferred_element_type, t) for t in allowed_types):\n",
                "      raise TypeError(\"Input type is incompatible with `preferred_element_type`. The compatible combinations of \"\n",
                "                      \"(input_type, preferred_element_type) are (integral, integral), (integral, floating), \"\n",
                "                      \"(floating, floating), (complex, complex.\")\n",
                "    if dtypes.issubdtype(input_dtype, np.signedinteger) and not dtypes.issubdtype(preferred_element_type, np.signedinteger):\n",
                "      raise TypeError(\"`preferred_element_type` must have the same signedness as the original type.\")\n",
                "  input_bitwidth = np.dtype(input_dtype).itemsize\n",
                "  preferred_bitwidth = np.dtype(preferred_element_type).itemsize\n",
                "  if preferred_bitwidth < input_bitwidth:\n",
                "    raise TypeError(\"`preferred_element_type` must not be narrower than the original type.\")\n",
                "\n",
                "def _precision_config(precision):\n",
                "  if precision is not None:\n",
                "    config = xla_client.PrecisionConfig()\n",
                "    if isinstance(precision, tuple):\n",
                "      config.operand_precision.extend(precision)\n",
                "    else:\n",
                "      config.operand_precision.extend((precision, precision))\n",
                "    return config\n",
                "  return None\n",
                "\n",
                "def _masked(padded_value, logical_shape, dimensions, value=0):\n",
                "  \"\"\"\n",
                "  Sets all padding to the given value (default is 0) in the given dimensions.\n",
                "  All values outside the logical shape are considered padding.\n",
                "  \"\"\"\n",
                "  if len(dimensions) == 0:\n",
                "    return padded_value\n",
                "\n",
                "  masks = [broadcasted_iota(np.int32, padded_value.shape, d) < logical_shape[d]\n",
                "           for d in dimensions]\n",
                "  mask_intersection = masks[0]\n",
                "  for mask in masks[1:]:\n",
                "    mask_intersection &= mask\n",
                "  return select(mask_intersection, padded_value, full_like(padded_value, value))\n",
                "\n",
                "\n",
                "def _dot_general_shape_rule(lhs, rhs, *, dimension_numbers, precision,\n",
                "                            preferred_element_type: Optional[DTypeLike]):\n",
                "  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers\n",
                "  if not all(np.all(np.greater_equal(d, 0)) and np.all(np.less(d, lhs.ndim))\n",
                "             for d in (lhs_contracting, lhs_batch)):\n",
                "    msg = (\"dot_general requires lhs dimension numbers to be nonnegative and \"\n",
                "           \"less than the number of axes of the lhs value, got \"\n",
                "           f\"lhs_batch of {lhs_batch} and lhs_contracting of {lhs_contracting} \"\n",
                "           f\"for lhs of rank {lhs.ndim}\")\n",
                "    raise TypeError(msg)\n",
                "  if not all(np.all(np.greater_equal(d, 0)) and np.all(np.less(d, rhs.ndim))\n",
                "             for d in (rhs_contracting, rhs_batch)):\n",
                "    msg = (\"dot_general requires rhs dimension numbers to be nonnegative and \"\n",
                "           \"less than the number of axes of the rhs value, got \"\n",
                "           f\"rhs_batch of {rhs_batch} and rhs_contracting of {rhs_contracting} \"\n",
                "           f\"for rhs of rank {rhs.ndim}\")\n",
                "    raise TypeError(msg)\n",
                "  if len(lhs_batch) != len(rhs_batch):\n",
                "    msg = (\"dot_general requires equal numbers of lhs_batch and rhs_batch \"\n",
                "           \"dimensions, got lhs_batch {} and rhs_batch {}.\")\n",
                "    raise TypeError(msg.format(lhs_batch, rhs_batch))\n",
                "  lhs_contracting_set, lhs_batch_set = set(lhs_contracting), set(lhs_batch)\n",
                "  rhs_contracting_set, rhs_batch_set = set(rhs_contracting), set(rhs_batch)\n",
                "  if len(lhs_batch_set) != len(lhs_batch):\n",
                "    msg = (\"dot_general requires lhs batch dimensions to be distinct, got \"\n",
                "           f\"lhs_batch {lhs_batch}.\")\n",
                "    raise TypeError(msg)\n",
                "  if len(rhs_batch_set) != len(rhs_batch):\n",
                "    msg = (\"dot_general requires rhs batch dimensions to be distinct, got \"\n",
                "           f\"rhs_batch {rhs_batch}.\")\n",
                "    raise TypeError(msg)\n",
                "  if len(lhs_contracting_set) != len(lhs_contracting):\n",
                "    msg = (\"dot_general requires lhs contracting dimensions to be distinct, \"\n",
                "           f\"got lhs_contracting {lhs_contracting}.\")\n",
                "    raise TypeError(msg)\n",
                "  if len(rhs_contracting_set) != len(rhs_contracting):\n",
                "    msg = (\"dot_general requires rhs contracting dimensions to be distinct, \"\n",
                "           f\"got rhs_contracting {rhs_contracting}.\")\n",
                "    raise TypeError(msg)\n",
                "  if lhs_contracting_set & lhs_batch_set:\n",
                "    msg = (\"dot_general requires lhs batch dimensions to be disjoint from \"\n",
                "           \"contracting dimensions, got lhs_batch {} and lhs_contracting {}.\")\n",
                "    raise TypeError(msg.format(lhs_batch, lhs_contracting))\n",
                "  if rhs_contracting_set & rhs_batch_set:\n",
                "    msg = (\"dot_general requires rhs batch dimensions to be disjoint from \"\n",
                "           \"contracting dimensions, got rhs_batch {} and rhs_contracting {}.\")\n",
                "    raise TypeError(msg.format(rhs_batch, rhs_contracting))\n",
                "  lhs_batch_shape = tuple(lhs.shape[i] for i in lhs_batch)\n",
                "  rhs_batch_shape = tuple(rhs.shape[i] for i in rhs_batch)\n",
                "  if not core.symbolic_equal_shape(lhs_batch_shape, rhs_batch_shape):\n",
                "    msg = (\"dot_general requires lhs batch dimensions and rhs batch dimensions \"\n",
                "           \"to have the same shape, got {} and {}.\")\n",
                "    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))\n",
                "  lhs_contracting_shape = tuple(lhs.shape[i] for i in lhs_contracting)\n",
                "  rhs_contracting_shape = tuple(rhs.shape[i] for i in rhs_contracting)\n",
                "  if not core.symbolic_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n",
                "    msg = (\"dot_general requires contracting dimensions to have the same \"\n",
                "           \"shape, got {} and {}.\")\n",
                "    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))\n",
                "\n",
                "  return _dot_general_shape_computation(lhs.shape, rhs.shape, dimension_numbers)\n",
                "\n",
                "def _dot_general_shape_computation(lhs_shape, rhs_shape, dimension_numbers):\n",
                "  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers\n",
                "  batch_shape = tuple(lhs_shape[i] for i in lhs_batch)\n",
                "  lhs_contract_or_batch = tuple(sorted(tuple(lhs_contracting) + tuple(lhs_batch)))\n",
                "  lhs_tensored_shape = tuple_delete(lhs_shape, lhs_contract_or_batch)\n",
                "  rhs_contract_or_batch = tuple(sorted(tuple(rhs_contracting) + tuple(rhs_batch)))\n",
                "  rhs_tensored_shape = tuple_delete(rhs_shape, rhs_contract_or_batch)\n",
                "  return batch_shape + lhs_tensored_shape + rhs_tensored_shape\n",
                "\n",
                "def tuple_delete(tup, idx):\n",
                "  idx_ = set(idx)\n",
                "  return tuple(tup[i] for i in range(len(tup)) if i not in idx_)\n",
                "\n",
                "\n",
                "def _dot_general_dtype_rule(lhs, rhs, *, dimension_numbers, precision,\n",
                "                            preferred_element_type: Optional[DTypeLike]):\n",
                "  input_dtype = naryop_dtype_rule(_input_dtype, [_any, _any], 'dot_general', lhs, rhs)\n",
                "  if preferred_element_type is None:\n",
                "    return input_dtype\n",
                "  _validate_preferred_element_type(input_dtype, preferred_element_type)\n",
                "  return preferred_element_type\n",
                "\n",
                "def _dot_general_transpose_lhs(g, y, *, dimension_numbers, precision,\n",
                "                               preferred_element_type: Optional[DTypeLike],\n",
                "                               swap_ans=False):\n",
                "  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers\n",
                "  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)\n",
                "  x_kept = remaining(range(x_ndim), x_contract, x_batch)\n",
                "  y_kept = remaining(range(y.ndim), y_contract, y_batch)\n",
                "  if swap_ans:\n",
                "    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)\n",
                "  else:\n",
                "    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)\n",
                "  dims = ((ans_y, y_kept), (ans_batch, y_batch))\n",
                "  x_contract_sorted_by_y = list(np.take(x_contract, np.argsort(y_contract)))  # type: ignore[arg-type]\n",
                "  out_axes = np.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)\n",
                "  return transpose(dot_general(g, y, dims, precision=precision,\n",
                "                               preferred_element_type=preferred_element_type),\n",
                "                   tuple(out_axes))\n",
                "\n",
                "def _dot_general_transpose_rhs(g, x, *, dimension_numbers, precision,\n",
                "                               preferred_element_type: Optional[DTypeLike]):\n",
                "  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers\n",
                "  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))\n",
                "  return _dot_general_transpose_lhs(\n",
                "    g, x, dimension_numbers=swapped_dimension_numbers, precision=precision,\n",
                "    preferred_element_type=preferred_element_type,\n",
                "    swap_ans=True)\n",
                "\n",
                "\n",
                "def _dot_general_batch_rule(batched_args, batch_dims, *, dimension_numbers,\n",
                "                            precision,\n",
                "                            preferred_element_type: Optional[DTypeLike]):\n",
                "  lhs, rhs = batched_args\n",
                "  lbd, rbd = batch_dims\n",
                "  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers\n",
                "  if (type(lbd) is type(rbd) is ConcatAxis and\n",
                "      lbd.axis in lhs_contract and rbd.axis in rhs_contract):\n",
                "    # first handle any other part of the dot with these as batch dims\n",
                "    lhs_contract_ = [d for d in lhs_contract if d != lbd.axis]\n",
                "    rhs_contract_ = [d for d in rhs_contract if d != rbd.axis]\n",
                "    lhs_batch_ = (lbd.axis, *lhs_batch)\n",
                "    rhs_batch_ = (rbd.axis, *rhs_batch)\n",
                "    new_dnums = ((lhs_contract_, rhs_contract_), (lhs_batch_, rhs_batch_))\n",
                "    out = dot_general(lhs, rhs, new_dnums, precision=precision,\n",
                "                      preferred_element_type=preferred_element_type)\n",
                "    # now a segment sum along that batch axis\n",
                "    return batching.segment_sum(out, lbd.segment_lengths), 0\n",
                "\n",
                "  new_dimension_numbers, result_batch_dim = _dot_general_batch_dim_nums(\n",
                "      (lhs.ndim, rhs.ndim), batch_dims, dimension_numbers)\n",
                "  batched_out = dot_general(lhs, rhs, new_dimension_numbers,\n",
                "                            precision=precision,\n",
                "                            preferred_element_type=preferred_element_type)\n",
                "  return batched_out, result_batch_dim\n",
                "\n",
                "def _dot_general_batch_dim_nums(ndims, batch_dims, dimension_numbers):\n",
                "  # there are three kinds of dimensions in a dot_general:\n",
                "  # - contraction dimensions appear in lhs and rhs but not the result\n",
                "  # - batch dimensions appear in lhs, rhs, and result\n",
                "  # - tensor product dimensions appear in the result and one of lhs or rhs\n",
                "  lhs_ndim, rhs_ndim = ndims\n",
                "  lbd, rbd = batch_dims\n",
                "  assert lbd is not None or rbd is not None\n",
                "  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers\n",
                "\n",
                "  def bump_dims(dims, b):\n",
                "    return tuple(np.add(dims, np.greater_equal(dims, b)))\n",
                "\n",
                "  if type(lbd) is type(rbd) is int:\n",
                "    # adding a batch dimension\n",
                "    lhs_batch = (lbd,) + bump_dims(lhs_batch, lbd)\n",
                "    rhs_batch = (rbd,) + bump_dims(rhs_batch, rbd)\n",
                "    lhs_contract = bump_dims(lhs_contract, lbd)\n",
                "    rhs_contract = bump_dims(rhs_contract, rbd)\n",
                "    result_batch_dim = 0\n",
                "  elif rbd is None and type(lbd) is ConcatAxis and lbd.axis not in lhs_contract:\n",
                "    if lbd.axis in lhs_batch:\n",
                "      axis = int(np.sum(np.less(lhs_batch, lbd.axis)))\n",
                "    else:\n",
                "      lhs_tensor = [d for d in range(lhs_ndim)\n",
                "                    if d not in lhs_batch and d not in lhs_contract]\n",
                "      axis = len(lhs_batch) + int(np.sum(np.less(lhs_tensor, lbd.axis)))\n",
                "    result_batch_dim = ConcatAxis(axis, lbd.segment_lengths)\n",
                "  elif lbd is None and type(rbd) is ConcatAxis and rbd.axis not in rhs_contract:\n",
                "    if rbd.axis in rhs_batch:\n",
                "      axis = int(np.sum(np.less(rhs_batch, rbd.axis)))\n",
                "    else:\n",
                "      rhs_tensor = [d for d in range(rhs_ndim)\n",
                "                    if d not in rhs_batch and d not in rhs_contract]\n",
                "      axis = (lhs_ndim - len(lhs_contract) +\n",
                "              int(sum(np.less(rhs_tensor, rbd.axis))))\n",
                "    result_batch_dim = ConcatAxis(axis, rbd.segment_lengths)\n",
                "  elif (type(lbd) is int and\n",
                "        (rbd is None or type(rbd) is ConcatAxis and\n",
                "         rbd.axis not in rhs_contract)):\n",
                "    lhs_tensor = [d for d in range(lhs_ndim)\n",
                "                  if d not in lhs_batch and d not in lhs_contract]\n",
                "    result_batch_dim = len(lhs_batch) + int(sum(np.less(lhs_tensor, lbd)))\n",
                "    lhs_batch = bump_dims(lhs_batch, lbd)\n",
                "    lhs_contract = bump_dims(lhs_contract, lbd)\n",
                "  elif (type(rbd) is int and\n",
                "        (lbd is None or type(lbd) is ConcatAxis and\n",
                "         lbd.axis not in lhs_contract)):\n",
                "    rhs_tensor = [d for d in range(rhs_ndim)\n",
                "                  if d not in rhs_batch and d not in rhs_contract]\n",
                "    result_batch_dim = (lhs_ndim - len(lhs_contract) +\n",
                "                        int(sum(np.less(rhs_tensor, rbd))))\n",
                "    rhs_batch = bump_dims(rhs_batch, rbd)\n",
                "    rhs_contract = bump_dims(rhs_contract, rbd)\n",
                "  else:\n",
                "    assert False\n",
                "\n",
                "  new_dimension_numbers = ((lhs_contract, rhs_contract), (lhs_batch, rhs_batch))\n",
                "  return new_dimension_numbers, result_batch_dim\n",
                "\n",
                "def _dot_general_padding_rule(in_avals, out_avals, lhs, rhs, *,\n",
                "                              dimension_numbers, **params):\n",
                "  lhs_aval, _ = in_avals\n",
                "  (lhs_contract, _), _ = dimension_numbers\n",
                "  padded_axes = [(i, lhs_aval.shape[i].val) for i in lhs_contract\n",
                "                 if isinstance(lhs_aval.shape[i], pe.BoundedAxisSize)]\n",
                "  lhs_ = _replace_masked_values(lhs, 0, padded_axes)\n",
                "  return [dot_general(lhs_, rhs, dimension_numbers=dimension_numbers, **params)]\n",
                "\n",
                "def _dot_general_pp_rule(eqn, context, settings):\n",
                "  # * suppress printing precision or preferred_element_type when None.\n",
                "  # * print dimension_numbers as list-of-lists to be shorter.\n",
                "  printed_params = {k: v for k, v in eqn.params.items() if v is not None}\n",
                "  (lhs_cont, rhs_cont), (lhs_batch, rhs_batch) = eqn.params['dimension_numbers']\n",
                "  printed_params['dimension_numbers'] = (\n",
                "      (list(lhs_cont), list(rhs_cont)), (list(lhs_batch), list(rhs_batch)))\n",
                "  lhs = core.pp_vars(eqn.outvars, context, print_shapes=settings.print_shapes)\n",
                "  rhs = [pp.text(eqn.primitive.name),\n",
                "         core.pp_kv_pairs(sorted(printed_params.items()), context, settings),\n",
                "         pp.text(\" \") + core.pp_vars(eqn.invars, context)]\n",
                "  annotation = (source_info_util.summarize(eqn.source_info)\n",
                "                if settings.source_info else None)\n",
                "  return [lhs, pp.text(\" = \", annotation=annotation), *rhs]\n",
                "\n",
                "\n",
                "dot_general_p = standard_primitive(_dot_general_shape_rule,\n",
                "                                   _dot_general_dtype_rule, 'dot_general')\n",
                "ad.defbilinear(dot_general_p,\n",
                "               _dot_general_transpose_lhs, _dot_general_transpose_rhs)\n",
                "batching.primitive_batchers[dot_general_p] = _dot_general_batch_rule\n",
                "pe.padding_rules[dot_general_p] = _dot_general_padding_rule\n",
                "core.pp_eqn_rules[dot_general_p] = _dot_general_pp_rule\n",
                "\n",
                "def precision_attr(precision: PrecisionType) -> ir.ArrayAttr:\n",
                "  if precision is None:\n",
                "    full_precision = (Precision.DEFAULT, Precision.DEFAULT)\n",
                "  elif not isinstance(precision, tuple):\n",
                "    full_precision = (precision, precision)\n",
                "  else:\n",
                "    full_precision = precision\n",
                "  return ir.ArrayAttr.get(\n",
                "      [hlo.PrecisionAttr.get(str(p)) for p in full_precision])\n",
                "\n",
                "\n",
                "\n",
                "def _dot_general_lower(ctx, lhs, rhs, *, dimension_numbers,\n",
                "                       precision, preferred_element_type: Optional[np.dtype]):\n",
                "  del preferred_element_type  # Implied by the output aval\n",
                "  lhs_aval, rhs_aval = ctx.avals_in\n",
                "  aval_out, = ctx.avals_out\n",
                "  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers\n",
                "\n",
                "  # TODO(b/195364460): Work around slow XLA/CPU implementation of float16 matmul\n",
                "  if ctx.module_context.platform == \"cpu\":\n",
                "    if lhs_aval.dtype == np.float16:\n",
                "      f32 = mlir.dtype_to_ir_type(np.dtype(np.float32))\n",
                "      lhs = hlo.ConvertOp(ir.RankedTensorType.get(lhs_aval.shape, f32),\n",
                "                          lhs).result\n",
                "    if rhs_aval.dtype == np.float16:\n",
                "      f32 = mlir.dtype_to_ir_type(np.dtype(np.float32))\n",
                "      rhs = hlo.ConvertOp(ir.RankedTensorType.get(rhs_aval.shape, f32),\n",
                "                          rhs).result\n",
                "  dot_dnums = hlo.DotDimensionNumbers.get(\n",
                "      lhs_batching_dimensions=list(lhs_batch),\n",
                "      rhs_batching_dimensions=list(rhs_batch),\n",
                "      lhs_contracting_dimensions=list(lhs_contracting),\n",
                "      rhs_contracting_dimensions=list(rhs_contracting))\n",
                "  return [\n",
                "      hlo.DotGeneralOp(\n",
                "          mlir.aval_to_ir_type(aval_out),\n",
                "          lhs,\n",
                "          rhs,\n",
                "          dot_dnums,\n",
                "          precision_config=precision_attr(precision)).result\n",
                "  ]\n",
                "\n",
                "mlir.register_lowering(dot_general_p, _dot_general_lower)\n",
                "\n",
                "\n",
                "def _broadcast_in_dim_shape_rule(operand, *, shape, broadcast_dimensions):\n",
                "  _check_shapelike('broadcast_in_dim', 'shape', shape)\n",
                "  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',\n",
                "                   broadcast_dimensions)\n",
                "  operand_ndim = np.ndim(operand)\n",
                "  if operand_ndim != len(broadcast_dimensions):\n",
                "    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '\n",
                "           'operand ndim; got broadcast_dimensions {} for operand ndim {}.')\n",
                "    raise TypeError(msg.format(broadcast_dimensions, operand_ndim))\n",
                "  if len(shape) < operand_ndim:\n",
                "    msg = ('broadcast_in_dim target broadcast shape must have equal or higher rank '\n",
                "           'to the operand shape; got operand ndim {} and target broadcast ndim {}.')\n",
                "    raise TypeError(msg.format(operand_ndim, len(shape)))\n",
                "  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):\n",
                "    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '\n",
                "           'dimensions, got {} for operand ndim {} and shape {}.')\n",
                "    raise TypeError(msg.format(broadcast_dimensions, operand_ndim, shape))\n",
                "  if not all(core.symbolic_equal_one_of_dim(operand.shape[i],\n",
                "                                            [1, shape[broadcast_dimensions[i]]])\n",
                "             for i in range(operand_ndim)):\n",
                "    msg = (\n",
                "        \"broadcast_in_dim operand dimension sizes must either be 1, or be \"\n",
                "        \"equal to their corresponding dimensions in the target broadcast \"\n",
                "        \"shape; got operand of shape {}, target broadcast shape {}, \"\n",
                "        \"broadcast_dimensions {} \")\n",
                "    raise TypeError(msg.format(operand.shape, shape, broadcast_dimensions))\n",
                "  if (len(broadcast_dimensions) != len(set(broadcast_dimensions)) or\n",
                "      tuple(broadcast_dimensions) != tuple(sorted(broadcast_dimensions))):\n",
                "    msg = (\"broadcast_in_dim broadcast_dimensions must be strictly increasing; \"\n",
                "           \"got broadcast_dimensions {}\")\n",
                "    raise TypeError(msg.format(broadcast_dimensions))\n",
                "\n",
                "  return shape\n",
                "\n",
                "def _broadcast_in_dim_typecheck_rule(\n",
                "    operand, *dyn_shape, shape, broadcast_dimensions):\n",
                "  if not dyn_shape:\n",
                "    out_aval, effects = broadcast_in_dim_p.abstract_eval(\n",
                "        operand.aval, shape=shape, broadcast_dimensions=broadcast_dimensions)\n",
                "    return [out_aval], effects\n",
                "  else:\n",
                "    # TODO(mattjj): perform more checks like _broadcast_in_dim_shape_rule\n",
                "    out_shape = _merge_dyn_shape(shape, dyn_shape)\n",
                "    out_shape = [x.val if type(x) is core.Literal else x for x in out_shape]  # pytype: disable=attribute-error\n",
                "    out_aval = core.DShapedArray(tuple(out_shape), operand.aval.dtype,\n",
                "                                 operand.aval.weak_type)\n",
                "    return [out_aval], core.no_effects\n",
                "\n",
                "def _broadcast_in_dim_transpose_rule(ct, operand, *dyn_shape,\n",
                "                                     shape, broadcast_dimensions):\n",
                "  if type(ct) is ad_util.Zero:\n",
                "    return [ad_util.Zero(operand.aval)]\n",
                "  unit_dims = [i for i, s in enumerate(operand.aval.shape)\n",
                "               if core.symbolic_equal_dim(s,  1)]\n",
                "  bdims = tuple(np.delete(broadcast_dimensions, unit_dims))\n",
                "  axes = tuple(np.delete(range(len(shape)), bdims))\n",
                "  return ([expand_dims(_reduce_sum(ct, axes), unit_dims)] +\n",
                "          [None] * len(dyn_shape))\n",
                "\n",
                "def _broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,\n",
                "                                 broadcast_dimensions):\n",
                "  operand, *dyn_shape = batched_args\n",
                "  operand_bdim, *dyn_shape_bdims = batch_dims\n",
                "  if len(dyn_shape) > 1: raise NotImplementedError\n",
                "  if (operand_bdim is not None and\n",
                "      (not dyn_shape_bdims or dyn_shape_bdims[0] is None)):\n",
                "    new_operand = batching.moveaxis(operand, operand_bdim, 0)\n",
                "    new_shape = (operand.shape[operand_bdim],) + shape\n",
                "    new_broadcast_dimensions = (0,) + tuple(np.add(1, broadcast_dimensions))\n",
                "    return broadcast_in_dim(new_operand, new_shape, new_broadcast_dimensions), 0\n",
                "  elif (operand_bdim is None and dyn_shape_bdims and\n",
                "        dyn_shape_bdims[0] is not None):\n",
                "    (d,), (d_bdim,) = dyn_shape, dyn_shape_bdims  # NotImplementedError above\n",
                "    assert d_bdim == 0  # must be scalar in the program to be batched\n",
                "    new_shape = _merge_dyn_shape(shape, (int(d.sum()),))\n",
                "    out = broadcast_in_dim(operand, new_shape, broadcast_dimensions)\n",
                "    idx, = (i for i, s in enumerate(shape) if s is None)\n",
                "    return out, batching.ConcatAxis(idx, d)\n",
                "  else:\n",
                "    raise NotImplementedError  # TODO(mattjj)\n",
                "\n",
                "def _broadcast_in_dim_fwd_rule(eqn):\n",
                "  v, *dyn = eqn.invars\n",
                "  if not dyn and core.symbolic_equal_shape(eqn.params['shape'], v.aval.shape):\n",
                "    return [v], None\n",
                "  else:\n",
                "    return [None], eqn\n",
                "\n",
                "def _broadcast_in_dim_staging_rule(\n",
                "    trace, x, *dyn, shape, broadcast_dimensions):\n",
                "  params = dict(shape=shape, broadcast_dimensions=broadcast_dimensions)\n",
                "  if not dyn:\n",
                "    return trace.default_process_primitive(broadcast_in_dim_p, (x,), params)\n",
                "  aval = core.DShapedArray(_merge_dyn_shape(shape, dyn), x.dtype, x.weak_type)\n",
                "  return _dyn_shape_staging_rule(trace, broadcast_in_dim_p, aval, x, *dyn,\n",
                "                                 **params)\n",
                "\n",
                "def _broadcast_in_dim_padding_rule(in_avals, out_avals, x, *dyn_shape,\n",
                "                                   shape, broadcast_dimensions):\n",
                "  del in_avals, dyn_shape\n",
                "  out_aval, = out_avals\n",
                "  new_shape = []\n",
                "  new_dyn_shape = []\n",
                "  for d in out_aval.shape:\n",
                "    if type(d) is pe.BoundedAxisSize:\n",
                "      new_shape.append(d.bound)\n",
                "    elif type(d) is int:\n",
                "      new_shape.append(d)\n",
                "    else:\n",
                "      assert isinstance(d, core.Tracer)\n",
                "      new_shape.append(None)\n",
                "      new_dyn_shape.append(d)\n",
                "  return [broadcast_in_dim_p.bind(x, *new_dyn_shape, shape=tuple(new_shape),\n",
                "                                  broadcast_dimensions=broadcast_dimensions)]\n",
                "\n",
                "def _broadcast_in_dim_jvp_rule(primals, tangents, *, shape, broadcast_dimensions):\n",
                "  operand, *dyn_shape = primals\n",
                "  operand_dot, *_ = tangents\n",
                "  y = broadcast_in_dim_p.bind(operand, *dyn_shape, shape=shape,\n",
                "                              broadcast_dimensions=broadcast_dimensions)\n",
                "  if type(operand_dot) is ad_util.Zero:\n",
                "    y_dot = ad_util.Zero.from_value(y)\n",
                "  else:\n",
                "    y_dot = broadcast_in_dim_p.bind(operand_dot, *dyn_shape, shape=shape,\n",
                "                                    broadcast_dimensions=broadcast_dimensions)\n",
                "  return y, y_dot\n",
                "\n",
                "def _broadcast_in_dim_partial_eval(\n",
                "    trace, operand, *dyn_shape, shape, broadcast_dimensions):\n",
                "  if not dyn_shape:\n",
                "    return trace.default_process_primitive(\n",
                "        broadcast_in_dim_p, (operand, *dyn_shape),\n",
                "        dict(shape=shape, broadcast_dimensions=broadcast_dimensions))\n",
                "  assert all(t.pval.is_known() for t in dyn_shape)\n",
                "  operand_tracer = trace.instantiate_const(operand)\n",
                "  dyn_shape_tracers = map(trace.instantiate_const, dyn_shape)\n",
                "  dyn_shape_tracers_ = iter(dyn_shape_tracers)\n",
                "  shape_ = [next(dyn_shape_tracers_) if d is None else d for d in shape]\n",
                "  out_aval = core.DShapedArray(tuple(shape_), operand.dtype, operand.weak_type)\n",
                "  out_tracer = pe.JaxprTracer(trace, pe.PartialVal.unknown(out_aval), None)\n",
                "  eqn = pe.new_eqn_recipe(\n",
                "      [operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p,\n",
                "      dict(shape=shape, broadcast_dimensions=broadcast_dimensions),\n",
                "      core.no_effects, source_info_util.current())\n",
                "  out_tracer.recipe = eqn\n",
                "  return out_tracer\n",
                "\n",
                "def _broadcast_in_dim_lower(ctx, x, *dyn_shape, shape, broadcast_dimensions) -> Sequence[ir.Value]:\n",
                "  aval_out, = ctx.avals_out\n",
                "  if dyn_shape:\n",
                "    aval_out = aval_out.update(shape=_merge_dyn_shape(shape, dyn_shape))\n",
                "\n",
                "\n",
                "  return [mlir.broadcast_in_dim(ctx, x, aval_out,\n",
                "                                broadcast_dimensions=broadcast_dimensions)]\n",
                "\n",
                "def _broadcast_in_dim_pp_rule(eqn, context, settings):\n",
                "  # Don't print shape or trivial broadcast_dimensions in params, since it can be\n",
                "  # inferred from the let-binder's type annotation.\n",
                "  printed_params = {}\n",
                "  if eqn.params['broadcast_dimensions']:\n",
                "    printed_params['broadcast_dimensions'] = eqn.params['broadcast_dimensions']\n",
                "  lhs = core.pp_vars(eqn.outvars, context, print_shapes=settings.print_shapes)\n",
                "  rhs = [pp.text(eqn.primitive.name),\n",
                "         core.pp_kv_pairs(sorted(printed_params.items()), context, settings),\n",
                "         pp.text(\" \") + core.pp_vars(eqn.invars[:1], context)]\n",
                "  annotation = (source_info_util.summarize(eqn.source_info)\n",
                "                if settings.source_info else None)\n",
                "  return [lhs, pp.text(\" = \", annotation=annotation), *rhs]\n",
                "\n",
                "def _broadcast_in_dim_abstract_eval(x, *dyn_shape, shape, broadcast_dimensions):\n",
                "  if dyn_shape: raise NotImplementedError\n",
                "  del dyn_shape\n",
                "  if not any(isinstance(d, core.DArray) and\n",
                "             type(core.get_aval(d).dtype) is core.bint for d in shape):\n",
                "    shape = _broadcast_in_dim_shape_rule(  # error checking\n",
                "        x, shape=shape, broadcast_dimensions=broadcast_dimensions)\n",
                "    return core.ShapedArray(shape, x.dtype, x.weak_type, x.named_shape)\n",
                "  # If any BInts in shape, produce a DShapedArray (even if x is a ShapedArray)\n",
                "  # TODO(mattjj): unify DShapedArray with ShapedArray, and remove this code\n",
                "  return core.DShapedArray(shape, x.dtype, x.weak_type)\n",
                "\n",
                "broadcast_in_dim_p = standard_primitive(\n",
                "    _broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')\n",
                "broadcast_in_dim_p.def_abstract_eval(_broadcast_in_dim_abstract_eval)\n",
                "ad.primitive_jvps[broadcast_in_dim_p] = _broadcast_in_dim_jvp_rule\n",
                "ad.primitive_transposes[broadcast_in_dim_p] = _broadcast_in_dim_transpose_rule\n",
                "batching.primitive_batchers[broadcast_in_dim_p] = _broadcast_in_dim_batch_rule\n",
                "pe.forwarding_rules[broadcast_in_dim_p] = _broadcast_in_dim_fwd_rule\n",
                "pe.custom_partial_eval_rules[broadcast_in_dim_p] = _broadcast_in_dim_partial_eval\n",
                "pe.custom_staging_rules[broadcast_in_dim_p] = _broadcast_in_dim_staging_rule\n",
                "pe.padding_rules[broadcast_in_dim_p] = _broadcast_in_dim_padding_rule\n",
                "core.custom_typechecks[broadcast_in_dim_p] = _broadcast_in_dim_typecheck_rule\n",
                "mlir.register_lowering(broadcast_in_dim_p, _broadcast_in_dim_lower)\n",
                "# TODO(mattjj): un-comment the next line\n",
                "# core.pp_eqn_rules[broadcast_in_dim_p] = _broadcast_in_dim_pp_rule\n",
                "\n",
                "\n",
                "def _clamp_shape_rule(min, operand, max):\n",
                "  if min.shape and min.shape != operand.shape:\n",
                "    raise TypeError(\"clamp requires min.shape == operand.shape or min.shape == \"\n",
                "                    f\"(), got min.shape={min.shape}, {operand.shape=}.\")\n",
                "  if max.shape and max.shape != operand.shape:\n",
                "    raise TypeError(\"clamp requires max.shape == operand.shape or max.shape == \"\n",
                "                    f\"(), got max.shape={max.shape}, {operand.shape=}.\")\n",
                "  return operand.shape\n",
                "\n",
                "_clamp_dtype_rule = partial(naryop_dtype_rule, _input_dtype, [_any, _any, _any],\n",
                "                            'clamp')\n",
                "\n",
                "def _clamp_batch_rule(batched_args, batch_dims, **params):\n",
                "  min, x, max = batched_args\n",
                "  min_bdim, x_bdim, max_bdim = batch_dims\n",
                "  size = next(x.shape[i] for x, i in zip(batched_args, batch_dims)\n",
                "              if i is not None)\n",
                "\n",
                "  # avoid transposes and some broadcasts in special cases\n",
                "  if min_bdim == x_bdim == max_bdim:\n",
                "    if np.shape(min) == np.shape(x) == np.shape(max):\n",
                "      return clamp_p.bind(min, x, max), x_bdim\n",
                "    elif np.ndim(min) == np.ndim(max) == 0:\n",
                "      return clamp_p.bind(min, x, max), x_bdim\n",
                "    elif np.ndim(min) == np.ndim(max) == 1:\n",
                "      min = broadcast_in_dim(min, x.shape, [min_bdim])\n",
                "      max = broadcast_in_dim(max, x.shape, [max_bdim])\n",
                "      return clamp_p.bind(min, x, max), x_bdim\n",
                "  elif np.ndim(min) == 0 and np.ndim(max) == 0 and x_bdim is not None:\n",
                "    return clamp_p.bind(min, x, max), x_bdim\n",
                "\n",
                "  min = batching.bdim_at_front(min, min_bdim, size) if np.shape(min) else min\n",
                "  max = batching.bdim_at_front(max, max_bdim, size) if np.shape(max) else max\n",
                "  x = batching.bdim_at_front(x, x_bdim, size) if np.shape(x) else x\n",
                "  if np.ndim(min) == 0 and np.ndim(x) > 0:\n",
                "    min = broadcast(min, x.shape)\n",
                "  if np.ndim(max) == 0 and np.ndim(x) > 0:\n",
                "    max = broadcast(max, x.shape)\n",
                "  if 0 < np.ndim(min) < np.ndim(x):\n",
                "    assert np.ndim(min) == 1, np.ndim(min)\n",
                "    min = broadcast_in_dim(min, x.shape, [0])\n",
                "  if 0 < np.ndim(max) < np.ndim(x):\n",
                "    assert np.ndim(max) == 1, np.ndim(max)\n",
                "    max = broadcast_in_dim(max, x.shape, [0])\n",
                "  if np.ndim(min) > np.ndim(x):\n",
                "    assert np.ndim(x) == 0, np.ndim(x)\n",
                "    x = broadcast(x, min.shape)\n",
                "  return clamp_p.bind(min, x, max), 0\n",
                "\n",
                "clamp_p = standard_primitive(_clamp_shape_rule, _clamp_dtype_rule, 'clamp')\n",
                "ad.defjvp(clamp_p,\n",
                "          lambda g, min, operand, max:\n",
                "          select(bitwise_and(gt(min, operand), lt(min, max)),\n",
                "                 g, _zeros(operand)),\n",
                "          lambda g, min, operand, max:\n",
                "          select(bitwise_and(gt(operand, min), lt(operand, max)),\n",
                "                 g, _zeros(operand)),\n",
                "          lambda g, min, operand, max:\n",
                "          select(lt(max, operand), g, _zeros(operand)))\n",
                "batching.primitive_batchers[clamp_p] = _clamp_batch_rule\n",
                "mlir.register_lowering(\n",
                "    clamp_p, partial(_nary_lower_hlo, hlo.ClampOp))\n",
                "pe.def_trivial_padding(clamp_p)\n",
                "\n",
                "def _concatenate_shape_rule(*operands, **kwargs):\n",
                "  dimension = kwargs.pop('dimension')\n",
                "  if not operands:\n",
                "    msg = \"concatenate expects at least one operand, got 0.\"\n",
                "    raise TypeError(msg)\n",
                "  if not all(isinstance(operand, UnshapedArray) for operand in operands):\n",
                "    msg = \"All objects to concatenate must be arrays, got {}.\"\n",
                "    op = next(op for op in operands if not isinstance(op, UnshapedArray))\n",
                "    raise TypeError(msg.format(type(op)))\n",
                "  if len({operand.ndim for operand in operands}) != 1:\n",
                "    msg = \"Cannot concatenate arrays with different numbers of dimensions: got {}.\"\n",
                "    raise TypeError(msg.format(\", \".join(str(o.shape) for o in operands)))\n",
                "  if not 0 <= dimension < operands[0].ndim:\n",
                "    msg = \"concatenate dimension out of bounds: dimension {} for shapes {}.\"\n",
                "    raise TypeError(msg.format(dimension, \", \".join([str(o.shape) for o in operands])))\n",
                "  shapes = [operand.shape[:dimension] + operand.shape[dimension+1:]\n",
                "            for operand in operands]\n",
                "  if not shapes[:-1] == shapes[1:]:\n",
                "    msg = (\"Cannot concatenate arrays with shapes that differ in dimensions \"\n",
                "           \"other than the one being concatenated: concatenating along \"\n",
                "           \"dimension {} for shapes {}.\")\n",
                "    shapes = [operand.shape for operand in operands]\n",
                "    raise TypeError(msg.format(dimension, \", \".join(map(str, shapes))))\n",
                "\n",
                "  concat_size = sum(o.shape[dimension] for o in operands)\n",
                "  ex_shape = operands[0].shape\n",
                "  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]\n",
                "\n",
                "def _concatenate_dtype_rule(*operands, **kwargs):\n",
                "  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))\n",
                "  return operands[0].dtype\n",
                "\n",
                "def _concatenate_transpose_rule(t, *operands, dimension):\n",
                "  operand_shapes = [o.aval.shape if ad.is_undefined_primal(o) else o.shape\n",
                "                    for o in operands]\n",
                "  if type(t) is ad_util.Zero:\n",
                "    return [ad_util.Zero(o.aval) if ad.is_undefined_primal(o) else None\n",
                "            for o in operands]\n",
                "  else:\n",
                "    limit_points = np.cumsum(\n",
                "        [shape[dimension] for shape in operand_shapes]).tolist()\n",
                "    starts = np.zeros((len(operands), t.ndim), dtype=int).tolist()\n",
                "    limits = np.tile(t.shape, (len(operands), 1)).tolist()\n",
                "\n",
                "    for i, s in enumerate(starts[1:]):\n",
                "      s[dimension] = limit_points[:-1][i]\n",
                "    for i, l in enumerate(limits):\n",
                "      l[dimension] = limit_points[i]\n",
                "\n",
                "    return [slicing.slice(t, start, limit) if ad.is_undefined_primal(o)\n",
                "            else None for o, start, limit in zip(operands, starts, limits)]\n",
                "\n",
                "def _concatenate_batch_rule(batched_args, batch_dims, *, dimension):\n",
                "  size = next(op.shape[bdim] for op, bdim in zip(batched_args, batch_dims)\n",
                "              if bdim is not None)\n",
                "  operands = [batching.moveaxis(op, bdim, 0) if bdim is not None\n",
                "              else broadcast(op, (size,))\n",
                "              for op, bdim in zip(batched_args, batch_dims)]\n",
                "  return concatenate(operands, dimension + 1), 0\n",
                "\n",
                "def _concatenate_pad_rule(in_avals, out_avals, *operands, dimension):\n",
                "  if all(isinstance(a.shape[dimension], (int, np.integer))\n",
                "         for a in in_avals):\n",
                "    return [concatenate(operands, dimension)]\n",
                "  else:\n",
                "    raise NotImplementedError  # TODO(mattjj)\n",
                "\n",
                "concatenate_p = standard_primitive(\n",
                "    _concatenate_shape_rule, _concatenate_dtype_rule, 'concatenate')\n",
                "ad.deflinear2(concatenate_p, _concatenate_transpose_rule)\n",
                "ad.primitive_transposes[concatenate_p] = _concatenate_transpose_rule\n",
                "batching.primitive_batchers[concatenate_p] = _concatenate_batch_rule\n",
                "pe.padding_rules[concatenate_p] = _concatenate_pad_rule\n",
                "\n",
                "def _concatenate_lower(ctx, *xs, dimension):\n",
                "  return hlo.ConcatenateOp(xs, mlir.i64_attr(dimension)).results\n",
                "mlir.register_lowering(concatenate_p, _concatenate_lower)\n",
                "\n",
                "\n",
                "def _pad_dtype_rule(operand, padding_value, *, padding_config):\n",
                "  if operand.dtype != padding_value.dtype:\n",
                "    msg = \"pad operand and padding_value must be same dtype: got {} and {}.\"\n",
                "    raise TypeError(msg.format(operand.dtype, padding_value.dtype))\n",
                "\n",
                "  return _input_dtype(operand, padding_value)\n",
                "\n",
                "def _pad_shape_rule(operand, padding_value, *, padding_config):\n",
                "  del padding_value\n",
                "  op_shape = np.shape(operand)\n",
                "  if not len(padding_config) == np.ndim(operand):\n",
                "    raise ValueError(\"length of padding_config must equal the number of axes \"\n",
                "                     f\"of operand, got padding_config {padding_config} \"\n",
                "                     f\"for operand shape {op_shape}\")\n",
                "  if not all(i >= 0 for _, _, i in padding_config):\n",
                "    raise ValueError(\"interior padding in padding_config must be nonnegative, \"\n",
                "                     f\"got padding_config {padding_config}\")\n",
                "  result = tuple(core.sum_dim(l, h, core.dilate_dim(d, i + 1))\n",
                "                 for (l, h, i), d in zip(padding_config, op_shape))\n",
                "  if not all(core.greater_equal_dim(d, 0) for d in result):\n",
                "    msg = (f\"Dimension size after padding is not at least 0, \"\n",
                "           f\"got result shape {result}, for padding_config {padding_config}\"\n",
                "           f\" and operand shape {op_shape}\")\n",
                "    raise ValueError(msg)\n",
                "  return result\n",
                "\n",
                "def _pad_transpose(t, operand, padding_value, *, padding_config):\n",
                "  if type(t) is ad_util.Zero:\n",
                "    t_operand = ad_util.Zero(operand.aval) if ad.is_undefined_primal(operand) else None\n",
                "    t_padv = ad_util.Zero(padding_value.aval) if ad.is_undefined_primal(padding_value) else None\n",
                "  else:\n",
                "    lo, hi, interior = util.unzip3(padding_config)\n",
                "    total = lambda x: _reduce_sum(x, list(range(t.ndim)))\n",
                "\n",
                "    def t_op():\n",
                "      unpad_config = safe_zip(np.negative(lo), np.negative(hi),\n",
                "                              np.zeros_like(interior))\n",
                "      unpadded = pad(t, np.array(0., t.dtype), unpad_config)\n",
                "      return slicing.slice(unpadded, np.zeros_like(lo), unpadded.shape,\n",
                "                           np.add(interior, 1))\n",
                "\n",
                "    t_operand = t_op() if ad.is_undefined_primal(operand) else None\n",
                "    t_padv = sub(total(t), total(t_operand)) if ad.is_undefined_primal(padding_value) else None\n",
                "  return [t_operand, t_padv]\n",
                "\n",
                "def _pad_batch_rule(batched_args, batch_dims, *, padding_config):\n",
                "  operand, padding_value = batched_args\n",
                "  operand_bdim, padding_value_bdim = batch_dims\n",
                "  if operand_bdim is None:\n",
                "    operand_bdim = 0\n",
                "    operand = broadcast(operand, (padding_value.shape[padding_value_bdim],))\n",
                "\n",
                "  padding_config = list(padding_config)\n",
                "  padding_config.insert(operand_bdim, (0, 0, 0))\n",
                "  if padding_value_bdim is None:\n",
                "    return pad(operand, padding_value, padding_config), operand_bdim\n",
                "\n",
                "  assert padding_value_bdim == 0, padding_value_bdim\n",
                "\n",
                "  x = pad(operand, _zero(operand), padding_config)\n",
                "  mask = pad(full_like(operand, True, np.bool_), False, padding_config)\n",
                "  broadcasted_padding = broadcast_in_dim(padding_value, x.shape,\n",
                "                                         (operand_bdim,))\n",
                "  return select(mask, x, broadcasted_padding), operand_bdim\n",
                "\n",
                "pad_p = standard_primitive(_pad_shape_rule, _pad_dtype_rule, 'pad')\n",
                "ad.deflinear2(pad_p, _pad_transpose)\n",
                "batching.primitive_batchers[pad_p] = _pad_batch_rule\n",
                "\n",
                "def _pad_lower(ctx, x, padding_value, *, padding_config):\n",
                "  aval_out, = ctx.avals_out\n",
                "  low, high, interior = util.unzip3(padding_config)\n",
                "  return [mlir.pad(ctx, aval_out, x, padding_value, low, high, interior)]\n",
                "\n",
                "mlir.register_lowering(pad_p, _pad_lower)\n",
                "\n",
                "\n",
                "# The squeeze primitive exists for the benefit of masking and other\n",
                "# transformations that need to keep track of axis identity.\n",
                "# For example, consider reshaping a 2D array with shape (1, N) into a 1D array\n",
                "# with shape (N,). This results in the following JAXpr:\n",
                "#   reshape[ dimension=None new_sizes=(N,) ]\n",
                "# For N > 1, we can match up the output array axis with the second axis of the\n",
                "# input. But for N = 1, it is not clear how axes match up: all we know from the\n",
                "# JAXpr is that we are reshaping from (1, 1) to (1,).\n",
                "# In constrast, squeeze[ dimensions=(0,) ] is unambiguous.\n",
                "\n",
                "\n",
                "def _squeeze_dtype_rule(operand, *, dimensions):\n",
                "  return operand.dtype\n",
                "\n",
                "def _squeeze_shape_rule(operand, *, dimensions):\n",
                "  return _compute_squeeze_shape(np.shape(operand), dimensions)\n",
                "\n",
                "def _compute_squeeze_shape(shape, dimensions):\n",
                "  dims_set = set(dimensions)\n",
                "  if len(dims_set) != len(dimensions):\n",
                "    raise ValueError(f\"dimensions are not unique: {dimensions}\")\n",
                "  if not all(0 <= d < len(shape) for d in dims_set):\n",
                "    raise ValueError(f\"dimensions outside range [0, ndim): {dimensions}\")\n",
                "  if any(not core.symbolic_equal_dim(shape[d], 1) for d in dimensions):\n",
                "    raise ValueError(\n",
                "        \"cannot select an axis to squeeze out which has size not equal to \"\n",
                "        f\"one, got {shape=} and {dimensions=}\")\n",
                "  return tuple(s for i, s in enumerate(shape) if i not in dims_set)\n",
                "\n",
                "def _squeeze_transpose_rule(t, operand, *, dimensions):\n",
                "  assert ad.is_undefined_primal(operand)\n",
                "  return [expand_dims(t, dimensions)]\n",
                "\n",
                "def _squeeze_batch_rule(batched_args, batch_dims, *, dimensions):\n",
                "  operand, = batched_args\n",
                "  bdim, = batch_dims\n",
                "  operand = batching.moveaxis(operand, bdim, 0)\n",
                "  dimensions = tuple(np.add(1, dimensions))\n",
                "  return squeeze(operand, dimensions=dimensions), 0\n",
                "\n",
                "squeeze_p = standard_primitive(_squeeze_shape_rule, _squeeze_dtype_rule,\n",
                "                               'squeeze')\n",
                "ad.deflinear2(squeeze_p, _squeeze_transpose_rule)\n",
                "batching.primitive_batchers[squeeze_p] = _squeeze_batch_rule\n",
                "pe.def_trivial_padding(squeeze_p)\n",
                "\n",
                "def _squeeze_lower(ctx, operand, *, dimensions):\n",
                "  del dimensions  # Implied by the output aval.\n",
                "  return [mlir.reshape(ctx, operand, ctx.avals_out[0])]\n",
                "\n",
                "mlir.register_lowering(squeeze_p, _squeeze_lower)\n",
                "\n",
                "\n",
                "def shape_as_value(shape: core.Shape):\n",
                "  \"\"\"Converts a shape that may contain Poly values into a JAX value.\"\"\"\n",
                "  if len(shape) == 0:\n",
                "    return full((0,), np.array(0, np.int64))\n",
                "  dims = [\n",
                "      expand_dims(convert_element_type(core.dimension_as_value(d), np.int64),\n",
                "                  (0,))\n",
                "      for d in shape\n",
                "  ]\n",
                "  return concatenate(dims, dimension=0)\n",
                "\n",
                "def _is_singleton_reshape(old, new):\n",
                "  # A singleton reshape is one where only singleton dimensions are added. We\n",
                "  # want to detect them because they can be expressed as (lazy) broadcasts.\n",
                "  old, new = iter(old), iter(new)\n",
                "  d1, d2 = next(old, None), next(new, None)\n",
                "  bcast_dims = []\n",
                "  i = 0\n",
                "  while True:\n",
                "    if d1 is d2 is None:\n",
                "      return bcast_dims\n",
                "    elif d1 == d2:\n",
                "      bcast_dims.append(i)\n",
                "      i += 1\n",
                "      d1, d2 = next(old, None), next(new, None)\n",
                "    elif d2 == 1:\n",
                "      i += 1\n",
                "      d2 = next(new, None)\n",
                "    else:\n",
                "      return None\n",
                "\n",
                "def _reshape_shape_rule(operand, *, new_sizes, dimensions):\n",
                "  if not all(core.greater_equal_dim(d, 0) for d in new_sizes):\n",
                "    msg = 'reshape new_sizes must all be positive, got {}.'\n",
                "    raise TypeError(msg.format(new_sizes))\n",
                "  # TODO(necula): re-enable this check\n",
                "  if (not config.jax_dynamic_shapes and\n",
                "      not core.same_shape_sizes(np.shape(operand), new_sizes)):\n",
                "    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'\n",
                "    raise TypeError(msg.format(new_sizes, np.shape(operand)))\n",
                "  if dimensions is not None:\n",
                "    if set(dimensions) != set(range(np.ndim(operand))):\n",
                "      msg = ('reshape dimensions must be a permutation of operand dimensions, '\n",
                "             'got dimensions {} for shape {}.')\n",
                "      raise TypeError(msg.format(dimensions, np.shape(operand)))\n",
                "  return tuple(new_sizes)\n",
                "\n",
                "def _reshape_typecheck_rule(operand, *dyn_shape, new_sizes, dimensions):\n",
                "  if not dyn_shape:\n",
                "    out_aval, effects = reshape_p.abstract_eval(\n",
                "        operand.aval, new_sizes=new_sizes, dimensions=dimensions)\n",
                "    return [out_aval], effects\n",
                "  else:\n",
                "    # TODO(mattjj, necula): perform more checks like _reshape_shape_rule\n",
                "    out_shape = _merge_dyn_shape(new_sizes, dyn_shape)\n",
                "    out_shape = [x.val if type(x) is core.Literal else x for x in out_shape]  # pytype: disable=attribute-error\n",
                "    out_aval = core.DShapedArray(tuple(out_shape), operand.aval.dtype,\n",
                "                                 operand.aval.weak_type)\n",
                "    return [out_aval], core.no_effects\n",
                "\n",
                "\n",
                "def _reshape_dtype_rule(operand, *, new_sizes, dimensions):\n",
                "  return operand.dtype\n",
                "\n",
                "def _reshape_transpose_rule(t, operand, *, new_sizes, dimensions):\n",
                "  assert ad.is_undefined_primal(operand)\n",
                "  if dimensions is None:\n",
                "    return [reshape(t, operand.aval.shape)]\n",
                "  else:\n",
                "    return [transpose(reshape(t, np.take(operand.aval.shape, dimensions)),\n",
                "                      np.argsort(dimensions))]\n",
                "\n",
                "def _reshape_batch_rule(batched_args, batch_dims, *, new_sizes, dimensions):\n",
                "  operand, = batched_args\n",
                "  bdim, = batch_dims\n",
                "  operand = batching.moveaxis(operand, bdim, 0)\n",
                "  if dimensions is not None:\n",
                "    dimensions = (0,) + tuple(np.add(1, dimensions))\n",
                "  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0\n",
                "\n",
                "\n",
                "def _reshape_lower(ctx, x, *dyn_shape, new_sizes, dimensions):\n",
                "  aval_out, = ctx.avals_out\n",
                "  if dimensions is not None:\n",
                "    x = hlo.TransposeOp(x, mlir.dense_int_elements(dimensions)).result\n",
                "  if dyn_shape:\n",
                "    aval_out = aval_out.update(shape=_merge_dyn_shape(new_sizes, dyn_shape))\n",
                "  return [mlir.reshape(ctx, x, aval_out)]\n",
                "\n",
                "def _reshape_staging_rule(\n",
                "    trace, x, *dyn, new_sizes, dimensions):\n",
                "  params = dict(new_sizes=new_sizes, dimensions=dimensions)\n",
                "  if not dyn:\n",
                "    return trace.default_process_primitive(reshape_p, (x,), params)\n",
                "  av = core.DShapedArray(_merge_dyn_shape(new_sizes, dyn), x.dtype, x.weak_type)\n",
                "  return _dyn_shape_staging_rule(trace, reshape_p, av, x, *dyn, **params)\n",
                "\n",
                "reshape_p = standard_primitive(_reshape_shape_rule, _reshape_dtype_rule,\n",
                "                               'reshape')\n",
                "ad.deflinear2(reshape_p, _reshape_transpose_rule)\n",
                "batching.primitive_batchers[reshape_p] = _reshape_batch_rule\n",
                "mlir.register_lowering(reshape_p, _reshape_lower)\n",
                "core.custom_typechecks[reshape_p] = _reshape_typecheck_rule\n",
                "pe.custom_staging_rules[reshape_p] = _reshape_staging_rule\n",
                "\n",
                "\n",
                "def _rev_shape_rule(operand, *, dimensions):\n",
                "  _check_shapelike('rev', 'dimensions', dimensions)\n",
                "  if len(set(dimensions)) != len(dimensions):\n",
                "    msg = 'rev dimensions must be unique, got {}.'\n",
                "    raise TypeError(msg.format(dimensions))\n",
                "  if dimensions and not _max(dimensions) < operand.ndim:\n",
                "    msg = ('rev dimensions must all be less than operand ndim, got dimensions '\n",
                "           '{} for operand ndim {}.')\n",
                "    raise TypeError(msg.format(dimensions, operand.ndim))\n",
                "  return operand.shape\n",
                "\n",
                "def _rev_batch_rule(batched_args, batch_dims, *, dimensions):\n",
                "  operand, = batched_args\n",
                "  bdim, = batch_dims\n",
                "  new_dimensions = [i + 1 if i >= bdim else i for i in dimensions]\n",
                "  return rev(operand, new_dimensions), bdim\n",
                "\n",
                "rev_p = standard_primitive(_rev_shape_rule, _input_dtype, 'rev')\n",
                "ad.deflinear2(rev_p, lambda t, _, dimensions: [rev(t, dimensions)])\n",
                "batching.primitive_batchers[rev_p] = _rev_batch_rule\n",
                "\n",
                "def _rev_lower(ctx, x, *, dimensions):\n",
                "  return hlo.ReverseOp(x, mlir.dense_int_elements(dimensions)).results\n",
                "mlir.register_lowering(rev_p, _rev_lower)\n",
                "\n",
                "\n",
                "def _transpose_shape_rule(operand, *, permutation):\n",
                "  if not isinstance(permutation, (tuple, list, np.ndarray)):\n",
                "    msg = \"transpose permutation must be a tuple/list/ndarray, got {}.\"\n",
                "    raise TypeError(msg.format(type(permutation)))\n",
                "  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):\n",
                "    msg = (\"transpose permutation isn't a permutation of operand dimensions, \"\n",
                "           \"got permutation {} for operand shape {}.\")\n",
                "    raise TypeError(msg.format(permutation, operand.shape))\n",
                "  return tuple(operand.shape[old_idx] for old_idx in permutation)\n",
                "\n",
                "def _transpose_batch_rule(batched_args, batch_dims, *, permutation):\n",
                "  operand, = batched_args\n",
                "  bdim, = batch_dims\n",
                "  perm = (bdim,) + tuple(i if i < bdim else i+1 for i in permutation)\n",
                "  return transpose(operand, perm), 0\n",
                "\n",
                "def _transpose_lower(ctx, x, *, permutation):\n",
                "  aval_out, = ctx.avals_out\n",
                "  if core.is_opaque_dtype(aval_out.dtype):\n",
                "    return [aval_out.dtype._rules.transpose_mlir(ctx, aval_out, x, permutation=permutation)]\n",
                "  return hlo.TransposeOp(x, mlir.dense_int_elements(permutation)).results\n",
                "\n",
                "transpose_p = standard_primitive(_transpose_shape_rule, _input_dtype,\n",
                "                                 'transpose')\n",
                "ad.deflinear2(transpose_p,\n",
                "              lambda t, _, permutation: [transpose(t, np.argsort(permutation))])  # type: ignore[arg-type]\n",
                "batching.primitive_batchers[transpose_p] = _transpose_batch_rule\n",
                "mlir.register_lowering(transpose_p, _transpose_lower)\n",
                "pe.def_trivial_padding(transpose_p)\n",
                "\n",
                "\n",
                "def _select_shape_rule(which, *cases):\n",
                "  if len(cases) == 0:\n",
                "    raise TypeError(\"select must have at least one case\")\n",
                "  if any(case.shape != cases[0].shape for case in cases[1:]):\n",
                "    msg = \"select cases must have the same shapes, got [{}].\"\n",
                "    raise TypeError(msg.format(\", \".join([str(c.shape) for c in cases])))\n",
                "  if which.shape and which.shape != cases[0].shape:\n",
                "    msg = (\"select `which` must be scalar or have the same shape as cases, \"\n",
                "           \"got `which` shape {} but case shape {}.\")\n",
                "    raise TypeError(msg.format(which.shape, cases[0].shape))\n",
                "  return cases[0].shape\n",
                "\n",
                "def _select_dtype_rule(which, *cases):\n",
                "  _check_same_dtypes(\"select\", False, *(c.dtype for c in cases))\n",
                "  if (not dtypes.issubdtype(which.dtype, np.bool_) and\n",
                "      not dtypes.issubdtype(which.dtype, np.integer)):\n",
                "    raise TypeError(\"select `which` must be boolean or integer type, got \"\n",
                "                    f\"{which.dtype}.\")\n",
                "  if dtypes.issubdtype(which.dtype, np.bool_) and len(cases) > 2:\n",
                "    raise TypeError(\"select with boolean `which` cannot have > 2 cases.\")\n",
                "  return cases[0].dtype\n",
                "\n",
                "def _select_weak_type_rule(which, *cases):\n",
                "  return all(c.weak_type for c in cases)\n",
                "\n",
                "def _select_transpose_rule(t, which, *cases):\n",
                "  assert not ad.is_undefined_primal(which)\n",
                "  if type(t) is ad_util.Zero:\n",
                "    return [None] + [ad_util.Zero(c.aval) if ad.is_undefined_primal(c) else None\n",
                "                     for c in cases]\n",
                "  else:\n",
                "    zeros = full_like(t, 0)\n",
                "    return [None] + [\n",
                "        select(eq(which, _const(which, i)), t, zeros)\n",
                "        if ad.is_undefined_primal(case) else None\n",
                "        for i, case in enumerate(cases)\n",
                "    ]\n",
                "\n",
                "def _select_batch_rule(batched_args, batch_dims, **unused_kwargs):\n",
                "  which, *cases = batched_args\n",
                "  which_bdim, *case_bdims = batch_dims\n",
                "  size = next(x.shape[i] for x, i in zip(batched_args, batch_dims)\n",
                "              if i is not None)\n",
                "\n",
                "  # avoid transposes and some broadcasts in special cases\n",
                "  if all(which_bdim == bdim for bdim in case_bdims):\n",
                "    if np.shape(which) == np.shape(cases[0]):\n",
                "      return select_n(which, *cases), which_bdim\n",
                "    else:\n",
                "      # vmapped function had a scalar which with nonscalar args\n",
                "      assert np.ndim(which) == 1\n",
                "      which = broadcast_in_dim(which, cases[0].shape, [which_bdim])\n",
                "      return select_n(which, *cases), which_bdim\n",
                "  elif np.ndim(which) == 0 and all(bdim is not None for bdim in case_bdims):\n",
                "    if all(case_bdims[0] == bdim for bdim in case_bdims[1:]):\n",
                "      return select_n(which, *cases), case_bdims[0]\n",
                "    elif all(np.shape(cases[0]) == np.shape(c) for c in cases):\n",
                "      bdim = case_bdims[0]\n",
                "      other_cases = [batching.moveaxis(c, c_bdim, bdim)\n",
                "                     for c, c_bdim in zip(cases[1:], case_bdims[1:])]\n",
                "      return select_n(which, cases[0], *other_cases), bdim\n",
                "\n",
                "  which = (batching.bdim_at_front(which, which_bdim, size) if np.shape(which)\n",
                "           else which)\n",
                "  if not all(() == np.shape(c) for c in cases):\n",
                "    cases = [batching.bdim_at_front(c, bdim, size)\n",
                "             for c, bdim in zip(cases, case_bdims)]\n",
                "  assert all(np.shape(cases[0]) == np.shape(c) for c in cases[1:])\n",
                "  if 0 < np.ndim(which) < np.ndim(cases[0]):\n",
                "    # vmapped function had a scalar which with nonscalar args\n",
                "    assert np.ndim(which) == 1\n",
                "    which = broadcast_in_dim(which, cases[0].shape, [0])\n",
                "  if np.ndim(which) > np.ndim(cases[0]):\n",
                "    assert np.ndim(cases[0]) == 0\n",
                "    cases = [broadcast(c, which.shape) for c in cases]\n",
                "  return select_n(which, *cases), 0\n",
                "\n",
                "def _select_jvp(primals, tangents):\n",
                "  which, *case_primals = primals\n",
                "  case_tangents = tangents[1:]\n",
                "  out = select_n(which, *case_primals)\n",
                "  if all(type(t) is ad_util.Zero for t in case_tangents):\n",
                "    out_dot = ad_util.Zero(case_tangents[0].aval)\n",
                "  else:\n",
                "    z = _zeros(next(t for t in case_tangents if type(t) is not ad_util.Zero))\n",
                "    case_tangents = [z if type(t) is ad_util.Zero else t for t in case_tangents]\n",
                "    out_dot = select_n(which, *case_tangents)\n",
                "  return out, out_dot\n",
                "\n",
                "def _select_hlo_lowering(ctx, which, *cases):\n",
                "  which_aval = ctx.avals_in[0]\n",
                "  if which_aval.dtype == np.dtype(np.bool_):\n",
                "    assert len(cases) <= 2\n",
                "    if len(cases) == 1: return cases\n",
                "    return hlo.SelectOp(which, cases[1], cases[0]).results\n",
                "\n",
                "  if dtypes.issubdtype(which_aval.dtype, np.signedinteger):\n",
                "    compare_type = 'SIGNED'\n",
                "  else:\n",
                "    compare_type = 'UNSIGNED'\n",
                "  lt = 'LT'\n",
                "\n",
                "  def _select(offset, cases):\n",
                "    assert len(cases) > 0\n",
                "    if len(cases) == 1:\n",
                "      return cases[0]\n",
                "    mid = len(cases) // 2\n",
                "    pred = mlir.compare_hlo(which,\n",
                "                            mlir.full_like_aval(ctx, offset + mid, which_aval),\n",
                "                            lt, compare_type)\n",
                "    return hlo.SelectOp(pred, _select(offset, cases[:mid]),\n",
                "                        _select(offset + mid, cases[mid:])).result\n",
                "\n",
                "  return [_select(0, cases)]\n",
                "\n",
                "select_n_p = standard_primitive(\n",
                "    _select_shape_rule, _select_dtype_rule, 'select_n',\n",
                "    weak_type_rule=_select_weak_type_rule)\n",
                "ad.primitive_jvps[select_n_p] = _select_jvp\n",
                "ad.primitive_transposes[select_n_p] = _select_transpose_rule\n",
                "batching.primitive_batchers[select_n_p] = _select_batch_rule\n",
                "mlir.register_lowering(select_n_p, _select_hlo_lowering)\n",
                "pe.def_trivial_padding(select_n_p)\n",
                "\n",
                "\n",
                "def _reduce_shape_rule(*avals, computation, jaxpr, consts, dimensions):\n",
                "  operand_avals, init_val_avals = split_list(avals, [len(avals) // 2])\n",
                "  if any(arg.shape != () for arg in init_val_avals):\n",
                "    init_val_shapes = [a.shape for a in init_val_avals]\n",
                "    raise ValueError(f'reduce found non-scalar initial value: {init_val_shapes}')\n",
                "  return [tuple(np.delete(op.shape, dimensions)) for op in operand_avals]\n",
                "\n",
                "def _reduce_dtype_rule(*avals, computation, jaxpr, consts, dimensions):\n",
                "  operand_avals, init_val_avals = split_list(avals, [len(avals) // 2])\n",
                "  operand_dtypes = [dtypes.canonicalize_dtype(op.dtype) for op in operand_avals]\n",
                "  init_val_dtypes = [dtypes.canonicalize_dtype(init.dtype) for init in init_val_avals]\n",
                "  if operand_dtypes != init_val_dtypes:\n",
                "    raise TypeError(\n",
                "        \"reduce operand dtypes should match corresponding initial value dtypes, \"\n",
                "        f\"got operands={operand_avals} and initial_values={init_val_avals}\")\n",
                "  return operand_dtypes\n",
                "\n",
                "def _reduce_weak_type_rule(*avals, computation, jaxpr, consts, dimensions):\n",
                "  operand_avals, init_val_avals = split_list(avals, [len(avals) // 2])\n",
                "  return [op.weak_type and init_val.weak_type\n",
                "          for op, init_val in safe_zip(operand_avals, init_val_avals)]\n",
                "\n",
                "def _reduce_batch_rule(batched_args, batch_dims, *, computation, jaxpr,\n",
                "                       consts, dimensions):\n",
                "  # TODO(mattjj,frostig): use batch_jaxpr, delete computation (assumes poly??)\n",
                "  num_operands = len(batched_args) // 2\n",
                "  operands, init_values = split_list(batched_args, [num_operands])\n",
                "  operand_bdims, init_value_bdims = split_list(batch_dims, [num_operands])\n",
                "  if all(init_value_bdim is batching.not_mapped\n",
                "         for init_value_bdim in init_value_bdims):\n",
                "    size = next(x.shape[ax] for x, ax in zip(batched_args, batch_dims)\n",
                "                if ax is not None)\n",
                "    operands = [batching.bdim_at_front(arg, bdim, size)\n",
                "                for arg, bdim in zip(operands, operand_bdims)]\n",
                "    new_dimensions = [d + 1 for d in dimensions]\n",
                "    new_operand_bdims = [0] * num_operands\n",
                "    return reduce_p.bind(*(operands + init_values),\n",
                "                         computation=computation,\n",
                "                         dimensions=tuple(new_dimensions),\n",
                "                         consts=consts,\n",
                "                         jaxpr=jaxpr), new_operand_bdims\n",
                "  else:\n",
                "    raise NotImplementedError  # loop and stack\n",
                "\n",
                "def _reduce_jvp(reducer, init_values, primals, tangents, axes):\n",
                "  input_shape = np.array(primals[0].shape, dtype=np.int_)\n",
                "\n",
                "  n = np.prod(input_shape[list(axes)])\n",
                "  non_axes = np.delete(np.arange(len(input_shape)), axes)\n",
                "\n",
                "  # Move the reduced axes to the front, and flatten them to 1D.\n",
                "  permutation = axes + tuple(non_axes)\n",
                "  new_shape = (n,) + tuple(input_shape[non_axes])\n",
                "  primals = tuple(reshape(x, new_shape, permutation) for x in primals)\n",
                "  tangents = tuple(reshape(t, new_shape, permutation) for t in tangents)\n",
                "\n",
                "  for d in range(len(non_axes) + 1):\n",
                "    reducer = api.vmap(reducer)\n",
                "  def _reduce_tree(*xs, axis=0):\n",
                "    \"\"\"Reduce by repeatedly splitting the array and multiplying.\"\"\"\n",
                "    while xs[0].shape[axis] > 1:\n",
                "      n = xs[0].shape[axis]\n",
                "      n1 = (n + 1) // 2\n",
                "      n2 = n - n1\n",
                "      xs1 = [slicing.slice_in_dim(x, 0, n1) for x in xs]\n",
                "      xs2 = [slicing.slice_in_dim(x, n1, None) for x in xs]\n",
                "      if n2 != n1:\n",
                "        paddings = [(0, 0, 0)] * len(xs[0].shape)\n",
                "        paddings[axis] = (0, 1, 0)\n",
                "        xs2 = [pad(x2, i, paddings) for x2, i in zip(xs2, init_values)]\n",
                "      xs = reducer(*(xs1 + xs2))\n",
                "    if xs[0].shape[axis] == 0:\n",
                "      return [full(input_shape[non_axes], i) for i in init_values]\n",
                "    return tuple(squeeze(x, (axis,)) for x in xs)\n",
                "\n",
                "  return api.jvp(_reduce_tree, primals, tangents)\n",
                "\n",
                "def _reduce_jvp_rule(primals, tangents, *, computation, jaxpr,\n",
                "                     consts, dimensions):\n",
                "  primal_xs, init_values = split_list(primals, [len(primals) // 2])\n",
                "  tangent_xs, tangent_init = split_list(tangents, [len(tangents) // 2])\n",
                "  # This test may be too strict, if a value is actually zero but we cannot prove\n",
                "  # it is symbolically zero.\n",
                "  if any(type(t) is not ad_util.Zero for t in tangent_init):\n",
                "    raise NotImplementedError(\n",
                "      \"Gradient of general lax.reduce with non-zero tangents for \"\n",
                "      \"initial values to reduction not implemented\")\n",
                "  reducer = core.jaxpr_as_fun(core.ClosedJaxpr(jaxpr, consts))\n",
                "  return _reduce_jvp(reducer, init_values, primal_xs, tangent_xs, dimensions)\n",
                "\n",
                "def _reduce_named_shape_rule(*avals, computation, jaxpr, consts, dimensions):\n",
                "  # TODO(mattjj,frostig): see the TODOs noting limitations/assumptions in\n",
                "  # _reduce_batching_rule. We're making the same assumptions here for now.\n",
                "  num_operands = len(avals) // 2\n",
                "  operand_avals, init_avals = split_list(avals, [num_operands])\n",
                "  if any(a.named_shape for a in init_avals):\n",
                "    raise NotImplementedError\n",
                "  named_shapes = [a.named_shape for a in operand_avals]\n",
                "  join = core.join_named_shapes(*(a.named_shape for a in operand_avals))\n",
                "  return [join] * len(named_shapes)\n",
                "\n",
                "\n",
                "reduce_p = core.Primitive('reduce')\n",
                "reduce_p.multiple_results = True\n",
                "reduce_p.def_impl(partial(xla.apply_primitive, reduce_p))\n",
                "reduce_p.def_abstract_eval(\n",
                "    partial(standard_multi_result_abstract_eval, reduce_p, _reduce_shape_rule,\n",
                "            _reduce_dtype_rule, _reduce_weak_type_rule,\n",
                "            _reduce_named_shape_rule))\n",
                "batching.primitive_batchers[reduce_p] = _reduce_batch_rule\n",
                "ad.primitive_jvps[reduce_p] = _reduce_jvp_rule\n",
                "\n",
                "def _reduce_lower(ctx, *values, computation, jaxpr, consts, dimensions):\n",
                "  assert all(isinstance(x, core.ShapedArray) for x in ctx.avals_in), ctx.avals_in\n",
                "  operands, init_values = util.split_list(values, [len(values) // 2])\n",
                "  init_value_avals = ctx.avals_in[len(values) // 2:]\n",
                "  op = hlo.ReduceOp([mlir.aval_to_ir_type(aval) for aval in ctx.avals_out],\n",
                "                    operands, init_values, mlir.dense_int_elements(dimensions))\n",
                "  ir_types = [mlir.aval_to_ir_type(aval) for aval in init_value_avals]\n",
                "  reducer = op.regions[0].blocks.append(*(ir_types + ir_types))\n",
                "  with ir.InsertionPoint(reducer):\n",
                "    reducer_ctx = ctx.module_context.replace(name_stack=util.new_name_stack())\n",
                "    if jaxpr.effects:\n",
                "      raise NotImplementedError('Cannot lower effectful `reduce`.')\n",
                "    out_nodes, _ = mlir.jaxpr_subcomp(reducer_ctx, jaxpr, mlir.TokenSet(), consts,\n",
                "                                      *([a] for a in reducer.arguments),\n",
                "                                      dim_var_values=ctx.dim_var_values)\n",
                "    hlo.ReturnOp(util.flatten(out_nodes))\n",
                "  return op.results\n",
                "\n",
                "mlir.register_lowering(reduce_p, _reduce_lower)\n",
                "\n",
                "\n",
                "def _reduce_number_dtype_rule(name, operand, *args, **kw):\n",
                "  if not dtypes.issubdtype(operand.dtype, np.number):\n",
                "    raise TypeError(\"{} does not accept dtype {}. Accepted dtypes are subtypes \"\n",
                "                    \"of number.\".format(name, np.dtype(operand.dtype).name))\n",
                "  return dtypes.canonicalize_dtype(operand.dtype)\n",
                "\n",
                "def _reduce_sum_shape_rule(operand, *, axes):\n",
                "  return _reduce_op_shape_rule(operand, axes=axes)\n",
                "\n",
                "def _reduce_sum_transpose_rule(cotangent, operand, *, axes):\n",
                "  assert ad.is_undefined_primal(operand)\n",
                "  input_shape = operand.aval.shape\n",
                "  broadcast_dimensions = tuple(np.delete(np.arange(len(input_shape)), axes))\n",
                "  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)\n",
                "  assert result.shape == input_shape\n",
                "  return [result]\n",
                "\n",
                "def _reducer_padding(traceable, ident, in_avals, out_avals, operand, *, axes):\n",
                "  del out_avals\n",
                "  aval, = in_avals\n",
                "  padded_axes = [(i, d.val) for i, d in enumerate(aval.shape)\n",
                "                 if isinstance(d, pe.BoundedAxisSize)]\n",
                "  operand_ = _replace_masked_values(operand, ident(aval.dtype), padded_axes)\n",
                "  return [traceable(operand_, axes)]\n",
                "\n",
                "def _replace_masked_values(x, val, padded_axes):\n",
                "  if not padded_axes: return x\n",
                "  dtype = dtypes._scalar_type_to_dtype(int)\n",
                "  masks = [broadcasted_iota(dtype, x.shape, i) < d for i, d in padded_axes]\n",
                "  return select(_reduce(operator.and_, masks), x, full_like(x, val))\n",
                "\n",
                "\n",
                "reduce_sum_p = standard_primitive(\n",
                "  _reduce_sum_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_sum'),\n",
                "  'reduce_sum')\n",
                "ad.deflinear2(reduce_sum_p, _reduce_sum_transpose_rule)\n",
                "batching.defreducer(reduce_sum_p)\n",
                "pe.padding_rules[reduce_sum_p] = partial(_reducer_padding, _reduce_sum,\n",
                "                                         _get_sum_identity)\n",
                "\n",
                "\n",
                "def _reduce_op_shape_rule(operand, *, axes, input_shape=None):\n",
                "  del input_shape  # Unused.\n",
                "  if len(axes) != len(set(axes)):\n",
                "    raise ValueError(f\"duplicate value in 'axes' of reduction: {axes}\")\n",
                "  if not all(0 <= a < operand.ndim for a in axes):\n",
                "    raise ValueError(f\"reduction axes {axes} contains out-of-bounds indices for {operand}.\")\n",
                "  axes = frozenset(axes)\n",
                "  return tuple(d for i, d in enumerate(operand.shape) if i not in axes)\n",
                "\n",
                "def _reduce_prod_jvp_rule(primals, tangents, *, axes):\n",
                "  reducer = lambda x, y: [mul(x, y)]\n",
                "  primals_out, tangents_out = _reduce_jvp(reducer, [_const(primals[0], 1)],\n",
                "                                          primals, tangents, axes)\n",
                "  return primals_out[0], tangents_out[0]\n",
                "\n",
                "reduce_prod_p = standard_primitive(\n",
                "  _reduce_op_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_prod'),\n",
                "  'reduce_prod')\n",
                "ad.primitive_jvps[reduce_prod_p] = _reduce_prod_jvp_rule\n",
                "batching.defreducer(reduce_prod_p)\n",
                "pe.padding_rules[reduce_prod_p] = partial(_reducer_padding, _reduce_prod,\n",
                "                                          _get_prod_identity)\n",
                "\n",
                "\n",
                "def _reduce_chooser_shape_rule(operand, *, axes):\n",
                "  return tuple(np.delete(operand.shape, axes))\n",
                "\n",
                "def _reduce_chooser_jvp_rule(g, ans, operand, *, axes):\n",
                "  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen\n",
                "  # locations in a single pass (rather than comparing equality) and use a\n",
                "  # gather, and/or even push along the chosen elements of g (b/112040122)\n",
                "  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]\n",
                "  location_indicators = convert_element_type(\n",
                "      _eq_meet(operand, reshape(ans, shape)), g.dtype)\n",
                "  counts = _reduce_sum(location_indicators, axes)\n",
                "  return div(_reduce_sum(mul(g, location_indicators), axes), counts)\n",
                "\n",
                "\n",
                "reduce_max_p = standard_primitive(_reduce_op_shape_rule, _input_dtype,\n",
                "                                  'reduce_max')\n",
                "ad.defjvp2(reduce_max_p, _reduce_chooser_jvp_rule)\n",
                "batching.defreducer(reduce_max_p)\n",
                "pe.padding_rules[reduce_max_p] = partial(_reducer_padding, _reduce_max,\n",
                "                                         _get_max_identity)\n",
                "\n",
                "\n",
                "reduce_min_p = standard_primitive(_reduce_op_shape_rule, _input_dtype,\n",
                "                                  'reduce_min')\n",
                "ad.defjvp2(reduce_min_p, _reduce_chooser_jvp_rule)\n",
                "batching.defreducer(reduce_min_p)\n",
                "pe.padding_rules[reduce_min_p] = partial(_reducer_padding, _reduce_min,\n",
                "                                         _get_min_identity)\n",
                "\n",
                "\n",
                "def _argminmax_shape_rule(operand, *, axes, index_dtype):\n",
                "  axis, = axes\n",
                "  if not (0 <= axis < len(operand.shape)):\n",
                "    raise ValueError(f\"Invalid axis {axis} for operand shape {operand.shape}\")\n",
                "  if not core.greater_equal_dim(operand.shape[axis], 1):\n",
                "    raise ValueError(\"argmin and argmax require non-empty reduced dimension. \"\n",
                "                     f\"operand.shape={operand.shape} {axis=}\")\n",
                "  return tuple(np.delete(operand.shape, axis))\n",
                "\n",
                "def _argminmax_dtype_rule(operand, *, axes, index_dtype):\n",
                "  if not dtypes.issubdtype(index_dtype, np.integer):\n",
                "    raise TypeError(\"index_dtype must be an integer type, but got {}\"\n",
                "                    .format(np.dtype(index_dtype).name))\n",
                "  return index_dtype\n",
                "\n",
                "def _compute_argminmax(value_comparator, get_identity,\n",
                "                       operand, *, index_dtype, axes):\n",
                "  # value_comparator is either lax.lt (for argmin) or lax.gt\n",
                "  # get_identity(operand.dtype) is inf for argmin or -inf for argmax\n",
                "  axis, = axes\n",
                "  indices = broadcasted_iota(index_dtype, np.shape(operand), axis)\n",
                "  def reducer_fn(op_val_index, acc_val_index):\n",
                "    op_val, op_index = op_val_index\n",
                "    acc_val, acc_index = acc_val_index\n",
                "    # Pick op_val if Lt (for argmin) or if NaN\n",
                "    pick_op_val = bitwise_or(value_comparator(op_val, acc_val),\n",
                "                             ne(op_val, op_val))\n",
                "    # If x and y are not NaN and x = y, then pick the first\n",
                "    pick_op_index = bitwise_or(pick_op_val,\n",
                "                               bitwise_and(eq(op_val, acc_val),\n",
                "                                           lt(op_index, acc_index)))\n",
                "    return (select(pick_op_val, op_val, acc_val),\n",
                "            select(pick_op_index, op_index, acc_index))\n",
                "  res = reduce([operand, indices],\n",
                "               [get_identity(operand.dtype), np.array(0, index_dtype)],\n",
                "               reducer_fn,\n",
                "               axes)\n",
                "  return res[1]\n",
                "\n",
                "argmin_p = standard_primitive(_argminmax_shape_rule, _argminmax_dtype_rule,\n",
                "                              'argmin', weak_type_rule=_strip_weak_type)\n",
                "batching.defreducer(argmin_p)\n",
                "ad.defjvp_zero(argmin_p)\n",
                "\n",
                "argmax_p = standard_primitive(_argminmax_shape_rule, _argminmax_dtype_rule,\n",
                "                              'argmax', weak_type_rule=_strip_weak_type)\n",
                "batching.defreducer(argmax_p)\n",
                "ad.defjvp_zero(argmax_p)\n",
                "\n",
                "mlir.register_lowering(argmin_p, mlir.cache_lowering(mlir.lower_fun(\n",
                "  partial(_compute_argminmax, lt, _get_min_identity),\n",
                "  multiple_results=False)))\n",
                "\n",
                "mlir.register_lowering(argmax_p, mlir.cache_lowering(mlir.lower_fun(\n",
                "  partial(_compute_argminmax, gt, _get_max_identity),\n",
                "  multiple_results=False)))\n",
                "\n",
                "\n",
                "def _reduce_logical_shape_rule(operand, *, axes):\n",
                "  if operand.dtype != np.bool_ and not np.issubdtype(operand.dtype, np.integer):\n",
                "    raise TypeError(f\"logical reduction requires operand dtype bool or int, got {operand.dtype}.\")\n",
                "  return tuple(np.delete(operand.shape, axes))\n",
                "\n",
                "reduce_or_p = standard_primitive(\n",
                "    _reduce_logical_shape_rule, _input_dtype, 'reduce_or',\n",
                "    weak_type_rule=_strip_weak_type)\n",
                "batching.defreducer(reduce_or_p)\n",
                "\n",
                "\n",
                "reduce_and_p = standard_primitive(\n",
                "    _reduce_logical_shape_rule, _input_dtype, 'reduce_and',\n",
                "    weak_type_rule=_strip_weak_type)\n",
                "batching.defreducer(reduce_and_p)\n",
                "\n",
                "\n",
                "reduce_xor_p = standard_primitive(\n",
                "    _reduce_logical_shape_rule, _input_dtype, 'reduce_xor',\n",
                "    weak_type_rule=_strip_weak_type)\n",
                "batching.defreducer(reduce_xor_p)\n",
                "\n",
                "\n",
                "def _unary_reduce_lower(reducer, unit_factory, ctx, x, *, axes):\n",
                "  aval_out, = ctx.avals_out\n",
                "  dtype = aval_out.dtype\n",
                "  op = hlo.ReduceOp([mlir.aval_to_ir_type(aval_out)], [x],\n",
                "                    mlir.ir_constants(unit_factory(aval_out.dtype)),\n",
                "                    mlir.dense_int_elements(axes))\n",
                "  scalar_type = mlir.aval_to_ir_type(core.ShapedArray((), dtype))\n",
                "  reducer_region = op.regions[0].blocks.append(scalar_type, scalar_type)\n",
                "  with ir.InsertionPoint(reducer_region):\n",
                "    add = reducer(*reducer_region.arguments)\n",
                "    hlo.ReturnOp(add.results)\n",
                "  return op.results\n",
                "\n",
                "mlir.register_lowering(reduce_sum_p, partial(_unary_reduce_lower, hlo.AddOp,\n",
                "                                             _get_sum_identity))\n",
                "mlir.register_lowering(reduce_prod_p, partial(_unary_reduce_lower, hlo.MulOp,\n",
                "                                              _get_prod_identity))\n",
                "mlir.register_lowering(reduce_or_p, partial(_unary_reduce_lower, hlo.OrOp,\n",
                "                                            _get_bitwise_or_identity))\n",
                "mlir.register_lowering(reduce_and_p, partial(_unary_reduce_lower, hlo.AndOp,\n",
                "                                             _get_bitwise_and_identity))\n",
                "mlir.register_lowering(reduce_xor_p, partial(_unary_reduce_lower, hlo.XorOp,\n",
                "                                             _get_bitwise_or_identity))\n",
                "mlir.register_lowering(reduce_min_p, partial(_unary_reduce_lower, mlir.min_hlo,\n",
                "                                             _get_min_identity))\n",
                "mlir.register_lowering(reduce_max_p, partial(_unary_reduce_lower, mlir.max_hlo,\n",
                "                                             _get_max_identity))\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "def _reduce_precision_shape_rule(operand, *, exponent_bits, mantissa_bits):\n",
                "  exponent_bits = operator.index(exponent_bits)\n",
                "  mantissa_bits = operator.index(mantissa_bits)\n",
                "  if exponent_bits < 1:\n",
                "    raise ValueError(f\"reduce_precision: exponent_bits must be positive; got {exponent_bits}\")\n",
                "  if mantissa_bits < 0:\n",
                "    raise ValueError(f\"reduce_precision: mantissa_bits must be non-negative; got {mantissa_bits}\")\n",
                "  return operand.shape\n",
                "\n",
                "\n",
                "reduce_precision_p = standard_primitive(\n",
                "    _reduce_precision_shape_rule,\n",
                "    partial(unop_dtype_rule, _identity, _float, 'reduce_precision'),\n",
                "    name='reduce_precision')\n",
                "batching.defvectorized(reduce_precision_p)\n",
                "\n",
                "def _reduce_precision_lower(ctx, operand, *, exponent_bits, mantissa_bits):\n",
                "  aval_out, = ctx.avals_out\n",
                "  return hlo.ReducePrecisionOp(operand, mlir.i32_attr(exponent_bits),\n",
                "                               mlir.i32_attr(mantissa_bits)).results\n",
                "\n",
                "mlir.register_lowering(reduce_precision_p, _reduce_precision_lower)\n",
                "\n",
                "\n",
                "\n",
                "_UINT_DTYPES = {\n",
                "  16: np.dtype(np.uint16),\n",
                "  32: np.dtype(np.uint32),\n",
                "  64: np.dtype(np.uint64),\n",
                "}\n",
                "\n",
                "_INT_DTYPES = {\n",
                "  16: np.dtype(np.int16),\n",
                "  32: np.dtype(np.int32),\n",
                "  64: np.dtype(np.int64),\n",
                "}\n",
                "\n",
                "\n",
                "def _sort_abstract_eval(*args, **kwargs):\n",
                "  args = tuple(raise_to_shaped(arg) for arg in args)\n",
                "  if any(arg.shape != args[0].shape for arg in args[1:]):\n",
                "    shapes = \" \".join(str(a.shape) for a in args)\n",
                "    raise TypeError(f\"Arguments to sort must have equal shapes, got: {shapes}\")\n",
                "  return args\n",
                "\n",
                "\n",
                "def _float_to_int_for_sort(x):\n",
                "  # Switch from a floating point value to a integer value in such a way that\n",
                "  # when using the integer value to compare, we get the same result for normal\n",
                "  # values, and -nan is treated as the smallest value, and nan is treated as\n",
                "  # the largest value.\n",
                "  # If f is a float, and\n",
                "  # x = bit_cast<int32>(f);\n",
                "  # y = x < 0 ? int32_max - x : x;\n",
                "  # then y is ordered as an int32 such that finite values have the obvious\n",
                "  # order. In this scheme, -0 would be before 0, and -NaN and NaN appear at\n",
                "  # the beginning and end of the ordering. This causes issues for stable\n",
                "  # sorts, so we avoid this by standardizing the representation of zeros\n",
                "  # and NaNs in the output.\n",
                "  # Note that in order to avoid -x to overflow, we calculate\n",
                "  # int32_max - x as unsigned, and then convert back to signed.\n",
                "  if x.dtype == dtypes.bfloat16:\n",
                "    x = convert_element_type(x, np.float32)\n",
                "  nbits = np.finfo(x).bits\n",
                "  signed_dtype = _INT_DTYPES[nbits]\n",
                "  unsigned_dtype = _UINT_DTYPES[nbits]\n",
                "\n",
                "  signed = bitcast_convert_type(x, signed_dtype)\n",
                "  unsigned = bitcast_convert_type(x, unsigned_dtype)\n",
                "\n",
                "  # We cannot standardize zeros in x because XLA elides this is some cases.\n",
                "  # We cannot standardize NaNs in x because it triggers jax.debug_nans\n",
                "  # So instead we do these replacements in the signed integer representation.\n",
                "\n",
                "  # Standardize zeros:\n",
                "  signed = select(eq(x, _zero(x)), _zeros(signed), signed)\n",
                "  # Standardize nans:\n",
                "  signed_nan = x.dtype.type(np.nan).view(signed_dtype)\n",
                "  signed = select(_isnan(x), full_like(signed, signed_nan), signed)\n",
                "\n",
                "  flipped = bitcast_convert_type(\n",
                "    sub(unsigned_dtype.type(np.iinfo(signed_dtype).max), unsigned), signed_dtype)\n",
                "  return select(lt(signed, _zero(signed)), flipped, signed)\n",
                "\n",
                "# Default comparator that sorts the operands lexicographically on the\n",
                "# first `num_keys` arguments.\n",
                "# For floating point types, a total order is created where\n",
                "# -infinity < ... < 0 < ... < infinity < NaN.\n",
                "# 0.0 and -0.0 are treated as equivalent, as are all NaN representations.\n",
                "# For complex types, the (real, imag) pairs are sorted lexicographically\n",
                "# (following NumPy's semantics).\n",
                "# This code adds complex-number support and lexicographic ordering to the algorithm from:\n",
                "# https://github.com/tensorflow/tensorflow/blob/ba43780830f09da72081fe5061c436f1c6203a92/tensorflow/compiler/xla/client/lib/comparators.h#L33\n",
                "def _sort_lt_comparator(*operands, num_keys=1):\n",
                "  x_keys, y_keys = _operands_to_keys(*operands, num_keys=num_keys)\n",
                "  p = None\n",
                "  for xk, yk in zip(x_keys[::-1], y_keys[::-1]):\n",
                "    p = (bitwise_or(lt(xk, yk), bitwise_and(eq(xk, yk), p)) if p is not None\n",
                "         else lt(xk, yk))\n",
                "  return p\n",
                "\n",
                "# Similar to sort_lt_comparator, but implements less than or equal. Used by\n",
                "# the searchsorted() implementation.\n",
                "def _sort_le_comparator(*operands, num_keys=1):\n",
                "  x_keys, y_keys = _operands_to_keys(*operands, num_keys=num_keys)\n",
                "  p = None\n",
                "  for xk, yk in zip(x_keys[::-1], y_keys[::-1]):\n",
                "    p = (bitwise_or(lt(xk, yk), bitwise_and(eq(xk, yk), p)) if p is not None\n",
                "         else le(xk, yk))\n",
                "  return p\n",
                "\n",
                "def _operands_to_keys(*operands, num_keys=1):\n",
                "  assert len(operands) >= 2 and len(operands) % 2 == 0, operands\n",
                "  assert len(operands) // 2 >= num_keys, (operands, num_keys)\n",
                "  x_keys, y_keys = [], []\n",
                "  for x, y in zip(operands[:2*num_keys:2], operands[1:2*num_keys:2]):\n",
                "    assert x.dtype == y.dtype, (x.dtype, y.dtype)\n",
                "    if dtypes.issubdtype(x.dtype, np.complexfloating):\n",
                "      x_keys.extend([_float_to_int_for_sort(real(x)), _float_to_int_for_sort(imag(x))])\n",
                "      y_keys.extend([_float_to_int_for_sort(real(y)), _float_to_int_for_sort(imag(y))])\n",
                "    elif dtypes.issubdtype(x.dtype, np.floating):\n",
                "      x_keys.append(_float_to_int_for_sort(x))\n",
                "      y_keys.append(_float_to_int_for_sort(y))\n",
                "    else:\n",
                "      x_keys.append(x)\n",
                "      y_keys.append(y)\n",
                "  return x_keys, y_keys\n",
                "\n",
                "\n",
                "def _sort_jvp(primals, tangents, *, dimension, is_stable, num_keys):\n",
                "  shape = primals[0].shape\n",
                "  iotas = []\n",
                "  for dim, size in enumerate(shape):\n",
                "    dtype = np.int32 if size < np.iinfo(np.int32).max else np.int64\n",
                "    iotas.append(broadcasted_iota(dtype, shape, dim))\n",
                "  primals = sort_p.bind(*(primals + (iotas[dimension],)), dimension=dimension,\n",
                "                        is_stable=is_stable, num_keys=num_keys)\n",
                "  idx = tuple(primals[-1] if i == dimension else iotas[i]\n",
                "              for i in range(len(shape)))\n",
                "  tangents_out = tuple(t if type(t) is ad_util.Zero else t[idx] for t in tangents)\n",
                "  return tuple(primals[:-1]), tangents_out\n",
                "\n",
                "def _sort_batch_rule(batched_args, batch_dims, *, dimension, is_stable, num_keys):\n",
                "  prototype_arg, new_bdim = next(\n",
                "    (a, b) for a, b in zip(batched_args, batch_dims) if b is not None)\n",
                "  new_args = []\n",
                "  for arg, bdim in zip(batched_args, batch_dims):\n",
                "    if bdim is None:\n",
                "      dims = np.delete(np.arange(prototype_arg.ndim), new_bdim)\n",
                "      new_args.append(broadcast_in_dim(arg, prototype_arg.shape, dims))\n",
                "    else:\n",
                "      new_args.append(batching.moveaxis(arg, bdim, new_bdim))\n",
                "  new_dimension = dimension + (new_bdim <= dimension)\n",
                "  bdims = (new_bdim,) * len(new_args)\n",
                "  return (sort_p.bind(*new_args, dimension=new_dimension, is_stable=is_stable, num_keys=num_keys),\n",
                "          bdims)\n",
                "\n",
                "\n",
                "sort_p = Primitive('sort')\n",
                "sort_p.multiple_results = True\n",
                "sort_p.def_impl(partial(xla.apply_primitive, sort_p))\n",
                "sort_p.def_abstract_eval(_sort_abstract_eval)\n",
                "ad.primitive_jvps[sort_p] = _sort_jvp\n",
                "batching.primitive_batchers[sort_p] = _sort_batch_rule\n",
                "\n",
                "\n",
                "def _sort_lower(ctx, *operands, dimension, is_stable, num_keys):\n",
                "  assert all(isinstance(x, core.ShapedArray) for x in ctx.avals_in), ctx.avals_in\n",
                "  sort = hlo.SortOp([mlir.aval_to_ir_type(aval) for aval in ctx.avals_out],\n",
                "                    mlir.flatten_lowering_ir_args(operands),\n",
                "                    dimension=mlir.i64_attr(dimension),\n",
                "                    is_stable=ir.BoolAttr.get(is_stable))\n",
                "  scalar_avals = [aval.update(shape=()) for aval in ctx.avals_in]\n",
                "  scalar_types = safe_map(mlir.aval_to_ir_type, scalar_avals)\n",
                "  comparator = sort.comparator.blocks.append(\n",
                "      *util.flatten(zip(scalar_types, scalar_types)))\n",
                "  with ir.InsertionPoint(comparator):\n",
                "    lower_comparator = mlir.lower_fun(partial(_sort_lt_comparator),\n",
                "                                      multiple_results=False)\n",
                "    sub_ctx = ctx.replace(primitive=None,\n",
                "                          avals_in=util.flatten(zip(scalar_avals, scalar_avals)),\n",
                "                          avals_out=[core.ShapedArray((), np.bool_)])\n",
                "\n",
                "    out = lower_comparator(sub_ctx, *[[a] for a in comparator.arguments],\n",
                "                           num_keys=num_keys)\n",
                "    hlo.ReturnOp(util.flatten(out))\n",
                "  return sort.results\n",
                "\n",
                "mlir.register_lowering(sort_p, _sort_lower)\n",
                "\n",
                "\n",
                "def _top_k_abstract_eval(operand, *, k):\n",
                "  if k < 0:\n",
                "    raise ValueError(f\"k argument to top_k must be nonnegative, got {k}\")\n",
                "  if len(operand.shape) == 0:\n",
                "    raise TypeError(\"top_k operand must have >= 1 dimension, got {}\"\n",
                "                    .format(operand.shape))\n",
                "  shape = list(operand.shape)\n",
                "  if shape[-1] < k:\n",
                "    msg = \"k argument to top_k must be no larger than minor dimension; {} vs {}\"\n",
                "    raise ValueError(msg.format(k, shape))\n",
                "  shape[-1] = k\n",
                "  return (operand.update(shape=shape, dtype=operand.dtype,\n",
                "                         weak_type=operand.weak_type),\n",
                "          operand.update(shape=shape, dtype=np.dtype(np.int32)))\n",
                "\n",
                "def _top_k_jvp(primals, tangents, *, k):\n",
                "  operand, = primals\n",
                "  tangent, = tangents\n",
                "  primals_out = top_k(operand, k)\n",
                "  if type(tangent) is ad_util.Zero:\n",
                "    tangent_out = ad_util.Zero.from_value(primals_out[0])\n",
                "  else:\n",
                "    _, k_idxs = primals_out\n",
                "    idx_shape = k_idxs.shape\n",
                "    rank = len(idx_shape)\n",
                "    gather_index_shape = idx_shape + (1,)\n",
                "    gather_indices = []\n",
                "    for i in range(rank-1):\n",
                "      _iota = iota(k_idxs.dtype, idx_shape[i])\n",
                "      _iota = broadcast_in_dim(_iota, gather_index_shape, (i,))\n",
                "      gather_indices.append(_iota)\n",
                "    gather_indices.append(reshape(k_idxs, gather_index_shape))\n",
                "    gather_indices = concatenate(gather_indices, dimension=rank)\n",
                "    slice_sizes = (1,) * rank\n",
                "    dnums = slicing.GatherDimensionNumbers(\n",
                "      offset_dims=(),\n",
                "      collapsed_slice_dims=tuple(range(rank)),\n",
                "      start_index_map=tuple(range(rank)))\n",
                "    tangent_out = slicing.gather(tangent, gather_indices, dnums, slice_sizes)\n",
                "  return primals_out, (tangent_out, ad_util.Zero.from_value(primals_out[1]))\n",
                "\n",
                "def _top_k_batch_rule(batched_args, batch_dims, *, k):\n",
                "  operand, = batched_args\n",
                "  bdim, = batch_dims\n",
                "  if bdim == operand.ndim-1:\n",
                "    perm = np.arange(operand.ndim)\n",
                "    perm[bdim-1], perm[bdim] = perm[bdim], perm[bdim-1]\n",
                "    top_k_v, top_k_i = top_k(transpose(operand, perm), k=k)\n",
                "    return (transpose(top_k_v, perm),\n",
                "            transpose(top_k_i, perm)), (bdim, bdim)\n",
                "  else:\n",
                "    return top_k(operand, k=k), (bdim, bdim)\n",
                "\n",
                "def _top_k_translation_rule(ctx, avals_in, avals_out, x, *, k):\n",
                "  return xla.xla_destructure(ctx.builder, xops.TopK(x, k))\n",
                "\n",
                "top_k_p = Primitive('top_k')\n",
                "top_k_p.multiple_results = True\n",
                "top_k_p.def_impl(partial(xla.apply_primitive, top_k_p))\n",
                "top_k_p.def_abstract_eval(_top_k_abstract_eval)\n",
                "def _top_k_lower(ctx, operand, k):\n",
                "  return chlo.TopKOp(operand, mlir.i64_attr(k)).results\n",
                "mlir.register_lowering(top_k_p, _top_k_lower)\n",
                "ad.primitive_jvps[top_k_p] = _top_k_jvp\n",
                "batching.primitive_batchers[top_k_p] = _top_k_batch_rule\n",
                "\n",
                "def _stop_gradient_jvp_rule(primals, tangents):\n",
                "  # if we don't call stop_gradient here, we'd only peel off one autodiff tracer\n",
                "  x, = primals\n",
                "  return stop_gradient(x), ad_util.Zero.from_value(x)\n",
                "\n",
                "def _stop_gradient_batch_rule(batched_args, batch_dims):\n",
                "  x, = batched_args\n",
                "  dim, = batch_dims\n",
                "  return stop_gradient(x), dim\n",
                "\n",
                "ad.primitive_jvps[ad_util.stop_gradient_p] = _stop_gradient_jvp_rule\n",
                "batching.primitive_batchers[ad_util.stop_gradient_p] = _stop_gradient_batch_rule\n",
                "pe.def_trivial_padding(ad_util.stop_gradient_p)\n",
                "\n",
                "\n",
                "def create_token(_=None):\n",
                "  \"\"\"Creates an XLA token value with no preconditions for sequencing effects.\n",
                "\n",
                "  Experimental.\n",
                "\n",
                "  The argument is ignored. It exists for backward compatibility.\n",
                "  \"\"\"\n",
                "  return create_token_p.bind()\n",
                "\n",
                "create_token_p = Primitive(\"create_token\")\n",
                "create_token_p.def_impl(partial(xla.apply_primitive, create_token_p))\n",
                "create_token_p.def_abstract_eval(lambda *_: abstract_token)\n",
                "\n",
                "def _create_token_lowering(ctx, *operands):\n",
                "  aval_out, = ctx.avals_out\n",
                "  if xc.mlir_api_version < 40:\n",
                "    return hlo.CreateTokenOp(mlir.aval_to_ir_type(aval_out)).results\n",
                "  else:\n",
                "    return hlo.CreateTokenOp().results\n",
                "mlir.register_lowering(create_token_p, _create_token_lowering)\n",
                "\n",
                "\n",
                "def after_all(*operands):\n",
                "  \"\"\"Merges one or more XLA token values. Experimental.\n",
                "\n",
                "  Wraps the XLA AfterAll operator.\"\"\"\n",
                "  return after_all_p.bind(*operands)\n",
                "\n",
                "def _after_all_abstract_eval(*operands):\n",
                "  if any(x is not abstract_token for x in operands):\n",
                "    raise TypeError(\"Arguments to after_all must be tokens\")\n",
                "  return abstract_token\n",
                "\n",
                "\n",
                "after_all_p = Primitive(\"after_all\")\n",
                "after_all_p.def_impl(partial(xla.apply_primitive, after_all_p))\n",
                "after_all_p.def_abstract_eval(_after_all_abstract_eval)\n",
                "\n",
                "def _after_all_lowering(ctx, *operands):\n",
                "  aval_out, = ctx.avals_out\n",
                "  if xc.mlir_api_version < 40:\n",
                "    return hlo.AfterAllOp(mlir.aval_to_ir_type(aval_out), operands).results\n",
                "  else:\n",
                "    return hlo.AfterAllOp(operands).results\n",
                "mlir.register_lowering(after_all_p, _after_all_lowering)\n",
                "\n",
                "\n",
                "InOutFeedEffect = enum.Enum('InOutFeedEffect', ['Infeed', 'Outfeed'])\n",
                "\n",
                "\n",
                "def infeed(token, shape=None, partitions=None):\n",
                "  \"\"\"Consumes an infeed value of `shape` from the host. Experimental.\n",
                "\n",
                "  `token` is used to sequence infeed and outfeed effects.\n",
                "  `partitions` may be specified inside a `sharded_jit` function.\n",
                "  \"\"\"\n",
                "  flat_shapes, treedef = pytree.flatten(shape)\n",
                "  for shape in flat_shapes:\n",
                "    if not isinstance(shape, ShapedArray):\n",
                "      raise TypeError(\"shape argument to infeed must be a pytree of \"\n",
                "                      \"ShapedArray values, got {}\".format(shape))\n",
                "  if partitions is not None:\n",
                "    # Always replicate token.\n",
                "    # We specifically use type() to raise an error for PartitionSpecs.\n",
                "    if type(partitions) != tuple:  # pylint: disable=unidiomatic-typecheck\n",
                "      raise ValueError(f\"'partitions' argument to infeed should be a tuple, \"\n",
                "                       f\"got {partitions}\")\n",
                "    partitions = partitions + (None,)\n",
                "  xs_and_token = infeed_p.bind(token, shapes=tuple(flat_shapes),\n",
                "                               partitions=partitions)\n",
                "  return (treedef.unflatten(xs_and_token[:-1]), xs_and_token[-1])\n",
                "\n",
                "def _infeed_abstract_eval(token, *, shapes, partitions):\n",
                "  if token is not abstract_token:\n",
                "    raise TypeError(\"First argument to infeed must be a token\")\n",
                "  return (*shapes, abstract_token), {InOutFeedEffect.Infeed}\n",
                "\n",
                "\n",
                "infeed_p = Primitive(\"infeed\")\n",
                "infeed_p.multiple_results = True\n",
                "infeed_p.def_impl(partial(xla.apply_primitive, infeed_p))\n",
                "infeed_p.def_effectful_abstract_eval(_infeed_abstract_eval)\n",
                "mlir.lowerable_effects.add(InOutFeedEffect.Infeed)\n",
                "\n",
                "\n",
                "def _infeed_lowering(ctx, token, *, shapes, partitions):\n",
                "  output_types = safe_map(mlir.aval_to_ir_types, ctx.avals_out[:-1])\n",
                "  flat_output_types = util.flatten(output_types)\n",
                "  # TODO(phawkins): verify `shapes` have a major-to-minor layout.\n",
                "  layouts = ir.ArrayAttr.get([\n",
                "      ir.ArrayAttr.get(\n",
                "          [mlir.i64_attr(i)\n",
                "           for i in range(len(aval.shape) - 1, -1, -1)])\n",
                "      for aval in shapes\n",
                "  ])\n",
                "  infeed = hlo.InfeedOp(\n",
                "      flat_output_types + [hlo.TokenType.get()],\n",
                "      token,\n",
                "      infeed_config=ir.StringAttr.get(''),\n",
                "      layout=layouts)\n",
                "  if partitions is not None:\n",
                "    mlir.set_sharding(infeed, xla.sharding_to_proto(partitions))\n",
                "  token = infeed.results[-1]\n",
                "  outs = infeed.results[:-1]\n",
                "  return util.unflatten(outs, safe_map(len, output_types)) + [[\n",
                "      token,\n",
                "  ]]\n",
                "\n",
                "mlir.register_lowering(infeed_p, _infeed_lowering)\n",
                "\n",
                "\n",
                "def outfeed(token, xs, partitions = None):\n",
                "  \"\"\"Outfeeds value `xs` to the host. Experimental.\n",
                "\n",
                "  `token` is used to sequence infeed and outfeed effects.\n",
                "  `partitions` may be specified inside a `sharded_jit` or `pjit` function.\n",
                "  \"\"\"\n",
                "  if partitions is not None:\n",
                "    # We specifically use type() to raise an error for PartitionSpecs.\n",
                "    if type(partitions) != tuple:  # pylint: disable=unidiomatic-typecheck\n",
                "      raise ValueError(f\"'partitions' argument to outfeed should be a tuple, \"\n",
                "                       f\"got {partitions}\")\n",
                "  flat_xs, _ = pytree.flatten(xs)\n",
                "  return outfeed_p.bind(token, *flat_xs, partitions=partitions)\n",
                "\n",
                "def _outfeed_abstract_eval(token, *xs, partitions):\n",
                "  if token is not abstract_token:\n",
                "    raise TypeError(\"First argument to outfeed must be a token\")\n",
                "  return abstract_token, {InOutFeedEffect.Outfeed}\n",
                "\n",
                "outfeed_p = Primitive(\"outfeed\")\n",
                "outfeed_p.def_impl(partial(xla.apply_primitive, outfeed_p))\n",
                "outfeed_p.def_effectful_abstract_eval(_outfeed_abstract_eval)\n",
                "mlir.lowerable_effects.add(InOutFeedEffect.Outfeed)\n",
                "\n",
                "\n",
                "def _outfeed_lowering(ctx, token, *xs, partitions):\n",
                "  token_aval = ctx.avals_in[0]\n",
                "  if xc.mlir_api_version < 40:\n",
                "    outfeed = hlo.OutfeedOp(\n",
                "        mlir.aval_to_ir_type(token_aval),\n",
                "        mlir.flatten_lowering_ir_args(xs),\n",
                "        token,\n",
                "        outfeed_config=ir.StringAttr.get(''))\n",
                "  else:\n",
                "    outfeed = hlo.OutfeedOp(\n",
                "        mlir.flatten_lowering_ir_args(xs),\n",
                "        token,\n",
                "        outfeed_config=ir.StringAttr.get(''))\n",
                "  if partitions is not None:\n",
                "    mlir.set_sharding(outfeed, xla.sharding_to_proto(partitions))\n",
                "  return outfeed.results\n",
                "\n",
                "mlir.register_lowering(outfeed_p, _outfeed_lowering)\n",
                "\n",
                "\n",
                "def rng_uniform(a, b, shape):\n",
                "  \"\"\"Stateful PRNG generator. Experimental and its use is discouraged.\n",
                "\n",
                "  Returns uniformly distributed random numbers in the range [a, b)\n",
                "\n",
                "  You should use jax.random for most purposes; this function exists only for\n",
                "  niche use cases with special performance requirements.\n",
                "\n",
                "  This API may be removed at any time.\n",
                "  \"\"\"\n",
                "  return rng_uniform_p.bind(a, b, shape=tuple(shape))\n",
                "\n",
                "def _rng_uniform_abstract_eval(a, b, *, shape):\n",
                "  if a.dtype != b.dtype:\n",
                "    raise ValueError(\n",
                "      \"Arguments to rng_uniform must have identical dtypes, got {} \"\n",
                "      \"and {}.\".format(a.dtype, b.dtype))\n",
                "  if a.shape != () or b.shape != ():\n",
                "    raise ValueError(\n",
                "      \"Arguments to rng_uniform must be scalars; got shapes {} and {}.\"\n",
                "      .format(a.shape, b.shape))\n",
                "  return a.update(shape=shape, dtype=a.dtype,\n",
                "                  weak_type=(a.weak_type and b.weak_type))\n",
                "\n",
                "rng_uniform_p = Primitive(\"rng_uniform\")\n",
                "rng_uniform_p.def_impl(partial(xla.apply_primitive, rng_uniform_p))\n",
                "rng_uniform_p.def_abstract_eval(_rng_uniform_abstract_eval)\n",
                "\n",
                "def _rng_uniform_lowering(ctx, a, b, *, shape):\n",
                "  aval_out, = ctx.avals_out\n",
                "  shape, = mlir.ir_constants(np.array(aval_out.shape, np.int64),\n",
                "                             canonicalize_types=False)\n",
                "  return hlo.RngOp(a, b, shape,\n",
                "                   hlo.RngDistributionAttr.get('UNIFORM')).results\n",
                "\n",
                "mlir.register_lowering(rng_uniform_p, _rng_uniform_lowering)\n",
                "\n",
                "\n",
                "def _rng_bit_generator_shape_rule(key, *, shape, dtype, algorithm):\n",
                "  del dtype, algorithm\n",
                "  return (key.shape, tuple(shape))\n",
                "\n",
                "def _rng_bit_generator_dtype_rule(key, *, shape, dtype, algorithm):\n",
                "  del shape, algorithm\n",
                "  return (key.dtype, dtype)\n",
                "\n",
                "def _rng_bit_generator_weak_type_rule(key, *, shape, dtype, algorithm):\n",
                "  del shape, dtype, algorithm\n",
                "  return (key.weak_type, False)\n",
                "\n",
                "RandomAlgorithm = xops.RandomAlgorithm\n",
                "RandomAlgorithm.__str__ = lambda algorithm: algorithm.name  # type: ignore[assignment]\n",
                "\n",
                "def _rng_algorithm(algorithm: RandomAlgorithm):\n",
                "  if algorithm == RandomAlgorithm.RNG_THREE_FRY:\n",
                "    return hlo.RngAlgorithmAttr.get(\"THREE_FRY\")\n",
                "  elif algorithm == RandomAlgorithm.RNG_PHILOX:\n",
                "    return hlo.RngAlgorithmAttr.get(\"PHILOX\")\n",
                "  elif algorithm == RandomAlgorithm.RNG_DEFAULT:\n",
                "    return hlo.RngAlgorithmAttr.get(\"DEFAULT\")\n",
                "  else:\n",
                "    assert False\n",
                "\n",
                "def _rng_bit_generator_lowering(\n",
                "    ctx, key, *, shape, dtype, algorithm):\n",
                "  key_type = ir.RankedTensorType(key.type)\n",
                "  key_shape, key_etype = key_type.shape, key_type.element_type\n",
                "  # While the RngBitGenerator HLO accepts a u64[2] key on all backends, we\n",
                "  # typically represent the key argument to this primitive as a u32[4] so as to\n",
                "  # sidestep issues with the jax_enable_x64=False configuration. As a result, we\n",
                "  # need to convert u32[4] -> u64[2] here in the translation rule. However, we\n",
                "  # also polymorphically allow a u64[2] for backward compatibility.\n",
                "  #\n",
                "  # Separately, xops.RngBitGenerator doesn't support generating u8 or\n",
                "  # u16, so we request u32 and truncate in that case.\n",
                "  u32_type = ir.IntegerType.get_unsigned(32)\n",
                "  u64_type = ir.IntegerType.get_unsigned(64)\n",
                "  assert ((key_shape == [4] and key_etype == u32_type) or\n",
                "          (key_shape == [2] and key_etype == u64_type)), (key_shape, key_etype)\n",
                "  dtype = np.dtype(dtype)\n",
                "  etype = mlir.dtype_to_ir_type(dtype)\n",
                "  if dtype == np.dtype('uint32') or dtype == np.dtype('uint64'):\n",
                "    rbg_etype = etype\n",
                "  else:\n",
                "    rbg_etype = u32_type\n",
                "  if key_etype == u32_type:\n",
                "    key = hlo.BitcastConvertOp(\n",
                "        ir.RankedTensorType.get([2], u64_type),\n",
                "        hlo.ReshapeOp(ir.RankedTensorType.get([2, 2], u32_type), key)).result\n",
                "  algorithm_attr = _rng_algorithm(algorithm)\n",
                "  out_key, out_vals = hlo.RngBitGeneratorOp(\n",
                "      key.type,\n",
                "      ir.RankedTensorType.get(shape, rbg_etype),\n",
                "      algorithm_attr, key).results\n",
                "  if key_etype == u32_type:\n",
                "    out_key = hlo.ReshapeOp(\n",
                "        ir.RankedTensorType.get([4], u32_type),\n",
                "        hlo.BitcastConvertOp(\n",
                "            ir.RankedTensorType.get([2, 2], u32_type), out_key)).result\n",
                "  if rbg_etype != etype:\n",
                "    out_vals = hlo.ConvertOp(\n",
                "      ir.RankedTensorType.get(ir.RankedTensorType(out_vals.type).shape, etype),\n",
                "      out_vals).result\n",
                "  return [out_key, out_vals]\n",
                "\n",
                "\n",
                "def _rng_bit_generator_named_shape_rule(key, *, shape, dtype, algorithm):\n",
                "  return [key.named_shape, key.named_shape]\n",
                "\n",
                "rng_bit_generator_p = Primitive(\"rng_bit_generator\")\n",
                "rng_bit_generator_p.multiple_results = True\n",
                "rng_bit_generator_p.def_impl(\n",
                "    partial(xla.apply_primitive, rng_bit_generator_p))\n",
                "rng_bit_generator_p.def_abstract_eval(\n",
                "    partial(standard_multi_result_abstract_eval, rng_bit_generator_p,\n",
                "            _rng_bit_generator_shape_rule, _rng_bit_generator_dtype_rule,\n",
                "            _rng_bit_generator_weak_type_rule,\n",
                "            _rng_bit_generator_named_shape_rule))\n",
                "mlir.register_lowering(rng_bit_generator_p,\n",
                "                       _rng_bit_generator_lowering)\n",
                "\n",
                "\n",
                "def _array_copy(arr: ArrayLike) -> Array:\n",
                "  return copy_p.bind(arr)\n",
                "\n",
                "\n",
                "def _which_dim_sharded(s: PmapSharding) -> Optional[int]:\n",
                "  sharded_dim = None\n",
                "  for i, s in enumerate(s.sharding_spec.sharding):\n",
                "    if isinstance(s, pxla.Unstacked):\n",
                "      sharded_dim = i\n",
                "      break\n",
                "  return sharded_dim\n",
                "\n",
                "\n",
                "def _identity_fn(x): return x\n",
                "\n",
                "\n",
                "def _copy_impl_pmap_sharding(sharded_dim, *args, **kwargs):\n",
                "  axis_name, static_broadcasted_tuple, donate_tuple = api._shared_code_pmap(\n",
                "    _identity_fn, None, (), (), sharded_dim, sharded_dim)\n",
                "  p = api._prepare_pmap(\n",
                "      _identity_fn, sharded_dim, sharded_dim, static_broadcasted_tuple,\n",
                "      donate_tuple, None, None, None, None, args, kwargs)\n",
                "  out_flat =  pxla.xla_pmap_impl(\n",
                "      p.flat_fun, *p.flat_args, backend=None, axis_name=axis_name,\n",
                "      axis_size=p.local_axis_size, global_axis_size=p.global_axis_size,\n",
                "      devices=p.devices, in_axes=p.in_axes_flat,\n",
                "      out_axes_thunk=p.out_axes_thunk, name=p.flat_fun.__name__,\n",
                "      donated_invars=p.donated_invars,\n",
                "      global_arg_shapes=p.global_arg_shapes_flat,\n",
                "      is_explicit_global_axis_size=p.is_explicit_global_axis_size,\n",
                "  )\n",
                "  return tree_util.tree_unflatten(p.out_tree(), out_flat)\n",
                "\n",
                "\n",
                "# TODO(https://github.com/google/jax/issues/13552): Look into making this a\n",
                "# method on jax.Array so that we can bypass the XLA compilation here.\n",
                "def _copy_impl(prim, *args, **kwargs):\n",
                "  a, = args\n",
                "  if (config.jax_array and isinstance(a, jax.Array) and\n",
                "      isinstance(a.sharding, PmapSharding)):\n",
                "    sharded_dim = _which_dim_sharded(a.sharding)\n",
                "    return _copy_impl_pmap_sharding(sharded_dim, *args, **kwargs)\n",
                "  return xla.apply_primitive(prim, *args, **kwargs)\n",
                "\n",
                "# The copy_p primitive exists for expressing making copies of runtime arrays.\n",
                "# For that reason we don't simplify it out of jaxprs (e.g. for jit invariance).\n",
                "# It's used in jnp.array(x, copy=True), which is the user-facing API.\n",
                "copy_p = core.Primitive('copy')\n",
                "copy_p.def_impl(partial(_copy_impl, copy_p))\n",
                "copy_p.def_abstract_eval(lambda x: x)\n",
                "mlir.register_lowering(copy_p, lambda ctx, x: [x])\n",
                "ad.deflinear(copy_p, lambda t: [copy_p.bind(t)])\n",
                "pe.def_trivial_padding(copy_p)\n",
                "batching.defvectorized(copy_p)\n",
                "\n",
                "\n",
                "def rng_bit_generator(key, shape, dtype=np.uint32,\n",
                "                      algorithm=RandomAlgorithm.RNG_DEFAULT):\n",
                "  \"\"\"Stateless PRNG bit generator. Experimental and its use is discouraged.\n",
                "\n",
                "  Returns uniformly distributed random bits with the specified shape and dtype\n",
                "  (what is required to be an integer type) using the platform specific\n",
                "  default algorithm or the one specified.\n",
                "\n",
                "  It provides direct access to the RngBitGenerator primitive exposed by XLA\n",
                "  (https://www.tensorflow.org/xla/operation_semantics#rngbitgenerator) for low\n",
                "  level API access.\n",
                "\n",
                "  Most users should use `jax.random` instead for a stable and more user\n",
                "  friendly API.\n",
                "  \"\"\"\n",
                "  shape = jax.core.canonicalize_shape(shape)\n",
                "  dtype = dtypes.canonicalize_dtype(dtype)\n",
                "  if np.dtype(dtype) not in {np.dtype('uint8'), np.dtype('uint16'),\n",
                "                             np.dtype('uint32'), np.dtype('uint64')}:\n",
                "    raise TypeError(f'rng_bit_generator: unsupported dtype {dtype}')\n",
                "  return tuple(\n",
                "      rng_bit_generator_p.bind(\n",
                "          key, shape=shape, dtype=dtype, algorithm=algorithm))\n",
                "\n",
                "\n",
                "def _iota_abstract_eval(*, dtype, shape, dimension):\n",
                "  _check_shapelike(\"iota\", \"shape\", shape)\n",
                "  if not any(dtypes.issubdtype(dtype, t) for t in _num):\n",
                "    msg = 'iota does not accept dtype {}. Accepted dtypes are subtypes of {}.'\n",
                "    typename = str(np.dtype(dtype).name)\n",
                "    accepted_typenames = (t.__name__ for t in _num)\n",
                "    raise TypeError(msg.format(typename, ', '.join(accepted_typenames)))\n",
                "  if not 0 <= dimension < len(shape):\n",
                "    raise ValueError(\"iota dimension must be between 0 and len(shape), got \"\n",
                "                     f\"{dimension=} for {shape=}\")\n",
                "  if not any(isinstance(d, core.DArray) and\n",
                "             type(core.get_aval(d).dtype) is core.bint for d in shape):\n",
                "    return ShapedArray(shape, dtype)\n",
                "  # TODO(mattjj): unify DShapedArray with ShapedArray, and remove this code\n",
                "  return core.DShapedArray(shape, dtype, False)\n",
                "\n",
                "iota_p = Primitive('iota')\n",
                "iota_p.def_impl(partial(xla.apply_primitive, iota_p))\n",
                "iota_p.def_abstract_eval(_iota_abstract_eval)\n",
                "\n",
                "def _iota_staging_rule(trace, *dyn_shape, dtype, shape, dimension):\n",
                "  params = dict(dtype=dtype, shape=shape, dimension=dimension)\n",
                "  if not dyn_shape:\n",
                "    return trace.default_process_primitive(iota_p, (), params)\n",
                "  aval = core.DShapedArray(_merge_dyn_shape(shape, dyn_shape), dtype, False)\n",
                "  return _dyn_shape_staging_rule(trace, iota_p, aval, *dyn_shape, **params)\n",
                "pe.custom_staging_rules[iota_p] = _iota_staging_rule\n",
                "\n",
                "def _iota_typecheck_rule(*dyn_shape, dtype, shape, dimension):\n",
                "  if not dyn_shape:\n",
                "    out_aval, effects = iota_p.abstract_eval(\n",
                "        dtype=dtype, shape=shape, dimension=dimension)\n",
                "    return [out_aval], effects\n",
                "  else:\n",
                "    out_shape = _merge_dyn_shape(shape, dyn_shape)\n",
                "    out_shape = [x.val if type(x) is core.Literal else x for x in out_shape]  # pytype: disable=attribute-error\n",
                "    out_aval = core.DShapedArray(tuple(out_shape), dtype, False)\n",
                "    return [out_aval], core.no_effects\n",
                "core.custom_typechecks[iota_p] = _iota_typecheck_rule\n",
                "\n",
                "def _iota_lower(ctx, *dyn_shape, dtype, shape, dimension):\n",
                "  del dtype\n",
                "  aval_out, = ctx.avals_out\n",
                "  if dyn_shape:\n",
                "    aval_out = aval_out.update(shape=_merge_dyn_shape(shape, dyn_shape))\n",
                "  if not core.is_constant_shape(aval_out.shape):\n",
                "    shape = mlir.eval_dynamic_shape(ctx, aval_out.shape)\n",
                "    return hlo.DynamicIotaOp(\n",
                "        mlir.aval_to_ir_type(aval_out),\n",
                "        mlir.shape_tensor(shape),\n",
                "        mlir.i64_attr(dimension),\n",
                "    ).results\n",
                "  else:\n",
                "    return hlo.IotaOp(mlir.aval_to_ir_type(aval_out),\n",
                "                      mlir.i64_attr(dimension)).results\n",
                "mlir.register_lowering(iota_p, _iota_lower)\n",
                "\n",
                "def _iota_batching_rule(in_vals, in_dims, *, dtype, shape, dimension):\n",
                "  (segment_lengths,), (ax,) = in_vals, in_dims\n",
                "  shapes = [_merge_dyn_shape(shape, (d,)) for d in segment_lengths]\n",
                "  iotas = [broadcasted_iota(dtype, s, dimension) for s in shapes]\n",
                "  return concatenate(iotas, dimension), batching.ConcatAxis(ax, segment_lengths)\n",
                "batching.primitive_batchers[iota_p] = _iota_batching_rule\n",
                "\n",
                "def _iota_pp_rule(eqn, context, settings):\n",
                "  printed_params = {}\n",
                "  if len(eqn.params['shape']) > 1:\n",
                "    printed_params['dimension'] = eqn.params['dimension']\n",
                "  lhs = core.pp_vars(eqn.outvars, context, print_shapes=settings.print_shapes)\n",
                "  rhs = [pp.text(eqn.primitive.name),\n",
                "         core.pp_kv_pairs(sorted(printed_params.items()), context, settings),\n",
                "         pp.text(\" \") + core.pp_vars(eqn.invars, context)]\n",
                "  annotation = (source_info_util.summarize(eqn.source_info)\n",
                "                if settings.source_info else None)\n",
                "  return [lhs, pp.text(\" = \", annotation=annotation), *rhs]\n",
                "# core.pp_eqn_rules[iota_p] = _iota_pp_rule\n",
                "\n",
                "def _iota_padding_rule(in_avals, out_avals, *dyn_shape, dtype, shape, dimension):\n",
                "  out_aval, = out_avals\n",
                "  new_shape = []\n",
                "  new_dyn_shape = []\n",
                "  for d in out_aval.shape:\n",
                "    if type(d) is pe.BoundedAxisSize:\n",
                "      new_shape.append(d.bound)\n",
                "    elif type(d) is int:\n",
                "      new_shape.append(d)\n",
                "    else:\n",
                "      assert isinstance(d, core.Tracer)\n",
                "      new_shape.append(None)\n",
                "      new_dyn_shape.append(d)\n",
                "  return [iota_p.bind(*new_dyn_shape, shape=tuple(new_shape),\n",
                "                      dtype=dtype, dimension=dimension)]\n",
                "pe.padding_rules[iota_p] = _iota_padding_rule\n",
                "\n",
                "\n",
                "### util\n",
                "\n",
                "_ndim = np.ndim\n",
                "\n",
                "\n",
                "def _dilate_shape(shape, dilation):\n",
                "  \"\"\"Utility function for computing the shape resulting from a dilation.\"\"\"\n",
                "  if not np.all(np.greater(dilation, 0)):\n",
                "    msg = \"All dilations must be positive, got {}.\"\n",
                "    raise TypeError(msg.format(dilation))\n",
                "  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)\n",
                "  return core.dilate_shape(shape, dilation)\n",
                "\n",
                "def _ceil_divide(x1, x2):\n",
                "  return -np.floor_divide(np.negative(x1), x2)\n",
                "\n",
                "def padtype_to_pads(in_shape, window_shape, window_strides, padding):\n",
                "  \"\"\"Convert padding string to list of pairs of pad values.\"\"\"\n",
                "  PaddingType = xla_client.PaddingType\n",
                "\n",
                "  if isinstance(padding, str):\n",
                "    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}\n",
                "    try:\n",
                "      padding = mapping[padding.upper()]\n",
                "    except KeyError as err:\n",
                "      msg = \"Unrecognized padding type: expected 'VALID' or 'SAME', got {}.\"\n",
                "      raise RuntimeError(msg.format(padding)) from err\n",
                "\n",
                "  if padding == PaddingType.SAME:\n",
                "    out_shape = _ceil_divide(in_shape, window_strides)\n",
                "    pad_sizes = np.maximum(0, (out_shape - 1) * window_strides +\n",
                "                                window_shape - in_shape)\n",
                "    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]\n",
                "  elif padding == PaddingType.VALID:\n",
                "    return [(0, 0)] * len(in_shape)\n",
                "  else:\n",
                "    msg = \"Unknown padding type: {}.\"\n",
                "    raise TypeError(msg.format(padding))\n",
                "\n",
                "\n",
                "# Map of lax function to equivalent jax.numpy function for use in error string below.\n",
                "_JNP_FUNCTION_EQUIVALENTS = {\n",
                "  'abs': 'fabs',\n",
                "  'acos': 'arccos',\n",
                "  'acosh': 'arccosh',\n",
                "  'add': 'add',\n",
                "  'asin': 'arcsin',\n",
                "  'asinh': 'arcsinh',\n",
                "  'atan': 'arctan',\n",
                "  'atan2': 'arctan2',\n",
                "  'atanh': 'arctanh',\n",
                "  'bitwise_and': 'bitwise_and',\n",
                "  'bitwise_not': 'bitwise_not',\n",
                "  'bitwise_or': 'bitwise_or',\n",
                "  'bitwise_xor': 'bitwise_xor',\n",
                "  'cbrt': 'cbrt',\n",
                "  'ceil': 'ceil',\n",
                "  'concatenate': 'concatenate',\n",
                "  'cos': 'cos',\n",
                "  'cosh': 'cosh',\n",
                "  'div': 'divide',\n",
                "  'eq': 'equal',\n",
                "  'exp': 'exp',\n",
                "  'expm1': 'expm1',\n",
                "  'floor': 'floor',\n",
                "  'greater': 'greater',\n",
                "  'greater_equal': 'greater_equal',\n",
                "  'less': 'less',\n",
                "  'less_equal': 'less_equal',\n",
                "  'log': 'log',\n",
                "  'logical_and': 'logical_and',\n",
                "  'logical_not': 'logical_not',\n",
                "  'logical_or': 'logical_or',\n",
                "  'logical_xor': 'logical_xor',\n",
                "  'log1p': 'log1p',\n",
                "  'max': 'maximum',\n",
                "  'min': 'minimum',\n",
                "  'mul': 'multiply',\n",
                "  'ne': 'not_equal',\n",
                "  'neg': 'negative',\n",
                "  'nextafter': 'nextafter',\n",
                "  'pow': 'float_power',\n",
                "  'rount': 'rount',\n",
                "  'select': 'where',\n",
                "  'shift_left': 'left_shift',\n",
                "  'shift_right_logical': 'right_shift',\n",
                "  'shift_right_arithmetic': 'right_shift',\n",
                "  'sign': 'sign',\n",
                "  'sin': 'sin',\n",
                "  'sinh': 'sinh',\n",
                "  'sqrt': 'sqrt',\n",
                "  'sub': 'subtract',\n",
                "  'tan': 'tan',\n",
                "  'tanh': 'tanh'\n",
                "}\n",
                "\n",
                "def _check_same_dtypes(name, ignore_fp_precision, *ttypes):\n",
                "  \"\"\"Check that dtypes agree, possibly ignoring float precision.\"\"\"\n",
                "  # the `ignore_fp_precision` flag exists because the XLA shape inference logic\n",
                "  # allows mixed floating point precision, but the HLO verifier often rejects it\n",
                "  if any(core.is_opaque_dtype(t) for t in ttypes):\n",
                "    return  # TODO(mattjj,frostig): do some checking, friend\n",
                "  types = map(np.dtype, ttypes)  # canonicalize\n",
                "  if ignore_fp_precision:\n",
                "    types = [\n",
                "        np.floating if dtypes.issubdtype(dtype, np.floating)\n",
                "        else np.complexfloating if dtypes.issubdtype(dtype, np.complexfloating)\n",
                "        else dtype for dtype in types]\n",
                "  if len({dtypes.canonicalize_dtype(t) for t in types}) != 1:\n",
                "    if ignore_fp_precision:\n",
                "      msg = (\"lax.{} requires arguments to have same dtypes up to floating point \"\n",
                "             \"precision, got {}.\")\n",
                "    else:\n",
                "      msg = \"lax.{} requires arguments to have the same dtypes, got {}.\"\n",
                "    if name in _JNP_FUNCTION_EQUIVALENTS:\n",
                "      equiv = _JNP_FUNCTION_EQUIVALENTS[name]\n",
                "      msg += f\" (Tip: jnp.{equiv} is a similar function that does automatic type promotion on inputs).\"\n",
                "    raise TypeError(msg.format(name, \", \".join(map(str, types))))\n",
                "\n",
                "\n",
                "def _check_shapelike(fun_name, arg_name, obj, non_zero_shape=False):\n",
                "  \"\"\"Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).\"\"\"\n",
                "  if not isinstance(obj, (tuple, list, np.ndarray)):\n",
                "    msg = \"{} {} must be of type tuple/list/ndarray, got {}.\"\n",
                "    raise TypeError(msg.format(fun_name, arg_name, type(obj)))\n",
                "  # bool(obj) for an ndarray raises an error, so we check len\n",
                "  if not len(obj):  # pylint: disable=g-explicit-length-test\n",
                "    return\n",
                "  if (config.jax_dynamic_shapes and isinstance(obj, (tuple, list)) and\n",
                "      any(isinstance(d, (core.Tracer, core.DArray)) for d in obj)):\n",
                "    return  # TODO(mattjj): handle more checks in the dynamic shape case\n",
                "  obj_arr = np.array(obj)\n",
                "  if obj_arr.ndim != 1:\n",
                "    msg = \"{} {} must be 1-dimensional, got {}.\"\n",
                "    raise TypeError(msg.format(obj_arr.ndim))\n",
                "  try:\n",
                "    canonicalize_shape(obj_arr)\n",
                "  except TypeError as err:\n",
                "    msg = \"{} {} must have every element be an integer type, got {}.\"\n",
                "    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj)))) from err\n",
                "  lower_bound, bound_error = (\n",
                "      (1, \"strictly positive\") if non_zero_shape else (0, \"nonnegative\"))\n",
                "  if not all(core.greater_equal_dim(d, lower_bound) for d in obj_arr):\n",
                "    msg = \"{} {} must have every element be {}, got {}.\"\n",
                "    raise TypeError(msg.format(fun_name, arg_name, bound_error, obj))\n",
                "\n",
                "\n",
                "def _const(example, val):\n",
                "  dtype = _dtype(example)\n",
                "  if dtypes.is_python_scalar(example):\n",
                "    val = dtypes.scalar_type_of(example)(val)\n",
                "    return val if dtype == _dtype(val) else np.array(val, dtype)\n",
                "  return np.array(val, dtype)\n",
                "\n",
                "_zeros: Callable = partial(full_like, fill_value=0)\n",
                "_zero: Callable = partial(full_like, shape=(), fill_value=0)\n",
                "_ones: Callable = partial(full_like, fill_value=1)\n",
                "_one: Callable = partial(full_like, shape=(), fill_value=1)\n",
                "_twos: Callable = partial(full_like, fill_value=2)\n",
                "_two: Callable = partial(full_like, shape=(), fill_value=2)\n",
                "\n",
                "dtype: Callable = partial(dtypes.dtype, canonicalize=True)\n",
                "_dtype: Callable = partial(dtypes.dtype, canonicalize=True)\n",
                "\n",
                "def _isnan(x: ArrayLike) -> Array:\n",
                "  return ne(x, x)\n",
                "\n",
                "def _iscomplex(x) -> bool:\n",
                "  return dtypes.issubdtype(_dtype(x), np.complexfloating)\n",
                "\n",
                "\n",
                "def ranges_like(*xs):\n",
                "  start = 0\n",
                "  for x in xs:\n",
                "    x_len = len(x)\n",
                "    yield range(start, start + x_len)\n",
                "    start += x_len\n",
                "\n",
                "\n",
                "def remaining(original, *removed_lists):\n",
                "  removed = set(itertools.chain(*removed_lists))\n",
                "  return [i for i in original if i not in removed]\n",
                "\n",
                "\n",
                "def canonicalize_precision(precision: PrecisionLike) -> Optional[Tuple[PrecisionType, PrecisionType]]:\n",
                "  \"\"\"Turns an API precision specification, into a pair of enumeration values.\n",
                "\n",
                "  The API can take the precision as a string, or int, and either as a single\n",
                "  value to apply to both operands, or as a sequence of two values.\n",
                "  \"\"\"\n",
                "  if precision is None:\n",
                "    if config.jax_default_matmul_precision is None:\n",
                "      return None\n",
                "    try:\n",
                "      return type_cast(\n",
                "          Tuple[PrecisionType, PrecisionType],\n",
                "          (Precision(config.jax_default_matmul_precision),\n",
                "           Precision(config.jax_default_matmul_precision)))\n",
                "    except TypeError:\n",
                "      raise ValueError(\n",
                "          \"jax_default_matmul_precision flag must be set to None or a value in \"\n",
                "          f\"{list(Precision._strings)}, but got {config.jax_default_matmul_precision}\"\n",
                "      ) from None\n",
                "  elif isinstance(precision, str) and precision in Precision._strings:\n",
                "    return type_cast(Tuple[PrecisionType, PrecisionType],\n",
                "                     (Precision(precision), Precision(precision)))\n",
                "  elif isinstance(precision, xla_client.PrecisionConfig.Precision):\n",
                "    return type_cast(Tuple[PrecisionType, PrecisionType], (precision, precision))\n",
                "  elif (isinstance(precision, (list, tuple)) and len(precision) == 2 and\n",
                "        all(isinstance(p, xla_client.PrecisionConfig.Precision) for p in precision)):\n",
                "    return type_cast(Tuple[PrecisionType, PrecisionType], precision)\n",
                "  elif (isinstance(precision, (list, tuple)) and len(precision) == 2 and\n",
                "        all(isinstance(s, str) for s in precision)):\n",
                "    s1, s2 = precision\n",
                "    p1 = type_cast(Tuple[PrecisionType, PrecisionType], canonicalize_precision(s1))[0]\n",
                "    p2 = type_cast(Tuple[PrecisionType, PrecisionType], canonicalize_precision(s2))[0]\n",
                "    return (p1, p2)\n",
                "  else:\n",
                "    raise ValueError(\n",
                "        f\"Precision argument must be None, a string in {list(Precision._strings)}, \"\n",
                "        \"a lax.Precision value or a tuple of two lax.Precision values or \"\n",
                "        f\"strings; got {precision}.\")\n",
                "\n",
                "def _balanced_eq(x, z, y):\n",
                "  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),\n",
                "             select(_eq_meet(y, z), _twos(z), _ones(z)))\n",
                "\n",
                "\n",
                "def _eq_meet(a, b):\n",
                "  a_dtype, b_dtype = _dtype(a), _dtype(b)\n",
                "  if a_dtype != b_dtype:\n",
                "    higher_dtype = dtypes.promote_types(a_dtype, b_dtype)\n",
                "    if higher_dtype == a_dtype:\n",
                "      a = convert_element_type(a, b_dtype)\n",
                "    else:\n",
                "      b = convert_element_type(b, a_dtype)\n",
                "  return eq(a, b)\n",
                "\n",
                "\n",
                "def _abstractify(x):\n",
                "  return raise_to_shaped(core.get_aval(x))\n",
                "\n",
                "\n",
                "# TODO(zhangqiaorjc): Update caller and remove this alias.\n",
                "_check_user_dtype_supported = dtypes.check_user_dtype_supported\n",
                "\n",
                "\n",
                "def empty(dtype):\n",
                "  return empty_p.bind(dtype=dtype)\n",
                "empty_p = core.Primitive('empty')\n",
                "empty_p.def_abstract_eval(lambda *, dtype: core.ShapedArray((), dtype))\n",
                "def _empty_lower(ctx, *, dtype):\n",
                "  if core.is_opaque_dtype(dtype):\n",
                "    return dtype._rules.empty_mlir(ctx, ctx.avals_out[0])\n",
                "  return mlir.ir_constants(np.zeros((), np.dtype(dtype)))\n",
                "mlir.register_lowering(empty_p, _empty_lower)\n",
                "\n",
                "\n",
                "class BIntRules:\n",
                "  @staticmethod\n"
            ],
            {
                "type": "replace",
                "before": [
                    "  def aval_to_ir_types(aval):\n"
                ],
                "after": [
                    "  def physical_avals(aval) -> Sequence[core.AbstractValue]:\n"
                ],
                "parent_version_range": {
                    "start": 4848,
                    "end": 4849
                },
                "child_version_range": {
                    "start": 4848,
                    "end": 4849
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "BIntRules",
                        "signature": "class BIntRules:",
                        "at_line": 4846
                    },
                    {
                        "type": "function",
                        "name": "aval_to_ir_types",
                        "signature": "def aval_to_ir_types(aval):",
                        "at_line": 4848
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: jax/_src/lax/lax.py\nCode:\n4845 4845    \n4846 4846    class BIntRules:\n4847 4847      @staticmethod\n4848       -   def aval_to_ir_types(aval):\n     4848  +   def physical_avals(aval) -> Sequence[core.AbstractValue]:\n4849 4849        dtype = dtypes._scalar_type_to_dtype(int)\n           ...\n",
                "file_path": "jax/_src/lax/lax.py",
                "identifiers_before": [
                    "aval",
                    "aval_to_ir_types"
                ],
                "identifiers_after": [
                    "AbstractValue",
                    "Sequence",
                    "aval",
                    "core",
                    "physical_avals"
                ],
                "prefix": [
                    "\n",
                    "class BIntRules:\n",
                    "  @staticmethod\n"
                ],
                "suffix": [
                    "    dtype = dtypes._scalar_type_to_dtype(int)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "aval",
                            "position": {
                                "start": {
                                    "line": 4848,
                                    "column": 23
                                },
                                "end": {
                                    "line": 4848,
                                    "column": 27
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/jax/jax/_src/lax/lax.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "aval",
                            "position": {
                                "start": {
                                    "line": 4848,
                                    "column": 21
                                },
                                "end": {
                                    "line": 4848,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/jax/jax/_src/lax/lax.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "    dtype = dtypes._scalar_type_to_dtype(int)\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    return (ir.RankedTensorType.get(aval.shape, mlir.dtype_to_ir_type(dtype)),)\n"
                ],
                "after": [
                    "    return [core.ShapedArray(aval.shape, dtype)]\n"
                ],
                "parent_version_range": {
                    "start": 4850,
                    "end": 4851
                },
                "child_version_range": {
                    "start": 4850,
                    "end": 4851
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "BIntRules",
                        "signature": "class BIntRules:",
                        "at_line": 4846
                    },
                    {
                        "type": "function",
                        "name": "aval_to_ir_types",
                        "signature": "def aval_to_ir_types(aval):",
                        "at_line": 4848
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: jax/_src/lax/lax.py\nCode:\n             class BIntRules:\n                 ...\n                 def aval_to_ir_types(aval):\n                     ...\n4849 4849        dtype = dtypes._scalar_type_to_dtype(int)\n4850       -     return (ir.RankedTensorType.get(aval.shape, mlir.dtype_to_ir_type(dtype)),)\n     4850  +     return [core.ShapedArray(aval.shape, dtype)]\n4851 4851    \n4852 4852      @staticmethod\n4853 4853      def result_handler(sticky_device, aval):\n           ...\n",
                "file_path": "jax/_src/lax/lax.py",
                "identifiers_before": [
                    "RankedTensorType",
                    "aval",
                    "dtype",
                    "dtype_to_ir_type",
                    "get",
                    "ir",
                    "mlir",
                    "shape"
                ],
                "identifiers_after": [
                    "ShapedArray",
                    "aval",
                    "core",
                    "dtype",
                    "shape"
                ],
                "prefix": [
                    "    dtype = dtypes._scalar_type_to_dtype(int)\n"
                ],
                "suffix": [
                    "\n",
                    "  @staticmethod\n",
                    "  def result_handler(sticky_device, aval):\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "aval",
                            "position": {
                                "start": {
                                    "line": 4850,
                                    "column": 36
                                },
                                "end": {
                                    "line": 4850,
                                    "column": 40
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/jax/jax/_src/lax/lax.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "aval",
                            "position": {
                                "start": {
                                    "line": 4850,
                                    "column": 29
                                },
                                "end": {
                                    "line": 4850,
                                    "column": 33
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/jax/jax/_src/lax/lax.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "\n",
                "  @staticmethod\n",
                "  def result_handler(sticky_device, aval):\n",
                "    def handler(_, buf):\n",
                "      buf.aval = core.ShapedArray(buf.shape, buf.dtype)\n",
                "      return core.DArray(aval, buf)\n",
                "    return handler\n",
                "\n",
                "core.bint._rules = BIntRules"
            ]
        ],
        "jax/_src/prng.py": [
            [
                "# Copyright 2021 The JAX Authors.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     https://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\n",
                "\n",
                "import abc\n",
                "from functools import partial, reduce\n",
                "import operator as op\n",
                "from typing import Any, Callable, Hashable, Iterator, NamedTuple, Sequence\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "import jax\n",
                "from jax import lax\n",
                "from jax import numpy as jnp\n",
                "from jax.config import config\n",
                "from jax.dtypes import float0\n",
                "from jax.interpreters import ad\n",
                "from jax.interpreters import batching\n",
                "from jax.interpreters import mlir\n",
                "from jax.interpreters import pxla\n",
                "from jax.interpreters import xla\n",
                "from jax._src import basearray\n",
                "from jax._src.sharding import (\n",
                "    NamedSharding, PmapSharding, OpShardingSharding)\n",
                "\n",
                "from jax._src import core\n",
                "from jax._src import dispatch\n",
                "from jax._src import dtypes\n",
                "from jax._src import pretty_printer as pp\n",
                "from jax._src.api import jit, vmap\n",
                "from jax._src.lax import lax as lax_internal\n",
                "from jax._src.lax import utils as lax_utils\n",
                "from jax._src.lib.mlir.dialects import hlo\n",
                "from jax._src.numpy import lax_numpy\n",
                "from jax._src.util import canonicalize_axis, prod, safe_map, safe_zip\n",
                "from jax._src.lib import gpu_prng, xla_extension_version\n",
                "\n",
                "map, unsafe_map = safe_map, map\n",
                "zip, unsafe_zip = safe_zip, zip\n",
                "\n",
                "\n",
                "UINT_DTYPES = {\n",
                "    8: jnp.uint8, 16: jnp.uint16, 32: jnp.uint32, 64: jnp.uint64}  # type: ignore[has-type]\n",
                "\n",
                "# -- PRNG implementation interface\n",
                "\n",
                "class PRNGImpl(NamedTuple):\n",
                "  \"\"\"Specifies PRNG key shape and operations.\n",
                "\n",
                "  A PRNG implementation is determined by a key type ``K`` and a\n",
                "  collection of functions that operate on such keys. The key type\n",
                "  ``K`` is an array type with element type uint32 and shape specified\n",
                "  by ``key_shape``. The type signature of each operations is::\n",
                "\n",
                "    seed :: int[] -> K\n",
                "    fold_in :: K -> int[] -> K\n",
                "    split[n] :: K -> K[n]\n",
                "    random_bits[shape, bit_width] :: K -> uint<bit_width>[shape]\n",
                "\n",
                "  A PRNG implementation is adapted to an array-like object of keys\n",
                "  ``K`` by the ``PRNGKeyArray`` class, which should be created via the\n",
                "  ``seed_with_impl`` function.\n",
                "  \"\"\"\n",
                "  key_shape: core.Shape\n",
                "  seed: Callable\n",
                "  split: Callable\n",
                "  random_bits: Callable\n",
                "  fold_in: Callable\n",
                "  tag: str = '?'\n",
                "\n",
                "  def __hash__(self) -> int:\n",
                "    return hash(self.tag)\n",
                "\n",
                "  def __str__(self) -> str:\n",
                "    return self.tag\n",
                "\n",
                "  def pprint(self):\n",
                "    return (pp.text(f\"{self.__class__.__name__} [{self.tag}]:\") +\n",
                "            pp.nest(2, pp.group(pp.brk() + pp.join(pp.brk(), [\n",
                "              pp.text(f\"{k} = {v}\") for k, v in self._asdict().items()\n",
                "            ]))))\n",
                "\n",
                "\n",
                "# -- PRNG key arrays\n",
                "\n",
                "def _check_prng_key_data(impl, key_data: jnp.ndarray):\n",
                "  ndim = len(impl.key_shape)\n",
                "  if not all(hasattr(key_data, attr) for attr in ['ndim', 'shape', 'dtype']):\n",
                "    raise TypeError(\"JAX encountered invalid PRNG key data: expected key_data \"\n",
                "                    f\"to have ndim, shape, and dtype attributes. Got {key_data}\")\n",
                "  if key_data.ndim < 1:\n",
                "    raise TypeError(\"JAX encountered invalid PRNG key data: expected \"\n",
                "                    f\"key_data.ndim >= 1; got ndim={key_data.ndim}\")\n",
                "  if key_data.shape[-ndim:] != impl.key_shape:\n",
                "    raise TypeError(\"JAX encountered invalid PRNG key data: expected key_data.shape to \"\n",
                "                    f\"end with {impl.key_shape}; got shape={key_data.shape} for {impl=}\")\n",
                "  if key_data.dtype not in [np.uint32, float0]:\n",
                "    raise TypeError(\"JAX encountered invalid PRNG key data: expected key_data.dtype = uint32; \"\n",
                "                    f\"got dtype={key_data.dtype}\")\n",
                "\n",
                "\n",
                "class PRNGKeyArrayMeta(abc.ABCMeta):\n",
                "  \"\"\"Metaclass for overriding PRNGKeyArray isinstance checks.\"\"\"\n",
                "\n",
                "  def __instancecheck__(self, instance):\n",
                "    try:\n",
                "      return (isinstance(instance.aval, core.ShapedArray) and\n",
                "              type(instance.aval.dtype) is KeyTy)\n",
                "    except AttributeError:\n",
                "      super().__instancecheck__(instance)\n",
                "\n",
                "\n",
                "class PRNGKeyArray(metaclass=PRNGKeyArrayMeta):\n",
                "  \"\"\"An array whose elements are PRNG keys.\n",
                "\n",
                "  This class lifts the definition of a PRNG, provided in the form of a\n",
                "  ``PRNGImpl``, into an array-like pytree class. Instances of this\n",
                "  class behave like an array whose base elements are keys, hiding the\n",
                "  fact that keys are typically arrays (of ``uint32`` dtype) themselves.\n",
                "\n",
                "  PRNGKeyArrays are also restricted relative to JAX arrays in that\n",
                "  they do not expose arithmetic operations. They instead expose\n",
                "  wrapper methods around the PRNG implementation functions (``split``,\n",
                "  ``random_bits``, ``fold_in``).\n",
                "  \"\"\"\n",
                "\n",
                "  impl: PRNGImpl\n",
                "  _base_array: jnp.ndarray\n",
                "\n",
                "  def __init__(self, impl, key_data: Any):\n",
                "    assert not isinstance(key_data, core.Tracer)\n",
                "    _check_prng_key_data(impl, key_data)\n",
                "    self.impl = impl\n",
                "    self._base_array = key_data\n",
                "\n",
                "  # TODO(frostig): rename to unsafe_base_array, or just offer base_array attr?\n",
                "  def unsafe_raw_array(self):\n",
                "    \"\"\"Access the raw numerical array that carries underlying key data.\n",
                "\n",
                "    Returns:\n",
                "      A uint32 JAX array whose leading dimensions are ``self.shape``.\n",
                "    \"\"\"\n",
                "    return self._base_array\n",
                "\n",
                "  def block_until_ready(self):\n",
                "    _ = self._base_array.block_until_ready()\n",
                "    return self\n",
                "\n",
                "  @property\n",
                "  def shape(self):\n",
                "    return base_arr_shape_to_keys_shape(self.impl, self._base_array.shape)\n",
                "\n",
                "  @property\n",
                "  def ndim(self):\n",
                "    return len(self.shape)\n",
                "\n",
                "  @property\n",
                "  def dtype(self):\n",
                "    return KeyTy(self.impl)\n",
                "\n",
                "  _device = property(op.attrgetter('_base_array._device'))\n",
                "  _committed = property(op.attrgetter('_base_array._committed'))\n",
                "  sharding = property(op.attrgetter('_base_array.sharding'))\n",
                "\n",
                "  def _is_scalar(self):\n",
                "    base_ndim = len(self.impl.key_shape)\n",
                "    return self._base_array.ndim == base_ndim\n",
                "\n",
                "  def __len__(self):\n",
                "    if self._is_scalar():\n",
                "      raise TypeError('len() of unsized object')\n",
                "    return len(self._base_array)\n",
                "\n",
                "  def __iter__(self) -> Iterator['PRNGKeyArray']:\n",
                "    if self._is_scalar():\n",
                "      raise TypeError('iteration over a 0-d key array')\n",
                "    # TODO(frostig): we may want to avoid iteration by slicing because\n",
                "    # a very common use of iteration is `k1, k2 = split(key)`, and\n",
                "    # slicing/indexing may be trickier to track for linearity checking\n",
                "    # purposes. Maybe we can:\n",
                "    # * introduce an unpack primitive+traceable (also allow direct use)\n",
                "    # * unpack upfront into shape[0] many keyarray slices\n",
                "    # * return iter over these unpacked slices\n",
                "    # Whatever we do, we'll want to do it by overriding\n",
                "    # ShapedArray._iter when the element type is KeyTy...\n",
                "    return (PRNGKeyArray(self.impl, k) for k in iter(self._base_array))\n",
                "\n",
                "  # TODO(frostig): are all of the stackable methods below (reshape,\n",
                "  # concat, broadcast_to, expand_dims), and the stackable registration,\n",
                "  # still needed? If, with some work, none are needed, then do we want\n",
                "  # to remove stackables altogether? This may be the only application.\n",
                "\n",
                "  # TODO(frostig): Remove? Overwritten below in particular\n",
                "  def reshape(self, newshape, order=None) -> 'PRNGKeyArray':\n",
                "    reshaped_base = jnp.reshape(self._base_array, (*newshape, -1), order=order)\n",
                "    return PRNGKeyArray(self.impl, reshaped_base)\n",
                "\n",
                "  def concatenate(self, key_arrs, axis, dtype=None):\n",
                "    if dtype is not None:\n",
                "      raise ValueError(\n",
                "          'dtype argument not supported for concatenating PRNGKeyArray')\n",
                "    axis = canonicalize_axis(axis, self.ndim)\n",
                "    arrs = [self._base_array, *[k._base_array for k in key_arrs]]\n",
                "    return PRNGKeyArray(self.impl, jnp.concatenate(arrs, axis))\n",
                "\n",
                "  def broadcast_to(self, shape):\n",
                "    if jnp.ndim(shape) == 0:\n",
                "      shape = (shape,)\n",
                "    new_shape = (*shape, *self.impl.key_shape)\n",
                "    return PRNGKeyArray(\n",
                "        self.impl, jnp.broadcast_to(self._base_array, new_shape))\n",
                "\n",
                "  def expand_dims(self, dimensions: Sequence[int]):\n",
                "    # follows lax.expand_dims, not jnp.expand_dims, so dimensions is a sequence\n",
                "    ndim_out = self.ndim + len(set(dimensions))\n",
                "    dimensions = [canonicalize_axis(d, ndim_out) for d in dimensions]\n",
                "    return PRNGKeyArray(\n",
                "        self.impl, lax.expand_dims(self._base_array, dimensions))\n",
                "\n",
                "  def __repr__(self):\n",
                "    return (f'{self.__class__.__name__}[{self.impl.tag}]'\n",
                "            f' {{ {self._base_array} }}')\n",
                "\n",
                "  def pprint(self):\n",
                "    pp_keys = pp.text('shape = ') + pp.text(str(self.shape))\n",
                "    pp_impl = pp.text('impl = ') + self.impl.pprint()\n",
                "    return str(pp.group(\n",
                "      pp.text('PRNGKeyArray:') +\n",
                "      pp.nest(2, pp.brk() + pp_keys + pp.brk() + pp_impl)))\n",
                "\n",
                "  # Hollow defs only for typing purposes, overwritten below\n",
                "  #\n",
                "  # TODO(frostig): there may be a better way to do this with\n",
                "  # `typing.type_check_only`.\n",
                "\n",
                "  @property\n",
                "  def T(self)                   -> 'PRNGKeyArray': assert False\n",
                "  def __getitem__(self, _)      -> 'PRNGKeyArray': assert False\n",
                "  def ravel(self, *_, **__)     -> 'PRNGKeyArray': assert False\n",
                "  def squeeze(self, *_, **__)   -> 'PRNGKeyArray': assert False\n",
                "  def swapaxes(self, *_, **__)  -> 'PRNGKeyArray': assert False\n",
                "  def take(self, *_, **__)      -> 'PRNGKeyArray': assert False\n",
                "  def transpose(self, *_, **__) -> 'PRNGKeyArray': assert False\n",
                "  def flatten(self, *_, **__)   -> 'PRNGKeyArray': assert False\n",
                "\n",
                "\n",
                "lax_numpy._set_device_array_base_attributes(PRNGKeyArray, include=[\n",
                "    '__getitem__', 'ravel', 'squeeze', 'swapaxes', 'take', 'reshape',\n",
                "    'transpose', 'flatten', 'T'])\n",
                "lax_numpy._register_stackable(PRNGKeyArray)\n",
                "basearray.Array.register(PRNGKeyArray)\n",
                "\n",
                "\n",
                "# TODO(frostig): remove, rerouting callers directly to random_seed\n",
                "def seed_with_impl(impl: PRNGImpl, seed: int) -> PRNGKeyArray:\n",
                "  return random_seed(seed, impl=impl)\n",
                "\n",
                "\n",
                "def keys_shaped_array(impl, shape):\n",
                "  return core.ShapedArray(shape, KeyTy(impl))\n",
                "\n",
                "def keys_aval_to_base_arr_aval(keys_aval):\n",
                "  shape = (*keys_aval.shape, *keys_aval.dtype.impl.key_shape)\n",
                "  return core.ShapedArray(shape, np.dtype('uint32'))\n",
                "\n",
                "def base_arr_shape_to_keys_shape(impl, base_arr_shape):\n",
                "  base_ndim = len(impl.key_shape)\n",
                "  return base_arr_shape[:-base_ndim]\n",
                "\n",
                "\n",
                "class KeyTyRules:\n",
                "\n",
                "  @staticmethod\n",
                "  def physical_avals(aval) -> Sequence[core.AbstractValue]:  # TODO(frostig): rename to `grounded_avals`\n",
                "    # TODO(frostig): dedup with `keys_aval_to_base_arr_aval``\n",
                "    return [core.ShapedArray((*aval.shape, *aval.dtype.impl.key_shape),  # type: ignore\n",
                "                             jnp.dtype('uint32'))]\n",
                "\n"
            ],
            {
                "type": "delete",
                "before": [
                    "  @staticmethod\n",
                    "  def aval_to_ir_types(aval: core.AbstractValue) -> Sequence[mlir.ir.Type]:\n",
                    "    phys_aval, = KeyTyRules.physical_avals(aval)\n",
                    "    return mlir.aval_to_ir_types(phys_aval)\n",
                    "\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 289,
                    "end": 294
                },
                "child_version_range": {
                    "start": 289,
                    "end": 289
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "KeyTyRules",
                        "signature": "class KeyTyRules:",
                        "at_line": 281
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: jax/_src/prng.py\nCode:\n           class KeyTyRules:\n               ...\n286 286        return [core.ShapedArray((*aval.shape, *aval.dtype.impl.key_shape),  # type: ignore\n287 287                                 jnp.dtype('uint32'))]\n288 288    \n289      -   @staticmethod\n290      -   def aval_to_ir_types(aval: core.AbstractValue) -> Sequence[mlir.ir.Type]:\n291      -     phys_aval, = KeyTyRules.physical_avals(aval)\n292      -     return mlir.aval_to_ir_types(phys_aval)\n293      - \n294 289      @staticmethod\n295 290      def physical_op_sharding(aval, sharding):\n296 291        op_sharding = sharding._to_xla_op_sharding(aval.ndim)\n         ...\n",
                "file_path": "jax/_src/prng.py",
                "identifiers_before": [
                    "AbstractValue",
                    "KeyTyRules",
                    "Sequence",
                    "Type",
                    "aval",
                    "aval_to_ir_types",
                    "core",
                    "ir",
                    "mlir",
                    "phys_aval",
                    "physical_avals",
                    "staticmethod"
                ],
                "identifiers_after": [],
                "prefix": [
                    "    return [core.ShapedArray((*aval.shape, *aval.dtype.impl.key_shape),  # type: ignore\n",
                    "                             jnp.dtype('uint32'))]\n",
                    "\n"
                ],
                "suffix": [
                    "  @staticmethod\n",
                    "  def physical_op_sharding(aval, sharding):\n",
                    "    op_sharding = sharding._to_xla_op_sharding(aval.ndim)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "  @staticmethod\n",
                "  def physical_op_sharding(aval, sharding):\n",
                "    op_sharding = sharding._to_xla_op_sharding(aval.ndim)\n",
                "    key_shape = aval.dtype.impl.key_shape\n",
                "\n",
                "    new_op_sharding = op_sharding.clone()\n",
                "    tad = list(new_op_sharding.tile_assignment_dimensions)\n",
                "    tad.extend([1] * len(key_shape))\n",
                "    new_op_sharding.tile_assignment_dimensions = tad\n",
                "    return new_op_sharding\n",
                "\n",
                "  @staticmethod\n",
                "  def result_handler(sticky_device, aval):\n",
                "    def handler(_, buf):\n",
                "      buf.aval = core.ShapedArray(buf.shape, buf.dtype)\n",
                "      return PRNGKeyArray(aval.dtype.impl, buf)\n",
                "    return handler\n",
                "\n",
                "  @staticmethod\n",
                "  def local_sharded_result_handler(aval, sharding, indices):\n",
                "    phys_aval, = KeyTyRules.physical_avals(aval)\n",
                "    key_shape = aval.dtype.impl.key_shape\n",
                "\n",
                "    # TODO(yashkatariya,frostig): remove this conditional and inline it when\n",
                "    # the transient config ever settles\n",
                "    if config.jax_array:\n",
                "      output_type = pxla.OutputType.Array\n",
                "    else:\n",
                "      output_type = pxla.OutputType.ShardedDeviceArray\n",
                "    phys_handler_maker = pxla.local_result_handlers[\n",
                "        (core.ShapedArray, output_type)]\n",
                "\n",
                "    # set up a grounded sharding (with a grounded sharding spec)\n",
                "    if isinstance(sharding, PmapSharding):\n",
                "      trailing_sharding = [pxla.NoSharding()] * len(key_shape)\n",
                "      phys_sharding_spec = pxla.ShardingSpec(\n",
                "          sharding=(*sharding.sharding_spec.sharding, *trailing_sharding),\n",
                "          mesh_mapping=sharding.sharding_spec.mesh_mapping)\n",
                "      phys_sharding = PmapSharding(devices=sharding.devices,\n",
                "                                   sharding_spec=phys_sharding_spec)\n",
                "    elif isinstance(sharding, NamedSharding):\n",
                "      trailing_spec = [None] * len(key_shape)\n",
                "      phys_sharding = NamedSharding(\n",
                "          sharding.mesh,\n",
                "          pxla.PartitionSpec(*sharding.spec, *trailing_spec))\n",
                "    else:\n",
                "      assert False, f'impossible sharding {sharding} in local sharded result handler'\n",
                "\n",
                "    # set up grounded indices\n",
                "    trailing_inds = [slice(None)] * len(key_shape)\n",
                "    phys_indices = [(*inds, *trailing_inds) for inds in indices]\n",
                "\n",
                "    # make a physical handler\n",
                "    phys_handler = phys_handler_maker(phys_aval, phys_sharding, phys_indices)\n",
                "\n",
                "    # set up a handler that calls the physical one and wraps back up\n",
                "    def handler(bufs):\n",
                "      return PRNGKeyArray(aval.dtype.impl, phys_handler(bufs))\n",
                "\n",
                "    return handler\n",
                "\n",
                "  @staticmethod\n",
                "  def global_sharded_result_handler(aval, out_sharding, committed,\n",
                "                                    is_out_sharding_from_xla):\n",
                "    phys_aval, = KeyTyRules.physical_avals(aval)\n",
                "    key_shape = aval.dtype.impl.key_shape\n",
                "\n",
                "    # TODO(yashkatariya,frostig): remove this conditional and inline it when\n",
                "    # the transient config ever settles\n",
                "    if config.jax_array:\n",
                "      output_type = pxla.OutputType.Array\n",
                "    else:\n",
                "      output_type = pxla.OutputType.GlobalDeviceArray\n",
                "\n",
                "    phys_handler_maker = pxla.global_result_handlers[\n",
                "        (core.ShapedArray, output_type)]\n",
                "\n",
                "    if dispatch.is_single_device_sharding(out_sharding):\n",
                "      phys_sharding = out_sharding\n",
                "    elif isinstance(out_sharding, NamedSharding):\n",
                "      trailing_spec = [None] * len(key_shape)\n",
                "      phys_sharding = NamedSharding(\n",
                "          out_sharding.mesh,\n",
                "          pxla.PartitionSpec(*out_sharding.spec, *trailing_spec))\n",
                "    else:\n",
                "      if is_out_sharding_from_xla:\n",
                "        phys_sharding = out_sharding\n",
                "      else:\n",
                "        phys_sharding = OpShardingSharding(\n",
                "            out_sharding._device_assignment,\n",
                "            KeyTyRules.physical_op_sharding(aval, out_sharding))\n",
                "\n",
                "    phys_handler = phys_handler_maker(phys_aval, phys_sharding, committed,\n",
                "                                      is_out_sharding_from_xla)\n",
                "    def handler(bufs):\n",
                "      return PRNGKeyArray(aval.dtype.impl, phys_handler(bufs))\n",
                "    return handler\n",
                "\n",
                "  # element-type-polymorphic primitive lowering rules\n",
                "\n",
                "  @staticmethod\n",
                "  def empty_mlir(ctx, aval_out) -> Sequence[mlir.ir.Value]:\n",
                "    return mlir.ir_constants(np.zeros(aval_out.dtype.impl.key_shape,\n",
                "                                      dtype=np.dtype('uint32')))\n",
                "\n",
                "  @staticmethod\n",
                "  def slice_mlir(ctx, aval_out, x, start_indices, limit_indices, strides) -> mlir.ir.Value:\n",
                "    key_shape = aval_out.dtype.impl.key_shape\n",
                "    trailing_zeros = [0] * len(key_shape)\n",
                "    trailing_ones  = [1] * len(key_shape)\n",
                "    start_indices = (*start_indices, *trailing_zeros)\n",
                "    limit_indices = (*limit_indices, *key_shape)\n",
                "    strides = (*strides, *trailing_ones)\n",
                "    physical_aval_out, = KeyTyRules.physical_avals(aval_out)\n",
                "    return mlir.slice_op(ctx, x, physical_aval_out,\n",
                "                         start_indices=start_indices, limit_indices=limit_indices, strides=strides)\n",
                "\n",
                "  @staticmethod\n",
                "  def dynamic_slice_mlir(ctx, aval_out, x, start_indices) -> mlir.ir.Value:\n",
                "    dtype = dtypes.canonicalize_dtype(np.dtype('int64'))\n",
                "    key_shape = aval_out.dtype.impl.key_shape\n",
                "    trailing_zeros = [mlir.ir_constant(np.array(0, dtype))] * len(key_shape)\n",
                "    start_indices = (*start_indices, *trailing_zeros)\n",
                "    physical_aval_out, = KeyTyRules.physical_avals(aval_out)\n",
                "    return mlir.dynamic_slice(ctx, physical_aval_out, x,\n",
                "                              start_indices=start_indices)\n",
                "\n",
                "  @staticmethod\n",
                "  def dynamic_update_slice_mlir(ctx, aval_out, x, update, *start_indices) -> mlir.ir.Value:\n",
                "    dtype = dtypes.canonicalize_dtype(np.dtype('int64'))\n",
                "    key_shape = aval_out.dtype.impl.key_shape\n",
                "    zeros = [mlir.ir_constant(np.array(0, dtype=dtype))] * len(key_shape)\n",
                "    start_indices = (*start_indices, *zeros)\n",
                "    physical_aval_out, = KeyTyRules.physical_avals(aval_out)\n",
                "    return mlir.dynamic_update_slice(ctx, physical_aval_out, x, update,\n",
                "                                     start_indices=start_indices)\n",
                "\n",
                "  @staticmethod\n",
                "  def broadcast_in_dim_mlir(ctx, aval_out, x,\n",
                "                            broadcast_dimensions) -> mlir.ir.Value:\n",
                "    key_shape = aval_out.dtype.impl.key_shape\n",
                "    trailing_dims = [aval_out.ndim + i for i in range(len(key_shape))]\n",
                "    broadcast_dimensions = [*broadcast_dimensions, *trailing_dims]\n",
                "    physical_aval_out, = KeyTyRules.physical_avals(aval_out)\n",
                "    return mlir.broadcast_in_dim(ctx, x, physical_aval_out, broadcast_dimensions=broadcast_dimensions)\n",
                "\n",
                "  @staticmethod\n",
                "  def transpose_mlir(ctx, aval_out, x, *, permutation) -> mlir.ir.Value:\n",
                "    key_shape = aval_out.dtype.impl.key_shape\n",
                "    trailing_dims = [aval_out.ndim + i for i in range(len(key_shape))]\n",
                "    perm = [*permutation, *trailing_dims]\n",
                "    return hlo.TransposeOp(x, mlir.dense_int_elements(perm)).result\n",
                "\n",
                "  @staticmethod\n",
                "  def gather_mlir(ctx, avals_in, aval_out, x, indices, *,\n",
                "                  dimension_numbers, slice_sizes, unique_indices,\n",
                "                  indices_are_sorted, mode, fill_value) -> mlir.ir.Value:\n",
                "    aval_x, aval_indices = avals_in\n",
                "    aval_y = aval_out\n",
                "    key_shape = aval_x.dtype.impl.key_shape\n",
                "    trailing_offset_dims = [aval_y.ndim + i for i in range(len(key_shape))]\n",
                "    dimension_numbers = dimension_numbers._replace(\n",
                "        offset_dims=(*dimension_numbers.offset_dims, *trailing_offset_dims))\n",
                "    slice_sizes = (*slice_sizes, *key_shape)\n",
                "    gather_lower = partial(\n",
                "        lax_internal.slicing._gather_lower, dimension_numbers=dimension_numbers,\n",
                "        slice_sizes=slice_sizes, unique_indices=unique_indices,\n",
                "        indices_are_sorted=indices_are_sorted, mode=mode, fill_value=fill_value)\n",
                "    res, = mlir.delegate_lowering(\n",
                "        ctx, gather_lower, x, indices,\n",
                "        avals_in=[keys_aval_to_base_arr_aval(aval_x), aval_indices],\n",
                "        avals_out=[keys_aval_to_base_arr_aval(aval_y)])\n",
                "    return res\n",
                "\n",
                "\n",
                "class KeyTy:\n",
                "  impl: Hashable  # prng.PRNGImpl. TODO(mattjj,frostig): protocol really\n",
                "  _rules = KeyTyRules\n",
                "\n",
                "  def __init__(self, impl):\n",
                "    self.impl = impl\n",
                "\n",
                "  @property\n",
                "  def name(self) -> str:\n",
                "    return f'key<{self.impl.tag}>'\n",
                "\n",
                "  def __repr__(self) -> str:\n",
                "    return self.name\n",
                "\n",
                "  def __eq__(self, other):\n",
                "    return type(other) is KeyTy and self.impl == other.impl\n",
                "\n",
                "  def __hash__(self) -> int:\n",
                "    return hash((self.__class__, self.impl))\n",
                "\n",
                "\n",
                "core.opaque_dtypes.add(KeyTy)\n",
                "\n",
                "\n",
                "core.pytype_aval_mappings[PRNGKeyArray] = (\n",
                "    lambda x: keys_shaped_array(x.impl, x.shape))\n",
                "\n",
                "xla.pytype_aval_mappings[PRNGKeyArray] = (\n",
                "    lambda x: keys_shaped_array(x.impl, x.shape))\n",
                "\n",
                "xla.canonicalize_dtype_handlers[PRNGKeyArray] = lambda x: x\n",
                "\n",
                "def device_put_key_array(x: PRNGKeyArray, device):\n",
                "  return dispatch.device_put(x.unsafe_raw_array(), device)\n",
                "dispatch.device_put_handlers[PRNGKeyArray] = device_put_key_array\n",
                "\n",
                "def key_array_shard_arg_handler(x: PRNGKeyArray, devices, indices):\n",
                "  # TODO(frostig): Remove the need for `core.get_aval`.\n",
                "  key_shape = core.get_aval(x).dtype.impl.key_shape\n",
                "  arr = x.unsafe_raw_array()\n",
                "\n",
                "  # TODO(yashkatariya,frostig): This assumes that the last dimensions are not\n",
                "  # sharded. This is only true when enable_custom_prng is True.\n",
                "  trailing_inds = [slice(None)] * len(key_shape)\n",
                "  phys_indices = [(*inds, *trailing_inds) for inds in indices]\n",
                "  return pxla.shard_arg_handlers[type(arr)](arr, devices, phys_indices)\n",
                "pxla.shard_arg_handlers[PRNGKeyArray] = key_array_shard_arg_handler\n",
                "\n",
                "\n",
                "def key_array_constant_handler(x, canonicalize_dtypes):\n",
                "  arr = x.unsafe_raw_array()\n",
                "  return mlir.get_constant_handler(type(arr))(arr, canonicalize_dtypes)\n",
                "mlir.register_constant_handler(PRNGKeyArray, key_array_constant_handler)\n",
                "\n",
                "\n",
                "# -- primitives\n",
                "\n",
                "def iterated_vmap_unary(n, f):\n",
                "  for _ in range(n):\n",
                "    f = jax.vmap(f)\n",
                "  return f\n",
                "\n",
                "# TODO(frostig): Revise the following two functions? These basically\n",
                "# undo the singleton dimensions added by `batching.defbroadcasting`.\n",
                "# It works, but introduces some possibly-redundant squeezes. Can we\n",
                "# borrow from other broadcasting primitives instead?\n",
                "\n",
                "def squeeze_vmap(f, left):\n",
                "  def squeeze_vmap_f(x, y):\n",
                "    if left:\n",
                "      x = jnp.squeeze(x, axis=0)\n",
                "      axes = (None, 0)\n",
                "    else:\n",
                "      y = jnp.squeeze(y, axis=0)\n",
                "      axes = (0, None)\n",
                "    return jax.vmap(f, in_axes=axes, out_axes=0)(x, y)\n",
                "  return squeeze_vmap_f\n",
                "\n",
                "def iterated_vmap_binary_bcast(shape1, shape2, f):\n",
                "  ndim1, ndim2 = len(shape1), len(shape2)\n",
                "  if ndim1 == ndim2 == 0:\n",
                "    return f\n",
                "  if 0 in [ndim1, ndim2]:\n",
                "    if ndim1 == 0:\n",
                "      return lambda x, y: iterated_vmap_unary(ndim2, lambda y: f(x, y))(y)\n",
                "    else:\n",
                "      return lambda x, y: iterated_vmap_unary(ndim1, lambda x: f(x, y))(x)\n",
                "  assert len(shape1) == len(shape2)\n",
                "  for sz1, sz2 in reversed(zip(shape1, shape2)):\n",
                "    if sz1 == sz2:\n",
                "      f = jax.vmap(f, out_axes=0)\n",
                "    else:\n",
                "      assert sz1 == 1 or sz2 == 1, (sz1, sz2)\n",
                "      f = squeeze_vmap(f, sz1 == 1)\n",
                "  return f\n",
                "\n",
                "\n",
                "def random_seed(seeds, impl):\n",
                "  # Avoid overflow error in X32 mode by first converting ints to int64.\n",
                "  # This breaks JIT invariance for large ints, but supports the common\n",
                "  # use-case of instantiating with Python hashes in X32 mode.\n",
                "  if isinstance(seeds, int):\n",
                "    seeds_arr = jnp.asarray(np.int64(seeds))\n",
                "  else:\n",
                "    seeds_arr = jnp.asarray(seeds)\n",
                "  return random_seed_p.bind(seeds_arr, impl=impl)\n",
                "\n",
                "random_seed_p = core.Primitive('random_seed')\n",
                "ad.defjvp_zero(random_seed_p)\n",
                "batching.defvectorized(random_seed_p)\n",
                "\n",
                "@random_seed_p.def_abstract_eval\n",
                "def random_seed_abstract_eval(seeds_aval, *, impl):\n",
                "  return keys_shaped_array(impl, seeds_aval.shape)\n",
                "\n",
                "@random_seed_p.def_impl\n",
                "def random_seed_impl(seeds, *, impl):\n",
                "  base_arr = random_seed_impl_base(seeds, impl=impl)\n",
                "  return PRNGKeyArray(impl, base_arr)\n",
                "\n",
                "def random_seed_impl_base(seeds, *, impl):\n",
                "  seed = iterated_vmap_unary(seeds.ndim, impl.seed)\n",
                "  return seed(seeds)\n",
                "\n",
                "def random_seed_lowering(ctx, seeds, *, impl):\n",
                "  aval, = ctx.avals_in\n",
                "  seed = iterated_vmap_unary(aval.ndim, impl.seed)\n",
                "  seed_lowering = mlir.lower_fun(seed, multiple_results=False)\n",
                "  return mlir.delegate_lowering(\n",
                "      ctx, seed_lowering, seeds,\n",
                "      avals_out=map(keys_aval_to_base_arr_aval, ctx.avals_out))\n",
                "\n",
                "mlir.register_lowering(random_seed_p, random_seed_lowering)\n",
                "\n",
                "\n",
                "def random_split(keys, count):\n",
                "  return random_split_p.bind(keys, count=count)\n",
                "\n",
                "random_split_p = core.Primitive('random_split')\n",
                "ad.defjvp_zero(random_split_p)\n",
                "batching.defvectorized(random_split_p)\n",
                "\n",
                "@random_split_p.def_abstract_eval\n",
                "def random_split_abstract_eval(keys_aval, *, count):\n",
                "  return keys_shaped_array(keys_aval.dtype.impl, (*keys_aval.shape, count))\n",
                "\n",
                "@random_split_p.def_impl\n",
                "def random_split_impl(keys, *, count):\n",
                "  base_arr = random_split_impl_base(\n",
                "      keys.impl, keys.unsafe_raw_array(), keys.ndim, count=count)\n",
                "  return PRNGKeyArray(keys.impl, base_arr)\n",
                "\n",
                "def random_split_impl_base(impl, base_arr, keys_ndim, *, count):\n",
                "  split = iterated_vmap_unary(keys_ndim, lambda k: impl.split(k, count))\n",
                "  return split(base_arr)\n",
                "\n",
                "def random_split_lowering(ctx, keys, *, count):\n",
                "  aval, = ctx.avals_in\n",
                "  impl = aval.dtype.impl\n",
                "  split = iterated_vmap_unary(aval.ndim, lambda k: impl.split(k, count))\n",
                "  split_lowering = mlir.lower_fun(split, multiple_results=False)\n",
                "  return mlir.delegate_lowering(\n",
                "      ctx, split_lowering, keys,\n",
                "      avals_in=[keys_aval_to_base_arr_aval(aval)],\n",
                "      avals_out=map(keys_aval_to_base_arr_aval, ctx.avals_out))\n",
                "\n",
                "mlir.register_lowering(random_split_p, random_split_lowering)\n",
                "\n",
                "\n",
                "def random_fold_in(keys, msgs):\n",
                "  return random_fold_in_p.bind(keys, jnp.asarray(msgs))\n",
                "\n",
                "random_fold_in_p = core.Primitive('random_fold_in')\n",
                "ad.defjvp_zero(random_fold_in_p)\n",
                "batching.defbroadcasting(random_fold_in_p)\n",
                "\n",
                "@random_fold_in_p.def_abstract_eval\n",
                "def random_fold_in_abstract_eval(keys_aval, msgs_aval):\n",
                "  shape = lax_internal.broadcasting_shape_rule(\n",
                "      'random_fold_in', keys_aval, msgs_aval)\n",
                "  named_shape = lax_utils.standard_named_shape_rule(keys_aval, msgs_aval)\n",
                "  return core.ShapedArray(shape, keys_aval.dtype, named_shape=named_shape)\n",
                "\n",
                "@random_fold_in_p.def_impl\n",
                "def random_fold_in_impl(keys, msgs):\n",
                "  base_arr = random_fold_in_impl_base(\n",
                "      keys.impl, keys.unsafe_raw_array(), msgs, keys.shape)\n",
                "  return PRNGKeyArray(keys.impl, base_arr)\n",
                "\n",
                "def random_fold_in_impl_base(impl, base_arr, msgs, keys_shape):\n",
                "  fold_in = iterated_vmap_binary_bcast(\n",
                "      keys_shape, np.shape(msgs), impl.fold_in)\n",
                "  return fold_in(base_arr, msgs)\n",
                "\n",
                "def random_fold_in_lowering(ctx, keys, msgs):\n",
                "  keys_aval, msgs_aval = ctx.avals_in\n",
                "  impl = keys_aval.dtype.impl\n",
                "  fold_in = iterated_vmap_binary_bcast(\n",
                "      keys_aval.shape, msgs_aval.shape, impl.fold_in)\n",
                "  fold_in_lowering = mlir.lower_fun(fold_in, multiple_results=False)\n",
                "  return mlir.delegate_lowering(\n",
                "      ctx, fold_in_lowering, keys, msgs,\n",
                "      avals_in=[keys_aval_to_base_arr_aval(keys_aval), msgs_aval],\n",
                "      avals_out=map(keys_aval_to_base_arr_aval, ctx.avals_out))\n",
                "\n",
                "mlir.register_lowering(random_fold_in_p, random_fold_in_lowering)\n",
                "\n",
                "\n",
                "def random_bits(keys, bit_width, shape):\n",
                "  shape = core.as_named_shape(shape)\n",
                "  for name, size in shape.named_items:\n",
                "    # TODO(frostig,mattjj,apaszke): Is this real_size check necessary,\n",
                "    # and is it meant to raise a user-facing ValueError? Should it be\n",
                "    # an `assert` (or RuntimeError) instead? Why do we check it in\n",
                "    # calls to `random_bits` instead of a more common paralleism path?\n",
                "    real_size = lax.psum(1, name)\n",
                "    if real_size != size:\n",
                "      raise ValueError(f\"The shape of axis {name} was specified as {size}, \"\n",
                "                       f\"but it really is {real_size}\")\n",
                "    axis_index = lax.axis_index(name)\n",
                "    keys = random_fold_in(keys, axis_index)\n",
                "  return random_bits_p.bind(keys, bit_width=bit_width, shape=shape.positional)\n",
                "\n",
                "random_bits_p = core.Primitive('random_bits')\n",
                "ad.defjvp_zero(random_bits_p)\n",
                "batching.defvectorized(random_bits_p)\n",
                "\n",
                "@random_bits_p.def_abstract_eval\n",
                "def random_bits_abstract_eval(keys_aval, *, bit_width, shape):\n",
                "  out_shape = (*keys_aval.shape, *shape)\n",
                "  out_dtype = dtypes.dtype(f'uint{bit_width}')\n",
                "  return core.ShapedArray(out_shape, out_dtype)\n",
                "\n",
                "@random_bits_p.def_impl\n",
                "def random_bits_impl(keys, *, bit_width, shape):\n",
                "  return random_bits_impl_base(keys.impl, keys.unsafe_raw_array(), keys.ndim,\n",
                "                               bit_width=bit_width, shape=shape)\n",
                "\n",
                "def random_bits_impl_base(impl, base_arr, keys_ndim, *, bit_width, shape):\n",
                "  bits = iterated_vmap_unary(\n",
                "      keys_ndim, lambda k: impl.random_bits(k, bit_width, shape))\n",
                "  return bits(base_arr)\n",
                "\n",
                "def random_bits_lowering(ctx, keys, *, bit_width, shape):\n",
                "  aval, = ctx.avals_in\n",
                "  impl = aval.dtype.impl\n",
                "  bits = iterated_vmap_unary(\n",
                "      aval.ndim, lambda k: impl.random_bits(k, bit_width, shape))\n",
                "  bits_lowering = mlir.lower_fun(bits, multiple_results=False)\n",
                "  ctx_new = ctx.replace(avals_in=[keys_aval_to_base_arr_aval(aval)])\n",
                "  out = bits_lowering(ctx_new, keys)\n",
                "  ctx.set_tokens_out(ctx_new.tokens_out)\n",
                "  return out\n",
                "\n",
                "mlir.register_lowering(random_bits_p, random_bits_lowering)\n",
                "\n",
                "\n",
                "# The following wrap/unwrap primitives are at least a stopgap for\n",
                "# backwards compatibility, namely when `config.jax_enable_custom_prng`\n",
                "# is False. We need to convert key arrays to and from underlying\n",
                "# uint32 base array, and we may need to do so under a jit. For\n",
                "# example, we want to support:\n",
                "#\n",
                "#   keys = jax.jit(random.split)(key)\n",
                "#\n",
                "# where `key` and `keys` are both acceptably old-style uint32 arrays\n",
                "# so long as enable_custom_prng is False. The way we handle this is\n",
                "# that `random.split` adapts the input/output by converting to/from\n",
                "# key arrays across its call to `random_split`. So we rely on these\n",
                "# wrap/unwrap casting primitives to allow that conversion under jit.\n",
                "#\n",
                "# We may want to keep both around for testing and debugging escape\n",
                "# hatches. We can rename them `unsafe` for emphasis, and/or issue a\n",
                "# warning on entry to the traceable.\n",
                "#\n",
                "# TODO(frostig): Consider removal once we always enable_custom_prng.\n",
                "\n",
                "def random_wrap(base_arr, *, impl):\n",
                "  _check_prng_key_data(impl, base_arr)\n",
                "  return random_wrap_p.bind(base_arr, impl=impl)\n",
                "\n",
                "random_wrap_p = core.Primitive('random_wrap')\n",
                "ad.defjvp_zero(random_wrap_p)\n",
                "\n",
                "@random_wrap_p.def_abstract_eval\n",
                "def random_wrap_abstract_eval(base_arr_aval, *, impl):\n",
                "  shape = base_arr_shape_to_keys_shape(impl, base_arr_aval.shape)\n",
                "  return keys_shaped_array(impl, shape)\n",
                "\n",
                "@random_wrap_p.def_impl\n",
                "def random_wrap_impl(base_arr, *, impl):\n",
                "  return PRNGKeyArray(impl, base_arr)\n",
                "\n",
                "def random_wrap_lowering(ctx, base_arr, *, impl):\n",
                "  return [base_arr]\n",
                "\n",
                "def random_wrap_batch_rule(batched_args, batch_dims, *, impl):\n",
                "  x, = batched_args\n",
                "  d, = batch_dims\n",
                "  x = batching.bdim_at_front(x, d, 1)\n",
                "  return random_wrap(x, impl=impl), 0\n",
                "\n",
                "mlir.register_lowering(random_wrap_p, random_wrap_lowering)\n",
                "batching.primitive_batchers[random_wrap_p] = random_wrap_batch_rule\n",
                "\n",
                "\n",
                "def random_unwrap(keys):\n",
                "  if not isinstance(keys, PRNGKeyArray):\n",
                "    raise TypeError(f'random_unwrap takes key array operand, got {type(keys)}')\n",
                "  return random_unwrap_p.bind(keys)\n",
                "\n",
                "random_unwrap_p = core.Primitive('random_unwrap')\n",
                "ad.defjvp_zero(random_unwrap_p)\n",
                "batching.defvectorized(random_unwrap_p)\n",
                "\n",
                "@random_unwrap_p.def_abstract_eval\n",
                "def random_unwrap_abstract_eval(keys_aval):\n",
                "  return keys_aval_to_base_arr_aval(keys_aval)\n",
                "\n",
                "@random_unwrap_p.def_impl\n",
                "def random_unwrap_impl(keys):\n",
                "  return keys.unsafe_raw_array()\n",
                "\n",
                "def random_unwrap_lowering(ctx, keys):\n",
                "  return [keys]\n",
                "\n",
                "mlir.register_lowering(random_unwrap_p, random_unwrap_lowering)\n",
                "\n",
                "\n",
                "# -- threefry2x32 PRNG implementation\n",
                "\n",
                "\n",
                "def _is_threefry_prng_key(key: jnp.ndarray) -> bool:\n",
                "  try:\n",
                "    return key.shape == (2,) and key.dtype == np.uint32\n",
                "  except AttributeError:\n",
                "    return False\n",
                "\n",
                "\n",
                "def threefry_seed(seed: jnp.ndarray) -> jnp.ndarray:\n",
                "  \"\"\"Create a single raw threefry PRNG key from an integer seed.\n",
                "\n",
                "  Args:\n",
                "    seed: a 64- or 32-bit integer used as the value of the key.\n",
                "\n",
                "  Returns:\n",
                "    The PRNG key contents, modeled as an array of shape (2,) and dtype\n",
                "    uint32. The key is constructed from a 64-bit seed by effectively\n",
                "    bit-casting to a pair of uint32 values (or from a 32-bit seed by\n",
                "    first padding out with zeros).\n",
                "  \"\"\"\n",
                "  if seed.shape:\n",
                "    raise TypeError(f\"PRNG key seed must be a scalar; got {seed!r}.\")\n",
                "  if not np.issubdtype(seed.dtype, np.integer):\n",
                "    raise TypeError(f\"PRNG key seed must be an integer; got {seed!r}\")\n",
                "  convert = lambda k: lax.reshape(lax.convert_element_type(k, np.uint32), [1])\n",
                "  k1 = convert(\n",
                "      lax.shift_right_logical(seed, lax_internal._const(seed, 32)))\n",
                "  with jax.numpy_dtype_promotion('standard'):\n",
                "    # TODO(jakevdp): in X64 mode, this can generate 64-bit computations for 32-bit\n",
                "    # inputs. We should avoid this.\n",
                "    k2 = convert(jnp.bitwise_and(seed, np.uint32(0xFFFFFFFF)))\n",
                "  return lax.concatenate([k1, k2], 0)\n",
                "\n",
                "\n",
                "def _make_rotate_left(dtype):\n",
                "  if not jnp.issubdtype(dtype, np.integer):\n",
                "    raise TypeError(\"_rotate_left only accepts integer dtypes.\")\n",
                "  nbits = np.array(jnp.iinfo(dtype).bits, dtype)\n",
                "\n",
                "  def _rotate_left(x, d):\n",
                "    if lax.dtype(d) != dtype:\n",
                "      d = lax.convert_element_type(d, dtype)\n",
                "    if lax.dtype(x) != dtype:\n",
                "      x = lax.convert_element_type(x, dtype)\n",
                "    return lax.shift_left(x, d) | lax.shift_right_logical(x, nbits - d)\n",
                "  return _rotate_left\n",
                "\n",
                "\n",
                "def _bit_stats(bits):\n",
                "  \"\"\"This is a debugging function to compute the statistics of bit fields.\"\"\"\n",
                "  return np.array([list(map(int, np.binary_repr(x, 64))) for x in bits]).mean(0)\n",
                "\n",
                "\n",
                "### hash function and split\n",
                "\n",
                "def _threefry2x32_abstract_eval(*args):\n",
                "  if any(a.dtype != jnp.uint32 for a in args):\n",
                "    raise TypeError(\"Arguments to threefry2x32 must have uint32 type, got {}\"\n",
                "                    .format(args))\n",
                "  if all(isinstance(arg, core.ShapedArray) for arg in args):\n",
                "    shape = lax_internal.broadcasting_shape_rule(*args)\n",
                "    named_shape = core.join_named_shapes(*(a.named_shape for a in args))\n",
                "    aval = core.ShapedArray(shape, jnp.dtype(jnp.uint32), named_shape=named_shape)\n",
                "  else:\n",
                "    aval = core.UnshapedArray(jnp.dtype(jnp.uint32))\n",
                "  return (aval,) * 2\n",
                "\n",
                "\n",
                "rotate_left = _make_rotate_left(np.uint32)\n",
                "\n",
                "\n",
                "def apply_round(v, rot):\n",
                "  v = v[:]\n",
                "  v[0] = v[0] + v[1]\n",
                "  v[1] = rotate_left(v[1], rot)\n",
                "  v[1] = v[0] ^ v[1]\n",
                "  return v\n",
                "\n",
                "\n",
                "def rotate_list(xs):\n",
                "  return xs[1:] + xs[:1]\n",
                "\n",
                "\n",
                "def rolled_loop_step(i, state):\n",
                "  x, ks, rotations = state\n",
                "  for r in rotations[0]:\n",
                "    x = apply_round(x, r)\n",
                "  new_x = [x[0] + ks[0], x[1] + ks[1] + jnp.asarray(i + 1, dtype=np.uint32)]\n",
                "  return new_x, rotate_list(ks), rotate_list(rotations)\n",
                "\n",
                "\n",
                "def _threefry2x32_lowering(key1, key2, x1, x2, use_rolled_loops=True):\n",
                "  \"\"\"Apply the Threefry 2x32 hash.\n",
                "\n",
                "  Args:\n",
                "    keypair: a pair of 32bit unsigned integers used for the key.\n",
                "    count: an array of dtype uint32 used for the counts.\n",
                "\n",
                "  Returns:\n",
                "    An array of dtype uint32 with the same shape as `count`.\n",
                "  \"\"\"\n",
                "  x = [x1, x2]\n",
                "\n",
                "  rotations = [np.array([13, 15, 26, 6], dtype=np.uint32),\n",
                "               np.array([17, 29, 16, 24], dtype=np.uint32)]\n",
                "  ks = [key1, key2, key1 ^ key2 ^ np.uint32(0x1BD11BDA)]\n",
                "\n",
                "  x[0] = x[0] + ks[0]\n",
                "  x[1] = x[1] + ks[1]\n",
                "\n",
                "  if use_rolled_loops:\n",
                "    x, _, _ = lax.fori_loop(0, 5, rolled_loop_step, (x, rotate_list(ks), rotations))\n",
                "\n",
                "  else:\n",
                "    for r in rotations[0]:\n",
                "      x = apply_round(x, r)\n",
                "    x[0] = x[0] + ks[1]\n",
                "    x[1] = x[1] + ks[2] + np.uint32(1)\n",
                "\n",
                "    for r in rotations[1]:\n",
                "      x = apply_round(x, r)\n",
                "    x[0] = x[0] + ks[2]\n",
                "    x[1] = x[1] + ks[0] + np.uint32(2)\n",
                "\n",
                "    for r in rotations[0]:\n",
                "      x = apply_round(x, r)\n",
                "    x[0] = x[0] + ks[0]\n",
                "    x[1] = x[1] + ks[1] + np.uint32(3)\n",
                "\n",
                "    for r in rotations[1]:\n",
                "      x = apply_round(x, r)\n",
                "    x[0] = x[0] + ks[1]\n",
                "    x[1] = x[1] + ks[2] + np.uint32(4)\n",
                "\n",
                "    for r in rotations[0]:\n",
                "      x = apply_round(x, r)\n",
                "    x[0] = x[0] + ks[2]\n",
                "    x[1] = x[1] + ks[0] + np.uint32(5)\n",
                "\n",
                "  return tuple(x)\n",
                "\n",
                "\n",
                "def _threefry2x32_gpu_lowering(lowering_func, ctx, k1, k2, x1, x2):\n",
                "  aval_out, _ = ctx.avals_out\n",
                "  k1_aval, k2_aval, x1_aval, x2_aval = ctx.avals_in\n",
                "  rank = len(aval_out.shape)\n",
                "  if 0 in aval_out.shape:\n",
                "    zeros = mlir.full_like_aval(ctx, 0, aval_out)\n",
                "    return [zeros, zeros]\n",
                "  def _broadcast(x, aval):\n",
                "    return mlir.broadcast_in_dim(ctx, x, aval_out,\n",
                "                                 broadcast_dimensions=range(rank - len(aval.shape), rank))\n",
                "\n",
                "  if xla_extension_version >= 113:\n",
                "    out_len = reduce(op.mul, aval_out.shape, 1)\n",
                "    if not core.is_constant_dim(out_len):\n",
                "      length = mlir.shape_tensor(mlir.eval_dynamic_shape(ctx, [out_len]))\n",
                "      length = mlir.hlo.ConvertOp(\n",
                "          mlir.ir.RankedTensorType.get((1,), mlir.ir.IntegerType.get_signless(64)),\n",
                "          length).result\n",
                "    else:\n",
                "      length = int(out_len)  # will be passed statically\n",
                "\n",
                "    return lowering_func(\n",
                "            (_broadcast(k1, k1_aval), _broadcast(k2, k2_aval)),\n",
                "            (_broadcast(x1, x1_aval), _broadcast(x2, x2_aval)), length)\n",
                "  else:\n",
                "    return lowering_func(\n",
                "            (_broadcast(k1, k1_aval), _broadcast(k2, k2_aval)),\n",
                "            (_broadcast(x1, x1_aval), _broadcast(x2, x2_aval)))\n",
                "\n",
                "threefry2x32_p = core.Primitive(\"threefry2x32\")\n",
                "threefry2x32_p.multiple_results = True\n",
                "threefry2x32_p.def_impl(partial(xla.apply_primitive, threefry2x32_p))\n",
                "threefry2x32_p.def_abstract_eval(_threefry2x32_abstract_eval)\n",
                "batching.defbroadcasting(threefry2x32_p)\n",
                "mlir.register_lowering(threefry2x32_p, mlir.lower_fun(\n",
                "    partial(_threefry2x32_lowering, use_rolled_loops=False),\n",
                "    multiple_results=True))\n",
                "mlir.register_lowering(threefry2x32_p, mlir.lower_fun(\n",
                "    partial(_threefry2x32_lowering, use_rolled_loops=True),\n",
                "    multiple_results=True), platform='cpu')\n",
                "mlir.register_lowering(\n",
                "    threefry2x32_p,\n",
                "    partial(_threefry2x32_gpu_lowering, gpu_prng.cuda_threefry2x32),\n",
                "    platform='cuda')\n",
                "mlir.register_lowering(\n",
                "    threefry2x32_p,\n",
                "    partial(_threefry2x32_gpu_lowering, gpu_prng.rocm_threefry2x32),\n",
                "    platform='rocm')\n",
                "\n",
                "\n",
                "def iota_2x32_shape(shape):\n",
                "  \"\"\"Reshaped ``uint64`` iota, as two parallel ``uint32`` arrays.\n",
                "\n",
                "  Setting aside representation, this function essentially computes the\n",
                "  equivalent of::\n",
                "\n",
                "    jax.lax.iota(dtype=np.uint64, size=np.prod(shape)).reshape(shape)\n",
                "\n",
                "  However:\n",
                "\n",
                "  * It returns two parallel ``uint32`` arrays instead of one\n",
                "    ``uint64`` array. This renders it invariant under either setting of\n",
                "    the system-wide ``jax_enable_x64`` configuration flag.\n",
                "\n",
                "  * It lowers in a way such that the compiler's automatic SPMD\n",
                "    partitioner recognizes its partitionability.\n",
                "\n",
                "  For example::\n",
                "\n",
                "    >>> import numpy as np\n",
                "    >>> from jax import lax\n",
                "    >>> from jax._src import prng\n",
                "\n",
                "    >>> prng.iota_2x32_shape((3, 4))\n",
                "    [Array([[0, 0, 0, 0],\n",
                "            [0, 0, 0, 0],\n",
                "            [0, 0, 0, 0]], dtype=uint32),\n",
                "     Array([[ 0,  1,  2,  3],\n",
                "            [ 4,  5,  6,  7],\n",
                "            [ 8,  9, 10, 11]], dtype=uint32)]\n",
                "\n",
                "    >>> def reshaped_iota(shape):\n",
                "    ...   return lax.iota(size=np.prod(shape), dtype=np.uint32).reshape(shape)\n",
                "    ...\n",
                "    >>> reshaped_iota((3, 4))\n",
                "    Array([[ 0,  1,  2,  3],\n",
                "           [ 4,  5,  6,  7],\n",
                "           [ 8,  9, 10, 11]], dtype=uint32)\n",
                "\n",
                "  Args:\n",
                "    shape: the output shape\n",
                "\n",
                "  Returns:\n",
                "    A pair of ``uint32`` arrays ``(counts_hi, counts_lo)``, both of\n",
                "    shape ``shape``, representing the higher-order and lower-order 32\n",
                "    bits of the 64 bit unsigned iota.\n",
                "  \"\"\"\n",
                "  if len(shape) == 0:\n",
                "    return (jnp.zeros((), np.dtype('uint32')),) * 2\n",
                "  return iota_2x32_shape_p.bind(shape=shape)\n",
                "\n",
                "iota_2x32_shape_p = core.Primitive('iota_2x32_shape')\n",
                "iota_2x32_shape_p.multiple_results = True\n",
                "iota_2x32_shape_p.def_impl(partial(xla.apply_primitive, iota_2x32_shape_p))\n",
                "\n",
                "@iota_2x32_shape_p.def_abstract_eval\n",
                "def iota_2x32_shape_abstract_eval(*, shape):\n",
                "  return (core.ShapedArray(shape, np.dtype('uint32')),) * 2\n",
                "\n",
                "def bcast_iotas_to_reshaped_iota(add, mul, shape, iotas):\n",
                "  strides = (*map(int, np.cumprod(shape[1:][::-1])[::-1]), 1)\n",
                "  return reduce(add, [mul(s, i) for i, s in zip(iotas, strides)])  # type: ignore\n",
                "\n",
                "def iota_2x32_shape_lowering(ctx, *, shape):\n",
                "  def _add(x, y):\n",
                "    return mlir.hlo.AddOp(x, y).result\n",
                "\n",
                "  def _mul(x, y):\n",
                "    x_const = mlir.ir_constant(np.array(x, np.dtype('uint64')),\n",
                "                               canonicalize_types=False)\n",
                "    x_bcast = mlir.hlo.BroadcastOp(x_const, mlir.dense_int_elements(shape))\n",
                "    return mlir.hlo.MulOp(x_bcast, y).result\n",
                "\n",
                "  assert len(shape) > 0\n",
                "  aval_out, _ = ctx.avals_out\n",
                "  aval_u64 = core.ShapedArray(shape, np.dtype('uint64'))\n",
                "  iotas = [mlir.hlo.IotaOp(mlir.aval_to_ir_type(aval_u64),\n",
                "                            mlir.i64_attr(dimension)).result\n",
                "           for dimension in range(len(shape))]\n",
                "  counts = bcast_iotas_to_reshaped_iota(_add, _mul, shape, iotas)\n",
                "  shift = mlir.ir_constant(np.array(32, np.dtype('uint64')),\n",
                "                           canonicalize_types=False)\n",
                "  shift = mlir.hlo.BroadcastOp(shift, mlir.dense_int_elements(shape)).result\n",
                "  counts_shifted = mlir.hlo.ShiftRightLogicalOp(counts, shift).result\n",
                "  counts_lo = mlir.hlo.ConvertOp(mlir.aval_to_ir_type(aval_out), counts).result\n",
                "  counts_hi = mlir.hlo.ConvertOp(mlir.aval_to_ir_type(aval_out),\n",
                "                                  counts_shifted).result\n",
                "  return counts_hi, counts_lo\n",
                "mlir.register_lowering(iota_2x32_shape_p, iota_2x32_shape_lowering)\n",
                "\n",
                "\n",
                "@partial(jit, inline=True)\n",
                "def threefry_2x32(keypair, count):\n",
                "  \"\"\"Apply the Threefry 2x32 hash.\n",
                "\n",
                "  Args:\n",
                "    keypair: a pair of 32bit unsigned integers used for the key.\n",
                "    count: an array of dtype uint32 used for the counts.\n",
                "\n",
                "  Returns:\n",
                "    An array of dtype uint32 with the same shape as `count`.\n",
                "  \"\"\"\n",
                "  key1, key2 = keypair\n",
                "  if not lax.dtype(key1) == lax.dtype(key2) == lax.dtype(count) == np.uint32:\n",
                "    msg = \"threefry_2x32 requires uint32 arguments, got {}\"\n",
                "    raise TypeError(msg.format([lax.dtype(x) for x in [key1, key2, count]]))\n",
                "\n",
                "  try:\n",
                "    odd_size = count.size % 2\n",
                "  except core.InconclusiveDimensionOperation as e:\n",
                "    msg = (\"jax.random functions have limited support for shape polymorphism. \"\n",
                "           \"In particular, the product of the known dimensions must be even.\")\n",
                "    raise core.InconclusiveDimensionOperation(msg) from e\n",
                "\n",
                "  if odd_size:\n",
                "    x = list(jnp.split(jnp.concatenate([count.ravel(), np.uint32([0])]), 2))\n",
                "  else:\n",
                "    x = list(jnp.split(count.ravel(), 2))\n",
                "\n",
                "  x = threefry2x32_p.bind(key1, key2, x[0], x[1])\n",
                "  out = jnp.concatenate(x)\n",
                "  assert out.dtype == np.uint32\n",
                "  return lax.reshape(out[:-1] if odd_size else out, count.shape)\n",
                "\n",
                "\n",
                "def threefry_split(key: jnp.ndarray, num: int) -> jnp.ndarray:\n",
                "  if config.jax_threefry_partitionable:\n",
                "    return _threefry_split_foldlike(key, int(num))  # type: ignore\n",
                "  else:\n",
                "    return _threefry_split_original(key, int(num))  # type: ignore\n",
                "\n",
                "@partial(jit, static_argnums=(1,), inline=True)\n",
                "def _threefry_split_original(key, num) -> jnp.ndarray:\n",
                "  counts = lax.iota(np.uint32, num * 2)\n",
                "  return lax.reshape(threefry_2x32(key, counts), (num, 2))\n",
                "\n",
                "@partial(jit, static_argnums=(1,), inline=True)\n",
                "def _threefry_split_foldlike(key, num) -> jnp.ndarray:\n",
                "  k1, k2 = key\n",
                "  counts1, counts2 = iota_2x32_shape((num,))\n",
                "  bits1, bits2 = threefry2x32_p.bind(k1, k2, counts1, counts2)\n",
                "  return jnp.stack([bits1, bits2], axis=1)\n",
                "\n",
                "\n",
                "def threefry_fold_in(key: jnp.ndarray, data: jnp.ndarray) -> jnp.ndarray:\n",
                "  assert not data.shape\n",
                "  return _threefry_fold_in(key, jnp.uint32(data))\n",
                "\n",
                "@partial(jit, inline=True)\n",
                "def _threefry_fold_in(key, data):\n",
                "  return threefry_2x32(key, threefry_seed(data))\n",
                "\n",
                "\n",
                "def threefry_random_bits(key: jnp.ndarray, bit_width, shape):\n",
                "  \"\"\"Sample uniform random bits of given width and shape using PRNG key.\"\"\"\n",
                "  if not _is_threefry_prng_key(key):\n",
                "    raise TypeError(\"threefry_random_bits got invalid prng key.\")\n",
                "  if bit_width not in (8, 16, 32, 64):\n",
                "    raise TypeError(\"requires 8-, 16-, 32- or 64-bit field width.\")\n",
                "\n",
                "  if (config.jax_threefry_partitionable and\n",
                "      not any(core.is_special_dim_size(d) for d in shape)):\n",
                "    return _threefry_random_bits_partitionable(key, bit_width, shape)\n",
                "  else:\n",
                "    return _threefry_random_bits_original(key, bit_width, shape)\n",
                "\n",
                "def _threefry_random_bits_partitionable(key: jnp.ndarray, bit_width, shape):\n",
                "  if all(core.is_constant_dim(d) for d in shape) and prod(shape) > 2 ** 64:\n",
                "    raise NotImplementedError('random bits array of size exceeding 2 ** 64')\n",
                "\n",
                "  k1, k2 = key\n",
                "  counts1, counts2 = iota_2x32_shape(shape)\n",
                "  bits1, bits2 = threefry2x32_p.bind(k1, k2, counts1, counts2)\n",
                "\n",
                "  dtype = UINT_DTYPES[bit_width]\n",
                "  if bit_width == 64:\n",
                "    bits_hi = lax.convert_element_type(bits1, dtype)\n",
                "    bits_lo = lax.convert_element_type(bits2, dtype)\n",
                "    return lax.shift_left(bits_hi, dtype(32)) | bits_lo\n",
                "  elif bit_width == 32:\n",
                "    return bits1 ^ bits2\n",
                "  else:\n",
                "    return lax.convert_element_type(bits1 ^ bits2, dtype)\n",
                "\n",
                "@partial(jit, static_argnums=(1, 2), inline=True)\n",
                "def _threefry_random_bits_original(key: jnp.ndarray, bit_width, shape):\n",
                "  size = prod(shape)\n",
                "  # Compute ceil(bit_width * size / 32) in a way that is friendly to shape\n",
                "  # polymorphism\n",
                "  max_count, r = divmod(bit_width * size, 32)\n",
                "  if r > 0:\n",
                "    max_count += 1\n",
                "\n",
                "  if core.is_constant_dim(max_count):\n",
                "    nblocks, rem = divmod(max_count, jnp.iinfo(np.uint32).max)\n",
                "  else:\n",
                "    nblocks, rem = 0, max_count\n",
                "\n",
                "  if not nblocks:\n",
                "    bits = threefry_2x32(key, lax.iota(np.uint32, rem))\n",
                "  else:\n",
                "    keys = threefry_split(key, nblocks + 1)\n",
                "    subkeys, last_key = keys[:-1], keys[-1]\n",
                "    blocks = vmap(threefry_2x32, in_axes=(0, None))(subkeys, lax.iota(np.uint32, jnp.iinfo(np.uint32).max))\n",
                "    last = threefry_2x32(last_key, lax.iota(np.uint32, rem))\n",
                "    bits = lax.concatenate([blocks.ravel(), last], 0)\n",
                "\n",
                "  dtype = UINT_DTYPES[bit_width]\n",
                "  if bit_width == 64:\n",
                "    bits = [lax.convert_element_type(x, dtype) for x in jnp.split(bits, 2)]\n",
                "    bits = lax.shift_left(bits[0], dtype(32)) | bits[1]\n",
                "  elif bit_width in [8, 16]:\n",
                "    # this is essentially bits.view(dtype)[:size]\n",
                "    bits = lax.bitwise_and(\n",
                "      np.uint32(np.iinfo(dtype).max),\n",
                "      lax.shift_right_logical(\n",
                "        lax.broadcast(bits, (1,)),\n",
                "        lax.mul(\n",
                "          np.uint32(bit_width),\n",
                "          lax.broadcasted_iota(np.uint32, (32 // bit_width, 1), 0)\n",
                "        )\n",
                "      )\n",
                "    )\n",
                "    bits = lax.reshape(bits, ((max_count * 32 // bit_width),), (1, 0))\n",
                "    bits = lax.convert_element_type(bits, dtype)[:size]\n",
                "  return lax.reshape(bits, shape)\n",
                "\n",
                "\n",
                "threefry_prng_impl = PRNGImpl(\n",
                "    key_shape=(2,),\n",
                "    seed=threefry_seed,\n",
                "    split=threefry_split,\n",
                "    random_bits=threefry_random_bits,\n",
                "    fold_in=threefry_fold_in,\n",
                "    tag='fry')\n",
                "\n",
                "\n",
                "# -- RngBitGenerator PRNG implementation\n",
                "\n",
                "# This code is experimental!\n",
                "# https://www.tensorflow.org/xla/operation_semantics#rngbitgenerator\n",
                "# Notice that the RngBitGenerator operations are not guaranteed to be\n",
                "# stable/deterministic across backends or compiler versions. Correspondingly, we\n",
                "# reserve the right to change any of these implementations at any time!\n",
                "\n",
                "def _rbg_seed(seed: jnp.ndarray) -> jnp.ndarray:\n",
                "  assert not seed.shape\n",
                "  halfkey = threefry_seed(seed)\n",
                "  return jnp.concatenate([halfkey, halfkey])\n",
                "\n",
                "def _rbg_split(key: jnp.ndarray, num: int) -> jnp.ndarray:\n",
                "  if config.jax_threefry_partitionable:\n",
                "    _threefry_split = _threefry_split_foldlike\n",
                "  else:\n",
                "    _threefry_split = _threefry_split_original\n",
                "  return vmap(\n",
                "      _threefry_split, (0, None), 1)(key.reshape(2, 2), num).reshape(num, 4)\n",
                "\n",
                "def _rbg_fold_in(key: jnp.ndarray, data: jnp.ndarray) -> jnp.ndarray:\n",
                "  assert not data.shape\n",
                "  return vmap(_threefry_fold_in, (0, None), 0)(key.reshape(2, 2), data).reshape(4)\n",
                "\n",
                "def _rbg_random_bits(key: jnp.ndarray, bit_width: int, shape: Sequence[int]\n",
                "                     ) -> jnp.ndarray:\n",
                "  if not key.shape == (4,) and key.dtype == jnp.dtype('uint32'):\n",
                "    raise TypeError(\"_rbg_random_bits got invalid prng key.\")\n",
                "  if bit_width not in (8, 16, 32, 64):\n",
                "    raise TypeError(\"requires 8-, 16-, 32- or 64-bit field width.\")\n",
                "  _, bits = lax.rng_bit_generator(key, shape, dtype=UINT_DTYPES[bit_width])\n",
                "  return bits\n",
                "\n",
                "rbg_prng_impl = PRNGImpl(\n",
                "    key_shape=(4,),\n",
                "    seed=_rbg_seed,\n",
                "    split=_rbg_split,\n",
                "    random_bits=_rbg_random_bits,\n",
                "    fold_in=_rbg_fold_in,\n",
                "    tag='rbg')\n",
                "\n",
                "def _unsafe_rbg_split(key: jnp.ndarray, num: int) -> jnp.ndarray:\n",
                "  # treat 10 iterations of random bits as a 'hash function'\n",
                "  _, keys = lax.rng_bit_generator(key, (10 * num, 4), dtype='uint32')\n",
                "  return keys[::10]\n",
                "\n",
                "def _unsafe_rbg_fold_in(key: jnp.ndarray, data: jnp.ndarray) -> jnp.ndarray:\n",
                "  assert not data.shape\n",
                "  _, random_bits = lax.rng_bit_generator(_rbg_seed(data), (10, 4), dtype='uint32')\n",
                "  return key ^ random_bits[-1]\n",
                "\n",
                "unsafe_rbg_prng_impl = PRNGImpl(\n",
                "    key_shape=(4,),\n",
                "    seed=_rbg_seed,\n",
                "    split=_unsafe_rbg_split,\n",
                "    random_bits=_rbg_random_bits,\n",
                "    fold_in=_unsafe_rbg_fold_in,\n",
                "    tag='urbg')"
            ]
        ],
        "jax/interpreters/mlir.py": [
            [
                "# Copyright 2021 The JAX Authors.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     https://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\n",
                "# Lowering and execution path that converts jaxprs into MLIR.\n",
                "from __future__ import annotations\n",
                "\n",
                "import collections\n",
                "import dataclasses\n",
                "import functools\n",
                "from functools import partial\n",
                "import io\n",
                "import itertools\n",
                "import operator\n",
                "import re\n",
                "import typing\n",
                "from typing import (Any, Callable, Dict, Iterator, List, NamedTuple, Optional,\n",
                "                    Protocol, Sequence, Set, Tuple, Type, Union, FrozenSet)\n",
                "import warnings\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "from jax._src import linear_util as lu\n",
                "from jax.config import config\n",
                "from jax.interpreters import ad\n",
                "from jax.interpreters import partial_eval as pe\n",
                "from jax.interpreters import xla\n",
                "from jax._src import ad_util\n",
                "from jax._src import core\n",
                "from jax._src import device_array\n",
                "from jax._src import dtypes\n",
                "from jax._src import source_info_util\n",
                "from jax._src import util\n",
                "from jax._src.lib import mlir_api_version\n",
                "from jax._src.lib import xla_bridge as xb\n",
                "from jax._src.lib import xla_client as xc\n",
                "from jax._src.lib.mlir import ir\n",
                "from jax._src.lib.mlir.dialects import hlo\n",
                "from jax._src.lib.mlir.dialects import func as func_dialect\n",
                "\n",
                "\n",
                "map, unsafe_map = util.safe_map, map\n",
                "zip, unsafe_zip = util.safe_zip, zip\n",
                "\n",
                "T = typing.TypeVar(\"T\")\n",
                "\n",
                "Value = Any  # = ir.Value\n",
                "\n",
                "# mypy implicitly sets this variable to true when type checking.\n",
                "MYPY = False\n",
                "\n",
                "lowerable_effects: Set[core.Effect] = set()\n",
                "\n",
                "\n",
                "# IR Helpers\n",
                "\n",
                "def dense_int_elements(xs) -> ir.DenseIntElementsAttr:\n",
                "  return ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))\n",
                "\n",
                "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr:\n",
                "  a = np.packbits(np.array(xs, np.bool_), bitorder='little')\n",
                "  # TODO(b/209005197): Work around for MLIR crash for non-splat single element\n",
                "  # buffers.\n",
                "  if len(xs) == 1:\n",
                "    a = np.array(0 if a.item() == 0 else 0xff, np.uint8)\n",
                "  return ir.DenseElementsAttr.get(\n",
                "      a, type=ir.IntegerType.get_signless(1), shape=[len(xs)])\n",
                "\n",
                "def i32_attr(i): return ir.IntegerAttr.get(ir.IntegerType.get_signless(32), i)\n",
                "def i64_attr(i): return ir.IntegerAttr.get(ir.IntegerType.get_signless(64), i)\n",
                "\n",
                "def shape_tensor(sizes: Sequence[Union[int, ir.RankedTensorType]]\n",
                "                 ) -> ir.RankedTensorType:\n",
                "  int1d = aval_to_ir_type(core.ShapedArray((1,), np.int32))\n",
                "  def lower_dim(d):\n",
                "    if type(d) is int:\n",
                "      return ir_constant(np.array([d], np.int32))\n",
                "    else:\n",
                "      return hlo.ReshapeOp(int1d, hlo.ConvertOp(aval_to_ir_type(core.ShapedArray((), np.int32)), d))\n",
                "  d, *ds = map(lower_dim, sizes)\n",
                "  if not ds:\n",
                "    return d\n",
                "  else:\n",
                "    return hlo.ConcatenateOp([d, *ds], i64_attr(0)).result\n",
                "\n",
                "\n",
                "def delegate_lowering(ctx, lowering_fun, *args, **ctx_override_kwargs):\n",
                "  \"\"\"Side-effects on `ctx`\"\"\"\n",
                "  ctx_new = ctx.replace(**ctx_override_kwargs)\n",
                "  out = lowering_fun(ctx_new, *args)\n",
                "  ctx.set_tokens_out(ctx_new.tokens_out)\n",
                "  return out\n",
                "\n",
                "\n",
                "# IR Types\n",
                "\n",
                "# Non-canonicalized dtype to IR type mapping.\n",
                "_dtype_to_ir_type : Dict[np.dtype, Callable[[], ir.Type]] = {\n",
                "  np.dtype(dtypes.float0): partial(ir.IntegerType.get_signless, 1),\n",
                "  np.dtype(np.bool_): partial(ir.IntegerType.get_signless, 1),\n",
                "  np.dtype(np.int8): partial(ir.IntegerType.get_signless, 8),\n",
                "  np.dtype(np.int16): partial(ir.IntegerType.get_signless, 16),\n",
                "  np.dtype(np.int32): partial(ir.IntegerType.get_signless, 32),\n",
                "  np.dtype(np.int64): partial(ir.IntegerType.get_signless, 64),\n",
                "  np.dtype(np.uint8): partial(ir.IntegerType.get_unsigned, 8),\n",
                "  np.dtype(np.uint16): partial(ir.IntegerType.get_unsigned, 16),\n",
                "  np.dtype(np.uint32): partial(ir.IntegerType.get_unsigned, 32),\n",
                "  np.dtype(np.uint64): partial(ir.IntegerType.get_unsigned, 64),\n",
                "  np.dtype(dtypes.bfloat16): ir.BF16Type.get,\n",
                "  np.dtype(np.float16): ir.F16Type.get,\n",
                "  np.dtype(np.float32): ir.F32Type.get,\n",
                "  np.dtype(np.float64): ir.F64Type.get,\n",
                "  np.dtype(np.complex64): lambda: ir.ComplexType.get(ir.F32Type.get()),\n",
                "  np.dtype(np.complex128): lambda: ir.ComplexType.get(ir.F64Type.get()),\n",
                "}\n",
                "\n",
                "if xc.mlir_api_version >= 43 and xc._version >= 113:\n",
                "  _dtype_to_ir_type.update({\n",
                "      np.dtype(dtypes.float8_e4m3fn): ir.Float8E4M3FNType.get,\n",
                "      np.dtype(dtypes.float8_e5m2): ir.Float8E5M2Type.get,\n",
                "  })\n",
                "\n",
                "def dtype_to_ir_type(dtype: Union[np.dtype, np.generic]) -> ir.Type:\n",
                "  assert isinstance(dtype, (np.dtype, np.generic)), type(dtype)\n",
                "  dtype = np.dtype(dtype)\n",
                "  try:\n",
                "    ir_type_factory = _dtype_to_ir_type[dtype]\n",
                "  except KeyError as err:\n",
                "    raise TypeError(\n",
                "        f\"No dtype_to_ir_type handler for dtype: {dtype}\") from err\n",
                "  return ir_type_factory()\n",
                "\n",
                "def _array_ir_types(aval: Union[core.ShapedArray, core.DShapedArray]\n",
                "                    ) -> Sequence[ir.Type]:\n",
                "  if core.is_opaque_dtype(aval.dtype):\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    return aval.dtype._rules.aval_to_ir_types(aval)\n"
                ],
                "after": [
                    "    phys_avals = aval.dtype._rules.physical_avals(aval)\n",
                    "    return tuple(itertools.chain(*map(_array_ir_types, phys_avals)))\n"
                ],
                "parent_version_range": {
                    "start": 145,
                    "end": 146
                },
                "child_version_range": {
                    "start": 145,
                    "end": 147
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if core.is_opaque_dtype(aval.dtype):",
                        "start_line": 144,
                        "end_line": 145
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "_array_ir_types",
                        "signature": "def _array_ir_types(aval: Union[core.ShapedArray, core.DShapedArray]\n                    )->Sequence[ir.Type]:",
                        "at_line": 142
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: jax/interpreters/mlir.py\nCode:\n142 142    def _array_ir_types(aval: Union[core.ShapedArray, core.DShapedArray]\n143 143                        ) -> Sequence[ir.Type]:\n144 144      if core.is_opaque_dtype(aval.dtype):\n145      -     return aval.dtype._rules.aval_to_ir_types(aval)\n    145  +     phys_avals = aval.dtype._rules.physical_avals(aval)\n    146  +     return tuple(itertools.chain(*map(_array_ir_types, phys_avals)))\n146 147      if not core.is_constant_shape(aval.shape):\n147 148        return _dynamic_array_ir_types(aval)  # type: ignore\n148 149      return (ir.RankedTensorType.get(aval.shape, dtype_to_ir_type(aval.dtype)),)\n         ...\n",
                "file_path": "jax/interpreters/mlir.py",
                "identifiers_before": [
                    "_rules",
                    "aval",
                    "aval_to_ir_types",
                    "dtype"
                ],
                "identifiers_after": [
                    "_array_ir_types",
                    "_rules",
                    "aval",
                    "chain",
                    "dtype",
                    "itertools",
                    "map",
                    "phys_avals",
                    "physical_avals",
                    "tuple"
                ],
                "prefix": [
                    "def _array_ir_types(aval: Union[core.ShapedArray, core.DShapedArray]\n",
                    "                    ) -> Sequence[ir.Type]:\n",
                    "  if core.is_opaque_dtype(aval.dtype):\n"
                ],
                "suffix": [
                    "  if not core.is_constant_shape(aval.shape):\n",
                    "    return _dynamic_array_ir_types(aval)  # type: ignore\n",
                    "  return (ir.RankedTensorType.get(aval.shape, dtype_to_ir_type(aval.dtype)),)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "  if not core.is_constant_shape(aval.shape):\n",
                "    return _dynamic_array_ir_types(aval)  # type: ignore\n",
                "  return (ir.RankedTensorType.get(aval.shape, dtype_to_ir_type(aval.dtype)),)\n",
                "\n",
                "def _dynamic_array_ir_types(aval: core.ShapedArray) -> Sequence[ir.Type]:\n",
                "  dyn_size = ir.ShapedType.get_dynamic_size()\n",
                "  shape = [d if type(d) is int else dyn_size for d in aval.shape]\n",
                "  return (ir.RankedTensorType.get(shape, dtype_to_ir_type(aval.dtype)),)\n",
                "\n",
                "ir_type_handlers: Dict[Type[core.AbstractValue],\n",
                "                        Callable[[Any], Sequence[ir.Type]]] = {}\n",
                "\n",
                "def aval_to_ir_types(aval: core.AbstractValue) -> Sequence[ir.Type]:\n",
                "  \"\"\"Converts a JAX aval to zero or more MLIR IR types.\n",
                "\n",
                "  In general, a JAX value may be represented by multiple IR values, so this\n",
                "  function returns multiple types.\"\"\"\n",
                "  try:\n",
                "    return ir_type_handlers[type(aval)](aval)\n",
                "  except KeyError as err:\n",
                "    raise TypeError(f\"No ir_type_handler for aval type: {type(aval)}\") from err\n",
                "\n",
                "ir_type_handlers[core.ShapedArray] = _array_ir_types\n",
                "ir_type_handlers[core.ConcreteArray] = _array_ir_types\n",
                "ir_type_handlers[core.AbstractToken] = lambda _: [hlo.TokenType.get()]\n",
                "ir_type_handlers[core.DShapedArray] = _dynamic_array_ir_types\n",
                "\n",
                "def aval_to_ir_type(aval: core.AbstractValue) -> ir.Type:\n",
                "  \"\"\"Convenience wrapper around aval_to_ir_types for single types.\n",
                "\n",
                "  For some common cases, e.g. dense arrays, we know JAX values are represented\n",
                "  by a single IR value.\"\"\"\n",
                "  types = aval_to_ir_types(aval)\n",
                "  if len(types) != 1:\n",
                "    raise TypeError(f\"aval_to_ir_type called on {aval} which corresponds to \"\n",
                "                    f\"multiple IR types {types}\")\n",
                "  return types[0]\n",
                "\n",
                "\n",
                "# Constants\n",
                "\n",
                "class ConstantHandler(Protocol):\n",
                "  def __call__(self, val: Any, canonicalize_types: bool) -> Sequence[ir.Value]:\n",
                "    \"\"\"Builds an IR representation for a constant `val`.\n",
                "\n",
                "    A JAX value is represented by zero or more IR values.\"\"\"\n",
                "\n",
                "_constant_handlers : Dict[type, ConstantHandler] = {}\n",
                "\n",
                "def register_constant_handler(type_: type, handler_fun: ConstantHandler):\n",
                "  _constant_handlers[type_] = handler_fun\n",
                "\n",
                "def get_constant_handler(type_: type) -> ConstantHandler:\n",
                "  return _constant_handlers[type_]\n",
                "\n",
                "def ir_constants(val: Any,\n",
                "                 canonicalize_types: bool = True) -> Sequence[ir.Value]:\n",
                "  \"\"\"Translate a Python `val` to an IR constant, canonicalizing its dtype.\n",
                "\n",
                "  Args:\n",
                "    val: a Python value to be translated to a constant.\n",
                "\n",
                "  Returns:\n",
                "    A representation of the constant as a list of IR values.\n",
                "  \"\"\"\n",
                "  for t in type(val).__mro__:\n",
                "    handler = _constant_handlers.get(t)\n",
                "    if handler:\n",
                "      out = handler(val, canonicalize_types)\n",
                "      assert all(isinstance(v, ir.Value) for v in out), (type(val), out)\n",
                "      return out\n",
                "  if hasattr(val, '__jax_array__'):\n",
                "    return ir_constants(val.__jax_array__(), canonicalize_types)\n",
                "  raise TypeError(f\"No constant handler for type: {type(val)}\")\n",
                "\n",
                "def ir_constant(val: Any, canonicalize_types: bool = True) -> ir.Value:\n",
                "  \"\"\"Convenience wrapper around ir_constants for singleton values.\"\"\"\n",
                "  values = ir_constants(val, canonicalize_types=canonicalize_types)\n",
                "  if len(values) != 1:\n",
                "    raise TypeError(f\"ir_constant called on {val} which corresponds to \"\n",
                "                    f\"multiple IR values {values}\")\n",
                "  return values[0]\n",
                "\n",
                "\n",
                "def _numpy_array_constant(x: np.ndarray, canonicalize_types\n",
                "                         ) -> Sequence[ir.Value]:\n",
                "  if canonicalize_types:\n",
                "    x = np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n",
                "  element_type = dtype_to_ir_type(x.dtype)\n",
                "  shape = x.shape\n",
                "  if x.dtype == np.bool_:\n",
                "    nelems = x.size\n",
                "    x = np.packbits(x, bitorder='little')\n",
                "    # TODO(b/209005197): Work around for MLIR crash for non-splat single element\n",
                "    # buffers.\n",
                "    if nelems == 1:\n",
                "      x = np.array(0 if x.item() == 0 else 0xff, np.uint8)\n",
                "  elif x.dtype == dtypes.bfloat16:\n",
                "    x = x.view(np.uint16)\n",
                "  x = np.ascontiguousarray(x)\n",
                "  attr = ir.DenseElementsAttr.get(x, type=element_type, shape=shape)\n",
                "  return (hlo.ConstantOp(attr).result,)\n",
                "\n",
                "\n",
                "def _masked_array_constant_handler(*args, **kwargs):\n",
                "  raise ValueError(\"numpy masked arrays are not supported as direct inputs to JAX functions. \"\n",
                "                   \"Use arr.filled() to convert the value to a standard numpy array.\")\n",
                "\n",
                "register_constant_handler(np.ma.MaskedArray, _masked_array_constant_handler)\n",
                "\n",
                "def _ndarray_constant_handler(val: np.ndarray, canonicalize_types\n",
                "                             ) -> Sequence[ir.Value]:\n",
                "  \"\"\"Constant handler for ndarray literals, handling zero-size strides.\n",
                "\n",
                "  In most cases this function calls _numpy_array_constant(val) except it has\n",
                "  special handling of arrays with any strides of size zero: for those, it\n",
                "  generates appropriate calls to NumpyArrayConstant, Broadcast, and Transpose\n",
                "  to avoid staging in large literals that might arise from np.zeros or np.ones\n",
                "  or the output of lax.broadcast (which uses np.broadcast_to which in turn\n",
                "  uses size-zero strides).\n",
                "\n",
                "  Args:\n",
                "    val: an ndarray.\n",
                "\n",
                "  Returns:\n",
                "    An XLA ComputationDataHandle / XlaOp representing the constant ndarray\n",
                "    staged into the XLA Computation.\n",
                "  \"\"\"\n",
                "  if dtypes.result_type(val) == dtypes.float0:\n",
                "    return _numpy_array_constant(np.zeros(val.shape, dtype=np.bool_),\n",
                "                                 canonicalize_types=False)\n",
                "  elif np.any(np.equal(0, val.strides)) and val.size > 0:\n",
                "    zero_stride_axes, = np.where(np.equal(0, val.strides))\n",
                "    other_axes, = np.where(np.not_equal(0, val.strides))\n",
                "    collapsed_val = val[tuple(0 if ax in zero_stride_axes else slice(None) # type: ignore\n",
                "                              for ax in range(val.ndim))]  # type: ignore\n",
                "    if canonicalize_types:\n",
                "      collapsed_val = np.asarray(\n",
                "          collapsed_val, dtypes.canonicalize_dtype(collapsed_val.dtype))\n",
                "    out = hlo.BroadcastInDimOp(\n",
                "        ir.RankedTensorType.get(\n",
                "            val.shape, dtype_to_ir_type(collapsed_val.dtype)),\n",
                "        _numpy_array_constant(collapsed_val, canonicalize_types=False)[0],\n",
                "        dense_int_elements(other_axes)).result\n",
                "    return (out,)\n",
                "  else:\n",
                "    return _numpy_array_constant(val, canonicalize_types)\n",
                "\n",
                "register_constant_handler(np.ndarray, _ndarray_constant_handler)\n",
                "\n",
                "for _scalar_type in [np.int8, np.int16, np.int32, np.int64,\n",
                "                     np.uint8, np.uint16, np.uint32, np.uint64,\n",
                "                     np.float16, np.float32, np.float64,\n",
                "                     np.complex64, np.complex128,\n",
                "                     np.bool_, np.longlong, dtypes.bfloat16]:\n",
                "  register_constant_handler(_scalar_type, _ndarray_constant_handler)\n",
                "\n",
                "def _python_scalar_handler(dtype, val, canonicalize_dtypes):\n",
                "  return _numpy_array_constant(np.array(val, dtype), canonicalize_dtypes)\n",
                "\n",
                "for ptype, dtype in dtypes.python_scalar_dtypes.items():\n",
                "  register_constant_handler(ptype, partial(_python_scalar_handler, dtype))\n",
                "\n",
                "def _device_array_constant_handler(val, canonicalize_types):\n",
                "  return _ndarray_constant_handler(np.asarray(val.device_buffer),\n",
                "                                   canonicalize_types)\n",
                "for t in device_array.device_array_types:\n",
                "  register_constant_handler(t, _device_array_constant_handler)\n",
                "\n",
                "def _token_constant_handler(val, canonicalize_types):\n",
                "  if mlir_api_version < 40:\n",
                "    return [hlo.CreateTokenOp(hlo.TokenType.get()).result]\n",
                "  else:\n",
                "    return [hlo.CreateTokenOp().result]\n",
                "register_constant_handler(core.Token, _token_constant_handler)\n",
                "\n",
                "# Source locations\n",
                "\n",
                "def _source_info_to_location(\n",
                "    primitive: core.Primitive, params: Dict,\n",
                "    source_info: source_info_util.SourceInfo,\n",
                "    name_stack: source_info_util.NameStack) -> ir.Location:\n",
                "  eqn_str = (f'{str(source_info.name_stack)}/'\n",
                "             f'{core.str_eqn_compact(primitive.name, params)}')\n",
                "  frame = source_info_util.user_frame(source_info)\n",
                "  if frame is None:\n",
                "    loc = ir.Location.unknown()\n",
                "  else:\n",
                "    loc = ir.Location.file(xla._get_canonical_source_file(frame),\n",
                "                           frame.start_line, frame.start_column)\n",
                "  loc = ir.Location.name(eqn_str, childLoc=loc)\n",
                "  # TODO(phawkins): also include primitive.name as the operator type.\n",
                "  return loc\n",
                "\n",
                "\n",
                "# Translation rules\n",
                "def make_ir_context() -> ir.Context:\n",
                "  \"\"\"Creates an MLIR context suitable for JAX IR.\"\"\"\n",
                "  from jax._src.lib.mlir import dialects\n",
                "  context = ir.Context()\n",
                "  dialects.mhlo.register_mhlo_dialect(context)\n",
                "  dialects.chlo.register_dialect(context)\n",
                "  dialects.stablehlo.register_dialect(context)\n",
                "  return context\n",
                "\n",
                "\n",
                "Mesh = Any\n",
                "MeshAxisName = Any\n",
                "\n",
                "@dataclasses.dataclass(frozen=True)\n",
                "class SPMDAxisContext:\n",
                "  \"\"\"A hardware axis context for parallel computations that use the GSPMD partitioner.\n",
                "\n",
                "  This includes the mesh that will later by used to execute this computation,\n",
                "  as well as a set of mesh axes that are currently (e.g. because the current lowering\n",
                "  is invoked inside an xmap) lowered in the MANUAL sharding mode.\n",
                "  \"\"\"\n",
                "  mesh: Mesh\n",
                "  manual_axes: FrozenSet[MeshAxisName] = frozenset()\n",
                "\n",
                "  @property\n",
                "  def axis_env(self):\n",
                "    # All collectives that touch axis_env should remember to set use_global_device_ids\n",
                "    # when this context is enabled!\n",
                "    if self.manual_axes != frozenset(self.mesh.axis_names):\n",
                "      raise NotImplementedError(\n",
                "          \"Collectives in manually partitioned computations are only supported \"\n",
                "          \"when all mesh axes are partitioned manually (no partial automatic sharding). \"\n",
                "          \"Make sure that you mention all mesh axes in axis_resources!\")\n",
                "    return self.unsafe_axis_env\n",
                "\n",
                "  @property\n",
                "  def unsafe_axis_env(self):\n",
                "    return xla.AxisEnv(\n",
                "        nreps=self.mesh.size,\n",
                "        names=self.mesh.axis_names,\n",
                "        sizes=tuple(self.mesh.shape.values()))\n",
                "\n",
                "  def extend_manual(self, axes: FrozenSet[MeshAxisName]) -> SPMDAxisContext:\n",
                "    return SPMDAxisContext(self.mesh, self.manual_axes | axes)\n",
                "\n",
                "\n",
                "@dataclasses.dataclass(frozen=True)\n",
                "class ReplicaAxisContext:\n",
                "  \"\"\"A hardware axis context for parallel computations that are partitioned by JAX.\n",
                "\n",
                "  Unlike in the SPMDAxisContext, this means that JAX might need to emit calls to\n",
                "  explicit collectives.\n",
                "  \"\"\"\n",
                "  axis_env: xla.AxisEnv\n",
                "\n",
                "\n",
                "@dataclasses.dataclass(frozen=True)\n",
                "class ShardingContext:\n",
                "  \"\"\"A hardware axis context for parallel computations that use the sharding\n",
                "  interface.\n",
                "\n",
                "  This context also uses the GSPMD partitioner.\n",
                "  \"\"\"\n",
                "  device_assignment: Sequence[xc.Device]\n",
                "\n",
                "  # Similar to SPMDContext as ShardingContext also uses the GSPMD partitioner.\n",
                "  @property\n",
                "  def axis_env(self):\n",
                "    return xla.AxisEnv(nreps=1, names=(), sizes=())\n",
                "\n",
                "\n",
                "AxisContext = Union[SPMDAxisContext, ReplicaAxisContext, ShardingContext]\n",
                "\n",
                "@dataclasses.dataclass\n",
                "class ModuleContext:\n",
                "  \"\"\"Module-wide context information for MLIR lowering.\"\"\"\n",
                "  context: ir.Context\n",
                "  module: ir.Module\n",
                "  ip: ir.InsertionPoint\n",
                "  symbol_table: ir.SymbolTable\n",
                "  backend_or_name: Optional[Union[str, xb.XlaBackend]]\n",
                "  platform: str\n",
                "  axis_context: AxisContext\n",
                "  name_stack: source_info_util.NameStack\n",
                "  keepalives: List[Any]\n",
                "  channel_iterator: Iterator[int]\n",
                "  host_callbacks: List[Any]\n",
                "  # The names of the dimension variables, sorted by name. This is the order in\n",
                "  # which they are passed to the IR functions that need them. This is only\n",
                "  # used for native serialization with polymorphic shapes when\n",
                "  # --jax_dynamic_shapes is off.\n",
                "  dim_vars: Sequence[str]\n",
                "\n",
                "  # Cached primitive lowerings.\n",
                "  cached_primitive_lowerings: Dict[Any, func_dialect.FuncOp]\n",
                "  cached_call_jaxpr_lowerings: Dict[Any, func_dialect.FuncOp]\n",
                "\n",
                "\n",
                "  @property\n",
                "  def axis_env(self) -> xla.AxisEnv:\n",
                "    return self.axis_context.axis_env\n",
                "\n",
                "  def __init__(\n",
                "      self,\n",
                "      backend_or_name: Optional[Union[str, xb.XlaBackend]],\n",
                "      platform: str,\n",
                "      axis_context: AxisContext,\n",
                "      name_stack: source_info_util.NameStack,\n",
                "      keepalives: List[Any],\n",
                "      channel_iterator: Iterator[int],\n",
                "      host_callbacks: List[Any],\n",
                "      context: Optional[ir.Context] = None,\n",
                "      module: Optional[ir.Module] = None,\n",
                "      ip: Optional[ir.InsertionPoint] = None,\n",
                "      symbol_table: Optional[ir.SymbolTable] = None,\n",
                "      cached_primitive_lowerings: Optional[Dict[Any,\n",
                "                                                func_dialect.FuncOp]] = None,\n",
                "      cached_call_jaxpr_lowerings: Optional[Dict[Any,\n",
                "                                                 func_dialect.FuncOp]] = None,\n",
                "      dim_vars: Sequence[str] = ()):\n",
                "    assert platform is not None\n",
                "    self.context = context or make_ir_context()\n",
                "    self.module = module or ir.Module.create(loc=ir.Location.unknown(self.context))\n",
                "    self.ip = ip or ir.InsertionPoint(self.module.body)\n",
                "    self.symbol_table = symbol_table or ir.SymbolTable(self.module.operation)\n",
                "    self.backend_or_name = backend_or_name\n",
                "    self.platform = platform\n",
                "    self.axis_context = axis_context\n",
                "    self.name_stack = name_stack\n",
                "    self.cached_primitive_lowerings = ({} if cached_primitive_lowerings is None\n",
                "                                       else cached_primitive_lowerings)\n",
                "    self.channel_iterator = channel_iterator\n",
                "    self.keepalives = keepalives\n",
                "    self.host_callbacks = host_callbacks\n",
                "    self.cached_call_jaxpr_lowerings = ({}\n",
                "                                        if cached_call_jaxpr_lowerings is None\n",
                "                                        else cached_call_jaxpr_lowerings)\n",
                "    self.dim_vars = dim_vars\n",
                "\n",
                "  @property\n",
                "  def backend(self) -> xb.XlaBackend:\n",
                "    if self.backend_or_name is None or isinstance(self.backend_or_name, str):\n",
                "      return xb.get_backend(self.backend_or_name)\n",
                "    return self.backend_or_name\n",
                "\n",
                "  def new_channel(self) -> int:\n",
                "    return next(self.channel_iterator)\n",
                "\n",
                "  def add_host_callback(self, host_callback: Any) -> None:\n",
                "    self.host_callbacks.append(host_callback)\n",
                "\n",
                "  def add_keepalive(self, keepalive: Any) -> None:\n",
                "    self.keepalives.append(keepalive)\n",
                "\n",
                "  def replace(self, **kw): return dataclasses.replace(self, **kw)\n",
                "\n",
                "\n",
                "@dataclasses.dataclass\n",
                "class LoweringRuleContext:\n",
                "  \"\"\"Per-rule context information for MLIR lowering.\"\"\"\n",
                "  module_context: ModuleContext\n",
                "  primitive: Optional[core.Primitive]\n",
                "  avals_in: Sequence[core.AbstractValue]\n",
                "  avals_out: Any  # Usually Sequence[core.AbstractValue], but sometimes None.\n",
                "  tokens_in: TokenSet\n",
                "  tokens_out: Optional[TokenSet]  # Mutable store for output containers\n",
                "  axis_size_env: Optional[Dict[core.Var, ir.Value]] = None  # Dynamic axis sizes\n",
                "  dim_var_values: Sequence[ir.Value] = ()  # The values for the dimension variables\n",
                "                                           # in same order as module_context.dim_vars\n",
                "\n",
                "  def set_tokens_out(self, tokens_out: TokenSet):\n",
                "    assert self.tokens_out is None, 'Should only set `tokens_out` once.'\n",
                "    self.tokens_out = tokens_out\n",
                "\n",
                "  def replace(self, **kw): return dataclasses.replace(self, **kw)\n",
                "\n",
                "\n",
                "if not MYPY:\n",
                "  class LoweringRule(Protocol):\n",
                "    def __call__(self, ctx: LoweringRuleContext,\n",
                "                 *args: Union[ir.Value, Sequence[ir.Value]],\n",
                "                 **kw) -> Sequence[Union[ir.Value, Sequence[ir.Value]]]:\n",
                "      \"\"\"Converts a JAX primitive invocation into MLIR.\"\"\"\n",
                "else:\n",
                "  LoweringRule = Any\n",
                "\n",
                "_lowerings: Dict[core.Primitive, LoweringRule] = {}\n",
                "_platform_specific_lowerings: Dict[str, Dict[core.Primitive, LoweringRule]]\n",
                "_platform_specific_lowerings = collections.defaultdict(dict)\n",
                "\n",
                "def register_lowering(prim: core.Primitive, rule: LoweringRule,\n",
                "                      platform: Optional[str] = None):\n",
                "  if platform is None:\n",
                "    _lowerings[prim] = rule\n",
                "  else:\n",
                "    # For backward compatibility reasons, we allow rules to be registered\n",
                "    # under \"gpu\" even though the platforms are now called \"cuda\" and \"rocm\".\n",
                "    # TODO(phawkins): fix up users to specify either \"cuda\" or \"rocm\" and remove\n",
                "    # this expansion.\n",
                "    for p in xb.expand_platform_alias(platform):\n",
                "      _platform_specific_lowerings[p][prim] = rule\n",
                "\n",
                "\n",
                "def _unwrap_singleton_ir_values(x): return x[0] if len(x) == 1 else x\n",
                "def wrap_singleton_ir_values(x: Union[ir.Value, Sequence[ir.Value]]\n",
                "                             ) -> Sequence[ir.Value]:\n",
                "  \"\"\"Adds a consistent tuples to a mixture of tupled and untuple values.\"\"\"\n",
                "  return (x,) if isinstance(x, ir.Value) else tuple(x)\n",
                "\n",
                "def flatten_lowering_ir_args(\n",
                "    xs: Sequence[Union[ir.Value, Sequence[ir.Value]]]\n",
                ") -> Sequence[Sequence[ir.Value]]:\n",
                "  return util.flatten(map(wrap_singleton_ir_values, xs))\n",
                "\n",
                "_module_name_regex = re.compile(r\"[^\\w.-]\")\n",
                "\n",
                "def sharded_aval(aval: core.ShapedArray,\n",
                "                 sharding: Optional[xc.OpSharding]) -> core.ShapedArray:\n",
                "  \"\"\"Returns the new aval sharded based on sharding proto.\"\"\"\n",
                "  if sharding is None:\n",
                "    return aval\n",
                "\n",
                "  if (sharding.type == xc.OpSharding.Type.REPLICATED or\n",
                "      sharding.type == xc.OpSharding.Type.MANUAL):\n",
                "    return aval\n",
                "\n",
                "  sharded_shape = []\n",
                "  tile_rank = len(sharding.tile_assignment_dimensions)\n",
                "  if sharding.replicate_on_last_tile_dim:\n",
                "    tile_rank -= 1\n",
                "  if sharding.last_tile_dims:\n",
                "    tile_rank -= len(sharding.last_tile_dims)\n",
                "  if tile_rank == 0:\n",
                "    return aval\n",
                "\n",
                "  for i in range(tile_rank):\n",
                "    partitions = sharding.tile_assignment_dimensions[i]\n",
                "    assert partitions > 0\n",
                "    sharded_shape.append((aval.shape[i] + partitions - 1) // partitions)\n",
                "  return aval.update(tuple(sharded_shape))\n",
                "\n",
                "\n",
                "class DimPolyEvaluator:\n",
                "  # A wrapper for an ir.Value that overloads + and * to be used for evaluating\n",
                "  # dimension polynomials.\n",
                "  def __init__(self, value: ir.Value):\n",
                "    self.value = value\n",
                "\n",
                "  def __add__(self, other: Union[np.int32, np.int64, DimPolyEvaluator]):\n",
                "    if not isinstance(other, DimPolyEvaluator):\n",
                "      other = DimPolyEvaluator(ir_constant(other))\n",
                "    return DimPolyEvaluator(hlo.AddOp(self.value, other.value).result)\n",
                "\n",
                "  def __radd__(self, other: Union[np.int32, np.int64]):\n",
                "    return DimPolyEvaluator(hlo.AddOp(ir_constant(other), self.value).result)\n",
                "\n",
                "  def __mul__(self, other: Union[np.int32, np.int64, DimPolyEvaluator]):\n",
                "    if not isinstance(other, DimPolyEvaluator):\n",
                "      other = DimPolyEvaluator(ir_constant(other))\n",
                "    return DimPolyEvaluator(hlo.MulOp(self.value, other.value).result)\n",
                "\n",
                "  def __rmul__(self, other: Union[np.int32, np.int64]):\n",
                "    return DimPolyEvaluator(hlo.MulOp(ir_constant(other), self.value).result)\n",
                "\n",
                "\n",
                "def eval_dynamic_shape(ctx: LoweringRuleContext,\n",
                "                       shape: core.Shape) -> Tuple[Union[int, Value], ...]:\n",
                "  # assert not core.is_constant_shape(shape)\n",
                "  if config.jax_dynamic_shapes:\n",
                "    return tuple(ctx.axis_size_env.get(d, d) for d in shape)  # type: ignore\n",
                "  else:\n",
                "    dim_var_env = {dv_name : DimPolyEvaluator(dv_val[0])\n",
                "                   for dv_name, dv_val in zip(ctx.module_context.dim_vars, ctx.dim_var_values)}\n",
                "    def eval_dim(d: core.DimSize) -> Union[int, ir.Value]:\n",
                "      try:\n",
                "        return operator.index(d)\n",
                "      except:\n",
                "        if isinstance(d, ir.Value):\n",
                "          return d\n",
                "        else:\n",
                "          # Is a dimension polynomial\n",
                "          return d.evaluate(dim_var_env).value  # type: ignore\n",
                "    return tuple(eval_dim(d) for d in shape)\n",
                "\n",
                "class LoweringResult(NamedTuple):\n",
                "  module: ir.Module\n",
                "  keepalive: Optional[Any]\n",
                "  host_callbacks: List[Any]\n",
                "\n",
                "\n",
                "_platforms_with_donation = [\"cpu\", \"cuda\", \"rocm\", \"tpu\"]\n",
                "\n",
                "\n",
                "def lower_jaxpr_to_module(\n",
                "    module_name: str,\n",
                "    jaxpr: core.ClosedJaxpr,\n",
                "    unordered_effects: List[core.Effect],\n",
                "    ordered_effects: List[core.Effect],\n",
                "    backend_or_name: Optional[Union[str, xb.XlaBackend]],\n",
                "    platform: str,\n",
                "    axis_context: AxisContext,\n",
                "    name_stack: source_info_util.NameStack,\n",
                "    donated_args: Sequence[bool],\n",
                "    replicated_args: Optional[Sequence[bool]] = None,\n",
                "    arg_shardings: Optional[Sequence[Optional[xc.OpSharding]]] = None,\n",
                "    result_shardings: Optional[Sequence[Optional[xc.OpSharding]]] = None\n",
                ") -> LoweringResult:\n",
                "  \"\"\"Lowers a top-level jaxpr to an MLIR module.\n",
                "\n",
                "  Handles the quirks of the argument/return value passing conventions of the\n",
                "  runtime.\n",
                "  \"\"\"\n",
                "  platform = xb.canonicalize_platform(platform)\n",
                "  if not xb.is_known_platform(platform):\n",
                "    raise ValueError(f\"Unknown platform {platform}\")\n",
                "  input_output_aliases = None\n",
                "  in_avals = jaxpr.in_avals\n",
                "  if arg_shardings is not None:\n",
                "    in_avals = [\n",
                "        sharded_aval(in_aval, in_sharding)\n",
                "        for in_aval, in_sharding in zip(in_avals, arg_shardings)\n",
                "    ]\n",
                "  out_avals = jaxpr.out_avals\n",
                "  if result_shardings is not None:\n",
                "    out_avals = []\n",
                "    for out_aval, out_sharding in zip(jaxpr.out_avals, result_shardings):\n",
                "      if (out_aval is not core.abstract_token and\n",
                "          core.is_opaque_dtype(out_aval.dtype)):\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        # TODO(frostig,mattjj,necula): asserts a single physical aval\n"
                ],
                "parent_version_range": {
                    "start": 670,
                    "end": 670
                },
                "child_version_range": {
                    "start": 671,
                    "end": 672
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if result_shardings is not None:",
                        "start_line": 665,
                        "end_line": 671
                    },
                    {
                        "type": "for_statement",
                        "statement": "for out_aval, out_sharding in zip(jaxpr.out_avals, result_shardings):",
                        "start_line": 667,
                        "end_line": 671
                    },
                    {
                        "type": "if_statement",
                        "statement": "if (out_aval is not core.abstract_token and\n          core.is_opaque_dtype(out_aval.dtype)):",
                        "start_line": 668,
                        "end_line": 670
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "lower_jaxpr_to_module",
                        "signature": "def lower_jaxpr_to_module(\n    module_name: str,\n    jaxpr: core.ClosedJaxpr,\n    unordered_effects: List[core.Effect],\n    ordered_effects: List[core.Effect],\n    backend_or_name: Optional[Union[str, xb.XlaBackend]],\n    platform: str,\n    axis_context: AxisContext,\n    name_stack: source_info_util.NameStack,\n    donated_args: Sequence[bool],\n    replicated_args: Optional[Sequence[bool]] = None,\n    arg_shardings: Optional[Sequence[Optional[xc.OpSharding]]] = None,\n    result_shardings: Optional[Sequence[Optional[xc.OpSharding]]] = None\n)->LoweringResult:",
                        "at_line": 635
                    },
                    {
                        "type": "call",
                        "name": "core.is_opaque_dtype",
                        "signature": "core.is_opaque_dtype(out_aval.dtype)",
                        "at_line": 669,
                        "argument": "out_aval.dtype"
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: jax/interpreters/mlir.py\nCode:\n           def lower_jaxpr_to_module(\n    module_name: str,\n    jaxpr: core.ClosedJaxpr,\n    unordered_effects: List[core.Effect],\n    ordered_effects: List[core.Effect],\n    backend_or_name: Optional[Union[str, xb.XlaBackend]],\n    platform: str,\n    axis_context: AxisContext,\n    name_stack: source_info_util.NameStack,\n    donated_args: Sequence[bool],\n    replicated_args: Optional[Sequence[bool]] = None,\n    arg_shardings: Optional[Sequence[Optional[xc.OpSharding]]] = None,\n    result_shardings: Optional[Sequence[Optional[xc.OpSharding]]] = None\n)->LoweringResult:\n               ...\n667 668        for out_aval, out_sharding in zip(jaxpr.out_avals, result_shardings):\n668 669          if (out_aval is not core.abstract_token and\n669 670              core.is_opaque_dtype(out_aval.dtype)):\n    671  +         # TODO(frostig,mattjj,necula): asserts a single physical aval\n670 672            out_aval, = out_aval.dtype._rules.physical_avals(out_aval)\n671 673          out_avals.append(sharded_aval(out_aval, out_sharding))\n672 674    \n         ...\n",
                "file_path": "jax/interpreters/mlir.py",
                "identifiers_before": [],
                "identifiers_after": [],
                "prefix": [
                    "    for out_aval, out_sharding in zip(jaxpr.out_avals, result_shardings):\n",
                    "      if (out_aval is not core.abstract_token and\n",
                    "          core.is_opaque_dtype(out_aval.dtype)):\n"
                ],
                "suffix": [
                    "        out_aval, = out_aval.dtype._rules.physical_avals(out_aval)\n",
                    "      out_avals.append(sharded_aval(out_aval, out_sharding))\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        out_aval, = out_aval.dtype._rules.physical_avals(out_aval)\n",
                "      out_avals.append(sharded_aval(out_aval, out_sharding))\n",
                "\n",
                "  if platform in _platforms_with_donation:\n",
                "    input_output_aliases, donated_args = _set_up_aliases(\n",
                "        in_avals, out_avals, donated_args)\n",
                "  if any(eff not in lowerable_effects for eff in jaxpr.effects):\n",
                "    raise ValueError(f'Cannot lower jaxpr with effects: {jaxpr.effects}')\n",
                "  if any(donated_args):\n",
                "    # TODO(tomhennigan): At call time we should mark these buffers as deleted.\n",
                "    unused_donations = [str(a) for a, d in zip(in_avals, donated_args)\n",
                "                        if d]\n",
                "    msg = \"See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.\"\n",
                "    if platform not in _platforms_with_donation:\n",
                "      msg = f\"Donation is not implemented for {platform}.\\n{msg}\"\n",
                "    warnings.warn(f\"Some donated buffers were not usable: {', '.join(unused_donations)}.\\n{msg}\")\n",
                "\n",
                "  # HLO channels need to start at 1\n",
                "  channel_iter = itertools.count(1)\n",
                "  # Create a keepalives list that will be mutated during the lowering.\n",
                "  keepalives: List[Any] = []\n",
                "  host_callbacks: List[Any] = []\n",
                "\n",
                "  dim_vars: Sequence[str]\n",
                "  if not config.jax_dynamic_shapes:\n",
                "    # Find the dimension variables\n",
                "    all_dim_poly = [d\n",
                "                    for aval in jaxpr.in_avals if hasattr(aval, \"shape\")\n",
                "                    for d in aval.shape if not core.is_constant_dim(d)]\n",
                "    dim_vars = tuple(sorted(functools.reduce(lambda acc, new: acc.union(new.get_vars()),\n",
                "                                             all_dim_poly, set())))\n",
                "  else:\n",
                "    dim_vars = ()\n",
                "\n",
                "  ctx = ModuleContext(backend_or_name, platform, axis_context, name_stack,\n",
                "                      keepalives, channel_iter, host_callbacks, dim_vars=dim_vars)\n",
                "  with ctx.context, ir.Location.unknown(ctx.context):\n",
                "    # Remove module name characters that XLA would alter. This ensures that\n",
                "    # XLA computation preserves the module name.\n",
                "    module_name = _module_name_regex.sub(\"_\", module_name)\n",
                "    ctx.module.operation.attributes[\"sym_name\"] = ir.StringAttr.get(\n",
                "        module_name)\n",
                "    unlowerable_effects = {eff for eff in jaxpr.effects\n",
                "                           if eff not in lowerable_effects}\n",
                "    if unlowerable_effects:\n",
                "      raise ValueError(\n",
                "          f'Cannot lower jaxpr with unlowerable effects: {unlowerable_effects}')\n",
                "    lower_jaxpr_to_fun(\n",
                "        ctx, \"main\", jaxpr, ordered_effects, public=True, create_tokens=True,\n",
                "        replace_tokens_with_dummy=True,\n",
                "        num_output_tokens=0,\n",
                "        replicated_args=replicated_args,\n",
                "        arg_shardings=arg_shardings, result_shardings=result_shardings,\n",
                "        input_output_aliases=input_output_aliases)\n",
                "\n",
                "  if not ctx.module.operation.verify():\n",
                "    module_string = module_to_string(ctx.module)\n",
                "    raise ValueError(\n",
                "        f\"Cannot lower jaxpr with verifier errors: {module_string}\")\n",
                "\n",
                "  return LoweringResult(ctx.module, ctx.keepalives, ctx.host_callbacks)\n",
                "\n",
                "def module_to_string(module: ir.Module) -> str:\n",
                "  output = io.StringIO()\n",
                "  module.operation.print(file=output, enable_debug_info=True,\n",
                "                         print_generic_op_form=False)\n",
                "  return output.getvalue()\n",
                "\n",
                "def module_to_bytecode(module: ir.Module) -> bytes:\n",
                "  output = io.BytesIO()\n",
                "  module.operation.write_bytecode(file=output)\n",
                "  return output.getvalue()\n",
                "\n",
                "\n",
                "def _set_up_aliases(avals_in, avals_out, donated_args):\n",
                "  input_output_aliases = [None] * len(avals_in)\n",
                "  # To match-up in-avals to out-avals we only care about the number of\n",
                "  # bytes, so we strip off unrelated aval metadata (eg. the named shape)\n",
                "  strip_metadata = lambda a: a.strip_named_shape().strip_weak_type()\n",
                "  avals_in = map(strip_metadata, avals_in)\n",
                "  avals_out = map(strip_metadata, avals_out)\n",
                "\n",
                "  donations = collections.defaultdict(collections.deque)\n",
                "  for i, (aval, donated) in enumerate(zip(avals_in, donated_args)):\n",
                "    if donated:\n",
                "      donations[aval].append(i)\n",
                "\n",
                "  out_donated_args = list(donated_args)\n",
                "  for i, aval in enumerate(avals_out):\n",
                "    if donations.get(aval, ()):\n",
                "      input_id = donations[aval].popleft()\n",
                "      input_output_aliases[input_id] = i\n",
                "      out_donated_args[input_id] = False\n",
                "\n",
                "  return input_output_aliases, out_donated_args\n",
                "\n",
                "Token = Sequence[ir.Value]\n",
                "\n",
                "def token_type() -> Sequence[ir.Type]:\n",
                "  return [hlo.TokenType.get()]\n",
                "\n",
                "def create_token() -> Token:\n",
                "  if mlir_api_version < 40:\n",
                "    return wrap_singleton_ir_values(\n",
                "        hlo.CreateTokenOp(hlo.TokenType.get()).result)\n",
                "  else:\n",
                "    return wrap_singleton_ir_values(hlo.CreateTokenOp().result)\n",
                "\n",
                "class TokenSet:\n",
                "  \"\"\"An immutable container of tokens to be used to lower effectful jaxprs. When lowering\n",
                "  effectful jaxprs, we need to thread HLO tokens to sequence them. Each effect\n",
                "  will need its own token that will be threaded in and out of the effectful\n",
                "  primitives. A `TokenSet` encapsulates a set of HLO tokens that will be\n",
                "  used by the lowering rules.\n",
                "  \"\"\"\n",
                "  _tokens: typing.OrderedDict[core.Effect, Token]\n",
                "\n",
                "  def __init__(self, *args, **kwargs):\n",
                "    self._tokens = collections.OrderedDict(*args, **kwargs)\n",
                "\n",
                "  def __len__(self):\n",
                "    return len(self._tokens)\n",
                "\n",
                "  def get(self, effect: core.Effect) -> Token:\n",
                "    return self._tokens[effect]\n",
                "\n",
                "  @classmethod\n",
                "  def create(cls, effects: Sequence[core.Effect]) -> TokenSet:\n",
                "    \"\"\"Creates a `TokenSet` corresponding to a list of `core.Effect`s.\"\"\"\n",
                "    tokens = [create_token() for _ in effects]\n",
                "    return TokenSet(zip(effects, tokens))\n",
                "\n",
                "  def items(self) -> Sequence[Tuple[core.Effect, Token]]:\n",
                "    return tuple(self._tokens.items())\n",
                "\n",
                "  def effects(self) -> Sequence[core.Effect]:\n",
                "    return tuple(self._tokens.keys())\n",
                "\n",
                "  def tokens(self) -> Sequence[Token]:\n",
                "    return tuple(self._tokens.values())\n",
                "\n",
                "  def subset(self, effects: Sequence[core.Effect]) -> TokenSet:\n",
                "    \"\"\"Return a subset of the `TokenSet` restricted to a set of `core.Effect`s.\"\"\"\n",
                "    return TokenSet((eff, self._tokens[eff]) for eff in effects)\n",
                "\n",
                "  def update_tokens(self, tokens: TokenSet) -> TokenSet:\n",
                "    \"\"\"Returns a new `TokenSet` with tokens replaced with ones from the input `TokenSet`.\"\"\"\n",
                "    new_tokens = []\n",
                "    for eff in self.effects():\n",
                "      if eff in tokens._tokens:\n",
                "        new_tokens.append(tokens._tokens[eff])\n",
                "      else:\n",
                "        new_tokens.append(self._tokens[eff])\n",
                "    return TokenSet(zip(self.effects(), new_tokens))\n",
                "\n",
                "def dummy_token_type() -> Sequence[ir.Type]:\n",
                "  return aval_to_ir_types(core.ShapedArray((0,), np.bool_))\n",
                "\n",
                "def dummy_token() -> Sequence[ir.Value]:\n",
                "  return ir_constants(np.zeros(0, np.bool_))\n",
                "\n",
                "def lower_jaxpr_to_fun(\n",
                "    ctx: ModuleContext,\n",
                "    name: str,\n",
                "    jaxpr: core.ClosedJaxpr,\n",
                "    effects: Sequence[core.Effect],\n",
                "    *,\n",
                "    create_tokens: bool = False,\n",
                "    public: bool = False,\n",
                "    replace_tokens_with_dummy: bool = False,\n",
                "    replicated_args: Optional[Sequence[bool]] = None,\n",
                "    arg_shardings: Optional[Sequence[Optional[xc.OpSharding]]] = None,\n",
                "    result_shardings: Optional[Sequence[Optional[xc.OpSharding]]] = None,\n",
                "    use_sharding_annotations: bool = True,\n",
                "    input_output_aliases: Optional[Sequence[Optional[int]]] = None,\n",
                "    num_output_tokens: int = 0,\n",
                "    api_name: str = 'jit',\n",
                ") -> func_dialect.FuncOp:\n",
                "  \"\"\"Lowers jaxpr and its callees to an IR function.\n",
                "\n",
                "  Assumes that an MLIR context, location, and insertion point are set.\n",
                "\n",
                "  Args:\n",
                "    ctx: the lowering context.\n",
                "    name: the function name. The name will be uniquified by the symbol table,\n",
                "      so it is ok to use the same name multiple times.\n",
                "    jaxpr: the jaxpr to lower.\n",
                "    effects: a sequence of `core.Effect`s corresponding to an ordering of tokens\n",
                "      that will be created in or used by the lowered function.\n",
                "    create_tokens: if true, the HLO will create tokens and ignore dummy input tokens.\n",
                "    public: if true, the function's visibility is set to \"public\".\n",
                "    replace_tokens_with_dummy: if true, token arguments/return values are\n",
                "      replaced with bool arrays of size [0].\n",
                "    replicated_args: if present, annotates arguments as replicated.\n",
                "    arg_shardings: sharding annotations for each argument (optional).\n",
                "    result_shardings: sharding annotations for each argument (optional).\n",
                "    use_sharding_annotations: if True, use \"mhlo.sharding\" annotations on\n",
                "      parameters and return values to express sharding. If False, use\n",
                "      hlo.custom_call operators with sharding annotations.\n",
                "      TODO(b/228598865): remove this option when \"mhlo.sharding\" annotations are\n",
                "      propagated on non-entry functions during MLIR->HLO conversion.\n",
                "    input_output_aliases: optional sequence that maps argument numbers to the\n",
                "      corresponding output that should alias them.\n",
                "    api_name: The name of the higher level primitive which should show up in the\n",
                "      name stack.\n",
                "  Returns the name of the function.\n",
                "  \"\"\"\n",
                "  def aval_to_types(aval):\n",
                "    if replace_tokens_with_dummy and aval is core.abstract_token:\n",
                "      aval = core.ShapedArray((), np.dtype(np.bool_))\n",
                "    return aval_to_ir_types(aval)\n",
                "\n",
                "  num_dim_vars = len(ctx.dim_vars)\n",
                "  dim_var_types = map(aval_to_types, [core.ShapedArray((), dtypes.canonicalize_dtype(np.int64))] * num_dim_vars)\n",
                "\n",
                "  # Function inputs: *dim_var_values, *tokens, *actual_inputs\n",
                "  input_types = map(aval_to_types, jaxpr.in_avals)\n",
                "  output_types = map(aval_to_types, jaxpr.out_avals)\n",
                "  num_tokens = len(effects)\n",
                "\n",
                "  if create_tokens:\n",
                "    # If we create the tokens they won't be inputs to the MLIR function.\n",
                "    token_types = [dummy_token_type() for _ in effects]\n",
                "    output_token_types = [dummy_token_type() for _ in range(num_output_tokens)]\n",
                "  else:\n",
                "    # If we aren't creating tokens they will be the initial inputs to the\n",
                "    # MLIR function.\n",
                "    output_token_types = []\n",
                "    num_tokens = len(effects)\n",
                "    token_types = [token_type() for _ in effects]\n",
                "  input_types = [*dim_var_types, *token_types, *input_types]\n",
                "  output_types = [*output_token_types, *token_types, *output_types]\n",
                "  if input_output_aliases is not None:\n",
                "    token_input_output_aliases = [None] * (num_dim_vars + num_tokens)\n",
                "    input_output_aliases = [*token_input_output_aliases, *input_output_aliases]\n",
                "    # Update the existing aliases to account for the new output values\n",
                "    input_output_aliases = [None if a is None\n",
                "                            else a + num_output_tokens + num_tokens\n",
                "                            for a in input_output_aliases]\n",
                "  if arg_shardings is not None:\n",
                "    token_shardings = [None] * (num_dim_vars + num_tokens)\n",
                "    arg_shardings = [*token_shardings, *arg_shardings]\n",
                "  if result_shardings is not None:\n",
                "    token_shardings = [None] * (num_tokens + num_output_tokens)\n",
                "    result_shardings = [*token_shardings, *result_shardings]\n",
                "  if replicated_args is not None:\n",
                "    token_replicated_args = [False] * (num_dim_vars + num_tokens)\n",
                "    replicated_args = [*token_replicated_args, *replicated_args]\n",
                "  flat_input_types = util.flatten(input_types)\n",
                "  flat_output_types = util.flatten(output_types)\n",
                "  ftype = ir.FunctionType.get(flat_input_types, flat_output_types)\n",
                "  func_op = func_dialect.FuncOp(name, ftype, ip=ctx.ip)\n",
                "  func_op.attributes[\"sym_visibility\"] = ir.StringAttr.get(\n",
                "      \"public\" if public else \"private\")\n",
                "  ctx.symbol_table.insert(func_op)\n",
                "  ir_arg_shardings = None\n",
                "  if arg_shardings is not None:\n",
                "    ir_arg_shardings = util.flatten(\n",
                "        [[sharding] * len(types) for sharding, types\n",
                "         in zip(arg_shardings, input_types)])\n",
                "  ir_result_shardings = None\n",
                "  if result_shardings is not None:\n",
                "    ir_result_shardings = util.flatten(\n",
                "        [[sharding] * len(types)\n",
                "         for sharding, types in zip(result_shardings, output_types)])\n",
                "\n",
                "  if (replicated_args is not None or ir_arg_shardings is not None\n",
                "      or input_output_aliases is not None):\n",
                "    arg_attrs: List[Dict[str, ir.Attribute]] = [\n",
                "        {} for _ in range(len(flat_input_types))]\n",
                "\n",
                "    if replicated_args is not None:\n",
                "      replicated_ir_args = [[replicated] * len(types) for replicated, types\n",
                "                            in zip(replicated_args, input_types)]\n",
                "      for attrs, replicated in zip(arg_attrs, util.flatten(replicated_ir_args)):\n",
                "        if replicated:\n",
                "          attrs[\"mhlo.is_same_data_across_replicas\"] = ir.UnitAttr.get()\n",
                "\n",
                "    if use_sharding_annotations and ir_arg_shardings is not None:\n",
                "      for attrs, sharding in zip(arg_attrs, ir_arg_shardings):\n",
                "        if sharding is not None:\n",
                "          attrs[\"mhlo.sharding\"] = ir.StringAttr.get(\n",
                "              sharding.SerializeToString())\n",
                "\n",
                "    if input_output_aliases is not None:\n",
                "      output_ids = util.unflatten(list(range(len(flat_output_types))),\n",
                "                                  map(len, output_types))\n",
                "      aliases: List[Optional[int]] = []\n",
                "      for types, alias in zip(input_types, input_output_aliases):\n",
                "        if alias is None:\n",
                "          aliases.extend([None] * len(types))\n",
                "        else:\n",
                "          aliases.extend(output_ids[alias])\n",
                "\n",
                "      for attrs, alias in zip(arg_attrs, aliases):\n",
                "        if alias is not None:\n",
                "          attrs[\"tf.aliasing_output\"] = i32_attr(alias)\n",
                "\n",
                "    func_op.arg_attrs = ir.ArrayAttr.get(\n",
                "        [ir.DictAttr.get(attrs) for attrs in arg_attrs])\n",
                "\n",
                "  if use_sharding_annotations and ir_result_shardings is not None:\n",
                "    func_op.result_attrs = ir.ArrayAttr.get([\n",
                "        ir.DictAttr.get(\n",
                "            {} if sharding is None else\n",
                "            {\"mhlo.sharding\": ir.StringAttr.get(sharding.SerializeToString())}\n",
                "        ) for sharding in ir_result_shardings\n",
                "    ])\n",
                "\n",
                "  entry_block = func_op.add_entry_block()\n",
                "  with ir.InsertionPoint(entry_block):\n",
                "    flat_args = entry_block.arguments\n",
                "    if not use_sharding_annotations and ir_arg_shardings is not None:\n",
                "      flat_args = [a if s is None else wrap_with_sharding_op(a, s)\n",
                "                   for a, s in zip(flat_args, ir_arg_shardings)]\n",
                "\n",
                "    unflattened_args = util.unflatten(flat_args, map(len, input_types))\n",
                "    # We separate out the dimension variable inputs, the token inputs and\n",
                "    # the usual inputs. The dimension variables and token inputs\n",
                "    # will be passed to `jaxpr_subcomp` separately from the `args`.\n",
                "    dim_var_values, token_args, unflattened_args = util.split_list(unflattened_args, [num_dim_vars, num_tokens])\n",
                "    if create_tokens:\n",
                "      tokens_in = TokenSet.create(effects)\n",
                "    else:\n",
                "      tokens_in = TokenSet(zip(effects, token_args))\n",
                "    args: List[List[ir.Value]] = []\n",
                "    for aval, arg in zip(jaxpr.in_avals, unflattened_args):\n",
                "      if replace_tokens_with_dummy and aval is core.abstract_token:\n",
                "        if mlir_api_version < 40:\n",
                "          args.append(hlo.CreateTokenOp(hlo.TokenType.get()).results)\n",
                "        else:\n",
                "          args.append(hlo.CreateTokenOp().results)\n",
                "      else:\n",
                "        args.append(arg)\n",
                "    callee_name_stack = xla.extend_name_stack(ctx.name_stack,\n",
                "                                              util.wrap_name(name, api_name))\n",
                "    out_vals, tokens_out = jaxpr_subcomp(ctx.replace(name_stack=callee_name_stack),\n",
                "                                         jaxpr.jaxpr, tokens_in, map(ir_constants, jaxpr.consts),\n",
                "                                         *args, dim_var_values=dim_var_values)\n",
                "    outs = []\n",
                "    if create_tokens:\n",
                "      for _ in range(num_output_tokens):\n",
                "        outs.append(dummy_token())\n",
                "      for _ in effects:\n",
                "        outs.append(dummy_token())\n",
                "    else:\n",
                "      for token in tokens_out.tokens():\n",
                "        outs.append(token)\n",
                "    for aval, out in zip(jaxpr.out_avals, out_vals):\n",
                "      if replace_tokens_with_dummy and aval is core.abstract_token:\n",
                "        outs.append(ir_constants(np.zeros((), np.bool_)))\n",
                "      else:\n",
                "        outs.append(out)\n",
                "    flat_outputs = util.flatten(outs)\n",
                "    if not use_sharding_annotations and ir_result_shardings is not None:\n",
                "      flat_outputs = [o if s is None else wrap_with_sharding_op(o, s)\n",
                "                      for o, s in zip(flat_outputs, ir_result_shardings)]\n",
                "\n",
                "    func_dialect.ReturnOp(flat_outputs)\n",
                "\n",
                "  return func_op\n",
                "\n",
                "def _emit_lowering_rule_as_fun(lowering_rule,\n",
                "                               ctx: LoweringRuleContext) -> func_dialect.FuncOp:\n",
                "  \"\"\"Emits the contents of a lowering rule as a private function.\"\"\"\n",
                "  num_dim_vars = len(ctx.module_context.dim_vars)\n",
                "  # TODO(necula) maybe only pass the dim_vars if they are needed?\n",
                "  dim_var_types = map(aval_to_ir_types, [core.ShapedArray((), dtypes.canonicalize_dtype(np.int64))] * num_dim_vars)\n",
                "\n",
                "  input_types = map(aval_to_ir_types, ctx.avals_in)\n",
                "  output_types = map(aval_to_ir_types, ctx.avals_out)\n",
                "  token_types = [token_type() for _ in ctx.tokens_in.items()]\n",
                "  input_types = [*dim_var_types, *token_types, *input_types]\n",
                "  output_types = [*token_types, *output_types]\n",
                "\n",
                "  flat_input_types = util.flatten(input_types)\n",
                "  flat_output_types = util.flatten(output_types)\n",
                "  ftype = ir.FunctionType.get(flat_input_types, flat_output_types)\n",
                "  assert ctx.primitive is not None\n",
                "  func_op = func_dialect.FuncOp(ctx.primitive.name, ftype,\n",
                "                                ip=ctx.module_context.ip)\n",
                "  func_op.attributes[\"sym_visibility\"] = ir.StringAttr.get(\"private\")\n",
                "  ctx.module_context.symbol_table.insert(func_op)\n",
                "  entry_block = func_op.add_entry_block()\n",
                "  with ir.InsertionPoint(entry_block):\n",
                "    unflattened_args = util.unflatten(entry_block.arguments,\n",
                "                                      map(len, input_types))\n",
                "    dim_var_values, token_args, unflattened_args = util.split_list(unflattened_args, [num_dim_vars, len(ctx.tokens_in)])\n",
                "    sub_ctx = ctx.replace(tokens_in=TokenSet(zip(ctx.tokens_in.effects(), token_args)),\n",
                "                          dim_var_values=dim_var_values)\n",
                "    outs = lowering_rule(sub_ctx, *_unwrap_singleton_ir_values(unflattened_args))\n",
                "    if sub_ctx.tokens_out:\n",
                "      outs = [*sub_ctx.tokens_out.tokens(), outs]\n",
                "    func_dialect.ReturnOp(util.flatten(map(wrap_singleton_ir_values, outs)))\n",
                "  return func_op\n",
                "\n",
                "def jaxpr_subcomp(ctx: ModuleContext, jaxpr: core.Jaxpr,\n",
                "                  tokens: TokenSet,\n",
                "                  consts: Sequence[Sequence[ir.Value]],\n",
                "                  *args: Sequence[ir.Value],\n",
                "                  dim_var_values: Sequence[ir.Value]\n",
                "                  ) -> Tuple[Sequence[Sequence[ir.Value]], TokenSet]:\n",
                "  \"\"\"Lowers a jaxpr into MLIR, inlined into an existing function.\n",
                "\n",
                "  Assumes that an MLIR context, location, and insertion point are set.\n",
                "\n",
                "  dim_var_values: the list of dimension variables values in the current\n",
                "    IR function, in the order of ctx.dim_vars.\n",
                "  \"\"\"\n",
                "  assert ctx.platform != \"gpu\"\n",
                "  def read(v: core.Atom) -> Sequence[ir.Value]:\n",
                "    if type(v) is core.Literal:\n",
                "      return ir_constants(v.val, canonicalize_types=True)\n",
                "    else:\n",
                "      assert isinstance(v, core.Var)\n",
                "      return env[v]\n",
                "\n",
                "  def aval(v: core.Atom) -> core.AbstractValue:\n",
                "    if type(v) is core.Literal:\n",
                "      return xla.abstractify(v.val)\n",
                "    else:\n",
                "      return v.aval\n",
                "\n",
                "  def write(v: core.Var, node: Sequence[ir.Value]):\n",
                "    assert node is not None\n",
                "    env[v] = tuple(node)\n",
                "\n",
                "\n",
                "  env: Dict[core.Var, Tuple[ir.Value, ...]] = {}\n",
                "\n",
                "  assert len(args) == len(jaxpr.invars), (jaxpr, args)\n",
                "  assert len(consts) == len(jaxpr.constvars), (jaxpr, consts)\n",
                "  assert all(isinstance(v, ir.Value) for vs in consts for v in vs), consts\n",
                "  assert len(ctx.dim_vars) == len(dim_var_values), (ctx.dim_vars, dim_var_values)\n",
                "  map(write, jaxpr.constvars, consts)\n",
                "  map(write, jaxpr.invars, args)\n",
                "  for eqn in jaxpr.eqns:\n",
                "    in_nodes = map(read, eqn.invars)\n",
                "    assert isinstance(ctx.name_stack, source_info_util.NameStack), type(ctx.name_stack)\n",
                "    source_info = eqn.source_info.replace(\n",
                "        name_stack=ctx.name_stack + eqn.source_info.name_stack)\n",
                "    loc = _source_info_to_location(eqn.primitive, eqn.params, source_info,\n",
                "                                   ctx.name_stack)\n",
                "    with source_info_util.user_context(eqn.source_info.traceback), loc:\n",
                "      if eqn.primitive in _platform_specific_lowerings[ctx.platform]:\n",
                "        rule = _platform_specific_lowerings[ctx.platform][eqn.primitive]\n",
                "      elif eqn.primitive in xla._backend_specific_translations[ctx.platform]:\n",
                "        rule = xla_fallback_lowering(eqn.primitive)\n",
                "      elif eqn.primitive in _lowerings:\n",
                "        rule = _lowerings[eqn.primitive]\n",
                "      elif eqn.primitive in xla._translations:\n",
                "        rule = xla_fallback_lowering(eqn.primitive)\n",
                "      else:\n",
                "        raise NotImplementedError(\n",
                "            f\"MLIR translation rule for primitive '{eqn.primitive.name}' not \"\n",
                "            f\"found for platform {ctx.platform}\")\n",
                "\n",
                "      eqn_ctx = ctx.replace(name_stack=source_info.name_stack)\n",
                "      effects = [eff for eff in eqn.effects if eff in core.ordered_effects]\n",
                "      tokens_in = tokens.subset(effects)\n",
                "      avals_in = map(aval, eqn.invars)\n",
                "      rule_ctx = LoweringRuleContext(\n",
                "          module_context=eqn_ctx, primitive=eqn.primitive, avals_in=avals_in,\n",
                "          avals_out=map(aval, eqn.outvars), tokens_in=tokens_in,\n",
                "          tokens_out=None, dim_var_values=dim_var_values)\n",
                "      if config.jax_dynamic_shapes:\n",
                "        axis_size_env = {d: read(d)[0]\n",
                "                         for a in avals_in if type(a) is core.DShapedArray\n",
                "                         for d in a.shape if type(d) is core.Var}\n",
                "        rule_ctx = rule_ctx.replace(axis_size_env=axis_size_env)\n",
                "      ans = rule(rule_ctx, *map(_unwrap_singleton_ir_values, in_nodes),\n",
                "                 **eqn.params)\n",
                "      if effects:\n",
                "        # If there were ordered effects in the primitive, there should be output\n",
                "        # tokens we need for subsequent ordered effects.\n",
                "        tokens_out = rule_ctx.tokens_out\n",
                "        if tokens_out is None:\n",
                "          raise ValueError(\n",
                "              f'Lowering rule for `{eqn.primitive}` needs to set `tokens_out` '\n",
                "              f'because it has effects: {eqn.effects}.')\n",
                "        if tokens_out.effects() != tokens_in.effects():\n",
                "          raise ValueError(\n",
                "              f'Lowering rule for `{eqn.primitive}` '\n",
                "              'returns incorrect set of output tokens. '\n",
                "              f'Expected: {tuple(tokens_in.effects())} vs. Actual: {tuple(tokens_out.effects())}')\n",
                "        tokens = tokens.update_tokens(tokens_out)\n",
                "\n",
                "    try:\n",
                "      out_nodes = tuple(map(wrap_singleton_ir_values, ans))\n",
                "    except TypeError as e:\n",
                "      raise ValueError(\"Output of translation rule must be iterable: \"\n",
                "                       f\"{eqn}, got output {ans}\") from e\n",
                "\n",
                "    assert all(isinstance(v, tuple) for v in out_nodes), (ans, eqn)\n",
                "    assert all(isinstance(v, ir.Value) for w in out_nodes for v in w), (ans, eqn)\n",
                "    assert len(ans) == len(eqn.outvars), (ans, eqn)\n",
                "    map(write, eqn.outvars, out_nodes)\n",
                "  return map(read, jaxpr.outvars), tokens\n",
                "\n",
                "def _ir_consts(consts):\n",
                "  unique_consts = {id(const): const for const in consts}\n",
                "  ir_consts = {\n",
                "      id_: ir_constants(const) for id_, const in unique_consts.items()}\n",
                "  return [ir_consts[id(const)] for const in consts]\n",
                "\n",
                "def lower_fun(fun: Callable, multiple_results: bool = True) -> Callable:\n",
                "  \"\"\"Converts a traceable JAX function `fun` into a lowering rule.\n",
                "\n",
                "  The returned function does not use `avals_out`, so callers may pass any value\n",
                "  as `avals_out`.\"\"\"\n",
                "  def f_lowered(ctx, *args, **params):\n",
                "    f = fun if multiple_results else lambda *args, **kw: (fun(*args, **kw),)\n",
                "    wrapped_fun = lu.wrap_init(f, params)\n",
                "\n",
                "    if config.jax_dynamic_shapes:\n",
                "      # We might be applying this function to arguments with dynamic shapes,\n",
                "      # i.e. there might be Vars in the shape tuples of ctx.avals_in. In that\n",
                "      # case, we need to form a jaxpr with leading binders for those axis size\n",
                "      # arguments (by computing an InputType and using trace_to_jaxpr_dynamic2),\n",
                "      # and we need to call jaxpr_subcomp with these arguments made explicit.\n",
                "      args = (*ctx.axis_size_env.values(), *args)\n",
                "      idx = {d: core.DBIdx(i) for i, d in enumerate(ctx.axis_size_env)}\n",
                "      i32_aval = core.ShapedArray((), np.dtype('int32'))\n",
                "      implicit_args = [(i32_aval, False)] * len(ctx.axis_size_env)\n",
                "      explicit_args = [(a.update(shape=tuple(idx.get(d, d) for d in a.shape))\n",
                "                        if type(a) is core.DShapedArray else a, True)\n",
                "                       for a in ctx.avals_in]\n",
                "      wrapped_fun = lu.annotate(wrapped_fun, (*implicit_args, *explicit_args))\n",
                "      jaxpr, _, consts = pe.trace_to_jaxpr_dynamic2(wrapped_fun)\n",
                "    else:\n",
                "      jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(wrapped_fun, ctx.avals_in)\n",
                "      # TODO(frostig,mattjj): check ctx.avals_out against jaxpr avals out?\n",
                "\n",
                "    out, tokens = jaxpr_subcomp(\n",
                "        ctx.module_context, jaxpr, ctx.tokens_in, _ir_consts(consts),\n",
                "        *map(wrap_singleton_ir_values, args), dim_var_values=ctx.dim_var_values)\n",
                "    ctx.set_tokens_out(tokens)\n",
                "    return out\n",
                "\n",
                "  return f_lowered\n",
                "\n",
                "\n",
                "def _lower_jaxpr_to_fun_cached(ctx, fn_name, call_jaxpr, effects):\n",
                "  if not call_jaxpr.consts:\n",
                "    # Cacheable.\n",
                "    key = (fn_name, call_jaxpr.jaxpr, tuple(effects))\n",
                "    try:\n",
                "      func_op = ctx.cached_call_jaxpr_lowerings[key]\n",
                "    except KeyError:\n",
                "      func_op = lower_jaxpr_to_fun(ctx, fn_name, call_jaxpr, effects)\n",
                "      ctx.cached_call_jaxpr_lowerings[key] = func_op\n",
                "  else:\n",
                "    func_op = lower_jaxpr_to_fun(ctx, fn_name, call_jaxpr, effects)\n",
                "  return func_op\n",
                "\n",
                "\n",
                "def _call_lowering(fn_name, stack_name, call_jaxpr, backend, ctx, avals_in,\n",
                "                   avals_out, tokens_in, *args,\n",
                "                   dim_var_values: Sequence[ir.Value]):\n",
                "  if isinstance(call_jaxpr, core.Jaxpr):\n",
                "    call_jaxpr = core.ClosedJaxpr(call_jaxpr, ())\n",
                "  xla.check_backend_matches(backend, ctx.platform)\n",
                "  effects = tokens_in.effects()\n",
                "  output_types = map(aval_to_ir_types, avals_out)\n",
                "  output_types = [token_type()] * len(effects) + output_types\n",
                "  flat_output_types = util.flatten(output_types)\n",
                "  symbol_name = _lower_jaxpr_to_fun_cached(ctx, fn_name, call_jaxpr, effects).name.value\n",
                "  args = tuple([*dim_var_values, *tokens_in.tokens(), *args])\n",
                "  call = func_dialect.CallOp(flat_output_types,\n",
                "                             ir.FlatSymbolRefAttr.get(symbol_name),\n",
                "                             flatten_lowering_ir_args(args))\n",
                "  out_nodes = util.unflatten(call.results, map(len, output_types))\n",
                "  tokens, out_nodes = util.split_list(out_nodes, [len(effects)])\n",
                "  tokens_out = tokens_in.update_tokens(TokenSet(zip(effects, tokens)))\n",
                "  return out_nodes, tokens_out\n",
                "\n",
                "def _xla_call_lower(ctx, *args,\n",
                "                    backend=None, name, call_jaxpr, donated_invars, inline=None,\n",
                "                    device=None, keep_unused=None):\n",
                "  del device, donated_invars, inline, keep_unused  # Ignored.\n",
                "  out_nodes, tokens = _call_lowering(\n",
                "      name, util.wrap_name(name, \"jit\"), call_jaxpr, backend,\n",
                "      ctx.module_context, ctx.avals_in, ctx.avals_out, ctx.tokens_in,\n",
                "      *args, dim_var_values=ctx.dim_var_values)\n",
                "  ctx.set_tokens_out(tokens)\n",
                "  return out_nodes\n",
                "\n",
                "register_lowering(xla.xla_call_p, _xla_call_lower)\n",
                "\n",
                "def _core_call_lowering(ctx, *args, name, backend=None, call_jaxpr):\n",
                "  out_nodes, tokens = _call_lowering(\n",
                "      name, name, call_jaxpr, backend, ctx.module_context,\n",
                "      ctx.avals_in, ctx.avals_out, ctx.tokens_in, *args,\n",
                "      dim_var_values=ctx.dim_var_values)\n",
                "  ctx.set_tokens_out(tokens)\n",
                "  return out_nodes\n",
                "\n",
                "register_lowering(core.call_p, partial(_core_call_lowering, name=\"core_call\"))\n",
                "register_lowering(core.closed_call_p,\n",
                "                  partial(_core_call_lowering, name=\"core_closed_call\"))\n",
                "\n",
                "def broadcast_in_dim(ctx: LoweringRuleContext, op, aval_out: core.AbstractValue, *,\n",
                "                     broadcast_dimensions) -> ir.Value:\n",
                "  # Lower a possibly-dynamic broadcast_in_dim\n",
                "  if core.is_opaque_dtype(aval_out.dtype):  # type: ignore\n",
                "    return aval_out.dtype._rules.broadcast_in_dim_mlir(  # type: ignore\n",
                "        ctx, aval_out, op,\n",
                "        broadcast_dimensions=broadcast_dimensions)\n",
                "  if not core.is_constant_shape(aval_out.shape):  # type: ignore\n",
                "    shape = eval_dynamic_shape(ctx, aval_out.shape)  # type: ignore\n",
                "    return hlo.DynamicBroadcastInDimOp(\n",
                "        aval_to_ir_type(aval_out), op,\n",
                "        shape_tensor(shape),\n",
                "        dense_int_elements(broadcast_dimensions),\n",
                "    ).result\n",
                "  else:\n",
                "    return hlo.BroadcastInDimOp(\n",
                "        aval_to_ir_type(aval_out), op,\n",
                "        dense_int_elements(broadcast_dimensions)).result\n",
                "\n",
                "def multi_broadcast_in_dim(ctx: LoweringRuleContext,\n",
                "                           ops: Sequence[ir.Value],\n",
                "                           ops_avals: Sequence[core.AbstractValue],\n",
                "                           out_shape: core.Shape) -> Sequence[ir.Value]:\n",
                "  \"\"\"Broadcasts multiple ops to the out_shape.\"\"\"\n",
                "  out = []\n",
                "  for op, op_aval in zip(ops, ops_avals):\n",
                "    op_aval_shape = op_aval.shape  # type: ignore\n",
                "    if core.symbolic_equal_shape(op_aval_shape, out_shape):  # type: ignore\n",
                "      out.append(op)\n",
                "    else:\n",
                "      assert len(op_aval_shape) <= len(out_shape), (op_aval_shape, out_shape)\n",
                "      broadcast_dimensions = list(range(len(out_shape) - len(op_aval_shape), len(out_shape)))\n",
                "      out.append(broadcast_in_dim(ctx, op,\n",
                "                                  core.ShapedArray(out_shape, op_aval.dtype),  # type: ignore\n",
                "                                  broadcast_dimensions=broadcast_dimensions))\n",
                "  return out\n",
                "\n",
                "def reshape(ctx: LoweringRuleContext, op, aval_out: core.AbstractValue) -> ir.Value:\n",
                "  if core.is_opaque_dtype(aval_out.dtype):  # type: ignore\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "    # TODO(frostig,mattjj,necula): asserts a single physical aval, and a\n",
                    "    # particular reshape rule (reshape to the output physical aval's shape)\n"
                ],
                "parent_version_range": {
                    "start": 1310,
                    "end": 1310
                },
                "child_version_range": {
                    "start": 1312,
                    "end": 1314
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if core.is_opaque_dtype(aval_out.dtype):",
                        "start_line": 1309,
                        "end_line": 1310
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "reshape",
                        "signature": "def reshape(ctx: LoweringRuleContext, op, aval_out: core.AbstractValue)->ir.Value:",
                        "at_line": 1308
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: jax/interpreters/mlir.py\nCode:\n1307 1309    \n1308 1310    def reshape(ctx: LoweringRuleContext, op, aval_out: core.AbstractValue) -> ir.Value:\n1309 1311      if core.is_opaque_dtype(aval_out.dtype):  # type: ignore\n     1312  +     # TODO(frostig,mattjj,necula): asserts a single physical aval, and a\n     1313  +     # particular reshape rule (reshape to the output physical aval's shape)\n1310 1314        aval_out, = aval_out.dtype._rules.physical_avals(aval_out)  # type: ignore\n1311 1315      if not core.is_constant_shape(aval_out.shape):  # type: ignore\n1312 1316        shape = eval_dynamic_shape(ctx, aval_out.shape)  # type: ignore\n           ...\n",
                "file_path": "jax/interpreters/mlir.py",
                "identifiers_before": [],
                "identifiers_after": [],
                "prefix": [
                    "\n",
                    "def reshape(ctx: LoweringRuleContext, op, aval_out: core.AbstractValue) -> ir.Value:\n",
                    "  if core.is_opaque_dtype(aval_out.dtype):  # type: ignore\n"
                ],
                "suffix": [
                    "    aval_out, = aval_out.dtype._rules.physical_avals(aval_out)  # type: ignore\n",
                    "  if not core.is_constant_shape(aval_out.shape):  # type: ignore\n",
                    "    shape = eval_dynamic_shape(ctx, aval_out.shape)  # type: ignore\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    aval_out, = aval_out.dtype._rules.physical_avals(aval_out)  # type: ignore\n",
                "  if not core.is_constant_shape(aval_out.shape):  # type: ignore\n",
                "    shape = eval_dynamic_shape(ctx, aval_out.shape)  # type: ignore\n",
                "    return hlo.DynamicReshapeOp(\n",
                "        aval_to_ir_type(aval_out), op,\n",
                "        shape_tensor(shape),\n",
                "    ).result\n",
                "  else:\n",
                "    return hlo.ReshapeOp(aval_to_ir_type(aval_out), op).result\n",
                "\n",
                "def slice_op(ctx: LoweringRuleContext, x, aval_out, *,\n",
                "             start_indices, limit_indices, strides) -> ir.Value:\n",
                "  if core.is_opaque_dtype(aval_out.dtype):\n",
                "    return [aval_out.dtype._rules.slice_mlir(\n",
                "        ctx, aval_out, x, start_indices, limit_indices, strides)]\n",
                "\n",
                "  if any(not core.is_constant_shape(s) for s in (start_indices, limit_indices, strides)):\n",
                "    start_indices = eval_dynamic_shape(ctx, start_indices)\n",
                "    limit_indices = eval_dynamic_shape(ctx, limit_indices)\n",
                "    strides = eval_dynamic_shape(ctx, strides)\n",
                "    return hlo.RealDynamicSliceOp(aval_to_ir_type(aval_out),\n",
                "                                  x,\n",
                "                                  shape_tensor(start_indices),\n",
                "                                  shape_tensor(limit_indices),\n",
                "                                  shape_tensor(strides)).result\n",
                "  else:\n",
                "    return hlo.SliceOp(x,\n",
                "                       dense_int_elements(start_indices),\n",
                "                       dense_int_elements(limit_indices),\n",
                "                       dense_int_elements(strides)).result\n",
                "\n",
                "def dynamic_slice(ctx: LoweringRuleContext, aval_out, x, *,\n",
                "                  start_indices) -> ir.Value:\n",
                "  if core.is_opaque_dtype(aval_out.dtype):\n",
                "    return aval_out.dtype._rules.dynamic_slice_mlir(ctx, aval_out, x,\n",
                "                                                    start_indices)\n",
                "  slice_sizes = aval_out.shape\n",
                "  if not core.is_constant_shape(slice_sizes):\n",
                "    slice_sizes = eval_dynamic_shape(ctx, slice_sizes)\n",
                "    return hlo.RealDynamicSliceOp(\n",
                "        aval_to_ir_type(aval_out), x,\n",
                "        shape_tensor(start_indices),\n",
                "        hlo.AddOp(shape_tensor(start_indices),\n",
                "                  shape_tensor(slice_sizes)).result,\n",
                "        shape_tensor([1] * len(slice_sizes))\n",
                "    ).result\n",
                "  else:\n",
                "    return hlo.DynamicSliceOp(x, start_indices,\n",
                "                              dense_int_elements(slice_sizes)).result\n",
                "\n",
                "def dynamic_update_slice(ctx: LoweringRuleContext, aval_out, x, update, *,\n",
                "                         start_indices) -> ir.Value:\n",
                "  if core.is_opaque_dtype(aval_out.dtype):\n",
                "    return aval_out.dtype._rules.dynamic_update_slice_mlir(\n",
                "        ctx, aval_out, x, update, *start_indices)\n",
                "\n",
                "  # TODO(necula): handle dynamic shapes\n",
                "  if mlir_api_version < 40:\n",
                "    return hlo.DynamicUpdateSliceOp(\n",
                "        aval_to_ir_type(aval_out), x, update, start_indices).result\n",
                "  else:\n",
                "    return hlo.DynamicUpdateSliceOp(x, update, start_indices).result\n",
                "\n",
                "def pad(ctx: LoweringRuleContext, aval_out,\n",
                "        x, padding_value,\n",
                "        padding_low, padding_high, padding_interior) -> ir.Value:\n",
                "  if all(core.is_constant_shape(s) for s in (padding_low,\n",
                "                                             padding_high, padding_interior)):\n",
                "    return hlo.PadOp(x, padding_value,\n",
                "                     dense_int_elements(padding_low),\n",
                "                     dense_int_elements(padding_high),\n",
                "                     dense_int_elements(padding_interior)).result\n",
                "  else:\n",
                "    padding_low = shape_tensor(eval_dynamic_shape(ctx, padding_low))\n",
                "    padding_high = shape_tensor(eval_dynamic_shape(ctx, padding_high))\n",
                "    padding_interior = shape_tensor(eval_dynamic_shape(ctx, padding_interior))\n",
                "    return hlo.DynamicPadOp(\n",
                "        aval_to_ir_type(aval_out),\n",
                "        x, padding_value, padding_low, padding_high, padding_interior).result\n",
                "\n",
                "def full_like_aval(ctx: LoweringRuleContext, value, aval: core.ShapedArray) -> ir.Value:\n",
                "  \"\"\"Returns an IR constant shaped full of `value` shaped like `aval`.\"\"\"\n",
                "  zero = ir_constant(np.array(value, aval.dtype))\n",
                "  return broadcast_in_dim(ctx, zero, aval, broadcast_dimensions=())\n",
                "\n",
                "def zeros_like_lowering(ctx, x):\n",
                "  aval, = ctx.avals_in\n",
                "  assert isinstance(aval, core.ShapedArray), aval\n",
                "  return [full_like_aval(ctx, 0, aval)]\n",
                "register_lowering(ad_util.zeros_like_p, zeros_like_lowering)\n",
                "\n",
                "def add_jaxvals_lowering(ctx, x, y):\n",
                "  return hlo.AddOp(x, y).results\n",
                "register_lowering(ad_util.add_jaxvals_p, add_jaxvals_lowering)\n",
                "\n",
                "register_lowering(ad_util.stop_gradient_p, lambda ctx, x: [x])\n",
                "\n",
                "\n",
                "def compare_hlo(x, y, direction: str, comparison_type: Optional[str] = None):\n",
                "  \"\"\"Creates CompareOp.\"\"\"\n",
                "  if comparison_type is None:\n",
                "    elem_type = ir.RankedTensorType(x.type).element_type\n",
                "    if ir.IntegerType.isinstance(elem_type):\n",
                "      comparison_type = (\"UNSIGNED\" if ir.IntegerType.is_unsigned(elem_type)\n",
                "                         else \"SIGNED\")\n",
                "    else:\n",
                "      comparison_type = \"FLOAT\"\n",
                "\n",
                "  return hlo.CompareOp(\n",
                "      x,\n",
                "      y,\n",
                "      hlo.ComparisonDirectionAttr.get(direction),\n",
                "      compare_type=hlo.ComparisonTypeAttr.get(comparison_type))\n",
                "\n",
                "def _minmax_hlo(op, cmp, x, y):\n",
                "  \"\"\"Min/max that compares complex values lexicographically as pairs.\"\"\"\n",
                "  tensor_type = ir.RankedTensorType(x.type)\n",
                "  if ir.ComplexType.isinstance(tensor_type.element_type):\n",
                "    rx = hlo.RealOp(x).result\n",
                "    ry = hlo.RealOp(y).result\n",
                "    real_eq = compare_hlo(rx, ry, \"EQ\", \"FLOAT\")\n",
                "    real_cmp = compare_hlo(rx, ry, cmp, \"FLOAT\")\n",
                "    imag_cmp = compare_hlo(\n",
                "        hlo.ImagOp(x).result,\n",
                "        hlo.ImagOp(y).result, cmp, \"FLOAT\")\n",
                "    which = hlo.SelectOp(real_eq, imag_cmp, real_cmp).result\n",
                "    return hlo.SelectOp(which, x, y)\n",
                "  else:\n",
                "    return op(x, y)\n",
                "\n",
                "min_hlo = partial(_minmax_hlo, hlo.MinOp, \"LT\")\n",
                "max_hlo = partial(_minmax_hlo, hlo.MaxOp, \"GT\")\n",
                "\n",
                "\n",
                "def convert_hlo(ctx: LoweringRuleContext, x, aval_in, aval_out):\n",
                "  \"\"\"Variant of convert that has HLO semantics.\n",
                "\n",
                "  In particular, treat casts to boolean as x != 0, rather than truncating\n",
                "  integer values (b/209440332).\"\"\"\n",
                "  if (not core.is_opaque_dtype(aval_out.dtype) and\n",
                "      aval_out.dtype == np.dtype(np.bool_)):\n",
                "    if dtypes.issubdtype(aval_in.dtype, np.inexact):\n",
                "      compare_type = \"FLOAT\"\n",
                "    elif dtypes.issubdtype(aval_in.dtype, np.signedinteger):\n",
                "      compare_type = \"SIGNED\"\n",
                "    else:\n",
                "      compare_type = \"UNSIGNED\"\n",
                "    return compare_hlo(x, full_like_aval(ctx, 0, aval_in), \"NE\",\n",
                "                       compare_type).result\n",
                "  return hlo.ConvertOp(aval_to_ir_type(aval_out), x).result\n",
                "\n",
                "def _wrap_with_spmd_op(name: str,\n",
                "                       result_type: ir.Type,\n",
                "                       x: ir.Value,\n",
                "                       sharding_proto: xc.OpSharding,\n",
                "                       unspecified_dims: Optional[Set[int]] = None):\n",
                "  # unspecified_dims indicate dimensions whose shardings are not specified and\n",
                "  # XLA sharding propagation can change them.\n",
                "  if unspecified_dims:\n",
                "    backend_config = \"unspecified_dims=[\" + \",\".join(\n",
                "        [str(i) for i in sorted(unspecified_dims)]) + \"]\"\n",
                "  else:\n",
                "    backend_config = \"\"\n",
                "  op = hlo.CustomCallOp([result_type], [x],\n",
                "                        call_target_name=ir.StringAttr.get(name),\n",
                "                        has_side_effect=ir.BoolAttr.get(False),\n",
                "                        backend_config=ir.StringAttr.get(backend_config),\n",
                "                        api_version=i32_attr(1),\n",
                "                        called_computations=ir.ArrayAttr.get([]),\n",
                "                        operand_layouts=None,\n",
                "                        result_layouts=None)\n",
                "  op.attributes[\"mhlo.sharding\"] = ir.StringAttr.get(\n",
                "      sharding_proto.SerializeToString())\n",
                "  return op.result\n",
                "\n",
                "def wrap_with_sharding_op(x: ir.Value,\n",
                "                          sharding_proto: xc.OpSharding,\n",
                "                          unspecified_dims: Optional[Set[int]] = None):\n",
                "  return _wrap_with_spmd_op(\"Sharding\", x.type, x, sharding_proto,\n",
                "                            unspecified_dims)\n",
                "\n",
                "wrap_with_full_to_shard_op = partial(_wrap_with_spmd_op, \"SPMDFullToShardShape\")\n",
                "wrap_with_shard_to_full_op = partial(_wrap_with_spmd_op, \"SPMDShardToFullShape\")\n",
                "\n",
                "def set_sharding(op, sharding_proto: xc.OpSharding):\n",
                "  op.attributes[\"mhlo.sharding\"] = ir.StringAttr.get(\n",
                "      sharding_proto.SerializeToString())\n",
                "\n",
                "# MLIR lowerings for lax primitives\n",
                "\n",
                "def cache_lowering(f):\n",
                "  \"\"\"Decorator that causes the contents of a lowering rule to be reused.\n",
                "\n",
                "  The lowering will be emitted out-of-line in a separate function, together with\n",
                "  a call to that function. If the same primitive is called with the same shapes\n",
                "  and parameters, a new call to the original function will be added, without\n",
                "  emitting a new function.\n",
                "  \"\"\"\n",
                "  @functools.wraps(f)\n",
                "  def cached_lowering(ctx, *args, **params):\n",
                "    assert ctx.primitive is not None\n",
                "    key = (ctx.primitive, tuple(ctx.avals_in), tuple(ctx.avals_out),\n",
                "           tuple(params.items()))\n",
                "    try:\n",
                "      func = ctx.module_context.cached_primitive_lowerings.get(key)\n",
                "    except TypeError:\n",
                "      # If the parameters aren't hashable, give up on caching.\n",
                "      # TODO(phawkins): switch to requiring hashability, when XLA fallback\n",
                "      # computations have been ported to MLIR.\n",
                "      return f(ctx, *args, **params)\n",
                "    if func is None:\n",
                "      func = _emit_lowering_rule_as_fun(partial(f, **params), ctx)\n",
                "      ctx.module_context.cached_primitive_lowerings[key] = func\n",
                "\n",
                "    output_types = map(aval_to_ir_types, ctx.avals_out)\n",
                "    args = tuple(ctx.dim_var_values) + args\n",
                "    flat_output_types = util.flatten(output_types)\n",
                "    call = func_dialect.CallOp(flat_output_types,\n",
                "                               ir.FlatSymbolRefAttr.get(func.name.value),\n",
                "                               flatten_lowering_ir_args(args))\n",
                "    return util.unflatten(call.results, map(len, output_types))\n",
                "  return cached_lowering\n",
                "\n",
                "\n",
                "\n",
                "def xla_computation_to_mlir_module(xla_computation: xc.XlaComputation\n",
                "                                  ) -> ir.Module:\n",
                "  module_str = xc._xla.mlir.xla_computation_to_mlir_module(xla_computation)\n",
                "  return ir.Module.parse(module_str)\n",
                "\n",
                "def merge_mlir_modules(dst_module: ir.Module,\n",
                "                       sym_name: str,\n",
                "                       src_module: ir.Module) -> str:\n",
                "  \"\"\"Returns the name of src_module's main() function, after renaming.\"\"\"\n",
                "  callee_name = None\n",
                "  assert dst_module.context == src_module.context\n",
                "  dst_symtab = ir.SymbolTable(dst_module.operation)\n",
                "\n",
                "  n = len(dst_module.body.operations)\n",
                "  for op in src_module.body.operations:\n",
                "    dst_module.body.append(op)\n",
                "  ops = list(dst_module.body.operations)[n:]\n",
                "\n",
                "  for op in ops:\n",
                "    op = typing.cast(func_dialect.FuncOp, op)\n",
                "    old_name = op.name.value\n",
                "    if op.name.value == \"main\":\n",
                "      dst_symtab.set_symbol_name(op, sym_name)\n",
                "      op.attributes[\"sym_visibility\"] = ir.StringAttr.get(\"private\")\n",
                "      callee_name = ir.StringAttr(dst_symtab.insert(op)).value\n",
                "      new_name = callee_name\n",
                "    else:\n",
                "      new_name = ir.StringAttr(dst_symtab.insert(op)).value\n",
                "\n",
                "    # Replace references to the symbol with the new name\n",
                "    for other_op in ops:\n",
                "      dst_symtab.replace_all_symbol_uses(\n",
                "          old_name, new_name, other_op.operation)\n",
                "\n",
                "\n",
                "  assert callee_name is not None\n",
                "  return callee_name\n",
                "\n",
                "\n",
                "def xla_fallback_lowering(prim: core.Primitive):\n",
                "  @cache_lowering\n",
                "  def fallback(ctx: LoweringRuleContext, *args, **params):\n",
                "    module_ctx = ctx.module_context\n",
                "    axis_ctx = module_ctx.axis_context\n",
                "    if isinstance(axis_ctx, SPMDAxisContext):\n",
                "      axis_env = axis_ctx.unsafe_axis_env\n",
                "    else:\n",
                "      axis_env = module_ctx.axis_env\n",
                "\n",
                "    if any(hasattr(a, \"shape\") and\n",
                "           not core.is_constant_shape(a.shape) for a in (ctx.avals_in + ctx.avals_out)):\n",
                "      raise NotImplementedError(\n",
                "          f\"Shape polymorphism for xla_fallback_lowering is not implemented ({ctx.primitive}); b/261682623\")\n",
                "\n",
                "    xla_computation = xla.primitive_subcomputation(\n",
                "        module_ctx.platform, axis_env, prim, ctx.avals_in,\n",
                "        ctx.avals_out, **params)\n",
                "    xla_module = xla_computation_to_mlir_module(xla_computation)\n",
                "    callee_name = merge_mlir_modules(\n",
                "        module_ctx.module, f\"xla_fallback_{prim.name}\", xla_module)\n",
                "    output_types = map(aval_to_ir_types, ctx.avals_out)\n",
                "    flat_output_types = util.flatten(output_types)\n",
                "    output_type = (ir.TupleType.get_tuple(flat_output_types)\n",
                "                   if prim.multiple_results else flat_output_types[0])\n",
                "\n",
                "    call = func_dialect.CallOp([output_type],\n",
                "                               ir.FlatSymbolRefAttr.get(callee_name),\n",
                "                               flatten_lowering_ir_args(args)).result\n",
                "    if not prim.multiple_results:\n",
                "      return [call]\n",
                "    flat_results = [hlo.GetTupleElementOp(call, i32_attr(i)).result\n",
                "                    for i in range(len(flat_output_types))]\n",
                "\n",
                "    return util.unflatten(flat_results, map(len, output_types))\n",
                "  return fallback\n",
                "\n",
                "register_lowering(ad.custom_lin_p, ad._raise_custom_vjp_error_on_jvp)\n",
                "\n",
                "DEVICE_TO_DEVICE_TYPE = 1\n",
                "SEND_TO_HOST_TYPE = 2\n",
                "RECV_FROM_HOST_TYPE = 3\n",
                "\n",
                "_dtype_to_xla_type_string_map = {\n",
                "    np.dtype(\"bool\"): \"pred\",\n",
                "    np.dtype(\"float16\"): \"f16\",\n",
                "    np.dtype(\"float32\"): \"f32\",\n",
                "    np.dtype(\"float64\"): \"f64\",\n",
                "    np.dtype(\"int8\"): \"s8\",\n",
                "    np.dtype(\"uint8\"): \"u8\",\n",
                "    np.dtype(\"int16\"): \"s16\",\n",
                "    np.dtype(\"uint16\"): \"u16\",\n",
                "    np.dtype(\"int32\"): \"s32\",\n",
                "    np.dtype(\"uint32\"): \"u32\",\n",
                "    np.dtype(\"int64\"): \"s64\",\n",
                "    np.dtype(\"uint64\"): \"u64\",\n",
                "    dtypes._bfloat16_dtype: \"bf16\",\n",
                "    np.dtype(\"complex64\"): \"c64\",\n",
                "    np.dtype(\"complex128\"): \"c128\",\n",
                "}\n",
                "\n",
                "def _dtype_to_xla_type_string(dtype: np.dtype) -> str:\n",
                "  if dtype not in _dtype_to_xla_type_string_map:\n",
                "    raise NotImplementedError(dtype)\n",
                "  return _dtype_to_xla_type_string_map[dtype]\n",
                "\n",
                "def send_to_host(channel: int, token: hlo.TokenType, operand: Any,\n",
                "                 aval: core.ShapedArray, name: str, *,\n",
                "                 sharding: Optional[xc.OpSharding] = None) -> ir.Value:\n",
                "  channel_handle = hlo.ChannelHandle.get(channel, SEND_TO_HOST_TYPE)\n",
                "  if mlir_api_version < 40:\n",
                "    send_op = hlo.SendOp(hlo.TokenType.get(), [operand], token, channel_handle,\n",
                "                         is_host_transfer=ir.BoolAttr.get(True))\n",
                "  else:\n",
                "    send_op = hlo.SendOp([operand], token, channel_handle,\n",
                "                         is_host_transfer=ir.BoolAttr.get(True))\n",
                "  dtype_str = _dtype_to_xla_type_string(aval.dtype)\n",
                "  if dtype_str in {\"f64\", \"s64\", \"u64\", \"c64\", \"c128\"}:\n",
                "    raise NotImplementedError(\"64-bit types not supported.\")\n",
                "  send_op.attributes[\"mhlo.frontend_attributes\"] = ir.DictAttr.get(\n",
                "      dict(\n",
                "          _xla_host_transfer_handler_name=ir.StringAttr.get(str(name)),\n",
                "          _xla_host_transfer_original_type=ir.StringAttr.get(dtype_str),\n",
                "          _xla_host_transfer_rendezvous=ir.StringAttr.get(str(name))))\n",
                "  if sharding is not None:\n",
                "    set_sharding(send_op, sharding)\n",
                "  return send_op.result\n",
                "\n",
                "\n",
                "def receive_from_host(channel: int, token: hlo.TokenType,\n",
                "                      out_aval: core.ShapedArray, name: str, *,\n",
                "                      sharding: Optional[xc.OpSharding] = None) -> ir.Value:\n",
                "  channel_handle = hlo.ChannelHandle.get(channel, RECV_FROM_HOST_TYPE)\n",
                "  recv_op = hlo.RecvOp([aval_to_ir_type(out_aval),\n",
                "                        hlo.TokenType.get()], token, channel_handle,\n",
                "                        is_host_transfer=ir.BoolAttr.get(True))\n",
                "  dtype_str = _dtype_to_xla_type_string(out_aval.dtype)\n",
                "  if dtype_str in {\"f64\", \"s64\", \"u64\", \"c64\", \"c128\"}:\n",
                "    raise NotImplementedError(\"64-bit types not supported.\")\n",
                "  recv_op.attributes[\"mhlo.frontend_attributes\"] = ir.DictAttr.get(\n",
                "      dict(\n",
                "          _xla_host_transfer_handler_name=ir.StringAttr.get(str(name)),\n",
                "          _xla_host_transfer_original_type=ir.StringAttr.get(dtype_str),\n",
                "          _xla_host_transfer_rendezvous=ir.StringAttr.get(str(name))))\n",
                "  if sharding is not None:\n",
                "    set_sharding(recv_op, sharding)\n",
                "  # Token should be at the end of the results\n",
                "  result, token = recv_op.results\n",
                "  return token, result\n",
                "\n",
                "\n",
                "def _emit_tpu_python_callback(\n",
                "    backend: xb.XlaBackend,\n",
                "    ctx: LoweringRuleContext,\n",
                "    callback,\n",
                "    token: Optional[Any],\n",
                "    operands: List[ir.Value],\n",
                "    operand_avals: List[core.ShapedArray],\n",
                "    operand_shapes: List[xc.Shape],\n",
                "    result_avals: List[core.ShapedArray],\n",
                "    result_shapes: List[xc.Shape],\n",
                "    *,\n",
                "    sharding: Optional[xc.OpSharding] = None\n",
                ") -> Tuple[List[ir.Value], Any, Any]:\n",
                "  if mlir_api_version < 40:\n",
                "    token = token or hlo.CreateTokenOp(hlo.TokenType.get()).result\n",
                "  else:\n",
                "    token = token or hlo.CreateTokenOp().result\n",
                "  _wrapped_callback = callback\n",
                "\n",
                "  send_channels = []\n",
                "  if not operand_avals:\n",
                "    # If there are no operands to the callback, we need to insert a dummy send\n",
                "    # op or the callback will never be triggered!\n",
                "    # TODO(sharadmv,chky): Enable this fix in the runtime as opposed to in\n",
                "    # MLIR builder.\n",
                "    callback_without_args = _wrapped_callback\n",
                "    def _wrapped_callback(*args):  # pylint: disable=function-redefined\n",
                "      del args\n",
                "      return callback_without_args()\n",
                "    send_channel = ctx.module_context.new_channel()\n",
                "    dummy_send_aval = core.ShapedArray((1,), np.float32)\n",
                "    dummy_send_val = ir_constant(np.zeros(1, np.float32))\n",
                "    operand_shapes = [*operand_shapes,\n",
                "                      xla.aval_to_xla_shapes(dummy_send_aval)[0]]\n",
                "    token = send_to_host(send_channel, token, dummy_send_val, dummy_send_aval,\n",
                "                         callback.__name__, sharding=sharding)\n",
                "    send_channels.append(send_channel)\n",
                "  else:\n",
                "    for operand, operand_aval in zip(operands, operand_avals):\n",
                "      if any(s == 0 for s in operand_aval.shape):\n",
                "        raise NotImplementedError(\n",
                "            \"Callbacks with zero-dimensional values not supported on TPU.\")\n",
                "      channel = ctx.module_context.new_channel()\n",
                "      token = send_to_host(channel, token, operand, operand_aval,\n",
                "                           callback.__name__, sharding=sharding)\n",
                "      send_channels.append(channel)\n",
                "\n",
                "  recv_channels = []\n",
                "  outputs = []\n",
                "  for result_aval in result_avals:\n",
                "    if any(s == 0 for s in result_aval.shape):\n",
                "      raise NotImplementedError(\n",
                "          \"Callbacks with zero-dimensional values not supported on TPU.\")\n",
                "    channel = ctx.module_context.new_channel()\n",
                "    assert isinstance(result_aval, core.ShapedArray)\n",
                "    token, out = receive_from_host(channel, token, result_aval,\n",
                "                                   callback.__name__, sharding=sharding)\n",
                "    outputs.append(out)\n",
                "    recv_channels.append(channel)\n",
                "  opaque = backend.make_python_callback_from_host_send_and_recv(\n",
                "      _wrapped_callback, operand_shapes, result_shapes, send_channels,\n",
                "      recv_channels)\n",
                "  ctx.module_context.add_host_callback(opaque)\n",
                "  return outputs, token, opaque\n",
                "\n",
                "def _layout_to_mlir_layout(minor_to_major: Optional[Sequence[int]]):\n",
                "  if minor_to_major is None:\n",
                "    # Needed for token layouts\n",
                "    layout = np.zeros((0,), dtype=\"int64\")\n",
                "  else:\n",
                "    layout = np.array(minor_to_major, dtype=\"int64\")\n",
                "  return ir.DenseIntElementsAttr.get(layout, type=ir.IndexType.get())\n",
                "\n",
                "def _aval_to_default_layout(aval):\n",
                "  # Row major order is default for `NumPy`.\n",
                "  return list(range(aval.ndim - 1, -1, -1))\n",
                "\n",
                "def emit_python_callback(\n",
                "    ctx: LoweringRuleContext, callback, token: Optional[Any],\n",
                "    operands: List[ir.Value], operand_avals: List[core.ShapedArray],\n",
                "    result_avals: List[core.ShapedArray],\n",
                "    has_side_effect: bool, *, sharding: Optional[xc.OpSharding] = None,\n",
                "    operand_layouts: Optional[Sequence[Optional[Sequence[int]]]] = None,\n",
                "    result_layouts: Optional[Sequence[Optional[Sequence[int]]]] = None,\n",
                "    ) -> Tuple[List[ir.Value], Any, Any]:\n",
                "  \"\"\"Emits MLIR that calls back to a provided Python function.\"\"\"\n",
                "  platform = ctx.module_context.platform\n",
                "  if platform not in {\"cpu\", \"cuda\", \"rocm\", \"tpu\"}:\n",
                "    raise ValueError(\n",
                "        f\"`EmitPythonCallback` not supported on {platform} backend.\")\n",
                "  backend = ctx.module_context.backend\n",
                "  result_shapes = util.flatten(\n",
                "      [xla.aval_to_xla_shapes(result_aval) for result_aval in result_avals])\n",
                "  operand_shapes = util.flatten(\n",
                "      [xla.aval_to_xla_shapes(op_aval) for op_aval in operand_avals])\n",
                "  # Handling layouts\n",
                "  if operand_layouts is None:\n",
                "    operand_layouts = map(_aval_to_default_layout, operand_avals)\n",
                "  operand_mlir_layouts = [\n",
                "      _layout_to_mlir_layout(_aval_to_default_layout(layout)) if layout is None\n",
                "      else _layout_to_mlir_layout(layout) for layout, aval\n",
                "      in zip(operand_layouts, operand_avals)]\n",
                "  if result_layouts is None:\n",
                "    result_layouts = map(_aval_to_default_layout, result_avals)\n",
                "  result_mlir_layouts = [\n",
                "      _layout_to_mlir_layout(_aval_to_default_layout(aval)) if layout is None\n",
                "      else _layout_to_mlir_layout(layout) for layout, aval\n",
                "      in zip(result_layouts, result_avals)]\n",
                "\n",
                "  # First we apply checks to ensure output shapes and dtypes match the expected\n",
                "  # ones.\n",
                "  def _wrapped_callback(*args):\n",
                "    out_vals = callback(*args)\n",
                "    if len(out_vals) != len(result_avals):\n",
                "      raise RuntimeError(\n",
                "          \"Mismatched number of outputs from callback. \"\n",
                "          \"Expected: {}, Actual: {}\".format(len(result_avals), len(out_vals)))\n",
                "    for i, (out_val, out_aval) in enumerate(zip(out_vals, result_avals)):\n",
                "      if out_val.shape != out_aval.shape:\n",
                "        raise RuntimeError(\n",
                "            f\"Incorrect output shape for return value {i}: \"\n",
                "            \"Expected: {}, Actual: {}\".format(out_aval.shape, out_val.shape))\n",
                "      if out_val.dtype != out_aval.dtype:\n",
                "        raise RuntimeError(\n",
                "            f\"Incorrect output dtype for return value {i}: \"\n",
                "            \"Expected: {}, Actual: {}\".format(out_aval.dtype, out_val.dtype))\n",
                "    return out_vals\n",
                "\n",
                "  if platform == \"tpu\":\n",
                "    return _emit_tpu_python_callback(backend, ctx, _wrapped_callback,  token,\n",
                "        operands, operand_avals, operand_shapes, result_avals, result_shapes,\n",
                "        sharding=sharding)\n",
                "  result_types = util.flatten([aval_to_ir_types(aval) for aval in result_avals])\n",
                "  if token:\n",
                "\n",
                "    callback_without_token = _wrapped_callback\n",
                "    def _wrapped_callback(token, *args):  # type: ignore  # pylint: disable=function-redefined\n",
                "      return (token, *callback_without_token(*args))\n",
                "\n",
                "    operand_shapes = [\n",
                "        xla.aval_to_xla_shapes(core.abstract_token)[0], *operand_shapes\n",
                "    ]\n",
                "    result_shapes = [\n",
                "        xla.aval_to_xla_shapes(core.abstract_token)[0], *result_shapes\n",
                "    ]\n",
                "    operands = [token, *operands]\n",
                "    result_types = [token_type()[0], *result_types]\n",
                "    operand_mlir_layouts = [_layout_to_mlir_layout(None), *operand_mlir_layouts]\n",
                "    result_mlir_layouts = [_layout_to_mlir_layout(None), *result_mlir_layouts]\n",
                "  callback_descriptor, keepalive = (\n",
                "      backend.get_emit_python_callback_descriptor(_wrapped_callback,\n",
                "                                                  operand_shapes,\n",
                "                                                  result_shapes))\n",
                "  descriptor_operand = ir_constant(\n",
                "      callback_descriptor, canonicalize_types=False)\n",
                "  callback_operands = [descriptor_operand, *operands]\n",
                "  if operand_mlir_layouts is not None:\n",
                "    operand_mlir_layouts = [_layout_to_mlir_layout([]), *operand_mlir_layouts]\n",
                "  result_type = ir.TupleType.get_tuple(result_types)\n",
                "  call_target_name = (\"xla_python_gpu_callback\"\n",
                "                     if platform in {\"cuda\", \"rocm\"} else \"xla_python_cpu_callback\")\n",
                "  result = hlo.CustomCallOp(\n",
                "      [result_type],\n",
                "      callback_operands,\n",
                "      call_target_name=ir.StringAttr.get(call_target_name),\n",
                "      has_side_effect=ir.BoolAttr.get(has_side_effect),\n",
                "      api_version=i32_attr(2),\n",
                "      called_computations=ir.ArrayAttr.get([]),\n",
                "      backend_config=ir.StringAttr.get(str(callback_descriptor)),\n",
                "      operand_layouts=(\n",
                "        None if operand_mlir_layouts is None\n",
                "        else ir.ArrayAttr.get(operand_mlir_layouts)),\n",
                "      result_layouts=(\n",
                "        None if result_mlir_layouts is None\n",
                "        else ir.ArrayAttr.get(result_mlir_layouts)))\n",
                "  if sharding is not None:\n",
                "    set_sharding(result, sharding)\n",
                "  results = [\n",
                "      hlo.GetTupleElementOp(result, i32_attr(i)).result\n",
                "      for i in range(len(result_types))\n",
                "  ]\n",
                "  if token:\n",
                "    token, *results = results\n",
                "  return results, token, keepalive\n",
                "\n",
                "def build_xla_computation_helper(\n",
                "    closed_jaxpr: core.ClosedJaxpr, *, name: str, platform: str,\n",
                "    backend_or_name: str, axis_context: AxisContext) -> xc.XlaComputation:\n",
                "  \"\"\"Helper to generate pmap-style XLA computations for custom partitioners.\"\"\"\n",
                "  if closed_jaxpr.effects:\n",
                "    raise NotImplementedError\n",
                "  lowering_result = lower_jaxpr_to_module(name, closed_jaxpr,\n",
                "      backend_or_name=backend_or_name, unordered_effects=[], ordered_effects=[],\n",
                "      name_stack=source_info_util.NameStack(),\n",
                "      donated_args=[False] * len(closed_jaxpr.jaxpr.invars),\n",
                "      axis_context=axis_context, platform=platform)\n",
                "  return xc._xla.mlir.mlir_module_to_xla_computation(\n",
                "      module_to_string(lowering_result.module), use_tuple_args=False,\n",
                "      return_tuple=False)\n",
                "\n",
                "# Lax ops missing MLIR lowerings.\n",
                "# # TODO(b/203775215): these are missing from the cHLO dialect. Either add\n",
                "# # them or port them to Python.\n",
                "# lax.igamma_p,\n",
                "# lax.igammac_p,\n",
                "# lax.igamma_grad_a,\n",
                "# lax.random_gamma_grad_p,\n",
                "# lax.bessel_i0e_p,\n",
                "# lax.bessel_i1e_p,\n",
                "# lax.erf_inv_p,\n",
                "# lax.regularized_incomplete_beta_p,\n",
                "\n",
                "# # CHLO doesn't have a legalization for bf16 (b/203774470)\n",
                "# lax.erf_p,\n",
                "# lax.erfc_p,"
            ]
        ],
        "tests/lax_test.py": [
            [
                "# Copyright 2018 The JAX Authors.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     https://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "from __future__ import annotations\n",
                "\n",
                "import collections\n",
                "from functools import partial\n",
                "import itertools\n",
                "import operator\n",
                "import types\n",
                "import unittest\n",
                "from unittest import SkipTest\n",
                "from typing import Tuple\n",
                "\n",
                "from absl.testing import absltest\n",
                "from absl.testing import parameterized\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "import jax\n",
                "from jax import core\n",
                "from jax import lax\n",
                "import jax.numpy as jnp\n",
                "from jax.test_util import check_grads\n",
                "from jax import tree_util\n",
                "import jax.util\n",
                "\n",
                "from jax.interpreters import xla\n",
                "from jax.interpreters import mlir\n",
                "from jax.interpreters import batching\n",
                "from jax.interpreters import pxla\n",
                "from jax._src import array\n",
                "from jax._src.lib.mlir.dialects import hlo\n",
                "from jax._src import dispatch\n",
                "from jax._src import dtypes\n",
                "from jax._src import test_util as jtu\n",
                "from jax._src import lax_reference\n",
                "from jax._src.util import prod\n",
                "from jax._src.lax import lax as lax_internal\n",
                "from jax._src.lib import xla_client as xc\n",
                "\n",
                "from jax.config import config\n",
                "config.parse_flags_with_absl()\n",
                "\n",
                "\n",
                "### lax tests\n",
                "\n",
                "# For standard unops and binops, we can generate a large number of tests on\n",
                "# arguments of appropriate shapes and dtypes using the following table.\n",
                "\n",
                "float_dtypes = jtu.dtypes.all_floating\n",
                "complex_elem_dtypes = jtu.dtypes.floating\n",
                "complex_dtypes = jtu.dtypes.complex\n",
                "inexact_dtypes = jtu.dtypes.all_inexact\n",
                "int_dtypes = jtu.dtypes.all_integer\n",
                "uint_dtypes = jtu.dtypes.all_unsigned\n",
                "bool_dtypes = jtu.dtypes.boolean\n",
                "default_dtypes = float_dtypes + int_dtypes\n",
                "all_dtypes = float_dtypes + complex_dtypes + int_dtypes + uint_dtypes + bool_dtypes\n",
                "python_scalar_types = [bool, int, float, complex]\n",
                "\n",
                "compatible_shapes = [[(3,)], [(3, 4), (3, 1), (1, 4)], [(2, 3, 4), (2, 1, 4)]]\n",
                "\n",
                "# We check cases where the preferred type is at least as wide as the input\n",
                "# type and where both are either both floating-point or both integral,\n",
                "# which are the only supported configurations.\n",
                "preferred_type_combinations = [\n",
                "  (np.float16, np.float16), (np.float16, np.float32), (np.float16, np.float64),\n",
                "  (dtypes.bfloat16, dtypes.bfloat16), (dtypes.bfloat16, np.float32),\n",
                "  (dtypes.bfloat16, np.float64), (np.float32, np.float32), (np.float32, np.float64),\n",
                "  (np.float64, np.float64), (np.int8, np.int8), (np.int8, np.int16), (np.int8, np.int32),\n",
                "  (np.int8, np.int64), (np.int16, np.int16), (np.int16, np.int32), (np.int16, np.int64),\n",
                "  (np.int32, np.int32), (np.int32, np.int64), (np.int64, np.int64),\n",
                "  (np.complex64, np.complex64), (np.complex64, np.complex128), (np.complex128, np.complex128),\n",
                "  (np.int8, np.float16), (np.int8, dtypes.bfloat16), (np.int8, np.float32), (np.int8, np.float64),\n",
                "  (np.int16, np.float16), (np.int16, dtypes.bfloat16), (np.int16, np.float32), (np.int16, np.float64),\n",
                "  (np.int32, np.float32), (np.int32, np.float64), (np.int64, np.float64)]\n",
                "\n",
                "\n",
                "OpRecord = collections.namedtuple(\n",
                "    \"OpRecord\", [\"op\", \"nargs\", \"dtypes\", \"rng_factory\", \"tol\"])\n",
                "\n",
                "def op_record(op, nargs, dtypes, rng_factory, tol=None):\n",
                "  return OpRecord(op, nargs, dtypes, rng_factory, tol)\n",
                "\n",
                "LAX_OPS = [\n",
                "    op_record(\"neg\", 1, default_dtypes + complex_dtypes, jtu.rand_small),\n",
                "    op_record(\"sign\", 1, default_dtypes + uint_dtypes, jtu.rand_small),\n",
                "    op_record(\"floor\", 1, float_dtypes, jtu.rand_small),\n",
                "    op_record(\"ceil\", 1, float_dtypes, jtu.rand_small),\n",
                "    op_record(\"round\", 1, float_dtypes, jtu.rand_default),\n",
                "    op_record(\"nextafter\", 2, [f for f in float_dtypes if f != dtypes.bfloat16],\n",
                "              jtu.rand_default, tol=0),\n",
                "\n",
                "    op_record(\"is_finite\", 1, float_dtypes, jtu.rand_small),\n",
                "\n",
                "    op_record(\"exp\", 1, float_dtypes + complex_dtypes, jtu.rand_small),\n",
                "    # TODO(b/142975473): on CPU, expm1 for float64 is only accurate to ~float32\n",
                "    # precision.\n",
                "    op_record(\"expm1\", 1, float_dtypes + complex_dtypes, jtu.rand_small,\n",
                "              {np.float64: 1e-8}),\n",
                "    op_record(\"log\", 1, float_dtypes + complex_dtypes, jtu.rand_positive),\n",
                "    op_record(\"log1p\", 1, float_dtypes + complex_dtypes, jtu.rand_positive),\n",
                "    # TODO(b/142975473): on CPU, tanh for complex128 is only accurate to\n",
                "    # ~float32 precision.\n",
                "    # TODO(b/143135720): on GPU, tanh has only ~float32 precision.\n",
                "    op_record(\"tanh\", 1, float_dtypes + complex_dtypes, jtu.rand_small,\n",
                "              {np.float64: 1e-9, np.complex128: 1e-7}),\n",
                "    op_record(\"logistic\", 1, float_dtypes + complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"sin\", 1, float_dtypes + complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"cos\", 1, float_dtypes + complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"atan2\", 2, float_dtypes, jtu.rand_default),\n",
                "\n",
                "    op_record(\"sqrt\", 1, float_dtypes, jtu.rand_positive),\n",
                "    op_record(\"sqrt\", 1, complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"rsqrt\", 1, float_dtypes, jtu.rand_positive),\n",
                "    op_record(\"rsqrt\", 1, complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"cbrt\", 1, float_dtypes, jtu.rand_default),\n",
                "    op_record(\"square\", 1, float_dtypes + complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"reciprocal\", 1, float_dtypes + complex_dtypes, jtu.rand_positive),\n",
                "    op_record(\"tan\", 1, float_dtypes + complex_dtypes, jtu.rand_default, {np.float32: 3e-5}),\n",
                "    op_record(\"asin\", 1, float_dtypes + complex_dtypes, jtu.rand_small, {np.complex128: 5e-12}),\n",
                "    op_record(\"acos\", 1, float_dtypes + complex_dtypes, jtu.rand_small),\n",
                "    op_record(\"atan\", 1, float_dtypes + complex_dtypes, jtu.rand_small),\n",
                "    op_record(\"asinh\", 1, float_dtypes + complex_dtypes, jtu.rand_default,\n",
                "              tol={np.complex64: 1E-4, np.complex128: 1E-5}),\n",
                "    op_record(\"acosh\", 1, float_dtypes + complex_dtypes, jtu.rand_positive),\n",
                "    # TODO(b/155331781): atanh has only ~float precision\n",
                "    op_record(\"atanh\", 1, float_dtypes + complex_dtypes, jtu.rand_small, {np.float64: 1e-9}),\n",
                "    op_record(\"sinh\", 1, float_dtypes + complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"cosh\", 1, float_dtypes + complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"lgamma\", 1, float_dtypes, jtu.rand_positive,\n",
                "              {np.float32: 1e-3 if jtu.device_under_test() == \"tpu\" else 1e-5,\n",
                "               np.float64: 1e-14}),\n",
                "    op_record(\"digamma\", 1, float_dtypes, jtu.rand_positive,\n",
                "              {np.float64: 1e-14}),\n",
                "    op_record(\"betainc\", 3, float_dtypes, jtu.rand_positive,\n",
                "              {np.float64: 1e-14}),\n",
                "    op_record(\"igamma\", 2,\n",
                "              [f for f in float_dtypes if f not in [dtypes.bfloat16, np.float16]],\n",
                "              jtu.rand_positive, {np.float64: 1e-14}),\n",
                "    op_record(\"igammac\", 2,\n",
                "              [f for f in float_dtypes if f not in [dtypes.bfloat16, np.float16]],\n",
                "              jtu.rand_positive, {np.float64: 1e-14}),\n",
                "    op_record(\"erf\", 1, float_dtypes, jtu.rand_small),\n",
                "    op_record(\"erfc\", 1, float_dtypes, jtu.rand_small),\n",
                "    # TODO(b/142976030): the approximation of erfinf used by XLA is only\n",
                "    # accurate to float32 precision.\n",
                "    op_record(\"erf_inv\", 1, float_dtypes, jtu.rand_small,\n",
                "              {np.float64: 1e-9}),\n",
                "    op_record(\"bessel_i0e\", 1, float_dtypes, jtu.rand_default),\n",
                "    op_record(\"bessel_i1e\", 1, float_dtypes, jtu.rand_default),\n",
                "\n",
                "    op_record(\"real\", 1, complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"imag\", 1, complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"complex\", 2, complex_elem_dtypes, jtu.rand_default),\n",
                "    op_record(\"conj\", 1, complex_elem_dtypes + complex_dtypes,\n",
                "              jtu.rand_default),\n",
                "    op_record(\"abs\", 1, default_dtypes + complex_dtypes, jtu.rand_default),\n",
                "    op_record(\"pow\", 2, float_dtypes + complex_dtypes, jtu.rand_positive),\n",
                "\n",
                "    op_record(\"bitwise_and\", 2, bool_dtypes, jtu.rand_small),\n",
                "    op_record(\"bitwise_not\", 1, bool_dtypes, jtu.rand_small),\n",
                "    op_record(\"bitwise_or\", 2, bool_dtypes, jtu.rand_small),\n",
                "    op_record(\"bitwise_xor\", 2, bool_dtypes, jtu.rand_small),\n",
                "    op_record(\"population_count\", 1, int_dtypes + uint_dtypes, jtu.rand_int),\n",
                "    op_record(\"clz\", 1, int_dtypes + uint_dtypes, jtu.rand_int),\n",
                "\n",
                "    op_record(\"add\", 2, default_dtypes + complex_dtypes, jtu.rand_small),\n",
                "    op_record(\"sub\", 2, default_dtypes + complex_dtypes, jtu.rand_small),\n",
                "    op_record(\"mul\", 2, default_dtypes + complex_dtypes, jtu.rand_small),\n",
                "    op_record(\"div\", 2, default_dtypes + complex_dtypes, jtu.rand_nonzero),\n",
                "    op_record(\"rem\", 2, default_dtypes, jtu.rand_nonzero),\n",
                "\n",
                "    op_record(\"max\", 2, all_dtypes, jtu.rand_small),\n",
                "    op_record(\"min\", 2, all_dtypes, jtu.rand_small),\n",
                "\n",
                "    op_record(\"eq\", 2, all_dtypes, jtu.rand_some_equal),\n",
                "    op_record(\"ne\", 2, all_dtypes, jtu.rand_small),\n",
                "    op_record(\"ge\", 2, default_dtypes, jtu.rand_small),\n",
                "    op_record(\"gt\", 2, default_dtypes, jtu.rand_small),\n",
                "    op_record(\"le\", 2, default_dtypes, jtu.rand_small),\n",
                "    op_record(\"lt\", 2, default_dtypes, jtu.rand_small),\n",
                "]\n",
                "\n",
                "ReducerOpRecord = collections.namedtuple(\n",
                "  \"ReducerOpRecord\", [\"op\", \"reference_op\", \"init_val\", \"dtypes\", \"primitive\"]\n",
                ")\n",
                "\n",
                "LAX_REDUCE_OPS = [\n",
                "  ReducerOpRecord(lax.add, np.add, 0, default_dtypes, lax.reduce_sum_p),\n",
                "  ReducerOpRecord(lax.mul, np.multiply, 1, default_dtypes, lax.reduce_prod_p),\n",
                "  ReducerOpRecord(lax.max, np.maximum, 0, uint_dtypes + bool_dtypes, lax.reduce_max_p),\n",
                "  ReducerOpRecord(lax.max, np.maximum, -np.inf, float_dtypes, lax.reduce_max_p),\n",
                "  ReducerOpRecord(lax.max, np.maximum, dtypes.iinfo(np.int32).min, [np.int32], lax.reduce_max_p),\n",
                "  ReducerOpRecord(lax.max, np.maximum, dtypes.iinfo(np.int64).min, [np.int64], lax.reduce_max_p),\n",
                "  ReducerOpRecord(lax.min, np.minimum, np.inf, float_dtypes, lax.reduce_min_p),\n",
                "  ReducerOpRecord(lax.min, np.minimum, dtypes.iinfo(np.int32).max, [np.int32], lax.reduce_min_p),\n",
                "  ReducerOpRecord(lax.min, np.minimum, dtypes.iinfo(np.int64).max, [np.int64], lax.reduce_min_p),\n",
                "  ReducerOpRecord(lax.min, np.minimum, dtypes.iinfo(np.uint32).max, [np.uint32], lax.reduce_min_p),\n",
                "  ReducerOpRecord(lax.min, np.minimum, dtypes.iinfo(np.uint64).max, [np.uint64], lax.reduce_min_p),\n",
                "  ReducerOpRecord(lax.bitwise_and, np.bitwise_and, -1, int_dtypes + uint_dtypes + bool_dtypes, lax.reduce_and_p),\n",
                "  ReducerOpRecord(lax.bitwise_or, np.bitwise_or, 0, int_dtypes + uint_dtypes + bool_dtypes, lax.reduce_or_p),\n",
                "  ReducerOpRecord(lax.bitwise_xor, np.bitwise_xor, 0, int_dtypes + uint_dtypes + bool_dtypes, lax.reduce_xor_p),\n",
                "]\n",
                "\n",
                "\n",
                "class LaxTest(jtu.JaxTestCase):\n",
                "  \"\"\"Numerical tests for LAX operations.\"\"\"\n",
                "\n",
                "  @parameterized.parameters(itertools.chain.from_iterable(\n",
                "      jtu.sample_product_testcases(\n",
                "        [dict(op_name=rec.op, rng_factory=rec.rng_factory)],\n",
                "        shapes=itertools.chain.from_iterable(\n",
                "          itertools.combinations_with_replacement(shape_group, rec.nargs)\n",
                "          for shape_group in compatible_shapes),\n",
                "        dtype=rec.dtypes)\n",
                "      for rec in LAX_OPS))\n",
                "  def testOp(self, op_name, rng_factory, shapes, dtype):\n",
                "    rng = rng_factory(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype) for shape in shapes]\n",
                "    op = getattr(lax, op_name)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  @parameterized.parameters(itertools.chain.from_iterable(\n",
                "      jtu.sample_product_testcases(\n",
                "        [dict(op_name=rec.op, rng_factory=rec.rng_factory, tol=rec.tol)],\n",
                "        shapes=itertools.chain.from_iterable(\n",
                "          itertools.combinations_with_replacement(shape_group, rec.nargs)\n",
                "          for shape_group in compatible_shapes),\n",
                "        dtype=rec.dtypes)\n",
                "      for rec in LAX_OPS))\n",
                "  def testOpAgainstNumpy(self, op_name, rng_factory, shapes, dtype, tol):\n",
                "    if (not config.x64_enabled and op_name == \"nextafter\"\n",
                "        and dtype == np.float64):\n",
                "      raise SkipTest(\"64-bit mode disabled\")\n",
                "    rng = rng_factory(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype) for shape in shapes]\n",
                "    op = getattr(lax, op_name)\n",
                "    numpy_op = getattr(lax_reference, op_name)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker, tol=tol)\n",
                "\n",
                "  # TODO test shift_left, shift_right_arithmetic, shift_right_logical\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(from_dtype=from_dtype, to_dtype=to_dtype)\n",
                "     for from_dtype, to_dtype in itertools.product(\n",
                "      [None, np.float32, np.int32, \"float32\", \"int32\"], repeat=2)],\n",
                "    weak_type=[False, True],\n",
                "  )\n",
                "  def testConvertElementType(self, from_dtype, to_dtype, weak_type):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng((2, 3), from_dtype)]\n",
                "    op = lambda x: lax_internal._convert_element_type(x, to_dtype, weak_type)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "    x = rng((1,), from_dtype)\n",
                "    out = op(x)\n",
                "    self.assertEqual(out.dtype, dtypes.canonicalize_dtype(to_dtype or x.dtype))\n",
                "    self.assertEqual(out.aval.weak_type, weak_type)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(from_dtype=from_dtype, to_dtype=to_dtype)\n",
                "     for from_dtype, to_dtype in itertools.product(\n",
                "      [np.float32, np.int32, \"float32\", \"int32\"], repeat=2)],\n",
                "  )\n",
                "  def testConvertElementTypeAgainstNumpy(self, from_dtype, to_dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng((2, 3), from_dtype)]\n",
                "    op = lambda x: lax.convert_element_type(x, to_dtype)\n",
                "    numpy_op = lambda x: lax_reference.convert_element_type(x, to_dtype)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(from_dtype=from_dtype, to_dtype=to_dtype)\n",
                "     for from_dtype, to_dtype in itertools.product(\n",
                "      [np.float32, np.int32, \"float32\", \"int32\"], repeat=2)],\n",
                "  )\n",
                "  def testBitcastConvertType(self, from_dtype, to_dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng((2, 3), from_dtype)]\n",
                "    op = lambda x: lax.bitcast_convert_type(x, to_dtype)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(from_dtype=from_dtype, to_dtype=to_dtype)\n",
                "     for from_dtype, to_dtype in itertools.product(\n",
                "       [np.float32, np.int32, \"float32\", \"int32\"], repeat=2)],\n",
                "  )\n",
                "  def testBitcastConvertTypeAgainstNumpy(self, from_dtype, to_dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng((2, 3), from_dtype)]\n",
                "    op = lambda x: lax.bitcast_convert_type(x, to_dtype)\n",
                "    numpy_op = lambda x: lax_reference.bitcast_convert_type(x, to_dtype)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(from_dtype=from_dtype, to_dtype=to_dtype)\n",
                "     for from_dtype, to_dtype in itertools.product(\n",
                "       [np.float32, np.int32, \"float32\", \"int32\"], repeat=2)],\n",
                "    weak_type=[False, True],\n",
                "  )\n",
                "  def testBitcastConvertWeakType(self, from_dtype, to_dtype, weak_type):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    x_in = lax_internal._convert_element_type(rng((2, 3), from_dtype),\n",
                "                                              weak_type=weak_type)\n",
                "    op = lambda x: lax.bitcast_convert_type(x, to_dtype)\n",
                "    self.assertEqual(dtypes.is_weakly_typed(x_in), weak_type)\n",
                "    x_out = op(x_in)\n",
                "    self.assertEqual(dtypes.is_weakly_typed(x_out), False)\n",
                "    x_out_jit = jax.jit(op)(x_in)\n",
                "    self.assertEqual(dtypes.is_weakly_typed(x_out_jit), False)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(min_shape=min_shape, operand_shape=operand_shape, max_shape=max_shape)\n",
                "     for min_shape, operand_shape, max_shape in [\n",
                "          [(), (2, 3), ()],\n",
                "          [(2, 3), (2, 3), ()],\n",
                "          [(), (2, 3), (2, 3)],\n",
                "          [(2, 3), (2, 3), (2, 3)],\n",
                "     ]],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testClamp(self, min_shape, operand_shape, max_shape, dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    shapes = [min_shape, operand_shape, max_shape]\n",
                "    args_maker = lambda: [rng(shape, dtype) for shape in shapes]\n",
                "    self._CompileAndCheck(lax.clamp, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(min_shape=min_shape, operand_shape=operand_shape, max_shape=max_shape)\n",
                "     for min_shape, operand_shape, max_shape in [\n",
                "          [(), (2, 3), ()],\n",
                "          [(2, 3), (2, 3), ()],\n",
                "          [(), (2, 3), (2, 3)],\n",
                "          [(2, 3), (2, 3), (2, 3)],\n",
                "     ]],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testClampAgainstNumpy(self, min_shape, operand_shape, max_shape, dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    shapes = [min_shape, operand_shape, max_shape]\n",
                "    args_maker = lambda: [rng(shape, dtype) for shape in shapes]\n",
                "    self._CheckAgainstNumpy(lax_reference.clamp, lax.clamp, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(base_shape=shape, dim=dim) for shape in [(4,), (3, 4), (2, 3, 4)]\n",
                "     for dim in range(len(shape))],\n",
                "    num_arrs=[3],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testConcatenate(self, dim, base_shape, dtype, num_arrs):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]\n",
                "              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]\n",
                "    args_maker = lambda: [rng(shape, dtype) for shape in shapes]\n",
                "    op = lambda *args: lax.concatenate(args, dim)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(base_shape=shape, dim=dim) for shape in [(4,), (3, 4), (2, 3, 4)]\n",
                "     for dim in range(len(shape))],\n",
                "    num_arrs=[3],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testConcatenateAgainstNumpy(self, dim, base_shape, dtype, num_arrs):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]\n",
                "              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]\n",
                "    args_maker = lambda: [rng(shape, dtype) for shape in shapes]\n",
                "    op = lambda *args: lax.concatenate(args, dim)\n",
                "    numpy_op = lambda *args: lax_reference.concatenate(args, dim)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=(b, i, 9, 10), rhs_shape=(j, i, 4, 5))\n",
                "     for b, i, j in itertools.product([2, 3], repeat=3)],\n",
                "    dtype=float_dtypes,\n",
                "    strides=[(1, 1), (1, 2), (2, 1)],\n",
                "    padding=[\"VALID\", \"SAME\"],\n",
                "  )\n",
                "  def testConv(self, lhs_shape, rhs_shape, dtype, strides, padding):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.conv(lhs, rhs, strides, padding)\n",
                "\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=(b, i, 9, 10), rhs_shape=(j, i, 4, 5))\n",
                "     for b, i, j in itertools.product([2, 3], repeat=3)],\n",
                "    [dict(dtype=dtype, preferred_element_type=preferred)\n",
                "     for dtype, preferred in preferred_type_combinations]\n",
                "  )\n",
                "  def testConvPreferredElement(self, lhs_shape, rhs_shape, dtype, preferred_element_type):\n",
                "    if (not config.x64_enabled and\n",
                "       (dtype == np.float64 or preferred_element_type == np.float64\n",
                "        or dtype == np.int64 or preferred_element_type == np.int64\n",
                "        or dtype == np.complex128 or preferred_element_type == np.complex128)):\n",
                "      raise SkipTest(\"64-bit mode disabled\")\n",
                "    if (jtu.device_under_test() == \"tpu\" and\n",
                "       (dtype == np.complex128 or preferred_element_type == np.complex128)):\n",
                "      raise SkipTest(\"np.complex128 is not yet supported on TPU\")\n",
                "    if jtu.device_under_test() == \"gpu\" and np.issubdtype(dtype, np.integer):\n",
                "      # TODO(b/183565702): Support integer convolutions on CPU/GPU.\n",
                "      raise SkipTest(\"Integer convolution not yet supported on GPU\")\n",
                "    # x64 implementation is only accurate to ~float32 precision for this case.\n",
                "    if dtype == np.complex64 and preferred_element_type == np.complex128:\n",
                "      tol = 1e-5\n",
                "    else:\n",
                "      tol = {np.float64: 1e-14}\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    x = rng(lhs_shape, dtype)\n",
                "    y = rng(rhs_shape, dtype)\n",
                "    # We first compute the conv when both inputs are a lower-precision type and\n",
                "    # preferred_element_type is a higher-precision type. We then compute results\n",
                "    # where the inputs are first upcast to the higher-precision type and no\n",
                "    # `preferred_element_type` is given. We expect the result to be extremely\n",
                "    # similar given the semantics of `preferred_element_type`.\n",
                "    result_with_preferred_type = lax.conv(\n",
                "      x, y, (1, 1), \"VALID\",\n",
                "      preferred_element_type=preferred_element_type)\n",
                "    result_with_upcast_inputs = lax.conv(\n",
                "      x.astype(preferred_element_type),\n",
                "      y.astype(preferred_element_type),\n",
                "      (1, 1), \"VALID\")\n",
                "    self.assertArraysAllClose(\n",
                "      result_with_preferred_type, result_with_upcast_inputs, rtol=tol, atol=tol)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=(b, i, 9, 10), rhs_shape=(j, i, 4, 5))\n",
                "     for b, i, j in itertools.product([2, 3], repeat=3)],\n",
                "    dtype=float_dtypes,\n",
                "    strides=[(1, 1), (1, 2), (2, 1)],\n",
                "    padding=[\"VALID\", \"SAME\"],\n",
                "  )\n",
                "  def testConvAgainstNumpy(self, lhs_shape, rhs_shape, dtype, strides, padding):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "    op = lambda lhs, rhs: lax.conv(lhs, rhs, strides, padding)\n",
                "    numpy_op = lambda lhs, rhs: lax_reference.conv(lhs, rhs, strides, padding)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=(b, i, 9, 10), rhs_shape=(j, i, 4, 5))\n",
                "     for b, i, j in itertools.product([1, 2, 3], repeat=3)],\n",
                "    dtype=float_dtypes,\n",
                "    strides=[(1, 1), (1, 2), (2, 1)],\n",
                "    padding=[((0, 0), (0, 0)), ((1, 2), (2, 0))],\n",
                "    lhs_dilation=[(1, 1), (1, 2), (2, 2)],\n",
                "    rhs_dilation=[(1, 1), (1, 2), (2, 2)],\n",
                "  )\n",
                "  def testConvWithGeneralPadding(self, lhs_shape, rhs_shape, dtype, strides,\n",
                "                                 padding, lhs_dilation, rhs_dilation):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.conv_with_general_padding(\n",
                "          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)\n",
                "\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=(b, i, 9, 10), rhs_shape=(j, i, 4, 5))\n",
                "     for b, i, j in itertools.product([1, 2, 3], repeat=3)],\n",
                "    dtype=[np.float32],\n",
                "    strides=[(1, 1), (1, 2), (2, 1)],\n",
                "    padding=[((0, 0), (0, 0)), ((1, 2), (2, 0))],\n",
                "    lhs_dilation=[(1, 1), (1, 2), (2, 2)],\n",
                "    rhs_dilation=[(1, 1), (1, 2), (2, 2)],\n",
                "  )\n",
                "  def testConvWithGeneralPaddingAgainstNumpy(\n",
                "      self, lhs_shape, rhs_shape, dtype, strides, padding, lhs_dilation,\n",
                "      rhs_dilation):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.conv_with_general_padding(\n",
                "          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation,\n",
                "          precision=lax.Precision.HIGHEST)\n",
                "\n",
                "    def numpy_fun(lhs, rhs):\n",
                "      return lax_reference.conv_with_general_padding(\n",
                "          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)\n",
                "\n",
                "    self._CheckAgainstNumpy(numpy_fun, fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=(b * batch_group_count, i * feature_group_count),\n",
                "          rhs_shape=(j * feature_group_count * batch_group_count, i),\n",
                "          batch_group_count=batch_group_count,\n",
                "          feature_group_count=feature_group_count)\n",
                "     for batch_group_count, feature_group_count in [(1, 1), (2, 1), (1, 2)]\n",
                "     for b, i, j in itertools.product([2, 3], repeat=3)],\n",
                "    [dict(dimension_numbers=(\"NC\", \"OI\", \"NC\"), perms=([0, 1], [0, 1]))],\n",
                "    dtype=all_dtypes,\n",
                "  )\n",
                "  def testConvGeneralDilated0D(self, lhs_shape, rhs_shape, dtype,\n",
                "                               feature_group_count, batch_group_count,\n",
                "                               dimension_numbers, perms):\n",
                "    if np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.bool_):\n",
                "      # TODO(b/183565702): Support integer convolutions on CPU/GPU.\n",
                "      if jtu.device_under_test() == \"gpu\":\n",
                "        raise SkipTest(\"Integer convolution not yet supported on GPU\")\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    lhs_perm, rhs_perm = perms  # permute to compatible shapes\n",
                "\n",
                "    def args_maker():\n",
                "      return [lax.transpose(rng(lhs_shape, dtype), lhs_perm),\n",
                "              lax.transpose(rng(rhs_shape, dtype), rhs_perm)]\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.conv_general_dilated(\n",
                "          lhs, rhs, window_strides=(), padding=(),\n",
                "          dimension_numbers=dimension_numbers,\n",
                "          feature_group_count=feature_group_count,\n",
                "          batch_group_count=batch_group_count)\n",
                "\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=(b * batch_group_count, i * feature_group_count, 9, w),\n",
                "          rhs_shape=(j * feature_group_count * batch_group_count, i, 4, 5),\n",
                "          batch_group_count=batch_group_count,\n",
                "          feature_group_count=feature_group_count)\n",
                "     for batch_group_count, feature_group_count in [(1, 1), (2, 1), (1, 2)]\n",
                "     for w in [0, 10]\n",
                "     for b, i, j in itertools.product([2, 3], repeat=3)],\n",
                "    [\n",
                "      dict(dimension_numbers=(\"NCHW\", \"OIHW\", \"NCHW\"),\n",
                "           perms=([0, 1, 2, 3], [0, 1, 2, 3])),\n",
                "      dict(dimension_numbers=(\"NHWC\", \"HWIO\", \"NHWC\"),\n",
                "           perms=([0, 2, 3, 1], [2, 3, 1, 0])),\n",
                "      dict(dimension_numbers=(\"NCHW\", \"HWIO\", \"NHWC\"),\n",
                "           perms=([0, 1, 2, 3], [2, 3, 1, 0])),\n",
                "    ],\n",
                "    dtype=all_dtypes,\n",
                "    strides=[(1, 1), (2, 1)],\n",
                "    padding=[((1, 2), (2, 0)), ((10, 8), (7, 13))],\n",
                "    lhs_dilation=[(1, 1), (1, 2), (1, 4)],\n",
                "    rhs_dilation=[(1, 1), (1, 2), (1, 4)],\n",
                "  )\n",
                "  def testConvGeneralDilated(self, lhs_shape, rhs_shape, dtype, strides,\n",
                "                             padding, lhs_dilation, rhs_dilation,\n",
                "                             feature_group_count, batch_group_count,\n",
                "                             dimension_numbers, perms):\n",
                "    if np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.bool_):\n",
                "      # TODO(b/183565702): Support integer convolutions on CPU/GPU.\n",
                "      if jtu.device_under_test() == \"gpu\":\n",
                "        raise SkipTest(\"Integer convolution not yet supported on GPU\")\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    lhs_perm, rhs_perm = perms  # permute to compatible shapes\n",
                "\n",
                "    def args_maker():\n",
                "      return [lax.transpose(rng(lhs_shape, dtype), lhs_perm),\n",
                "              lax.transpose(rng(rhs_shape, dtype), rhs_perm)]\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.conv_general_dilated(\n",
                "          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation,\n",
                "          dimension_numbers, feature_group_count=feature_group_count,\n",
                "          batch_group_count=batch_group_count)\n",
                "\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  def testConvGeneralDilatedPatchesOverlapping1D(self):\n",
                "    lhs = np.array([[1]], np.float32).reshape((1, 1))\n",
                "    patches = lax.conv_general_dilated_patches(\n",
                "      lhs=lhs,\n",
                "      filter_shape=(),\n",
                "      window_strides=(),\n",
                "      padding='SAME'\n",
                "    )\n",
                "    self.assertAllClose(lhs, patches)\n",
                "\n",
                "    dn = ('NHC', 'OIH', 'NHC')\n",
                "    lhs = np.array([1, 2, 3, 4, 5], np.float32).reshape((1, -1, 1))\n",
                "\n",
                "    patches = lax.conv_general_dilated_patches(\n",
                "        lhs=lhs,\n",
                "        filter_shape=(2,),\n",
                "        window_strides=(2,),\n",
                "        padding='VALID',\n",
                "        dimension_numbers=dn\n",
                "    )\n",
                "    self.assertAllClose(\n",
                "        np.array([[1, 2],\n",
                "                  [3, 4]], np.float32).reshape((1, 2, 2)), patches)\n",
                "\n",
                "    patches = lax.conv_general_dilated_patches(\n",
                "        lhs=lhs,\n",
                "        filter_shape=(3,),\n",
                "        window_strides=(1,),\n",
                "        padding='SAME',\n",
                "        dimension_numbers=dn\n",
                "    )\n",
                "    self.assertAllClose(\n",
                "        np.array([[0, 1, 2],\n",
                "                  [1, 2, 3],\n",
                "                  [2, 3, 4],\n",
                "                  [3, 4, 5],\n",
                "                  [4, 5, 0]], np.float32).reshape((1, 5, 3)), patches)\n",
                "\n",
                "    patches = lax.conv_general_dilated_patches(\n",
                "        lhs=lhs,\n",
                "        filter_shape=(3,),\n",
                "        window_strides=(1,),\n",
                "        padding='SAME',\n",
                "        rhs_dilation=(2,),\n",
                "        dimension_numbers=dn\n",
                "    )\n",
                "    self.assertAllClose(\n",
                "        np.array([[0, 1, 3],\n",
                "                  [0, 2, 4],\n",
                "                  [1, 3, 5],\n",
                "                  [2, 4, 0],\n",
                "                  [3, 5, 0]], np.float32).reshape((1, 5, 3)), patches)\n",
                "\n",
                "  def testConvGeneralDilatedPatchesOverlapping2D(self):\n",
                "    lhs = np.array([[1, 2, 3],\n",
                "                    [4, 5, 6]], np.float32).reshape((1, 2, 3, 1))\n",
                "    patches = lax.conv_general_dilated_patches(\n",
                "        lhs=lhs,\n",
                "        filter_shape=(2, 2),\n",
                "        window_strides=(1, 1),\n",
                "        padding='SAME',\n",
                "        dimension_numbers=('NHWC', 'OIHW', 'NHWC')\n",
                "    )\n",
                "    self.assertAllClose(np.array([[1, 2, 4, 5],\n",
                "                                  [2, 3, 5, 6],\n",
                "                                  [3, 0, 6, 0],\n",
                "                                  [4, 5, 0, 0],\n",
                "                                  [5, 6, 0, 0],\n",
                "                                  [6, 0, 0, 0]],\n",
                "                                 np.float32).reshape((1, 2, 3, 4)), patches)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, filter_shape=filter_shape, strides=strides,\n",
                "          padding=padding, dimension_numbers=dim_nums)\n",
                "     for lhs_shape, filter_shape, strides, padding, dim_nums in [\n",
                "          ((2, 5), (), (), [], (\"NC\", \"OI\", \"CN\")),\n",
                "          ((2, 3, 4), (2,), (2,), [(0, 2)], (\"CNH\", \"OHI\", \"HNC\")),\n",
                "          ((3, 1, 4, 5), (1, 3), (1, 3), [(3, 1), (2, 2)],\n",
                "           (\"NCHW\", \"OIHW\", \"NCHW\")),\n",
                "          ((3, 2, 5, 6), (4, 3), (4, 3), [(5, 2), (2, 4)],\n",
                "           None),\n",
                "          ((1, 2, 3, 4), (1, 1), (1, 1), [(0, 0), (0, 0)],\n",
                "           (\"NCWH\", \"OHWI\", \"CNHW\")),\n",
                "          ((1, 2, 3, 4), (3, 2), (1, 1), [(0, 0), (0, 0)],\n",
                "           (\"CWHN\", \"HOWI\", \"NCHW\")),\n",
                "          ((2, 3, 4, 5, 6), (2, 1, 3), (2, 1, 3), [(1, 2), (5, 3), (3, 5)],\n",
                "           (\"NHWDC\", \"HDIWO\", \"DCWNH\"))\n",
                "     ]],\n",
                "     dtype=all_dtypes,\n",
                "     precision=[None, lax.Precision.DEFAULT, lax.Precision.HIGH,\n",
                "                lax.Precision.HIGHEST],\n",
                "  )\n",
                "  def testConvGeneralDilatedPatchesNonOverlapping(self,\n",
                "                                                  lhs_shape,\n",
                "                                                  filter_shape,\n",
                "                                                  dtype,\n",
                "                                                  strides,\n",
                "                                                  padding,\n",
                "                                                  dimension_numbers,\n",
                "                                                  precision):\n",
                "    if np.issubdtype(dtype, np.integer) or np.issubdtype(dtype, np.bool_):\n",
                "      # TODO(b/183565702): Support integer convolutions on CPU/GPU.\n",
                "      if jtu.device_under_test() == \"gpu\":\n",
                "        raise SkipTest(\"Integer convolution not yet supported on GPU\")\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    lhs = rng(lhs_shape, dtype)\n",
                "\n",
                "    if dimension_numbers is None:\n",
                "      lhs_spec, rhs_spec, out_spec = \"NCHW\", \"OIHW\", \"NCHW\"\n",
                "    else:\n",
                "      lhs_spec, rhs_spec, out_spec = dimension_numbers\n",
                "\n",
                "    filter_spec = ''.join(c for c in rhs_spec if c not in ('I', 'O'))\n",
                "    patches_spec = out_spec.replace('C', 'C' + filter_spec.lower())\n",
                "\n",
                "    full_padding = []\n",
                "    for c in lhs_spec:\n",
                "      if c in ('N', 'C'):\n",
                "        full_padding += [(0, 0)]\n",
                "      else:\n",
                "        full_padding += [padding[filter_spec.index(c)]]\n",
                "\n",
                "    lhs_padded = np.pad(lhs, full_padding, 'constant')\n",
                "    out = lax.transpose(lhs_padded, [lhs_spec.index(c) for c in out_spec])\n",
                "\n",
                "    patches = lax.conv_general_dilated_patches(\n",
                "        lhs=lhs,\n",
                "        filter_shape=filter_shape,\n",
                "        window_strides=strides,\n",
                "        padding=padding,\n",
                "        dimension_numbers=dimension_numbers,\n",
                "        precision=precision\n",
                "    )\n",
                "\n",
                "    source = []\n",
                "\n",
                "    # Test that output spatial shape is factored into `#patches x patch_size`.\n",
                "    for c in out_spec:\n",
                "      out_c = out.shape[out_spec.index(c)]\n",
                "      patch_c = patches.shape[out_spec.index(c)]\n",
                "\n",
                "      if c == 'N':\n",
                "        self.assertEqual(out_c, patch_c)\n",
                "      elif c == 'C':\n",
                "        self.assertEqual(out_c * np.prod(filter_shape), patch_c)\n",
                "      else:\n",
                "        self.assertEqual(out_c, patch_c * filter_shape[filter_spec.index(c)])\n",
                "\n",
                "        source += [patches_spec.index(c), patches_spec.index(c.lower())]\n",
                "\n",
                "    # Test that stacking patches together gives the source image, padded.\n",
                "    c = out_spec.index('C')\n",
                "    patches = patches.reshape(patches.shape[:c] +\n",
                "                              (lhs_shape[lhs_spec.index('C')],) +\n",
                "                              filter_shape +\n",
                "                              patches.shape[c + 1:]\n",
                "                              )\n",
                "    patches = np.moveaxis(patches, source, range(len(source)))\n",
                "    for i in range(len(filter_shape)):\n",
                "      patches = patches.reshape(patches.shape[:i] + (-1,) +\n",
                "                                patches.shape[2 + i:])\n",
                "    patches = np.moveaxis(\n",
                "        patches,\n",
                "        range(len(filter_shape)),\n",
                "        [out_spec.index(c) for c in out_spec if c not in ('N', 'C')])\n",
                "    self.assertAllClose(out, patches)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(n=n, lhs_spec=lhs_spec, rhs_spec=rhs_spec, out_spec=out_spec)\n",
                "     for n in [1, 2]\n",
                "     for lhs_spec in [''.join(s)\n",
                "                      for s in itertools.permutations('NCHWD'[:n + 2])]\n",
                "     for rhs_spec in [''.join(s)\n",
                "                      for s in itertools.permutations('OIHWDX'[:n + 2])]\n",
                "     for out_spec in [''.join(s)\n",
                "                      for s in itertools.permutations('NCHWDX'[:n + 2])]],\n",
                "    dtype=inexact_dtypes,\n",
                "    precision=[None, lax.Precision.DEFAULT, lax.Precision.HIGH,\n",
                "               lax.Precision.HIGHEST,\n",
                "               (lax.Precision.DEFAULT, lax.Precision.HIGHEST)],\n",
                "    padding=['SAME', 'VALID'],\n",
                "  )\n",
                "  def testConvGeneralDilatedLocal(self, dtype, precision, n, padding, lhs_spec,\n",
                "                                  rhs_spec, out_spec):\n",
                "    \"\"\"Make sure LCN with tiled CNN kernel matches CNN.\"\"\"\n",
                "    lhs_spec_default = 'NCHWDX'[:n + 2]\n",
                "    rhs_spec_default = 'OIHWDX'[:n + 2]\n",
                "\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "\n",
                "    lhs_default = rng((2, 4, 7, 6, 5, 8)[:n + 2], dtype)\n",
                "    rhs_default = rng((5, 4, 2, 3, 1, 2)[:n + 2], dtype)\n",
                "\n",
                "    window_strides = (1, 2, 3, 4)[:n]\n",
                "    rhs_dilation = (2, 1, 3, 2)[:n]\n",
                "\n",
                "    lhs_perm = [lhs_spec_default.index(c) for c in lhs_spec]\n",
                "    lhs = np.transpose(lhs_default, lhs_perm)\n",
                "\n",
                "    rhs_perm = [rhs_spec_default.index(c) for c in rhs_spec]\n",
                "    rhs = np.transpose(rhs_default, rhs_perm)\n",
                "\n",
                "    kwargs = dict(\n",
                "        lhs=lhs,\n",
                "        window_strides=window_strides,\n",
                "        padding=padding,\n",
                "        rhs_dilation=rhs_dilation,\n",
                "        dimension_numbers=(lhs_spec, rhs_spec, out_spec),\n",
                "        precision=precision\n",
                "    )\n",
                "\n",
                "    out_conv = lax.conv_general_dilated(rhs=rhs, **kwargs)\n",
                "\n",
                "    rhs_local = np.moveaxis(rhs, (rhs_spec.index('O'), rhs_spec.index('I')),\n",
                "                            (0, 1))\n",
                "    rhs_local = rhs_local.reshape((rhs_local.shape[0], -1) + (1,) * n)\n",
                "\n",
                "    rhs_shape = (rhs_local.shape[:2] +\n",
                "                 tuple(out_conv.shape[out_spec.index(c)]\n",
                "                       for c in rhs_spec_default[2:]))\n",
                "\n",
                "    rhs_local = np.broadcast_to(rhs_local, rhs_shape)\n",
                "    rhs_local = np.transpose(rhs_local, rhs_perm)\n",
                "\n",
                "    filter_shape = [rhs.shape[i]\n",
                "                    for i in range(n + 2) if rhs_spec[i] not in ('O', 'I')]\n",
                "    out_local = lax.conv_general_dilated_local(rhs=rhs_local,\n",
                "                                               filter_shape=filter_shape,\n",
                "                                               **kwargs)\n",
                "\n",
                "    self.assertAllClose(out_conv, out_local)\n",
                "\n",
                "  # TODO(mattjj): test conv_general_dilated against numpy\n",
                "\n",
                "  def testConv0DIsDot(self):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    def args_maker():\n",
                "      return [rng((10, 5), np.float32), rng((5, 7), np.float32)]\n",
                "    jnp_fun = partial(lax.conv_general_dilated, window_strides=(),\n",
                "                      padding='VALID', dimension_numbers=('NC', 'IO', 'NC'))\n",
                "    self._CompileAndCheck(jnp_fun, args_maker)\n",
                "    self._CheckAgainstNumpy(np.dot, jnp_fun, args_maker, tol=.1)\n",
                "\n",
                "  def testGradConv0D(self):\n",
                "    # Reproduces a failure in neural_tangents not caught in our presubmit tests\n",
                "    # See cl/367416742.\n",
                "    lhs = np.ones((2, 5), dtype=np.float32)\n",
                "    rhs = np.ones((5, 10), dtype=np.float32)\n",
                "\n",
                "    def f_jax(lhs, rhs):\n",
                "      return lax.conv_general_dilated(\n",
                "          lhs, rhs, window_strides=(),\n",
                "          padding=(), lhs_dilation=(), rhs_dilation=(),\n",
                "          dimension_numbers=lax.ConvDimensionNumbers((0, 1), (1, 0), (0, 1)),\n",
                "          batch_group_count=1, feature_group_count=1, precision=None,\n",
                "          preferred_element_type=None)\n",
                "    res, pullback = jax.vjp(f_jax, lhs, rhs)\n",
                "    grad = pullback(np.ones_like(res))\n",
                "    self.assertAllClose((lhs * 10., rhs * 2.), grad)\n",
                "\n",
                "  @staticmethod\n",
                "  def _conv_transpose_via_grad(data, kernel, strides, padding,\n",
                "                               rhs_dilation=None, dimension_numbers=None):\n",
                "    \"\"\"Helper method: calculates conv transpose via grad for testing.\"\"\"\n",
                "    assert len(data.shape) == len(kernel.shape)\n",
                "    nspatial = len(data.shape) - 2\n",
                "    one = (1,) * nspatial\n",
                "    rhs_dilation = rhs_dilation or one\n",
                "    dn = lax.conv_dimension_numbers(data.shape, kernel.shape,\n",
                "                                    dimension_numbers)\n",
                "    in_shape = np.take(data.shape, dn.lhs_spec)\n",
                "    in_sdims = in_shape[2:]\n",
                "    k_shape = np.take(kernel.shape, dn.rhs_spec)\n",
                "    k_sdims = k_shape[2:]\n",
                "    e_k_sdims = [(k-1) * r + 1 for k, r in zip(k_sdims, rhs_dilation)]\n",
                "    if padding == 'VALID':\n",
                "      o_sdims = [in_sdims[i]*strides[i] + max(e_k_sdims[i]-strides[i],0)\n",
                "                 for i in range(nspatial)]\n",
                "    elif padding == 'SAME':\n",
                "      o_sdims = [in_sdims[i]*strides[i] for i in range(nspatial)]\n",
                "    o_shape =  [in_shape[0], k_shape[1]] + o_sdims\n",
                "    out_spec_inv = [x[0] for x in\n",
                "                    sorted(enumerate(dn.out_spec), key=lambda x: x[1])]\n",
                "    o_layout = np.take(np.array(o_shape), out_spec_inv)\n",
                "    placeholder = np.ones(o_layout, data.dtype)\n",
                "    conv = lambda x: lax.conv_general_dilated(x, kernel, strides, padding,\n",
                "                                              one, rhs_dilation, dn)\n",
                "    _, g = jax.vjp(conv, placeholder)\n",
                "    return g(data)[0]\n",
                "\n",
                "  @staticmethod\n",
                "  def _transpose_conv_kernel(data, kernel, dimension_numbers):\n",
                "    dn = lax.conv_dimension_numbers(data.shape, kernel.shape,\n",
                "                                    dimension_numbers)\n",
                "    spatial_axes = np.array(dn.rhs_spec)[2:]\n",
                "    for axis in spatial_axes:\n",
                "      kernel = np.flip(kernel, axis)\n",
                "    kernel = np.swapaxes(kernel, dn.rhs_spec[0], dn.rhs_spec[1])\n",
                "    return kernel\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape)\n",
                "     for lhs_shape, rhs_shape in [\n",
                "          ((b, 9, 10, i), (k, k, j, i))  # NB: i,j flipped in RHS for transpose\n",
                "          for b, i, j, k in itertools.product([2,3],[2,3],[2,3],[3,4,5])]],\n",
                "    dtype=float_dtypes,\n",
                "    strides=[(1, 1), (1, 2), (2, 1), (2, 2), (3, 3)],\n",
                "    padding=[\"VALID\", \"SAME\"],\n",
                "    dspec=[('NHWC', 'HWIO', 'NHWC'),],\n",
                "    rhs_dilation=[None, (2, 2)],\n",
                "  )\n",
                "  @jtu.skip_on_flag(\"jax_skip_slow_tests\", True)\n",
                "  def testConvTranspose2DT(self, lhs_shape, rhs_shape, dtype, strides,\n",
                "                          padding, dspec, rhs_dilation):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "\n",
                "    # NB: this test calculates conv_transpose performing identically to the\n",
                "    # lhs-grad of conv.\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.conv_transpose(lhs, rhs, strides, padding,\n",
                "                                rhs_dilation=rhs_dilation,\n",
                "                                dimension_numbers=dspec,\n",
                "                                transpose_kernel=True)\n",
                "\n",
                "    def fun_via_grad(lhs, rhs):\n",
                "      return self._conv_transpose_via_grad(lhs, rhs, strides, padding,\n",
                "                                           rhs_dilation=rhs_dilation,\n",
                "                                           dimension_numbers=dspec)\n",
                "\n",
                "    # NB: below just checks for agreement, we're not calling numpy.\n",
                "    self._CheckAgainstNumpy(fun_via_grad, fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape)\n",
                "     for lhs_shape, rhs_shape in [\n",
                "          ((b, 9, 10, i), (k, k, i, j))\n",
                "          for b, i, j, k in itertools.product([2,3],[2,3],[2,3],[3,4,5])]],\n",
                "    dtype=float_dtypes,\n",
                "    strides=[(1, 1), (1, 2), (2, 1), (2, 2), (3, 3)],\n",
                "    padding=[\"VALID\", \"SAME\"],\n",
                "    dspec=[('NHWC', 'HWIO', 'NHWC'),],\n",
                "    rhs_dilation=[None, (2, 2)],\n",
                "  )\n",
                "  @jtu.skip_on_flag(\"jax_skip_slow_tests\", True)\n",
                "  def testConvTranspose2D(self, lhs_shape, rhs_shape, dtype, strides,\n",
                "                          padding, dspec, rhs_dilation):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.conv_transpose(lhs, rhs, strides, padding,\n",
                "                                rhs_dilation=rhs_dilation,\n",
                "                                dimension_numbers=dspec,\n",
                "                                transpose_kernel=False)\n",
                "\n",
                "    def fun_via_grad(lhs, rhs):\n",
                "      rhs_t = self._transpose_conv_kernel(lhs, rhs, dimension_numbers=dspec)\n",
                "      return self._conv_transpose_via_grad(lhs, rhs_t, strides, padding,\n",
                "                                           rhs_dilation=rhs_dilation,\n",
                "                                           dimension_numbers=dspec)\n",
                "\n",
                "    # NB: below just checks for agreement, we're not calling numpy.\n",
                "    self._CheckAgainstNumpy(fun_via_grad, fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape)\n",
                "     for lhs_shape, rhs_shape in [\n",
                "          ((b, 10, i), (k, i, j))\n",
                "          for b, i, j, k in itertools.product([2,3],[2,3],[2,3],[3,4,5])]],\n",
                "    dtype=float_dtypes,\n",
                "    strides=[(1,), (2,), (3,)],\n",
                "    padding=[\"VALID\", \"SAME\"],\n",
                "    dspec=[('NHC', 'HIO', 'NHC'),],\n",
                "    rhs_dilation=[None, (2,)],\n",
                "  )\n",
                "  def testConvTranspose1D(self, lhs_shape, rhs_shape, dtype, strides,\n",
                "                          padding, dspec, rhs_dilation):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.conv_transpose(lhs, rhs, strides, padding,\n",
                "                                dimension_numbers=dspec,\n",
                "                                rhs_dilation=rhs_dilation,\n",
                "                                transpose_kernel=False)\n",
                "\n",
                "    def fun_via_grad(lhs, rhs):\n",
                "      rhs_t = self._transpose_conv_kernel(lhs, rhs, dimension_numbers=dspec)\n",
                "      return self._conv_transpose_via_grad(lhs, rhs_t, strides, padding,\n",
                "                                           rhs_dilation=rhs_dilation,\n",
                "                                           dimension_numbers=dspec)\n",
                "\n",
                "    # NB: below just checks for agreement, we're not calling numpy.\n",
                "    self._CheckAgainstNumpy(fun_via_grad, fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape)\n",
                "     for lhs_shape, rhs_shape in [\n",
                "          ((b, i), (i, j))\n",
                "          for b, i, j in itertools.product([2,3],[2,3],[2,3])]],\n",
                "    dtype=float_dtypes,\n",
                "    strides=[()],\n",
                "    padding=[\"VALID\", \"SAME\"],\n",
                "    dspec=[('NC', 'IO', 'NC'),],\n",
                "    rhs_dilation=[None, ()],\n",
                "  )\n",
                "  def testConvTranspose0D(self, lhs_shape, rhs_shape, dtype, strides,\n",
                "                          padding, dspec, rhs_dilation):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.conv_transpose(lhs, rhs, strides, padding,\n",
                "                                dimension_numbers=dspec,\n",
                "                                rhs_dilation=rhs_dilation,\n",
                "                                transpose_kernel=False)\n",
                "\n",
                "    def fun_via_grad(lhs, rhs):\n",
                "      rhs_t = self._transpose_conv_kernel(lhs, rhs, dimension_numbers=dspec)\n",
                "      return self._conv_transpose_via_grad(lhs, rhs_t, strides, padding,\n",
                "                                           rhs_dilation=rhs_dilation,\n",
                "                                           dimension_numbers=dspec)\n",
                "\n",
                "    # NB: below just checks for agreement, we're not calling numpy.\n",
                "    self._CheckAgainstNumpy(fun_via_grad, fun, args_maker)\n",
                "\n",
                "  def testConvTransposePaddingList(self):\n",
                "    # Regression test for https://github.com/google/jax/discussions/8695\n",
                "    a = jnp.ones((28,28))\n",
                "    b = jnp.ones((3,3))\n",
                "    c = lax.conv_general_dilated(a[None, None], b[None, None], (1,1), [(0,0),(0,0)], (1,1))\n",
                "    self.assertAllClose(c, 9 * jnp.ones((1, 1, 26, 26)))\n",
                "\n",
                "  def testConvInvalidPadding(self):\n",
                "    x = jnp.ones((1, 10, 10, 5), dtype=jnp.bfloat16)\n",
                "    with self.assertRaisesRegex(ValueError,\n",
                "                                r\"padding argument.*, got \\(3, 3\\)\"):\n",
                "      jax.lax.conv_general_dilated_patches(x, (5, 5), window_strides=(1, 1),\n",
                "                                           padding=(3, 3))\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape)\n",
                "     for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]],\n",
                "    dtype=all_dtypes,\n",
                "    precision=[None, lax.Precision.DEFAULT, lax.Precision.HIGH,\n",
                "               lax.Precision.HIGHEST,\n",
                "               (lax.Precision.DEFAULT, lax.Precision.HIGHEST)],\n",
                "  )\n",
                "  def testDot(self, lhs_shape, rhs_shape, dtype, precision):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "    self._CompileAndCheck(partial(lax.dot, precision=precision), args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape)\n",
                "     for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]],\n",
                "    [dict(dtype=d, preferred_element_type=p)\n",
                "     for d, p in preferred_type_combinations],\n",
                "  )\n",
                "  def testDotPreferredElement(self, lhs_shape, rhs_shape, dtype, preferred_element_type):\n",
                "    if (not config.x64_enabled and\n",
                "       (dtype == np.float64 or preferred_element_type == np.float64\n",
                "        or dtype == np.int64 or preferred_element_type == np.int64)):\n",
                "      raise SkipTest(\"64-bit mode disabled\")\n",
                "    if (jtu.device_under_test() == \"tpu\" and\n",
                "       (dtype == np.complex128 or preferred_element_type == np.complex128)):\n",
                "      raise SkipTest(\"np.complex128 is not yet supported on TPU\")\n",
                "    if jtu.device_under_test() == \"gpu\":\n",
                "      # TODO(b/189287598)\n",
                "      raise SkipTest(\"dot_general with preferred_element_type returns NaN non-deterministically on GPU\")\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    x = rng(lhs_shape, dtype)\n",
                "    y = rng(rhs_shape, dtype)\n",
                "    # We first compute the dot when both inputs are a lower-precision type and\n",
                "    # preferred_element_type is a higher-precision type. We then compute results\n",
                "    # where the inputs are first upcast to the higher-precision type and no\n",
                "    # `preferred_element_type` is given. We expect the result to be extremely\n",
                "    # similar given the semantics of `preferred_element_type`.\n",
                "    result_with_preferred_type = lax.dot(x, y, preferred_element_type=preferred_element_type)\n",
                "    result_with_upcast_inputs = lax.dot(\n",
                "      x.astype(preferred_element_type),\n",
                "      y.astype(preferred_element_type))\n",
                "    self.assertArraysAllClose(result_with_preferred_type, result_with_upcast_inputs)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape)\n",
                "     for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]],\n",
                "    dtype=all_dtypes\n",
                "  )\n",
                "  def testDotAgainstNumpy(self, lhs_shape, rhs_shape, dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "    tol = {\n",
                "      np.float16: 1e-2,\n",
                "      np.float64: max(jtu.default_tolerance()[np.dtype(np.float64)], 1e-14),\n",
                "      np.complex128: max(jtu.default_tolerance()[np.dtype(np.complex128)],\n",
                "                          1e-14)\n",
                "    }\n",
                "    lax_op = partial(lax.dot, precision=lax.Precision.HIGHEST)\n",
                "    self._CheckAgainstNumpy(lax_reference.dot, lax_op, args_maker, tol=tol)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape,\n",
                "          lhs_contracting=lhs_contracting, rhs_contracting=rhs_contracting)\n",
                "     for lhs_shape, rhs_shape, lhs_contracting, rhs_contracting in [\n",
                "          [(5,), (5,), [0], [0]],\n",
                "          [(5, 7), (5,), [0], [0]],\n",
                "          [(7, 5), (5,), [1], [0]],\n",
                "          [(3, 5), (2, 5), [1], [1]],\n",
                "          [(5, 3), (5, 2), [0], [0]],\n",
                "          [(5, 3, 2), (5, 2, 4), [0], [0]],\n",
                "          [(5, 3, 2), (5, 2, 4), [0,2], [0,1]],\n",
                "          [(5, 3, 2), (3, 5, 2, 4), [0,2], [1,2]],\n",
                "          [(1, 2, 2, 3), (1, 2, 3, 1), [1], [1]],\n",
                "          [(3, 2), (2, 4), [1], [0]],\n",
                "      ]],\n",
                "    dtype=all_dtypes,\n",
                "  )\n",
                "  def testDotGeneralContractOnly(self, lhs_shape, rhs_shape, dtype,\n",
                "                                 lhs_contracting, rhs_contracting):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "    dimension_numbers = ((lhs_contracting, rhs_contracting), ([], []))\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.dot_general(lhs, rhs, dimension_numbers)\n",
                "\n",
                "    self._CompileAndCheck(fun, args_maker, check_dtypes=False)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape,\n",
                "          dimension_numbers=dimension_numbers)\n",
                "      for lhs_shape, rhs_shape, dimension_numbers in [\n",
                "          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),\n",
                "          ((3, 3, 2), (2, 3, 4), (([2], [0]), ([0], [1]))),\n",
                "          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),\n",
                "      ]],\n",
                "    dtype=all_dtypes,\n",
                "  )\n",
                "  def testDotGeneralContractAndBatch(self, lhs_shape, rhs_shape, dtype,\n",
                "                                     dimension_numbers):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "\n",
                "    def fun(lhs, rhs):\n",
                "      return lax.dot_general(lhs, rhs, dimension_numbers)\n",
                "\n",
                "    self._CompileAndCheck(fun, args_maker, check_dtypes=False)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape,\n",
                "          dimension_numbers=dimension_numbers)\n",
                "      for lhs_shape, rhs_shape, dimension_numbers in [\n",
                "          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),\n",
                "          ((3, 3, 2), (2, 3, 4), (([2], [0]), ([0], [1]))),\n",
                "          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),\n",
                "      ]],\n",
                "    dtype=all_dtypes,\n",
                "  )\n",
                "  def testDotGeneralAgainstNumpy(self, lhs_shape, rhs_shape, dtype,\n",
                "                                 dimension_numbers):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "    op = lambda x, y: lax.dot_general(x, y, dimension_numbers)\n",
                "    numpy_op = lambda x, y: lax_reference.dot_general(x, y, dimension_numbers)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    shape=[(), (2, 3)],\n",
                "    dtype=default_dtypes,\n",
                "    broadcast_sizes=[(), (2,), (1, 2)],\n",
                "  )\n",
                "  def testBroadcast(self, shape, dtype, broadcast_sizes):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    op = lambda x: lax.broadcast(x, broadcast_sizes)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    shape=[(), (2, 3)],\n",
                "    dtype=default_dtypes,\n",
                "    broadcast_sizes=[(), (2,), (1, 2)],\n",
                "  )\n",
                "  def testBroadcastAgainstNumpy(self, shape, dtype, broadcast_sizes):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    op = lambda x: lax.broadcast(x, broadcast_sizes)\n",
                "    numpy_op = lambda x: lax_reference.broadcast(x, broadcast_sizes)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(inshape=inshape, outshape=outshape, dimensions=dimensions)\n",
                "      for inshape, outshape, dimensions in [\n",
                "          ([2], [2, 2], [0]),\n",
                "          ([2], [2, 2], [1]),\n",
                "          ([2], [2, 3], [0]),\n",
                "          ([], [2, 3], []),\n",
                "          ([1], [2, 3], [1]),\n",
                "      ]],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testBroadcastInDim(self, inshape, dtype, outshape, dimensions):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(inshape, dtype)]\n",
                "    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  def testBroadcastInDimOperandShapeTranspose(self):\n",
                "    # Regression test for https://github.com/google/jax/issues/5276\n",
                "    def f(x):\n",
                "      return lax.broadcast_in_dim(x, (2, 3, 4), broadcast_dimensions=(0, 1, 2)).sum()\n",
                "    def g(x):\n",
                "      return lax.broadcast_in_dim(x.reshape((3,)), (2, 3, 4), broadcast_dimensions=(1,)).sum()\n",
                "    x = np.ones((1, 3, 1))\n",
                "    self.assertArraysEqual(jax.grad(f)(x), jax.grad(g)(x))\n",
                "\n",
                "  @parameterized.parameters(\n",
                "    {\"inshape\": inshape, \"outshape\": outshape,\n",
                "      \"broadcast_dimensions\": broadcast_dimensions, \"err_msg\": err_msg}\n",
                "    for inshape, outshape, broadcast_dimensions, err_msg in [\n",
                "      ([2], [2, 2], [0, 1], ('broadcast_dimensions must have length equal to '\n",
                "                              'operand ndim')),\n",
                "      ([2, 2], [2], [0, 1], ('target broadcast shape must have equal or higher rank '\n",
                "                             'to the operand shape')),\n",
                "      ([2], [2, 3], [2], ('broadcast_in_dim broadcast_dimensions must be a subset of output '\n",
                "                          'dimensions')),\n",
                "      ([2], [3], [0], ('operand dimension sizes must either be 1, or be '\n",
                "                       'equal to their corresponding dimensions in the target broadcast shape')),\n",
                "      ([2, 2], [2, 2], [1, 0], ('broadcast_dimensions must be strictly increasing')),\n",
                "    ])\n",
                "  def testBroadcastInDimShapeCheck(self, inshape, outshape, broadcast_dimensions, err_msg):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    x = rng(inshape, np.float32)\n",
                "    with self.assertRaisesRegex(TypeError, err_msg):\n",
                "      lax.broadcast_in_dim(x, shape=outshape, broadcast_dimensions=broadcast_dimensions)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(inshape=inshape, outshape=outshape, dimensions=dimensions)\n",
                "      for inshape, outshape, dimensions in [\n",
                "          ([2], [2, 2], [0]),\n",
                "          ([2], [2, 2], [1]),\n",
                "          ([2], [2, 3], [0]),\n",
                "          ([], [2, 3], []),\n",
                "          ([1], [2, 3], [1]),\n",
                "      ]],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testBroadcastInDimAgainstNumpy(self, inshape, dtype, outshape, dimensions):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(inshape, dtype)]\n",
                "    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)\n",
                "    numpy_op = lambda x: lax_reference.broadcast_in_dim(x, outshape, dimensions)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @parameterized.parameters(\n",
                "    {\"inshape\": inshape, \"dimensions\": dimensions, \"error_type\": error_type,\n",
                "     \"err_msg\": err_msg}\n",
                "    for inshape, dimensions, error_type, err_msg in [\n",
                "      ((1, 2, 3), (0, 0), ValueError, 'dimensions are not unique'),\n",
                "      ((1, 2, 3), (3,), ValueError, 'axis 3 is out of bounds'),\n",
                "      ((1, 2, 3), (-4,), ValueError, 'axis -4 is out of bounds'),\n",
                "      ((1, 2, 3), (1,), ValueError, 'cannot select an axis to squeeze out'),\n",
                "      ((1, 2, 3), (None,), TypeError, 'cannot be interpreted as an integer'),\n",
                "    ])\n",
                "  def testSqueezeShapeCheck(self, inshape, dimensions, error_type, err_msg):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    x = rng(inshape, np.float32)\n",
                "    with self.assertRaisesRegex(error_type, err_msg):\n",
                "      lax.squeeze(x, dimensions=dimensions)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(arg_shape=arg_shape, dimensions=dimensions)\n",
                "      for arg_shape, dimensions in [\n",
                "        [(1,), (0,)],\n",
                "        [(1,), (-1,)],\n",
                "        [(2, 1, 4), (1,)],\n",
                "        [(2, 1, 3, 1), (1,)],\n",
                "        [(2, 1, 3, 1), (1, 3)],\n",
                "        [(2, 1, 3, 1), (3,)],\n",
                "      ]],\n",
                "  )\n",
                "  def testSqueeze(self, arg_shape, dimensions):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(arg_shape, np.float32)]\n",
                "    op = lambda x: lax.squeeze(x, dimensions)\n",
                "    numpy_op = lambda x: lax_reference.squeeze(x, dimensions)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "    check_grads(op, args_maker(), 2, [\"fwd\", \"rev\"], eps=1.)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    input_type=[\"np.array\", \"jnp.array\", \"float\", \"np.float32\"],\n",
                "    jit=[True, False],\n",
                "  )\n",
                "  def testEmptySqueezeReturnType(self, input_type, jit):\n",
                "    if input_type == \"np.array\":\n",
                "      operand = np.arange(5)\n",
                "    elif input_type == \"jnp.array\":\n",
                "      operand = jnp.arange(5)\n",
                "    elif input_type == \"float\":\n",
                "      operand = 2.0\n",
                "    elif input_type == \"np.float32\":\n",
                "      operand = np.float32(2.0)\n",
                "    else:\n",
                "      raise ValueError(f\"Unrecognized {input_type=}\")\n",
                "\n",
                "    op = lambda x: lax.squeeze(x, dimensions=())\n",
                "    if jit:\n",
                "      op = jax.jit(op)\n",
                "    result = op(operand)\n",
                "    self.assertIsInstance(result, jax.Array)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(arg_shape=arg_shape, out_shape=out_shape)\n",
                "     for arg_shape, out_shape in [\n",
                "        [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]\n",
                "     ]],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testReshape(self, arg_shape, out_shape, dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(arg_shape, dtype)]\n",
                "    op = lambda x: lax.reshape(x, out_shape)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(arg_shape=arg_shape, out_shape=out_shape)\n",
                "     for arg_shape, out_shape in [\n",
                "        [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]\n",
                "     ]],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testReshapeAgainstNumpy(self, arg_shape, out_shape, dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(arg_shape, dtype)]\n",
                "    op = lambda x: lax.reshape(x, out_shape)\n",
                "    numpy_op = lambda x: lax_reference.reshape(x, out_shape)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  def testRoundRoundingMethods(self):\n",
                "    x = np.array([-2.5, -1.5, -0.5, 0.5, 1.5, 2.5], dtype=np.float32)\n",
                "    self.assertAllClose(lax.round(x, lax.RoundingMethod.AWAY_FROM_ZERO),\n",
                "                        np.array([-3, -2, -1, 1, 2, 3], dtype=np.float32))\n",
                "    self.assertAllClose(lax.round(x, lax.RoundingMethod.TO_NEAREST_EVEN),\n",
                "                        np.array([-2, -2, 0, 0, 2, 2], dtype=np.float32))\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, pads=pads) for shape, pads in [\n",
                "        ((0, 2), [(1, 2, 1), (0, 1, 0)]),\n",
                "        ((2, 3), [(1, 2, 1), (0, 1, 0)]),\n",
                "        ((2,), [(1, 2, 0)]),\n",
                "        ((1, 2), [(1, 2, 0), (3, 4, 0)]),\n",
                "        ((1, 2), [(0, 0, 0), (0, 0, 0)]),\n",
                "        ((2,), [(1, 2, 3),]),\n",
                "        ((3, 2), [(1, 2, 1), (3, 4, 2)]),\n",
                "        ((2,), [(-1, 2, 0),]),\n",
                "        ((4, 2), [(-1, -2, 0), (1, 2, 0)]),\n",
                "        ((4, 2), [(-1, 2, 0), (1, 2, 2)]),\n",
                "        ((5,), [(-1, -2, 2),]),\n",
                "        ((4, 2), [(-1, -2, 1), (1, 2, 2)])\n",
                "      ]\n",
                "    ],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testPad(self, shape, dtype, pads):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    fun = lambda operand: lax.pad(operand, np.array(0, dtype), pads)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    shape=[(2, 3)],\n",
                "    dtype=default_dtypes,\n",
                "    pads=[\n",
                "        [(0, 0, 0), (0, 0, 0)],  # no padding\n",
                "        [(1, 1, 0), (2, 2, 0)],  # only positive edge padding\n",
                "        [(1, 2, 1), (0, 1, 0)],  # edge padding and interior padding\n",
                "        [(0, 0, 0), (-1, -1, 0)],  # negative padding\n",
                "        [(0, 0, 0), (-2, -2, 4)],  # add big dilation then remove from edges\n",
                "        [(0, 0, 0), (-2, -3, 1)],  # remove everything in one dimension\n",
                "    ]\n",
                "  )\n",
                "  def testPadAgainstNumpy(self, shape, dtype, pads):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    op = lambda x: lax.pad(x, np.array(0, dtype), pads)\n",
                "    numpy_op = lambda x: lax_reference.pad(x, np.array(0, dtype), pads)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  def testPadErrors(self):\n",
                "    with self.assertRaisesRegex(ValueError, \"padding_config\"):\n",
                "      lax.pad(np.zeros(2), 0., [(0, 1, 0), (0, 1, 0)])\n",
                "    with self.assertRaisesRegex(ValueError, \"interior padding in padding_config must be nonnegative\"):\n",
                "      lax.pad(np.zeros(2), 0., [(0, 1, -1)])\n",
                "    with self.assertRaisesRegex(ValueError, \"Dimension size after padding is not at least 0\"):\n",
                "      lax.pad(np.zeros(2), 0., [(-3, 0, 0)])\n",
                "    with self.assertRaisesRegex(ValueError, \"Dimension size after padding is not at least 0\"):\n",
                "      lax.pad(np.zeros(2), 0., [(-4, 0, 1)])\n",
                "\n",
                "  def testReverse(self):\n",
                "    rev = jax.jit(lambda operand: lax.rev(operand, dimensions))\n",
                "\n",
                "    dimensions = []\n",
                "    self.assertAllClose(np.array([0, 1, 2, 3]), rev(np.array([0, 1, 2, 3])),\n",
                "                        check_dtypes=False)\n",
                "\n",
                "    dimensions = [0]\n",
                "    self.assertAllClose(np.array([3, 2, 1]), rev(np.array([1, 2, 3])),\n",
                "                        check_dtypes=False)\n",
                "\n",
                "    dimensions = [0, 1]\n",
                "    self.assertAllClose(np.array([[6, 5, 4], [3, 2, 1]]),\n",
                "                        rev(np.array([[1, 2, 3], [4, 5, 6]])),\n",
                "                        check_dtypes=False)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(arg_shape=arg_shape, pred_shape=pred_shape)\n",
                "      for arg_shape in [(), (3,), (2, 3)]\n",
                "      for pred_shape in ([(), arg_shape] if arg_shape else [()])\n",
                "    ],\n",
                "    arg_dtype=default_dtypes,\n",
                "  )\n",
                "  def testSelect(self, pred_shape, arg_shape, arg_dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    def args_maker():\n",
                "      return [rng(pred_shape, np.bool_), rng(arg_shape, arg_dtype),\n",
                "              rng(arg_shape, arg_dtype)]\n",
                "    return self._CheckAgainstNumpy(lax_reference.select, lax.select, args_maker)\n",
                "    return self._CompileAndCheck(lax.select, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(arg_shape=arg_shape, pred_shape=pred_shape)\n",
                "      for arg_shape in [(), (3,), (2, 3)]\n",
                "      for pred_shape in ([(), arg_shape] if arg_shape else [()])\n",
                "    ],\n",
                "    [dict(pred_dtype=pred_dtype, num_args=num_args)\n",
                "     for (pred_dtype, num_args) in (\n",
                "          list(itertools.product([np.dtype(np.bool_), np.dtype(np.int32)],\n",
                "                                 [1, 2])) +\n",
                "          [(np.dtype(np.int32), 6)])],\n",
                "    arg_dtype=default_dtypes,\n",
                "  )\n",
                "  def testSelectN(self, pred_dtype, pred_shape, arg_shape, arg_dtype, num_args):\n",
                "    if pred_dtype == np.bool_:\n",
                "      pred_rng = jtu.rand_default(self.rng())\n",
                "    else:\n",
                "      pred_rng = jtu.rand_int(self.rng(), low=-1, high=num_args + 1)\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    def args_maker():\n",
                "      return [pred_rng(pred_shape, pred_dtype)] + (\n",
                "          [rng(arg_shape, arg_dtype) for _ in range(num_args)])\n",
                "    return self._CheckAgainstNumpy(lambda c, *xs: np.choose(c, xs, mode='clip'),\n",
                "                                   lax.select_n, args_maker)\n",
                "    return self._CompileAndCheck(lax.select_n, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, starts=indices, limits=limit_indices,\n",
                "          strides=strides)\n",
                "      for shape, indices, limit_indices, strides in [\n",
                "        [(3,), (1,), (2,), None],\n",
                "        [(7,), (4,), (7,), None],\n",
                "        [(5,), (1,), (5,), (2,)],\n",
                "        [(8,), (1,), (6,), (2,)],\n",
                "        [(5, 3), (1, 1), (3, 2), None],\n",
                "        [(5, 3), (1, 1), (3, 1), None],\n",
                "        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],\n",
                "        [(5, 3), (1, 1), (2, 1), (1, 1)],\n",
                "        [(5, 3), (1, 1), (5, 3), (2, 1)],\n",
                "      ]\n",
                "    ],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testSlice(self, shape, dtype, starts, limits, strides):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    op = lambda x: lax.slice(x, starts, limits, strides)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, starts=indices, limits=limit_indices,\n",
                "          strides=strides)\n",
                "      for shape, indices, limit_indices, strides in [\n",
                "        [(3,), (1,), (2,), None],\n",
                "        [(7,), (4,), (7,), None],\n",
                "        [(5,), (1,), (5,), (2,)],\n",
                "        [(8,), (1,), (6,), (2,)],\n",
                "        [(5, 3), (1, 1), (3, 2), None],\n",
                "        [(5, 3), (1, 1), (3, 1), None],\n",
                "        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],\n",
                "        [(5, 3), (1, 1), (2, 1), (1, 1)],\n",
                "        [(5, 3), (1, 1), (5, 3), (2, 1)],\n",
                "      ]\n",
                "    ],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testSliceAgainstNumpy(self, shape, dtype, starts, limits, strides):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    op = lambda x: lax.slice(x, starts, limits, strides)\n",
                "    numpy_op = lambda x: lax_reference.slice(x, starts, limits, strides)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, indices=indices, size_indices=size_indices)\n",
                "      for shape, indices, size_indices in [\n",
                "        [(3,), np.array((1,)), (1,)],\n",
                "        [(5, 3), (1, 1), (3, 1)],\n",
                "        [(5, 3), np.array((1, 1)), (3, 1)],\n",
                "        [(7, 5, 3), np.array((4, 1, 0)), (2, 0, 1)],\n",
                "      ]\n",
                "    ],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testDynamicSlice(self, shape, dtype, indices, size_indices):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype), np.array(indices)]\n",
                "    op = lambda x, starts: lax.dynamic_slice(x, starts, size_indices)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, indices=indices, size_indices=size_indices)\n",
                "      for shape, indices, size_indices in [\n",
                "        [(3,), (1,), (1,)],\n",
                "        [(5, 3), (1, 1), (3, 1)],\n",
                "        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],\n",
                "      ]\n",
                "    ],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testDynamicSliceAgainstNumpy(self, shape, dtype, indices, size_indices):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype), np.array(indices)]\n",
                "    op = lambda x, s: lax.dynamic_slice(x, s, size_indices)\n",
                "    numpy_op = lambda x, s: lax_reference.dynamic_slice(x, s, size_indices)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  def testDynamicSliceInDim(self):\n",
                "    # Regression test for mixed type problem in dynamic_slice_in_dim.\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    x = rng((6, 7), np.int32)\n",
                "    np.testing.assert_equal(lax.dynamic_slice_in_dim(x, 2, 3), x[2:5])\n",
                "\n",
                "  def testDynamicSliceArraySliceSizes(self):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    x = rng((6, 7), np.int32)\n",
                "    np.testing.assert_equal(lax.dynamic_slice(x, [2, 3], jnp.array([2, 2])),\n",
                "                            x[2:4, 3:5])\n",
                "\n",
                "  def testDynamicSliceWithNonScalarIndex(self):\n",
                "    x = jnp.ones((6, 7), np.int32)\n",
                "    with self.assertRaises(TypeError):\n",
                "      lax.dynamic_slice_in_dim(x, jnp.array([2, 2]), 3)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, indices=indices, update_shape=update_shape)\n",
                "      for shape, indices, update_shape in [\n",
                "        [(3,), (1,), (1,)],\n",
                "        [(5, 3), (1, 1), (3, 1)],\n",
                "        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],\n",
                "      ]\n",
                "    ],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testDynamicUpdateSlice(self, shape, dtype, indices, update_shape):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "\n",
                "    def args_maker():\n",
                "      return [rng(shape, dtype), rng(update_shape, dtype), np.array(indices)]\n",
                "\n",
                "    self._CompileAndCheck(lax.dynamic_update_slice, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, indices=indices, update_shape=update_shape)\n",
                "      for shape, indices, update_shape in [\n",
                "        [(3,), (1,), (1,)],\n",
                "        [(5, 3), (1, 1), (3, 1)],\n",
                "        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],\n",
                "      ]\n",
                "    ],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testDynamicUpdateSliceAgainstNumpy(self, shape, dtype, indices,\n",
                "                                         update_shape):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "\n",
                "    def args_maker():\n",
                "      return [rng(shape, dtype), rng(update_shape, dtype), np.array(indices)]\n",
                "\n",
                "    self._CheckAgainstNumpy(lax_reference.dynamic_update_slice,\n",
                "                            lax.dynamic_update_slice, args_maker)\n",
                "\n",
                "  def testDynamicUpdateSliceBatched(self):\n",
                "    # Regression test for https://github.com/google/jax/issues/9083\n",
                "    x = jnp.arange(5)\n",
                "    y = jnp.arange(6, 9)\n",
                "    ind = jnp.arange(6)\n",
                "    expected = jnp.vstack([lax.dynamic_update_slice(x, y, (i,)) for i in ind])\n",
                "    actual = jax.vmap(lax.dynamic_update_slice, (None, None, 0))(x, y, (ind,))\n",
                "    self.assertAllClose(expected, actual)\n",
                "\n",
                "  def testDynamicUpdateSliceWithNonScalarIndex(self):\n",
                "    x = jnp.ones((6, 7), np.int32)\n",
                "    with self.assertRaises(TypeError):\n",
                "      lax.dynamic_update_slice_in_dim(x, jnp.ones((2, 7), np.int32),\n",
                "                                      jnp.array([2, 2]), axis=0)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, perm=perm)\n",
                "      for shape, perm in [\n",
                "        [(3, 4), (1, 0)],\n",
                "        [(3, 4), (0, 1)],\n",
                "        [(3, 4, 5), (2, 1, 0)],\n",
                "        [(3, 4, 5), (1, 0, 2)],\n",
                "      ]\n",
                "     ],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testTranspose(self, shape, dtype, perm):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    op = lambda x: lax.transpose(x, perm)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  def testTransposeWithArrayPermutation(self):\n",
                "    x = lax.transpose(np.ones((2, 3)), jnp.array([1, 0]))\n",
                "    self.assertEqual((3, 2), x.shape)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, perm=perm)\n",
                "      for shape, perm in [\n",
                "        [(3, 4), (1, 0)],\n",
                "        [(3, 4), (0, 1)],\n",
                "        [(3, 4, 5), (2, 1, 0)],\n",
                "        [(3, 4, 5), (1, 0, 2)],\n",
                "      ]\n",
                "     ],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testTransposeAgainstNumpy(self, shape, dtype, perm):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    op = lambda x: lax.transpose(x, perm)\n",
                "    numpy_op = lambda x: lax_reference.transpose(x, perm)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(op=rec.op, reference_op=rec.reference_op, init_val=rec.init_val,\n",
                "          primitive=rec.primitive, dtype=dtype)\n",
                "     for rec in LAX_REDUCE_OPS for dtype in rec.dtypes],\n",
                "    [dict(shape=shape, dims=dims)\n",
                "      for shape, dims in [\n",
                "        [(3, 4, 5), (0,)],\n",
                "        [(3, 4, 5), (1, 2)],\n",
                "        [(3, 4, 5), (0, 2)],\n",
                "        [(3, 4, 5), (0, 1, 2)],\n",
                "      ]\n",
                "    ],\n",
                "  )\n",
                "  def testReduce(self, op, reference_op, init_val, shape, dtype, dims, primitive):\n",
                "    if not config.x64_enabled and dtype in (np.float64, np.int64, np.uint64):\n",
                "      raise SkipTest(\"x64 mode is disabled.\")\n",
                "    def reference_fun(operand):\n",
                "      if hasattr(reference_op, \"reduce\"):\n",
                "        initial = np.array(init_val, dtype=dtype)\n",
                "        result = reference_op.reduce(operand, axis=dims, initial=initial)\n",
                "      else:\n",
                "        result = reference_op(operand, axis=dims)\n",
                "\n",
                "      return result.astype(dtype)\n",
                "\n",
                "    rng_factory = (jtu.rand_default if dtypes.issubdtype(dtype, np.integer)\n",
                "                   else jtu.rand_small)\n",
                "    rng = rng_factory(self.rng())\n",
                "    init_val = np.asarray(init_val).astype(dtype)\n",
                "    fun = lambda operand, init_val: lax.reduce(operand, init_val, op, dims)\n",
                "    args_maker = lambda: [rng(shape, dtype), init_val]\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "    # we separately test the version that uses a concrete init_val because it\n",
                "    # can hit different code paths\n",
                "    fun = lambda operand: lax.reduce(operand, init_val, op, dims)\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "    self._CheckAgainstNumpy(reference_fun, fun, args_maker)\n",
                "\n",
                "    # check that the correct monoid reducer primitive is used inside the jaxpr.\n",
                "    # This requires the init_val (monoid identity element) to be static\n",
                "    jaxpr = jax.make_jaxpr(fun)(rng(shape, dtype))\n",
                "    self.assertEqual(jaxpr.eqns[0].primitive, primitive)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    op=[\"add\", \"mul\"],\n",
                "    op_namespace=[lax, operator],\n",
                "    arr_weak_type=[False, True],\n",
                "    init_weak_type=[False, True],\n",
                "  )\n",
                "  def testReduceWeakType(self, op_namespace, op, arr_weak_type, init_weak_type):\n",
                "    op = getattr(op_namespace, op)\n",
                "    arr = lax_internal._convert_element_type(np.arange(10), int,\n",
                "                                             weak_type=arr_weak_type)\n",
                "    init = lax_internal._convert_element_type(1, int, weak_type=init_weak_type)\n",
                "    fun = lambda arr, init: lax.reduce(arr, init, op, (0,))\n",
                "    out = fun(arr, init)\n",
                "    self.assertEqual(dtypes.is_weakly_typed(out), arr_weak_type and init_weak_type)\n",
                "    out_jit = jax.jit(fun)(arr, init)\n",
                "    self.assertEqual(dtypes.is_weakly_typed(out_jit), arr_weak_type and init_weak_type)\n",
                "\n",
                "  def testReduceWindowScalar(self):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    dtype = jnp.float32\n",
                "    init_val = np.asarray(0, dtype=dtype)\n",
                "    op = lax.add\n",
                "\n",
                "    def fun(operand, init_val):\n",
                "      return lax.reduce_window(\n",
                "          operand, init_val, op, window_dimensions=(), window_strides=(),\n",
                "          padding=(), base_dilation=(), window_dilation=())\n",
                "\n",
                "    def reference_fun(operand, init_val):\n",
                "      return lax_reference.reduce_window(\n",
                "          operand, init_val, op, window_dimensions=(), window_strides=(),\n",
                "          padding=(), base_dilation=())\n",
                "\n",
                "    args_maker = lambda: [rng((), dtype), init_val]\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "    self._CheckAgainstNumpy(reference_fun, fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(init_val=init_val, op=op, dtype=dtype)\n",
                "      for init_val, op, dtypes in [\n",
                "          (0, lax.add, [np.float32]),\n",
                "          (-np.inf, lax.max, [np.float32]),\n",
                "          (np.inf, lax.min, [np.float32]),\n",
                "      ]\n",
                "      for dtype in dtypes\n",
                "    ],\n",
                "    [dict(shape=shape, dims=dims, strides=strides, padding=padding,\n",
                "          base_dilation=base_dilation, window_dilation=window_dilation)\n",
                "      for shape, dims, strides, padding, base_dilation, window_dilation in (\n",
                "        itertools.chain(\n",
                "          itertools.product(\n",
                "            [(4, 6)],\n",
                "            [(2, 1), (1, 2)],\n",
                "            [(1, 1), (2, 1), (1, 2)],\n",
                "            [\"VALID\", \"SAME\", [(0, 3), (1, 2)]],\n",
                "            [(1, 1), (2, 3)],\n",
                "            [(1, 1), (1, 2)]),\n",
                "          itertools.product(\n",
                "            [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],\n",
                "            [(1, 2, 2, 1), (1, 1, 1, 1)],\n",
                "            [\"VALID\", \"SAME\", [(0, 1), (1, 0), (2, 3), (0, 2)]],\n",
                "            [(1, 1, 1, 1), (2, 1, 3, 2)],\n",
                "            [(1, 1, 1, 1), (1, 2, 2, 1)])))\n",
                "    ],\n",
                "  )\n",
                "  def testReduceWindow(self, op, init_val, dtype, shape, dims, strides, padding,\n",
                "                       base_dilation, window_dilation):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    init_val = np.asarray(init_val, dtype=dtype)\n",
                "\n",
                "    def fun(operand, init_val):\n",
                "      return lax.reduce_window(operand, init_val, op, dims, strides, padding,\n",
                "                               base_dilation, window_dilation)\n",
                "\n",
                "    def reference_fun(operand, init_val):\n",
                "      return lax_reference.reduce_window(operand, init_val, op, dims, strides,\n",
                "                                         padding, base_dilation)\n",
                "\n",
                "    args_maker = lambda: [rng(shape, dtype), init_val]\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "    if all(d == 1 for d in window_dilation):\n",
                "      self._CheckAgainstNumpy(reference_fun, fun, args_maker)\n",
                "\n",
                "    # we separately test the version that uses a concrete init_val because it\n",
                "    # can hit different code paths\n",
                "    def fun(operand):\n",
                "      return lax.reduce_window(operand, init_val, op, dims, strides, padding,\n",
                "                               base_dilation, window_dilation)\n",
                "\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, dims=dims, strides=strides, padding=padding,\n",
                "          base_dilation=base_dilation, window_dilation=window_dilation)\n",
                "      for shape, dims, strides, padding, base_dilation, window_dilation in (\n",
                "        itertools.chain(\n",
                "          itertools.product(\n",
                "            [(4, 6)],\n",
                "            [(2, 1), (1, 2)],\n",
                "            [(1, 1), (2, 1), (1, 2)],\n",
                "            [\"VALID\", \"SAME\", [(0, 3), (1, 2)]],\n",
                "            [(1, 1), (2, 3)],\n",
                "            [(1, 1), (1, 2)]),\n",
                "          itertools.product(\n",
                "            [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],\n",
                "            [(1, 2, 2, 1), (1, 1, 1, 1)],\n",
                "            [\"VALID\", \"SAME\", [(0, 1), (1, 0), (2, 3), (0, 2)]],\n",
                "            [(1, 1, 1, 1), (2, 1, 3, 2)],\n",
                "            [(1, 1, 1, 1), (1, 2, 2, 1)])))\n",
                "    ],\n",
                "    dtype=[np.float32],\n",
                "  )\n",
                "  # TODO(b/183233858): variadic reduce-window is not implemented on XLA:GPU\n",
                "  @jtu.skip_on_devices(\"gpu\")\n",
                "  def testReduceWindowVariadic(self, dtype, shape, dims, strides, padding,\n",
                "                               base_dilation, window_dilation):\n",
                "    if (jtu.device_under_test() == \"tpu\" and\n",
                "        any(d != 1 for d in window_dilation)):\n",
                "      raise SkipTest(\"TPU support missing for arbitrary window dilation.\")\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    init_values = (np.asarray(0, dtype=dtype), np.array(-np.inf, dtype=dtype))\n",
                "\n",
                "    def reducer(xs, ys):\n",
                "      x1, x2 = xs\n",
                "      y1, y2 = ys\n",
                "      return (x1 + y1, lax.max(x2, y2))\n",
                "\n",
                "    def fun(*operands):\n",
                "      return lax.reduce_window(operands, init_values, reducer, dims, strides,\n",
                "                               padding, base_dilation, window_dilation)\n",
                "\n",
                "    def reference_fun(*operands):\n",
                "      return [\n",
                "          lax_reference.reduce_window(operand, init_val, op, dims, strides,\n",
                "                                      padding, base_dilation)\n",
                "          for operand, init_val, op in zip(operands, init_values,\n",
                "                                           [np.add, np.maximum])]\n",
                "\n",
                "    args_maker = lambda: [rng(shape, dtype), rng(shape, dtype)]\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "    if all(d == 1 for d in window_dilation):\n",
                "      self._CheckAgainstNumpy(reference_fun, fun, args_maker)\n",
                "\n",
                "\n",
                "  def testReduceWindowFailures(self):\n",
                "    def empty_window_test():\n",
                "      return lax.reduce_window(np.ones((1,)), 0., lax.add, padding='VALID',\n",
                "                               window_dimensions=(0,), window_strides=(1,))\n",
                "\n",
                "    def zero_stride_test():\n",
                "      return lax.reduce_window(np.ones((1,)), 0., lax.add, padding='VALID',\n",
                "                               window_dimensions=(1,), window_strides=(0,))\n",
                "\n",
                "    for failure_fun in [empty_window_test, zero_stride_test]:\n",
                "      with self.assertRaisesRegex(TypeError, \"must have every element be\"):\n",
                "        failure_fun()\n",
                "\n",
                "    with self.assertRaisesRegex(\n",
                "        ValueError,\n",
                "        \"reduce_window output must have the same tree structure as the \"\n",
                "        \"operands.*\"):\n",
                "      return lax.reduce_window(\n",
                "          np.ones((1,)), 0., lambda x, y: [x + y],\n",
                "          padding='VALID', window_dimensions=(1,), window_strides=(1,))\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, window_dimensions=window_dimensions,\n",
                "          base_dilation=base_dilation, window_dilation=window_dilation)\n",
                "      for shape, window_dimensions, base_dilation, window_dilation in (\n",
                "        itertools.chain(\n",
                "          itertools.product(\n",
                "            [(4, 6)],\n",
                "            [(1, 1), (3, 4)],\n",
                "            [(1, 1), (1, 2), (2, 13), (40, 60)],\n",
                "            [(1, 1), (1, 2), (2, 13), (40, 60)]),\n",
                "          itertools.product(\n",
                "            [(3, 2, 4, 6)],\n",
                "            [(1, 1, 1, 1), (2, 1, 2, 1)],\n",
                "            [(1, 1, 1, 1), (1, 2, 2, 1), (30, 40, 3, 2)],\n",
                "            [(1, 1, 1, 1), (1, 2, 2, 1), (30, 40, 3, 2)])))\n",
                "     ],\n",
                "  )\n",
                "  def testReduceWindowShapeDilation(self, shape, window_dimensions,\n",
                "                                    base_dilation, window_dilation):\n",
                "    operand, padding, strides = np.ones(shape), 'SAME', (1,) * len(shape)\n",
                "    result = lax.reduce_window(operand, 0., lax.add, padding=padding,\n",
                "                               window_strides=strides,\n",
                "                               window_dimensions=window_dimensions)\n",
                "    # With a stride of 1 in each direction and a padding of 'SAME', the\n",
                "    # shape of the input should be equal to the shape of the result according\n",
                "    # to https://www.tensorflow.org/xla/operation_semantics#reducewindow.\n",
                "    self.assertEqual(shape, result.shape)\n",
                "\n",
                "  def testReduceWindowWithEmptyOutput(self):\n",
                "    # https://github.com/google/jax/issues/10315\n",
                "    shape = (5, 3, 2)\n",
                "    operand, padding, strides = np.ones(shape), 'VALID', (1,) * len(shape)\n",
                "    out = jax.eval_shape(lambda x: lax.reduce_window(x, 0., lax.add, padding=padding,\n",
                "                         window_strides=strides,\n",
                "                         window_dimensions=(3, 1, 1),\n",
                "                         window_dilation=(3, 1, 1)), operand)\n",
                "    self.assertEqual((0, 3, 2), out.shape)\n",
                "\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(op=op, np_op=np_op) for op, np_op in [\n",
                "      (lax.cumsum, np.cumsum),\n",
                "      (lax.cumprod, np.cumprod),\n",
                "      (lax.cummax, np.maximum.accumulate),\n",
                "      (lax.cummin, np.minimum.accumulate),\n",
                "    ]],\n",
                "    [dict(shape=shape, axis=axis)\n",
                "     for shape in [[10], [3, 4, 5]] for axis in range(len(shape))],\n",
                "    dtype=default_dtypes,\n",
                "    reverse=[False, True],\n",
                "  )\n",
                "  def testCumulativeReduce(self, op, np_op, shape, dtype, axis, reverse):\n",
                "    rng_factory = (jtu.rand_default if dtypes.issubdtype(dtype, np.integer)\n",
                "                   else jtu.rand_small)\n",
                "    rng = rng_factory(self.rng())\n",
                "    fun = partial(op, axis=axis, reverse=reverse)\n",
                "    def np_fun(x):\n",
                "      if reverse:\n",
                "        return np.flip(np_op(np.flip(x, axis), axis=axis, dtype=dtype), axis)\n",
                "      else:\n",
                "        return np_op(x, axis=axis, dtype=dtype)\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "    self._CheckAgainstNumpy(np_fun, fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, axis=axis)\n",
                "     for shape in [[10], [3, 4, 5]] for axis in range(len(shape))],\n",
                "    dtype=float_dtypes,\n",
                "    reverse=[False, True],\n",
                "  )\n",
                "  def testCumulativeLogSumExp(self, shape, dtype, axis, reverse):\n",
                "    # This op only works on floating-point types, so we've separated out the\n",
                "    # test.\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    fun = partial(lax.cumlogsumexp, axis=axis, reverse=reverse)\n",
                "    def np_fun(x):\n",
                "      if reverse:\n",
                "        return np.flip(np.logaddexp.accumulate(\n",
                "            np.flip(x, axis), axis=axis, dtype=dtype), axis)\n",
                "      else:\n",
                "        return np.logaddexp.accumulate(x, axis=axis, dtype=dtype)\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "    self._CheckAgainstNumpy(np_fun, fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    shape=[(), (3,), (3, 4)],\n",
                "    dtype=float_dtypes,\n",
                "    out_dtype=float_dtypes,\n",
                "  )\n",
                "  def testReducePrecision(self, shape, dtype, out_dtype):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    info = dtypes.finfo(out_dtype)\n",
                "    fun = lambda x: lax.reduce_precision(x, info.nexp, info.nmant)\n",
                "    np_fun = lambda x: np.asarray(x).astype(out_dtype).astype(dtype)\n",
                "    self._CheckAgainstNumpy(np_fun, fun, args_maker)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, axis=axis)\n",
                "     for shape in [(5,), (5, 7)] for axis in [-1, len(shape) - 1]],\n",
                "    dtype=all_dtypes,\n",
                "    is_stable=[False, True],\n",
                "  )\n",
                "  def testSort(self, shape, dtype, axis, is_stable):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    fun = lambda x: lax.sort(x, dimension=axis, is_stable=is_stable)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(dtype=float_dtypes)\n",
                "  def testSortFloatSpecialValues(self, dtype):\n",
                "    # Test confirms that\n",
                "    # - NaNs are sorted to the end, regardless of representation\n",
                "    # - sign bit of 0.0 is ignored\n",
                "    x = jnp.array([-np.inf, 0.0, -0.0, np.inf, np.nan, -np.nan], dtype=dtype)\n",
                "    index = lax.iota(dtypes.int_, x.size)\n",
                "    argsort = lambda x: lax.sort_key_val(x, lax.iota(dtypes.int_, x.size), is_stable=True)[1]\n",
                "    self.assertArraysEqual(argsort(x), index)\n",
                "    self.assertArraysEqual(jax.jit(argsort)(x), index)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, axis=axis)\n",
                "     for shape in [(5,), (5, 7)] for axis in [-1, len(shape) - 1]],\n",
                "    dtype=all_dtypes,\n",
                "    is_stable=[False, True],\n",
                "  )\n",
                "  def testSortAgainstNumpy(self, shape, dtype, axis, is_stable):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    op = lambda x: lax.sort(x, dimension=axis, is_stable=is_stable)\n",
                "    def numpy_op(x):\n",
                "      if is_stable:\n",
                "        return lax_reference.sort(x, axis, kind='stable')\n",
                "      else:\n",
                "        return lax_reference.sort(x, axis)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, axis=axis)\n",
                "     for shape in [(3,), (5, 3)] for axis in [-1, len(shape) - 1]],\n",
                "    key_dtype=float_dtypes + complex_dtypes + int_dtypes + uint_dtypes,\n",
                "    val_dtype=[np.float32, np.int32, np.uint32],\n",
                "    is_stable=[False, True],\n",
                "  )\n",
                "  def testSortKeyVal(self, shape, key_dtype, val_dtype, axis, is_stable):\n",
                "    if (np.issubdtype(key_dtype, np.complexfloating) and\n",
                "        jtu.device_under_test() == \"cpu\"):\n",
                "      raise SkipTest(\"Complex-valued sort not implemented\")\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    # This test relies on the property that wherever keys are tied, values are\n",
                "    # too, since we don't guarantee the same ordering of values with equal keys.\n",
                "    # To avoid that case, we generate unique keys (globally in the key array).\n",
                "    def args_maker():\n",
                "      flat_keys = np.arange(prod(shape), dtype=key_dtype)\n",
                "      keys = self.rng().permutation(flat_keys).reshape(shape)\n",
                "      values = rng(shape, val_dtype)\n",
                "      return keys, values\n",
                "\n",
                "    fun = lambda keys, values: lax.sort_key_val(keys, values, axis, is_stable)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, num_keys=num_keys)\n",
                "     for shape in [(3, 5), (4, 3)] for num_keys in range(1, shape[0] + 1)],\n",
                "    dtype=all_dtypes,\n",
                "  )\n",
                "  def testSortNumKeys(self, shape, dtype, num_keys):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    args_maker = lambda: [rng(shape, dtype)]\n",
                "    lax_fun = lambda x: lax.sort(tuple(x), num_keys=num_keys)\n",
                "    numpy_fun = lambda x: tuple(x[:, np.lexsort(x[:num_keys][::-1])])\n",
                "    # self._CompileAndCheck(lax_fun, args_maker)\n",
                "    self._CheckAgainstNumpy(numpy_fun, lax_fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, axis=axis)\n",
                "     for shape in [(3,), (5, 3)] for axis in [-1, len(shape) - 1]],\n",
                "    key_dtype=float_dtypes + complex_dtypes + int_dtypes + uint_dtypes,\n",
                "    val_dtype=[np.float32, np.int32, np.uint32],\n",
                "  )\n",
                "  def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis):\n",
                "    if (np.issubdtype(key_dtype, np.complexfloating) and\n",
                "        jtu.device_under_test() == \"cpu\"):\n",
                "      raise SkipTest(\"Complex-valued sort not implemented\")\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    # This test relies on the property that wherever keys are tied, values are\n",
                "    # too, since we don't guarantee the same ordering of values with equal keys.\n",
                "    # To avoid that case, we generate unique keys (globally in the key array).\n",
                "    def args_maker():\n",
                "      flat_keys = np.arange(prod(shape), dtype=key_dtype)\n",
                "      keys = self.rng().permutation(flat_keys).reshape(shape)\n",
                "      values = rng(shape, val_dtype)\n",
                "      return keys, values\n",
                "\n",
                "    op = lambda ks, vs: lax.sort_key_val(ks, vs, axis)\n",
                "    numpy_op = lambda ks, vs: lax_reference.sort_key_val(ks, vs, axis)\n",
                "    self._CheckAgainstNumpy(numpy_op, op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    dtype=[np.float32, np.int32, np.uint32],\n",
                "    shape=[(3,), (5, 3)],\n",
                "    k=[1, 3],\n",
                "  )\n",
                "  def testTopK(self, shape, dtype, k):\n",
                "    def args_maker():\n",
                "      flat_values = np.arange(prod(shape), dtype=dtype)\n",
                "      values = self.rng().permutation(flat_values).reshape(shape)\n",
                "      return [values]\n",
                "    def reference_top_k(x):\n",
                "      bcast_idxs = np.broadcast_to(np.arange(shape[-1], dtype=np.int32), shape)\n",
                "      sorted_vals, sorted_idxs = lax_reference.sort_key_val(x, bcast_idxs)\n",
                "      return sorted_vals[..., :-k-1:-1], sorted_idxs[..., :-k-1:-1]\n",
                "    op = lambda vs: lax.top_k(vs, k=k)\n",
                "    self._CheckAgainstNumpy(op, reference_top_k, args_maker)\n",
                "    self._CompileAndCheck(op, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(lhs_shape=lhs_shape, rhs_shape=rhs_shape)\n",
                "      for lhs_shape, rhs_shape in [((3, 2), (2, 4)),\n",
                "                                   ((5, 3, 2), (5, 2, 4)),\n",
                "                                   ((1, 2, 2, 3), (1, 2, 3, 1))]],\n",
                "    dtype=float_dtypes,\n",
                "  )\n",
                "  def testBatchMatMul(self, lhs_shape, rhs_shape, dtype):\n",
                "    rng = jtu.rand_small(self.rng())\n",
                "    arg_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]\n",
                "    self._CompileAndCheck(lax.batch_matmul, arg_maker)\n",
                "\n",
                "  def testCollapse(self):\n",
                "\n",
                "    @jax.jit\n",
                "    def collapse_first_two(x):\n",
                "      return lax.collapse(x, 0, 2)\n",
                "\n",
                "    self.assertEqual((6,), collapse_first_two(np.zeros((2, 3))).shape)\n",
                "    self.assertEqual((6, 4), collapse_first_two(np.zeros((2, 3, 4))).shape)\n",
                "    self.assertEqual((2, 3, 4),\n",
                "                     collapse_first_two(np.zeros((1, 2, 3, 4))).shape)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, idxs=idxs, axes=axes)\n",
                "      for shape, idxs, axes in [\n",
                "          [(3, 4, 5), (np.array([0, 2, 1]),), (0,)],\n",
                "          [(3, 4, 5), (np.array([-1, -2]),), (0,)],\n",
                "          [(3, 4, 5), (np.array([0, 2]), np.array([1, 3])), (0, 1)],\n",
                "          [(3, 4, 5), (np.array([0, 2]), np.array([1, 3])), [0, 2]],\n",
                "    ]],\n",
                "    dtype=all_dtypes,\n",
                "  )\n",
                "  def testIndexTake(self, shape, dtype, idxs, axes):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    rand_idxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)\n",
                "    args_maker = lambda: [rng(shape, dtype), rand_idxs()]\n",
                "    fun = lambda src, idxs: lax.index_take(src, idxs, axes)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, idxs=idxs, dnums=dnums, slice_sizes=slice_sizes)\n",
                "      for shape, idxs, dnums, slice_sizes in [\n",
                "          ((5,), np.array([[0], [2]]), lax.GatherDimensionNumbers(\n",
                "            offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)),\n",
                "            (1,)),\n",
                "          ((10,), np.array([[0], [0], [0]]), lax.GatherDimensionNumbers(\n",
                "            offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)),\n",
                "            (2,)),\n",
                "          ((10, 5,), np.array([[0], [2], [1]]), lax.GatherDimensionNumbers(\n",
                "            offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)),\n",
                "            (1, 3)),\n",
                "          ((10, 5), np.array([[0, 2], [1, 0]]), lax.GatherDimensionNumbers(\n",
                "            offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)),\n",
                "            (1, 3)),\n",
                "    ]],\n",
                "    dtype=all_dtypes,\n",
                "  )\n",
                "  def testGather(self, shape, dtype, idxs, dnums, slice_sizes):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    rng_idx = jtu.rand_int(self.rng(), high=max(shape))\n",
                "    rand_idxs = lambda: rng_idx(idxs.shape, idxs.dtype)\n",
                "    args_maker = lambda: [rng(shape, dtype), rand_idxs()]\n",
                "    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  # These tests are adapted from the corresponding tests in\n",
                "  # tensorflow/compiler/xla/service/shape_inference_test.cc with slight\n",
                "  # variations to account for the implicit setting of index_vector_dim in JAX.\n",
                "  @parameterized.named_parameters(\n",
                "      {\"testcase_name\": f\"_{testcase_name}\", \"operand_shape\": operand_shape,\n",
                "       \"indices_shape\": indices_shape,\n",
                "       \"dimension_numbers\": lax.GatherDimensionNumbers(\n",
                "          offset_dims=offset_dims,\n",
                "          collapsed_slice_dims=collapsed_slice_dims,\n",
                "          start_index_map=start_index_map),\n",
                "       \"slice_sizes\": slice_sizes, \"msg\": msg}\n",
                "      for (testcase_name, operand_shape, indices_shape, offset_dims,\n",
                "           collapsed_slice_dims, start_index_map, slice_sizes, msg) in [\n",
                "        (\"NonAscendingWindowIndices\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 1),\n",
                "         (4, 5, 6, 8, 7), (), (0, 1, 2, 3, 4), (10, 9, 8, 7, 6),\n",
                "         \"offset_dims in gather op must be sorted\"),\n",
                "        (\"RepeatedWindowIndices\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 1),\n",
                "         (4, 5, 6, 7, 7), (), (0, 1, 2, 3, 4), (10, 9, 8, 7, 6),\n",
                "         \"offset_dims in gather op must not repeat\"),\n",
                "        (\"WindowIndexOutOfBounds\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 1),\n",
                "         (4, 5, 100, 101, 102), (), (0, 1, 2, 3, 4), (10, 9, 8, 7, 6),\n",
                "         \"Offset dimension 2 in gather op is out of bounds\"),\n",
                "        (\"WindowIndexBarelyOutOfBounds\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 1),\n",
                "         (4, 5, 6, 7, 9), (), (0, 1, 2, 3, 4), (10, 9, 8, 7, 6),\n",
                "         \"Offset dimension 4 in gather op is out of bounds\"),\n",
                "        (\"MismatchingElidedWindowDims\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7, 8), (4,), (0, 1, 2, 3, 4), (10, 9, 8, 7, 6),\n",
                "         (\"All components of the offset index in a gather op must either be a \"\n",
                "          \"offset dimension or explicitly collapsed\")),\n",
                "        (\"OutOfBoundsWindowToInputMapping\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7, 8), (0, 1, 2, 3, 19), (0, 1, 2, 3, 4), (10, 9, 8, 7, 6),\n",
                "         \"Invalid collapsed_slice_dims set in gather op; valid range is\"),\n",
                "        (\"RepeatedWindowToInputMapping\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7, 8), (0, 1, 2, 3, 3), (0, 1, 2, 3, 4), (10, 9, 8, 7, 6),\n",
                "         \"collapsed_slice_dims in gather op must not repeat\"),\n",
                "        (\"MismatchingGatherToInputMapping\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7, 8), (), (0, 1, 2, 3), (10, 9, 8, 7, 6),\n",
                "         (\"Gather op has 4 elements in start_index_map and the bound of \"\n",
                "          \"dimension index_vector_dim=4 of indices is 5. These two \"\n",
                "          \"numbers must be equal.\")),\n",
                "        (\"OutOfBoundsGatherToInputMapping\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7, 8), (), (0, 1, 2, 3, 7), (10, 9, 8, 7, 6),\n",
                "         \"Invalid start_index_map\"),\n",
                "        (\"RepeatedGatherToInputMapping\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7, 8), (), (0, 1, 2, 3, 3), (10, 9, 8, 7, 6),\n",
                "         \"start_index_map in gather op must not repeat\"),\n",
                "        (\"NonAscendingElidedWindowDims\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7, 8), (2, 1), (0, 1, 2, 3, 4), (10, 9, 8, 7, 6),\n",
                "         \"collapsed_slice_dims in gather op must be sorted\"),\n",
                "        (\"WindowBoundsTooLarge\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7), (2,), (0, 1, 2, 3, 4), (10, 9, 8, 100, 6),\n",
                "         \"Slice size at index 3 in gather op is out of range\"),\n",
                "        (\"MismatchingNumberOfWindowBounds\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7), (), (0, 1, 2, 3, 4), (10, 9, 8, 7),\n",
                "         \"Gather op must have one slice size for every input dimension\"),\n",
                "        (\"WindowBoundsNot1ForElidedDim\", (10, 9, 8, 7, 6), (5, 4, 3, 2, 5),\n",
                "         (4, 5, 6, 7), (1,), (0, 1, 2, 3, 4), (10, 9, 8, 7, 6),\n",
                "         (\"Gather op can only collapse slice dims with bound 1, but bound \"\n",
                "          \"is 9 for index 1 at position 0.\"))\n",
                "      ]\n",
                "  )\n",
                "  def testGatherShapeCheckingRule(self, operand_shape, indices_shape,\n",
                "                                  dimension_numbers, slice_sizes, msg):\n",
                "    operand = np.ones(operand_shape, dtype=np.int32)\n",
                "    indices = np.ones(indices_shape, dtype=np.int32)\n",
                "\n",
                "    with self.assertRaisesRegex(TypeError, msg):\n",
                "      lax.gather(operand, indices, dimension_numbers, slice_sizes)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(arg_shape=arg_shape, idxs=idxs, update_shape=update_shape,\n",
                "          dnums=dnums)\n",
                "      for arg_shape, idxs, update_shape, dnums in [\n",
                "          ((5,), np.array([[0], [2]]), (2,), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(), inserted_window_dims=(0,),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "          ((10,), np.array([[0], [0], [0]]), (3, 2), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(1,), inserted_window_dims=(),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "          ((10, 5,), np.array([[0], [2], [1]]), (3, 3), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(1,), inserted_window_dims=(0,),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "    ]],\n",
                "    dtype=inexact_dtypes,\n",
                "    mode=[\"clip\", \"fill\", None],\n",
                "  )\n",
                "  def testScatterAdd(self, arg_shape, dtype, idxs, update_shape, dnums, mode):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    rng_idx = jtu.rand_int(self.rng(), high=max(arg_shape))\n",
                "    rand_idxs = lambda: rng_idx(idxs.shape, idxs.dtype)\n",
                "    args_maker = lambda: [rng(arg_shape, dtype), rand_idxs(),\n",
                "                          rng(update_shape, dtype)]\n",
                "    fun = partial(lax.scatter_add, dimension_numbers=dnums, mode=mode)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(arg_shape=arg_shape, idxs=idxs, update_shape=update_shape,\n",
                "          dnums=dnums)\n",
                "      for arg_shape, idxs, update_shape, dnums in [\n",
                "          ((5,), np.array([[0], [2]]), (2,), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(), inserted_window_dims=(0,),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "          ((10,), np.array([[0], [0], [0]]), (3, 2), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(1,), inserted_window_dims=(),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "          ((10, 5,), np.array([[0], [2], [1]], dtype=np.uint64), (3, 3), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(1,), inserted_window_dims=(0,),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "    ]],\n",
                "    dtype=float_dtypes,\n",
                "  )\n",
                "  def testScatterMin(self, arg_shape, dtype, idxs, update_shape, dnums):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    rng_idx = jtu.rand_int(self.rng(), high=max(arg_shape))\n",
                "    rand_idxs = lambda: rng_idx(idxs.shape, idxs.dtype)\n",
                "    args_maker = lambda: [rng(arg_shape, dtype), rand_idxs(),\n",
                "                          rng(update_shape, dtype)]\n",
                "    fun = partial(lax.scatter_min, dimension_numbers=dnums)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(arg_shape=arg_shape, idxs=idxs, update_shape=update_shape,\n",
                "          dnums=dnums)\n",
                "      for arg_shape, idxs, update_shape, dnums in [\n",
                "          ((5,), np.array([[0], [2]]), (2,), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(), inserted_window_dims=(0,),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "          ((10,), np.array([[0], [0], [0]]), (3, 2), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(1,), inserted_window_dims=(),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "          ((10, 5,), np.array([[0], [2], [1]]), (3, 3), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(1,), inserted_window_dims=(0,),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "    ]],\n",
                "    dtype=float_dtypes,\n",
                "  )\n",
                "  def testScatterMax(self, arg_shape, dtype, idxs, update_shape, dnums):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    rng_idx = jtu.rand_int(self.rng(), high=max(arg_shape))\n",
                "    rand_idxs = lambda: rng_idx(idxs.shape, idxs.dtype)\n",
                "    args_maker = lambda: [rng(arg_shape, dtype), rand_idxs(),\n",
                "                          rng(update_shape, dtype)]\n",
                "    fun = partial(lax.scatter_max, dimension_numbers=dnums)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(arg_shape=arg_shape, idxs=idxs, update_shape=update_shape,\n",
                "          dnums=dnums)\n",
                "      for arg_shape, idxs, update_shape, dnums in [\n",
                "          ((5,), np.array([[0], [2]]), (2,), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(), inserted_window_dims=(0,),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "          ((10,), np.array([[0], [0], [0]]), (3, 2), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(1,), inserted_window_dims=(),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "          ((10, 5,), np.array([[0], [2], [1]]), (3, 3), lax.ScatterDimensionNumbers(\n",
                "            update_window_dims=(1,), inserted_window_dims=(0,),\n",
                "            scatter_dims_to_operand_dims=(0,))),\n",
                "    ]],\n",
                "    dtype=float_dtypes,\n",
                "  )\n",
                "  def testScatter(self, arg_shape, dtype, idxs, update_shape, dnums):\n",
                "    rng = jtu.rand_default(self.rng())\n",
                "    rng_idx = jtu.rand_int(self.rng(), high=max(arg_shape))\n",
                "    rand_idxs = lambda: rng_idx(idxs.shape, idxs.dtype)\n",
                "    args_maker = lambda: [rng(arg_shape, dtype), rand_idxs(),\n",
                "                          rng(update_shape, dtype)]\n",
                "    fun = partial(lax.scatter, dimension_numbers=dnums)\n",
                "    self._CompileAndCheck(fun, args_maker)\n",
                "\n",
                "  # These tests are adapted from the corresponding tests in\n",
                "  # tensorflow/compiler/xla/service/shape_inference_test.cc with slight\n",
                "  # variations to account for the implicit setting of index_vector_dim in JAX.\n",
                "  @parameterized.named_parameters(\n",
                "      {\"testcase_name\": f\"_{testcase_name}\", \"operand_shape\": operand_shape,\n",
                "       \"indices\": indices, \"update_shape\": update_shape,\n",
                "       \"dimension_numbers\": lax.ScatterDimensionNumbers(\n",
                "          update_window_dims=update_window_dims,\n",
                "          inserted_window_dims=inserted_window_dims,\n",
                "          scatter_dims_to_operand_dims=scatter_dims_to_operand_dims),\n",
                "       \"msg\": msg}\n",
                "      for (testcase_name, operand_shape, indices, update_shape,\n",
                "           update_window_dims, inserted_window_dims,\n",
                "           scatter_dims_to_operand_dims, msg) in [\n",
                "              (\"ScatterWithUpdatesBiggerThanInput\", (64, 48), np.zeros((32, 1)),\n",
                "               (65, 32), (0,), (1,), (1,), \"Bounds of the window dimensions\"),\n",
                "              (\"ScatterWithUpdatesBiggerThanInputV2\", (64, 48),\n",
                "               np.zeros((32, 1)), (32, 49), (1,), (0,), (1,),\n",
                "               \"Bounds of the window dimensions\"),\n",
                "              (\"ScatterWithUpdatesNotMatchingIndices\", (64, 48),\n",
                "               np.zeros((32, 1)), (64, 31), (0,), (1,), (1,),\n",
                "               \"Bounds of the scatter dimensions\"),\n",
                "              (\"ScatterWithUpdatesNotMatchingIndicesV2\", (64, 48),\n",
                "               np.zeros((32, 1)), (31, 48), (1,), (0,), (1,),\n",
                "               \"Bounds of the scatter dimensions\"),\n",
                "              (\"ScatterNdWithUpdatesBiggerThanInput\", (64, 48),\n",
                "               np.zeros((10, 9, 8, 7, 1)), (10, 9, 8, 7, 65), (4,), (1,),\n",
                "               (0,), \"Bounds of the window dimensions\"),\n",
                "              (\"ScatterNdWithUpdatesNotMatchingIndices\", (64, 48),\n",
                "               np.zeros((10, 9, 8, 7, 1)), (9, 9, 8, 7, 64), (4,), (1,), (0,),\n",
                "               \"Bounds of the scatter dimensions\"),\n",
                "              (\"InvalidUpdates\", (50, 49, 48, 47, 46),\n",
                "               np.zeros((10, 9, 8, 7, 5)), (10, 9, 8, 7, 3, 2, 4, 1),\n",
                "               (4, 5, 6), (1, 2), (0, 1, 2, 3, 4),\n",
                "               \"Updates tensor must be of rank 7; got 8.\"),\n",
                "              (\"NonAscendingUpdateWindowDims\", (6, 5, 4, 3, 2),\n",
                "               np.zeros((5, 4, 3, 2, 1)), (10, 9, 8, 7, 6, 5, 4, 3, 2),\n",
                "               (4, 5, 6, 8, 7), (), (0, 1, 2, 3, 4),\n",
                "               \"update_window_dims in scatter op must be sorted\"),\n",
                "              (\"RepeatedUpdateWindowDims\", (6, 5, 4, 3, 2),\n",
                "               np.zeros((5, 4, 3, 2, 1)), (10, 9, 8, 7, 6, 5, 4, 3, 2),\n",
                "               (4, 5, 6, 7, 7), (), (0, 1, 2, 3, 4),\n",
                "               \"update_window_dims in scatter op must not repeat\"),\n",
                "              (\"OutOfBoundsUpdateWindowDims\", (6, 5, 4, 3, 2),\n",
                "               np.zeros((5, 4, 3, 2, 1)), (10, 9, 8, 7, 6, 5, 4, 3, 2),\n",
                "               (4, 5, 6, 7, 9), (), (0, 1, 2, 3, 4),\n",
                "               \"Invalid update_window_dims set in scatter op\"),\n",
                "              (\"NonAscendingInsertedWindowDims\", (50, 49, 48, 47, 46),\n",
                "               np.zeros((10, 9, 8, 7, 5)), (10, 9, 8, 7, 3, 2, 4),\n",
                "               (4, 5, 6), (2, 1), (0, 1, 2, 3, 4),\n",
                "               \"inserted_window_dims in scatter op must be sorted\"),\n",
                "              (\"RepeatedInsertedWindowDims\", (50, 49, 48, 47, 46),\n",
                "               np.zeros((10, 9, 8, 7, 5)), (10, 9, 8, 7, 3, 2, 4),\n",
                "               (4, 5, 6), (1, 1), (0, 1, 2, 3, 4),\n",
                "               \"inserted_window_dims in scatter op must not repeat\"),\n",
                "              (\"OutOfBoundsInsertedWindowDims\", (50, 49, 48, 47, 46),\n",
                "               np.zeros((10, 9, 8, 7, 5)), (10, 9, 8, 7, 3, 2, 4),\n",
                "               (4, 5, 6), (1, 5), (0, 1, 2, 3, 4),\n",
                "               \"Invalid inserted_window_dims set in scatter op\"),\n",
                "              (\"MismatchingScatterDimsToOperandDims\", (50, 49, 48, 47, 46),\n",
                "               np.zeros((10, 9, 8, 7, 5)), (10, 9, 8, 7, 3, 2, 4),\n",
                "               (4, 5, 6), (1, 2), (0, 1, 2, 3),\n",
                "               (\"Scatter op has 4 elements in scatter_dims_to_operand_dims and \"\n",
                "                \"the bound of dimension index_vector_dim=4 of indices \"\n",
                "                \"is 5. These two numbers must be equal\")),\n",
                "              (\"OutOfBoundsScatterDimsToOperandDims\", (50, 49, 48, 47, 46),\n",
                "               np.zeros((10, 9, 8, 7, 5)), (10, 9, 8, 7, 3, 2, 4),\n",
                "               (4, 5, 6), (1, 2), (0, 1, 2, 3, 10),\n",
                "               \"Invalid scatter_dims_to_operand_dims mapping\"),\n",
                "              (\"RepeatedValuesInScatterDimsToOperandDims\", (50, 49, 48, 47, 46),\n",
                "               np.zeros((10, 9, 8, 7, 5)), (10, 9, 8, 7, 3, 2, 4),\n",
                "               (4, 5, 6), (1, 2), (0, 1, 2, 2, 3),\n",
                "               \"scatter_dims_to_operand_dims in scatter op must not repeat\"),\n",
                "              (\"InsufficientWindowDims\", (50, 49, 48, 47, 46),\n",
                "               np.zeros((10, 9, 8, 7, 5)), (10, 9, 8, 7, 3, 2, 4),\n",
                "               (4, 5, 6), (1,), (0, 1, 2, 3),\n",
                "               (\"Scatter op has window of size 4; doesn't match operand of \"\n",
                "                \"rank 5.\"))\n",
                "           ]\n",
                "      )\n",
                "  def testScatterShapeCheckingRule(self, operand_shape, indices,\n",
                "                                   update_shape, dimension_numbers, msg):\n",
                "\n",
                "    def f(x, y):\n",
                "      operand = lax.broadcast(x, operand_shape)\n",
                "      updates = lax.broadcast(y, update_shape)\n",
                "      return lax.scatter(operand, indices, updates, dimension_numbers)\n",
                "    with self.assertRaisesRegex(TypeError, msg):\n",
                "      jax.eval_shape(f, np.int32(1), np.int32(1))\n",
                "\n",
                "  def testIssue831(self):\n",
                "    # Tests the DeviceTuple constant handler\n",
                "    def f(x):\n",
                "      g = lambda *args: args[1]\n",
                "      return jax.jit(lax.fori_loop, static_argnums=(2,))( 0, 10, g, x)\n",
                "\n",
                "    jax.jit(f)(1.)  # doesn't crash\n",
                "\n",
                "  def testReshapeWithUnusualShapes(self):\n",
                "    ans = lax.reshape(np.ones((3,), np.float32), (lax.add(1, 2), 1))\n",
                "    self.assertAllClose(ans, np.ones((3, 1), np.float32))\n",
                "\n",
                "    self.assertRaisesRegex(\n",
                "      TypeError,\n",
                "      \"Shapes must be 1D sequences of concrete values of integer type.*\",\n",
                "      lambda: lax.reshape(np.ones(3,), (np.array([3, 1]),)))\n",
                "\n",
                "    self.assertRaisesRegex(\n",
                "      TypeError,\n",
                "      \"Shapes must be 1D sequences of concrete values of integer type.*\",\n",
                "      lambda: lax.reshape(np.ones(3,), (1.5, 2.0)))\n",
                "\n",
                "  def testDynamicSliceTypeErrors(self):\n",
                "    self.assertRaisesRegex(\n",
                "      TypeError,\n",
                "      \"index arguments to dynamic_slice must be integers of the same type\",\n",
                "      lambda: lax.dynamic_slice(np.ones((3, 4), dtype=np.float32),\n",
                "                                (np.int32(1), np.int16(2)), (2, 2)))\n",
                "\n",
                "  def testDynamicUpdateSliceTypeErrors(self):\n",
                "    self.assertRaisesRegex(\n",
                "      TypeError,\n",
                "      \"index arguments to dynamic_update_slice must be integers of the same \"\n",
                "      \"type\",\n",
                "      lambda: lax.dynamic_update_slice(np.ones((3, 4), dtype=np.float32),\n",
                "                                       np.zeros((2, 2), dtype=np.float32),\n",
                "                                       (np.int32(1), np.int16(2))))\n",
                "\n",
                "  def test_tie_in_error(self):\n",
                "    raise SkipTest(\"test no longer needed after trivializing tie_in\")\n",
                "    # with core.skipping_checks():\n",
                "    #   with self.assertRaisesRegex(\n",
                "    #       TypeError, \".* of type .*tuple.* is not a valid JAX type\"):\n",
                "    #     jax.make_jaxpr(lambda x: lax.tie_in((x, x), 1))(1.)\n",
                "\n",
                "  def test_primitive_jaxtype_error(self):\n",
                "    with jax.enable_checks(False):\n",
                "      with self.assertRaisesRegex(\n",
                "          TypeError, \"Argument .* of type .* is not a valid JAX type\"):\n",
                "        lax.add(1, 'hi')\n",
                "\n",
                "  def test_reduction_with_repeated_axes_error(self):\n",
                "    with self.assertRaisesRegex(ValueError, \"duplicate value in 'axes' .*\"):\n",
                "      lax.reduce(np.arange(3), 0, lax.add, (0, 0))\n",
                "\n",
                "  @parameterized.parameters([lax.rem, lax.lt, lax.gt, lax.ge, lax.le])\n",
                "  def test_ops_do_not_accept_complex_dtypes(self, op):\n",
                "    with self.assertRaisesRegex(TypeError, \".*does not accept dtype complex.*\"):\n",
                "      op(2+3j, 4+5j)\n",
                "\n",
                "  def test_population_count_booleans_not_supported(self):\n",
                "    # https://github.com/google/jax/issues/3886\n",
                "    msg = \"population_count does not accept dtype bool\"\n",
                "    with self.assertRaisesRegex(TypeError, msg):\n",
                "      lax.population_count(True)\n",
                "\n",
                "  def test_conv_general_dilated_different_input_ranks_error(self):\n",
                "    # https://github.com/google/jax/issues/4316\n",
                "    msg = (\"conv_general_dilated lhs and rhs must have the same number of \"\n",
                "           \"dimensions\")\n",
                "    dimension_numbers = lax.ConvDimensionNumbers(lhs_spec=(0, 1, 2),\n",
                "                                                 rhs_spec=(0, 1, 2),\n",
                "                                                 out_spec=(0, 1, 2))\n",
                "    kwargs = { 'window_strides': (1,)\n",
                "             , 'padding': ((0, 0),)\n",
                "             , 'lhs_dilation': (1,)\n",
                "             , 'rhs_dilation': (1,)\n",
                "             , 'dimension_numbers': dimension_numbers\n",
                "             , 'feature_group_count': 1\n",
                "             , 'batch_group_count': 1\n",
                "             , 'precision': None\n",
                "             }\n",
                "    lhs, rhs = np.ones((1, 1, 1)), np.ones((1, 1, 1, 1))\n",
                "    with self.assertRaisesRegex(ValueError, msg):\n",
                "      lax.conv_general_dilated(lhs, rhs, **kwargs)\n",
                "\n",
                "  def test_window_strides_dimension_shape_rule(self):\n",
                "    # https://github.com/google/jax/issues/5087\n",
                "    msg = (\"conv_general_dilated window and window_strides must have \"\n",
                "           \"the same number of dimensions\")\n",
                "    lhs = jax.numpy.zeros((1, 1, 3, 3))\n",
                "    rhs = np.zeros((1, 1, 1, 1))\n",
                "    with self.assertRaisesRegex(ValueError, msg):\n",
                "      jax.lax.conv(lhs, rhs, [1], 'SAME')\n",
                "\n",
                "  def test_reduce_window_scalar_init_value_shape_rule(self):\n",
                "    # https://github.com/google/jax/issues/4574\n",
                "    args = { \"operand\": np.ones((4, 4), dtype=np.int32)\n",
                "           , \"init_value\": np.zeros((1,), dtype=np.int32)\n",
                "           , \"computation\": lax.max\n",
                "           , \"window_dimensions\": (2, 2)\n",
                "           , \"window_strides\": (2, 2)\n",
                "           , \"padding\": \"VALID\"\n",
                "           , \"base_dilation\": (1, 1)\n",
                "           , \"window_dilation\": (1, 1)\n",
                "           }\n",
                "\n",
                "    msg = (r\"reduce_window expected init_values to be scalars but init_values \"\n",
                "           r\"have shapes \\[\\(1,\\)\\].\")\n",
                "    with self.assertRaisesRegex(TypeError, msg):\n",
                "      lax.reduce_window(**args)\n",
                "\n",
                "  def test_reduce_correctly_works_with_pytrees(self):\n",
                "    operands = {'x': [np.ones(5), np.arange(5)]}\n",
                "    init_values = {'x': [0., 0]}\n",
                "    result = lax.reduce(operands, init_values,\n",
                "                        lambda x, y: tree_util.tree_map(lax.add, x, y),\n",
                "                        [0])\n",
                "    self.assertDictEqual(result, {'x': [5., 10]})\n",
                "\n",
                "  def test_reduce_with_mismatched_pytrees_errors(self):\n",
                "    operands = {'x': np.ones(5)}\n",
                "    bad_init_values = {'y': 0.}\n",
                "\n",
                "    with self.assertRaisesRegex(ValueError, 'Operands must have the same '\n",
                "                                'tree structure as init_values'):\n",
                "      lax.reduce(operands, bad_init_values,\n",
                "                 lambda x, y: dict(x=x['x'] + y['x']), [0])\n",
                "\n",
                "  def test_reduce_with_nonscalar_inits_errors(self):\n",
                "    operands = {'x': np.ones(5)}\n",
                "    bad_init_values = {'x': np.ones(5)}\n",
                "\n",
                "    with self.assertRaisesRegex(ValueError,\n",
                "                                'reduce found non-scalar initial value'):\n",
                "      lax.reduce(operands, bad_init_values,\n",
                "                 lambda x, y: dict(x=x['x'] + y['x']), [0])\n",
                "\n",
                "  def test_select_jvp_complexity(self):\n",
                "    jaxpr = jax.make_jaxpr(lambda x: jax.jvp(lambda x: lax.select(True, x, x),\n",
                "                                             (x,), (1.,)))(1.)\n",
                "    self.assertLen(jaxpr.jaxpr.eqns, 2)\n",
                "\n",
                "  def testRngBitGenerator(self):\n",
                "    # This test covers the original behavior of lax.rng_bit_generator, which\n",
                "    # required x64=True, and only checks shapes and jit invariance.\n",
                "    if not config.x64_enabled:\n",
                "      raise SkipTest(\"RngBitGenerator requires 64bit key\")\n",
                "\n",
                "    key = np.array((1, 2)).astype(np.uint64)\n",
                "    def fn(k):\n",
                "      return lax.rng_bit_generator(\n",
                "          k, shape=(5, 7), algorithm=lax.RandomAlgorithm.RNG_THREE_FRY)\n",
                "\n",
                "    out = fn(key)\n",
                "    out_jit = jax.jit(fn)(key)\n",
                "    self.assertEqual(out[0].shape, (2,))\n",
                "    self.assertEqual(out[1].shape, (5, 7))\n",
                "    self.assertArraysEqual(out[0], out_jit[0])\n",
                "    self.assertArraysEqual(out[1], out_jit[1])\n",
                "\n",
                "  def testRngBitGenerator2(self):\n",
                "    def f(key):\n",
                "      return lax.rng_bit_generator(key, shape=(5, 7))\n",
                "\n",
                "    key = np.array((1, 2, 3, 4)).astype(np.uint32)\n",
                "    out1 = f(key)\n",
                "    out2 = jax.jit(f)(key)\n",
                "    self.assertEqual(out1[0].shape, (4,))\n",
                "    self.assertEqual(out1[1].shape, (5, 7))\n",
                "    self.assertArraysEqual(out1[0], out2[0])\n",
                "    self.assertArraysEqual(out1[1], out2[1])\n",
                "\n",
                "  @jtu.skip_on_devices(\"tpu\")\n",
                "  def testRngBitGeneratorReturnedKey(self):\n",
                "    # This test ensures that the key bit-packing/unpacking operations used in\n",
                "    # the translation rule for rng_bit_generator, on older jaxlibs and at time\n",
                "    # of writing on GPU, are inverses of one another.\n",
                "    key = np.array([3, 1, 4, 2], dtype=np.dtype('uint32'))\n",
                "    new_key, _ = lax.rng_bit_generator(key, (0,))\n",
                "    self.assertAllClose(key, new_key)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    dtype=all_dtypes + python_scalar_types,\n",
                "    weak_type=[True, False],\n",
                "  )\n",
                "  def test_const(self, dtype, weak_type):\n",
                "    if dtype in set(python_scalar_types):\n",
                "      val = dtype(0)\n",
                "    else:\n",
                "      val = lax_internal._convert_element_type(0, dtype, weak_type=weak_type)\n",
                "\n",
                "    const = lax_internal._const(val, 0)\n",
                "    self.assertEqual(dtypes.dtype(val, canonicalize=True),\n",
                "                     dtypes.dtype(const, canonicalize=True))\n",
                "\n",
                "\n",
                "  def testIgammaSpecial(self):\n",
                "    self.assertEqual(lax.igamma(1., np.inf), 1.)\n",
                "    self.assertEqual(lax.igammac(1., np.inf), 0.)\n",
                "\n",
                "  def testRegressionIssue5728(self):\n",
                "    # The computation in this test gave garbage data on CPU due to an LLVM bug.\n",
                "    @jax.jit\n",
                "    def f(inputs):\n",
                "      out_action_2 = lax.slice_in_dim(inputs, 0, 15, axis=-1)\n",
                "      mask = lax.slice_in_dim(inputs, 7, 22, axis=-1)\n",
                "      out_action_2 = lax.select(lax.eq(mask, np.float32(0)),\n",
                "                                lax.broadcast(np.float32(42), (1, 15)),\n",
                "                                out_action_2)\n",
                "      return lax.pad(out_action_2, np.float32(42), [(0, 0, 0), (0, 15, 0)])\n",
                "    self.assertArraysEqual(np.full((1, 30), np.float32(42)),\n",
                "                           f(np.zeros((1, 24), dtype=np.float32)))\n",
                "\n",
                "  def testDynamicSliceU8Index(self):\n",
                "    # Regression test for u8 index in dynamic-slice (#6122)\n",
                "    # TODO(b/183216273): enable this test for CPU & GPU when possible.\n",
                "    if jtu.device_under_test() == \"cpu\":\n",
                "      raise unittest.SkipTest(\"DynamicSliceU8Index test is a known failure on CPU.\")\n",
                "    if jtu.device_under_test() == \"gpu\":\n",
                "      raise unittest.SkipTest(\"DynamicSliceU8Index test is a known failure on GPU.\")\n",
                "    x = np.arange(200)\n",
                "    np.testing.assert_equal(\n",
                "        np.array(lax.dynamic_slice(x, np.uint8([128]), (1,))), [128])\n",
                "\n",
                "\n",
                "class LazyConstantTest(jtu.JaxTestCase):\n",
                "  def _Check(self, make_const, expected):\n",
                "    # check casting to ndarray works\n",
                "    asarray_result = np.asarray(make_const())\n",
                "\n",
                "    # check passing as an argument works (should hit constant handler)\n",
                "    zero = np.array(0, expected.dtype)\n",
                "    argument_result = lax.add(zero, make_const())\n",
                "\n",
                "    # check looping into a compiled computation works\n",
                "    jit_result = jax.jit(lambda x: lax.add(x, make_const()))(zero)\n",
                "\n",
                "    # ensure they're all the same\n",
                "    self.assertAllClose(asarray_result, expected)\n",
                "    self.assertAllClose(argument_result, expected)\n",
                "    self.assertAllClose(jit_result, expected)\n",
                "\n",
                "    # ensure repr doesn't crash\n",
                "    repr(make_const())\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    dtype=default_dtypes + [None],\n",
                "    shape=[(), (3,), (2, 3), (2, 3, 4), (1001, 1001)],\n",
                "    fill_value=[0, 1, np.pi],\n",
                "  )\n",
                "  def testFilledConstant(self, shape, fill_value, dtype):\n",
                "    make_const = lambda: lax.full(shape, fill_value, dtype)\n",
                "    expected = np.full(shape, fill_value,\n",
                "                        dtype or dtypes.dtype(fill_value))\n",
                "    self._Check(make_const, expected)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, dimension=dimension)\n",
                "     for shape in [(), (3,), (2, 3), (2, 3, 4)]\n",
                "     # TODO(mattjj): re-enable (1001, 1001), (101, 101, 101),\n",
                "     for dimension in range(len(shape))],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testIotaConstant(self, dtype, shape, dimension):\n",
                "    make_const = lambda: lax.broadcasted_iota(dtype, shape, dimension)\n",
                "\n",
                "    arr = np.arange(shape[dimension], dtype=dtypes.canonicalize_dtype(dtype))\n",
                "    singleton_shape = [1] * len(shape)\n",
                "    singleton_shape[dimension] = shape[dimension]\n",
                "    expected = np.broadcast_to(arr.reshape(singleton_shape), shape)\n",
                "\n",
                "    self._Check(make_const, expected)\n",
                "\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    [dict(shape=shape, axes=axes)\n",
                "      for shape, axes in [\n",
                "          [(2, 3), (0, 1)],\n",
                "          [(2, 3, 4), (0, 1)],\n",
                "          [(2, 3, 4), (0, 2)],\n",
                "          [(2, 3, 4), (1, 2)],\n",
                "          [(2, 3, 4), (0, 1, 2)],\n",
                "          [(2, 3, 4, 2), (0, 1, 2)],\n",
                "          [(2, 3, 4, 2), (0, 2, 3)],\n",
                "          [(1001, 1001), (0, 1)],\n",
                "      ]],\n",
                "    dtype=default_dtypes,\n",
                "  )\n",
                "  def testDeltaConstant(self, dtype, shape, axes):\n",
                "    make_const = lambda: lax_internal._delta(dtype, shape, axes)\n",
                "    # don't check the asarray case, just assume it's right\n",
                "    expected = np.asarray(make_const())\n",
                "    self._Check(make_const, expected)\n",
                "\n",
                "  def testBroadcastInDim(self):\n",
                "    arr = lax.full((2, 1), 1.) + 1.\n",
                "    arr_np = np.full((2, 1), 1.) + 1.\n",
                "    expected = lax_reference.broadcast_in_dim(arr_np, (2, 1, 3), (0, 2))\n",
                "    make_const = lambda: lax.broadcast_in_dim(arr, (2, 1, 3), (0, 2))\n",
                "    self._Check(make_const, expected)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    input_type=[int, float, np.int32, np.float32, np.array],\n",
                "    dtype=[np.int32, np.float32],\n",
                "    jit=[True, False],\n",
                "    value=[0, 1],\n",
                "  )\n",
                "  def testConvertElementReturnType(self, input_type, dtype, value, jit):\n",
                "    op = lambda x: lax.convert_element_type(x, dtype)\n",
                "    if jit:\n",
                "      op = jax.jit(op)\n",
                "    result = op(input_type(value))\n",
                "    assert isinstance(result, jax.Array)\n",
                "\n",
                "  @jtu.sample_product(dtype_in=all_dtypes, dtype_out=all_dtypes)\n",
                "  @jtu.ignore_warning(category=np.ComplexWarning)\n",
                "  def testConvertElementTypeAvoidsCopies(self, dtype_in, dtype_out):\n",
                "    x = jax.device_put(np.zeros(5, dtype_in))\n",
                "    self.assertEqual(x.dtype, dtype_in)\n",
                "    y = lax.convert_element_type(x, dtype_out)\n",
                "    self.assertEqual(y.dtype, dtype_out)\n",
                "    if config.jax_array:\n",
                "      x_buf = x._arrays[0]\n",
                "      y_buf = y._arrays[0]\n",
                "    else:\n",
                "      x_buf = x.device_buffer\n",
                "      y_buf = y.device_buffer\n",
                "    if np.dtype(dtype_in) == np.dtype(dtype_out):\n",
                "      self.assertEqual(x_buf.unsafe_buffer_pointer(),\n",
                "                       y_buf.unsafe_buffer_pointer())\n",
                "    else:\n",
                "      self.assertNotEqual(x_buf.unsafe_buffer_pointer(),\n",
                "                          y_buf.unsafe_buffer_pointer())\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    index_dtype=jtu.dtypes.all_inexact + jtu.dtypes.boolean,\n",
                "    jax_fn=[lax.argmin, lax.argmax],\n",
                "  )\n",
                "  def testArgMinMaxIndexDtypeError(self, jax_fn, index_dtype):\n",
                "    with self.assertRaisesRegex(TypeError,\n",
                "                                \"index_dtype must be an integer type\"):\n",
                "      jax_fn(np.ones((2, 2)), axis=0, index_dtype=index_dtype)\n",
                "\n",
                "  @parameterized.parameters([lax.argmin, lax.argmax])\n",
                "  def testArgMinMaxEmptyError(self, jax_fn):\n",
                "    with self.assertRaisesRegex(ValueError,\n",
                "                                \"require non-empty reduced dimension\"):\n",
                "      jax_fn(np.ones((0, 2)), axis=0, index_dtype=np.int32)\n",
                "\n",
                "  @parameterized.parameters([lax.argmin, lax.argmax])\n",
                "  def testArgMinMaxInvalidAxisError(self, jax_fn):\n",
                "    with self.assertRaisesRegex(ValueError,\n",
                "                                \"Invalid axis -1 for operand\"):\n",
                "      jax_fn(np.ones((2, 3)), axis=-1, index_dtype=np.int32)\n",
                "\n",
                "  @jtu.sample_product(\n",
                "    jax_fn=[lax.argmin, lax.argmax],\n",
                "    weak_type=[False, True],\n",
                "  )\n",
                "  def testArgMinMaxWeakType(self, jax_fn, weak_type):\n",
                "    op = lambda x: jax_fn(x, axis=0, index_dtype=np.int32)\n",
                "    x_in = lax_internal._convert_element_type(np.ones((2, 2)),\n",
                "                                              weak_type=weak_type)\n",
                "    self.assertEqual(dtypes.is_weakly_typed(x_in), weak_type)\n",
                "    x_out = op(x_in)\n",
                "    self.assertEqual(dtypes.is_weakly_typed(x_out), False)\n",
                "    x_out_jit = jax.jit(op)(x_in)\n",
                "    self.assertEqual(dtypes.is_weakly_typed(x_out_jit), False)\n",
                "\n",
                "  def testArgMaxOfNanChoosesNaN(self):\n",
                "    self.assertEqual(lax.argmax(np.array([0., np.nan]), axis=0,\n",
                "                                index_dtype=np.int32), 1)\n",
                "\n",
                "  unary_op_types = {}\n",
                "  for r in LAX_OPS:\n",
                "    if r.nargs == 1:\n",
                "      unary_op_types[r.op] = (unary_op_types.get(r.op, set()) |\n",
                "                              {np.dtype(t) for t in r.dtypes})\n",
                "\n",
                "  @parameterized.named_parameters(\n",
                "      {\"testcase_name\": f\"_{op}\", \"op_name\": op, \"rec_dtypes\": dtypes}\n",
                "      for op, dtypes in unary_op_types.items())\n",
                "  def testUnaryWeakTypes(self, op_name, rec_dtypes):\n",
                "    \"\"\"Test that all lax unary ops propagate weak_type information appropriately.\"\"\"\n",
                "    if op_name == \"bitwise_not\":\n",
                "      raise unittest.SkipTest(\"https://github.com/google/jax/issues/12066\")\n",
                "    # Find a valid dtype for the function.\n",
                "    for dtype in [np.float_, np.int_, np.complex_, np.bool_]:\n",
                "      dtype = dtypes.canonicalize_dtype(dtype)\n",
                "      if dtype in rec_dtypes:\n",
                "        py_val = dtype.type(1).item()\n",
                "        lax_val = lax.full((), py_val, dtype)\n",
                "        break\n",
                "    else:\n",
                "      raise ValueError(f\"no available dtypes in {rec_dtypes}\")\n",
                "\n",
                "    op = getattr(lax, op_name)\n",
                "    py_op = op(py_val)\n",
                "    lax_op = op(lax_val)\n",
                "\n",
                "    self.assertAllClose(py_op, lax_op, check_dtypes=True)\n",
                "    self.assertFalse(lax_op.aval.weak_type)\n",
                "    if type(py_val) == bool:\n",
                "      # Booleans should have weak types stripped.\n",
                "      self.assertFalse(py_op.aval.weak_type)\n",
                "    else:\n",
                "      self.assertTrue(py_op.aval.weak_type)\n",
                "\n",
                "  def testCumsumLengthOne(self):\n",
                "    # regression test for issue 4672\n",
                "    x = lax.full((1,), 1)\n",
                "    out = lax.cumsum(x)\n",
                "    self.assertArraysEqual(out, x)\n",
                "\n",
                "  def testLog1pNearOne(self):\n",
                "    expected = np.log1p(np.float32(1e-5))\n",
                "    np.testing.assert_array_almost_equal_nulp(\n",
                "        expected.astype(np.float32), lax.log1p(np.float32(1e-5)))\n",
                "    np.testing.assert_array_almost_equal_nulp(\n",
                "        expected.astype(np.complex64), lax.log1p(np.complex64(1e-5)))\n",
                "\n",
                "\n",
                "class LaxNamedShapeTest(jtu.JaxTestCase):\n",
                "\n",
                "  def test_abstract_eval(self):\n",
                "    aval1 = core.ShapedArray((2, 3), np.float32, False, {'i': 10})\n",
                "    out, _ = lax.sin_p.abstract_eval(aval1)\n",
                "    self.assertEqual(out, aval1)\n",
                "\n",
                "    aval1 = core.ShapedArray((2, 3), np.float32, False, {'i': 10})\n",
                "    aval2 = core.ShapedArray((2, 3), np.float32, False, {'j': 5})\n",
                "    expected = core.ShapedArray((2, 3), np.float32, False, {'i': 10, 'j': 5})\n",
                "    out, _ = lax.add_p.abstract_eval(aval1, aval2)\n",
                "    self.assertEqual(out, expected)\n",
                "\n",
                "  def test_abstract_eval_collective(self):\n",
                "    with core.extend_axis_env('i', 10, None):\n",
                "      aval1 = core.ShapedArray((2, 3), np.float32, False, {'i': 10, 'j': 5})\n",
                "      expected = core.ShapedArray((2, 3), np.float32, False, {'j': 5})\n",
                "      (out,), _ = lax.psum_p.abstract_eval(aval1, axes=('i',), axis_index_groups=None)\n",
                "      self.assertEqual(out, expected)\n",
                "\n",
                "class FooTyRules:\n",
                "  # handlers\n",
                "\n",
                "  @staticmethod\n",
                "  def physical_avals(aval):\n",
                "    return [core.ShapedArray((*aval.shape, 2), jnp.dtype('uint32'))]\n",
                "\n"
            ],
            {
                "type": "delete",
                "before": [
                    "  @staticmethod\n",
                    "  def aval_to_ir_types(aval):\n",
                    "    aval2, = FooTyRules.physical_avals(aval)\n",
                    "    return mlir.aval_to_ir_types(aval2)\n",
                    "\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 2833,
                    "end": 2838
                },
                "child_version_range": {
                    "start": 2833,
                    "end": 2833
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "FooTyRules",
                        "signature": "class FooTyRules:",
                        "at_line": 2826
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: tests/lax_test.py\nCode:\n             class FooTyRules:\n                 ...\n2830 2830      def physical_avals(aval):\n2831 2831        return [core.ShapedArray((*aval.shape, 2), jnp.dtype('uint32'))]\n2832 2832    \n2833       -   @staticmethod\n2834       -   def aval_to_ir_types(aval):\n2835       -     aval2, = FooTyRules.physical_avals(aval)\n2836       -     return mlir.aval_to_ir_types(aval2)\n2837       - \n2838 2833      @staticmethod\n2839 2834      def physical_op_sharding(aval, sharding):\n2840 2835        return sharding._to_xla_op_sharding(aval.ndim)\n           ...\n",
                "file_path": "tests/lax_test.py",
                "identifiers_before": [
                    "FooTyRules",
                    "aval",
                    "aval2",
                    "aval_to_ir_types",
                    "mlir",
                    "physical_avals",
                    "staticmethod"
                ],
                "identifiers_after": [],
                "prefix": [
                    "  def physical_avals(aval):\n",
                    "    return [core.ShapedArray((*aval.shape, 2), jnp.dtype('uint32'))]\n",
                    "\n"
                ],
                "suffix": [
                    "  @staticmethod\n",
                    "  def physical_op_sharding(aval, sharding):\n",
                    "    return sharding._to_xla_op_sharding(aval.ndim)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "  @staticmethod\n",
                "  def physical_op_sharding(aval, sharding):\n",
                "    return sharding._to_xla_op_sharding(aval.ndim)\n",
                "\n",
                "  @staticmethod\n",
                "  def result_handler(sticky_device, aval):\n",
                "    def handler(_, buf):\n",
                "      buf.aval = core.ShapedArray(buf.shape, buf.dtype)\n",
                "      return FooArray(aval.shape, buf)\n",
                "    return handler\n",
                "\n",
                "  @staticmethod\n",
                "  def global_sharded_result_handler(aval, out_sharding, committed,\n",
                "                                    is_out_sharding_from_xla):\n",
                "    def handler(bufs):\n",
                "      buf, = bufs\n",
                "      buf.aval = core.ShapedArray(buf.shape, buf.dtype)\n",
                "      return FooArray(aval.shape, buf)\n",
                "    return handler\n",
                "\n",
                "  # element-type-polymorphic primitive lowering rules\n",
                "\n",
                "  @staticmethod\n",
                "  def empty_mlir(ctx, aval_out):\n",
                "    return mlir.ir_constants(np.zeros((2,), dtype=np.dtype('uint32')))\n",
                "\n",
                "  @staticmethod\n",
                "  def slice_mlir(ctx, aval_out, x, start_indices, limit_indices, strides):\n",
                "    start_indices = (*start_indices, 0)\n",
                "    limit_indices = (*limit_indices, 2)\n",
                "    strides = (*strides, 1)\n",
                "    return hlo.SliceOp(x,\n",
                "                       mlir.dense_int_elements(start_indices),\n",
                "                       mlir.dense_int_elements(limit_indices),\n",
                "                       mlir.dense_int_elements(strides)).result\n",
                "\n",
                "  @staticmethod\n",
                "  def dynamic_slice_mlir(ctx, aval_out, x, start_indices):\n",
                "    dtype = dtypes.canonicalize_dtype(np.dtype('int64'))\n",
                "    start_indices = (*start_indices, mlir.ir_constant(np.array(0, dtype=dtype)))\n",
                "    slice_sizes_ = mlir.dense_int_elements((*aval_out.shape, 2))\n",
                "    return hlo.DynamicSliceOp(x, start_indices, slice_sizes_).result\n",
                "\n",
                "  @staticmethod\n",
                "  def dynamic_update_slice_mlir(ctx, aval_out, x, update, *start_indices):\n",
                "    aval_out, = ctx.avals_out\n",
                "    dtype = dtypes.canonicalize_dtype(np.dtype('int64'))\n",
                "    start_indices = (*start_indices, mlir.ir_constant(np.array(0, dtype=dtype)))\n",
                "    if xc.mlir_api_version < 40:\n",
                "      return hlo.DynamicUpdateSliceOp(\n",
                "          mlir.aval_to_ir_type(aval_out), x, update, start_indices).result\n",
                "    else:\n",
                "      return hlo.DynamicUpdateSliceOp(x, update, start_indices).result\n",
                "\n",
                "  @staticmethod\n",
                "  def broadcast_in_dim_mlir(ctx, aval_out, x, broadcast_dimensions):\n",
                "    broadcast_dimensions = [*broadcast_dimensions, aval_out.ndim]\n",
                "    return hlo.BroadcastInDimOp(\n",
                "        mlir.aval_to_ir_type(aval_out), x,\n",
                "        mlir.dense_int_elements(broadcast_dimensions)).result\n",
                "\n",
                "  @staticmethod\n",
                "  def transpose_mlir(ctx, aval_out, x, *, permutation):\n",
                "    perm = [*permutation, len(permutation)]\n",
                "    return hlo.TransposeOp(x, mlir.dense_int_elements(perm)).result\n",
                "\n",
                "  @staticmethod\n",
                "  def gather_mlir(ctx, avals_in, aval_out, x, indices, *,\n",
                "                  dimension_numbers, slice_sizes, unique_indices,\n",
                "                  indices_are_sorted, mode, fill_value):\n",
                "    aval_x, aval_indices = avals_in\n",
                "    aval_y = aval_out\n",
                "    dimension_numbers = dimension_numbers._replace(\n",
                "        offset_dims=(*dimension_numbers.offset_dims, aval_y.ndim))\n",
                "    slice_sizes = (*slice_sizes, 2)\n",
                "    gather_lower = partial(\n",
                "        lax_internal.slicing._gather_lower, dimension_numbers=dimension_numbers,\n",
                "        slice_sizes=slice_sizes, unique_indices=unique_indices,\n",
                "        indices_are_sorted=indices_are_sorted, mode=mode, fill_value=fill_value)\n",
                "    aval_x_raw = core.ShapedArray((*aval_x.shape, 2), np.dtype('uint32'))\n",
                "    aval_y_raw = core.ShapedArray((*aval_y.shape, 2), np.dtype('uint32'))\n",
                "    return mlir.delegate_lowering(ctx, gather_lower, x, indices,\n",
                "                                  avals_in=[aval_x_raw, aval_indices],\n",
                "                                  avals_out=[aval_y_raw])[0]\n",
                "\n",
                "\n",
                "class FooTy:\n",
                "  name = 'foo'\n",
                "  _rules = FooTyRules\n",
                "\n",
                "  def __hash__(self) -> int:\n",
                "    return hash(FooTy)\n",
                "  def __eq__(self, other) -> bool:\n",
                "    return type(other) is FooTy\n",
                "  def __repr__(self) -> str:\n",
                "    return self.name\n",
                "  __str__ = __repr__\n",
                "\n",
                "# primitives\n",
                "\n",
                "make_p = core.Primitive('make')\n",
                "bake_p = core.Primitive('bake')\n",
                "take_p = core.Primitive('take')\n",
                "\n",
                "def make(shape): return make_p.bind(shape=tuple(shape))\n",
                "def bake(k):     return bake_p.bind(k)\n",
                "def take(k):     return take_p.bind(k)\n",
                "\n",
                "@make_p.def_abstract_eval\n",
                "def make_abstract_eval(*, shape):\n",
                "  return core.ShapedArray(shape, FooTy())\n",
                "\n",
                "@bake_p.def_abstract_eval\n",
                "def bake_abstract_eval(x):\n",
                "  if type(x.dtype) != FooTy: raise TypeError\n",
                "  return core.ShapedArray(tuple(reversed(x.shape)), FooTy())\n",
                "\n",
                "@take_p.def_abstract_eval\n",
                "def take_abstract_eval(x):\n",
                "  return core.ShapedArray(x.shape, jnp.dtype('float32'))\n",
                "\n",
                "# runtime ('outside jit') data types\n",
                "\n",
                "class FooArray:\n",
                "  shape: Tuple[int, ...]\n",
                "  data: jnp.ndarray\n",
                "\n",
                "  def __init__(self, shape, data):\n",
                "    assert data.shape == (*shape, 2)\n",
                "    self.shape = shape\n",
                "    self.data = data\n",
                "\n",
                "  def __repr__(self) -> str:\n",
                "    shape = ','.join(map(str, self.shape))\n",
                "    return f'foo[{shape}] with value\\n{self.data}'\n",
                "\n",
                "  size = property(lambda self: self.data.size // 2)\n",
                "  ndim = property(lambda self: self.data.ndim - 1)\n",
                "\n",
                "def device_put_foo_array(x: FooArray, device):\n",
                "  if isinstance(x.data, array.ArrayImpl):\n",
                "    return array._device_put_array(x.data, device)\n",
                "  return dispatch._device_put_array(x.data, device)\n",
                "\n",
                "def shard_foo_array_handler(x, devices, indices):\n",
                "  device, = devices\n",
                "  if isinstance(x.data, array.ArrayImpl):\n",
                "    return dispatch._device_put_jax_array(x.data, device)\n",
                "  return dispatch._device_put_array(x.data, device)\n",
                "\n",
                "def foo_array_constant_handler(x, c):\n",
                "  if config.jax_array:\n",
                "    return array._array_mlir_constant_handler(x.data, c)\n",
                "  return mlir._device_array_constant_handler(x.data, c)\n",
                "\n",
                "def make_lowering(*, shape):\n",
                "  return jnp.zeros((*shape, 2), 'uint32')\n",
                "\n",
                "def bake_lowering(k):\n",
                "  return k.T\n",
                "\n",
                "def take_lowering(k):\n",
                "  return jnp.broadcast_to(jnp.float32(k.size), k.shape)\n",
                "\n",
                "\n",
                "def bake_vmap(batched_args, batch_dims):\n",
                "  xs, = batched_args\n",
                "  bdim_in, = batch_dims\n",
                "  ys = bake(xs)\n",
                "  perm = list(reversed(range(xs.ndim)))\n",
                "  bdim_out = perm[bdim_in]\n",
                "  return ys, bdim_out\n",
                "\n",
                "\n",
                "class CustomElementTypesTest(jtu.JaxTestCase):\n",
                "\n",
                "  def setUp(self):\n",
                "    core.opaque_dtypes.add(FooTy)\n",
                "    core.pytype_aval_mappings[FooArray] = \\\n",
                "        lambda x: core.ShapedArray(x.shape, FooTy())\n",
                "    xla.canonicalize_dtype_handlers[FooArray] = lambda x: x\n",
                "    xla.pytype_aval_mappings[FooArray] = \\\n",
                "        lambda x: core.ShapedArray(x.shape, FooTy())\n",
                "    dispatch.device_put_handlers[FooArray] = device_put_foo_array\n",
                "    pxla.shard_arg_handlers[FooArray] = shard_foo_array_handler\n",
                "    mlir._constant_handlers[FooArray] = foo_array_constant_handler\n",
                "    mlir.register_lowering(make_p, mlir.lower_fun(make_lowering, False))\n",
                "    mlir.register_lowering(bake_p, mlir.lower_fun(bake_lowering, False))\n",
                "    mlir.register_lowering(take_p, mlir.lower_fun(take_lowering, False))\n",
                "    batching.defvectorized(take_p)\n",
                "    batching.primitive_batchers[bake_p] = bake_vmap\n",
                "\n",
                "  def tearDown(self):\n",
                "    core.opaque_dtypes.remove(FooTy)\n",
                "    del core.pytype_aval_mappings[FooArray]\n",
                "    del xla.canonicalize_dtype_handlers[FooArray]\n",
                "    del xla.pytype_aval_mappings[FooArray]\n",
                "    del dispatch.device_put_handlers[FooArray]\n",
                "    del mlir._constant_handlers[FooArray]\n",
                "    del mlir._lowerings[make_p]\n",
                "    del mlir._lowerings[bake_p]\n",
                "    del mlir._lowerings[take_p]\n",
                "    del batching.primitive_batchers[take_p]\n",
                "    del batching.primitive_batchers[bake_p]\n",
                "\n",
                "  def test_shaped_array_construction(self):\n",
                "    aval = core.ShapedArray((), FooTy())\n",
                "    self.assertEqual(aval.str_short(), 'foo[]')\n",
                "    aval = core.ShapedArray((3, 4), FooTy())\n",
                "    self.assertEqual(aval.str_short(), 'foo[3,4]')\n",
                "\n",
                "  def test_make_jaxpr_identity(self):\n",
                "    x = types.SimpleNamespace(shape=(3,), dtype=FooTy())\n",
                "    jaxpr = jax.make_jaxpr(lambda x: x)(x).jaxpr\n",
                "    # { lambda ; a:foo[3]. let  in (a,) }\n",
                "    self.assertLen(jaxpr.invars, 1)\n",
                "    a, = jaxpr.invars\n",
                "    self.assertEqual(a.aval, core.ShapedArray((3,), FooTy()))\n",
                "    self.assertLen(jaxpr.outvars, 1)\n",
                "    a, = jaxpr.outvars\n",
                "    self.assertEqual(a.aval, core.ShapedArray((3,), FooTy()))\n",
                "\n",
                "  # tests after here need the primitives\n",
                "\n",
                "  def test_make_jaxpr_with_primitives(self):\n",
                "    def f():\n",
                "      k1 = make((3, 4))\n",
                "      k2 = bake(k1)\n",
                "      x  = take(k2)\n",
                "      return x\n",
                "\n",
                "    jaxpr = jax.make_jaxpr(f)().jaxpr\n",
                "    # { lambda ; . let\n",
                "    #     a:foo[3,4] = make[shape=(3, 4)]\n",
                "    #     b:foo[4,3] = bake a\n",
                "    #     c:f32[4,3] = take b\n",
                "    #   in (c,) }\n",
                "    self.assertLen(jaxpr.invars, 0)\n",
                "    self.assertLen(jaxpr.eqns, 3)\n",
                "    e1, e2, e3 = jaxpr.eqns\n",
                "\n",
                "    self.assertIs(e1.primitive, make_p)\n",
                "    self.assertLen(e1.outvars, 1)\n",
                "    a, = e1.outvars\n",
                "    self.assertEqual(a.aval, core.ShapedArray((3, 4), FooTy()))\n",
                "\n",
                "    self.assertIs(e2.primitive, bake_p)\n",
                "    self.assertLen(e2.outvars, 1)\n",
                "    b, = e2.outvars\n",
                "    self.assertEqual(b.aval, core.ShapedArray((4, 3), FooTy()))\n",
                "\n",
                "    self.assertIs(e3.primitive, take_p)\n",
                "    self.assertLen(e3.outvars, 1)\n",
                "    c, = e3.outvars\n",
                "    self.assertEqual(c.aval, core.ShapedArray((4, 3), np.dtype('float32')))\n",
                "\n",
                "  # tests after here need FooArray and lowerings\n",
                "\n",
                "  def test_jit_closure(self):\n",
                "    k = FooArray((), jnp.arange(2, dtype='uint32'))\n",
                "\n",
                "    @jax.jit\n",
                "    def f():\n",
                "      jnp.add(1, 1)  # make jit not hit trivial dispatch path\n",
                "      return k\n",
                "\n",
                "    y = f()  # doesn't crash\n",
                "    self.assertIsInstance(y, FooArray)\n",
                "    self.assertEqual(y.shape, ())\n",
                "\n",
                "  def test_jit_identity(self):\n",
                "    k = FooArray((), jnp.arange(2, dtype='uint32'))\n",
                "\n",
                "    @jax.jit\n",
                "    def f(k):\n",
                "      jnp.add(1, 1)  # make jit not hit trivial dispatch path\n",
                "      return k\n",
                "\n",
                "    y = f(k)  # doesn't crash\n",
                "    self.assertIsInstance(y, FooArray)\n",
                "    self.assertEqual(y.shape, ())\n",
                "\n",
                "  def test_jit_multiple_primitives(self):\n",
                "    @jax.jit\n",
                "    def f():\n",
                "      k1 = make((3,))\n",
                "      k2 = bake(k1)\n",
                "      y  = take(k2)\n",
                "      return y\n",
                "\n",
                "    y = f()\n",
                "    self.assertArraysAllClose(y, jnp.array([3., 3., 3.]), check_dtypes=False)\n",
                "\n",
                "  def test_scan_jaxpr(self):\n",
                "    ks = jax.jit(lambda: make((3, 4)))()\n",
                "    f = lambda ks: jax.lax.scan(lambda _, k: (None, bake(k)), None, ks)\n",
                "    jaxpr = jax.make_jaxpr(f)(ks).jaxpr\n",
                "    # { lambda ; a:foo[3,4]. let\n",
                "    #     b:foo[3,4] = scan[\n",
                "    #       jaxpr={ lambda ; c:foo[4]. let d:foo[4] = bake c in (d,) }\n",
                "    #     ] a\n",
                "    #   in (b,) }\n",
                "    self.assertLen(jaxpr.invars, 1)\n",
                "    a, = jaxpr.invars\n",
                "    self.assertEqual(a.aval, core.ShapedArray((3, 4), FooTy()))\n",
                "    self.assertLen(jaxpr.eqns, 1)\n",
                "    e, = jaxpr.eqns\n",
                "    self.assertLen(e.outvars, 1)\n",
                "    b, = e.outvars\n",
                "    self.assertEqual(b.aval, core.ShapedArray((3, 4), FooTy()))\n",
                "\n",
                "  def test_scan_lowering(self):\n",
                "    ks = jax.jit(lambda: make((3, 4)))()\n",
                "    f = lambda ks: jax.lax.scan(lambda _, k: (None, bake(k)), None, ks)\n",
                "    _, out = jax.jit(f)(ks)  # doesn't crash\n",
                "    self.assertIsInstance(out, FooArray)\n",
                "    self.assertEqual(out.shape, (3, 4))\n",
                "\n",
                "  def test_vmap(self):\n",
                "    ks = jax.jit(lambda: make((3, 4, 5)))()\n",
                "    ys = jax.vmap(jax.jit(lambda k: take(bake(k))))(ks)\n",
                "    expected = jnp.broadcast_to(3 * 4 * 5, (3, 5, 4)).astype('float32')\n",
                "    self.assertAllClose(ys, expected)\n",
                "\n",
                "  def test_slice(self):\n",
                "    ks = jax.jit(lambda: make((3, 4)))()\n",
                "    ys = jax.jit(lambda x: lax.slice_in_dim(x, 1, 3))(ks)\n",
                "    self.assertIsInstance(ys, FooArray)\n",
                "    self.assertEqual(ys.shape, (2, 4))\n",
                "\n",
                "  def test_dynamic_slice(self):\n",
                "    ks = jax.jit(lambda: make((3, 4)))()\n",
                "    ys = jax.jit(lambda x, i: lax.dynamic_slice_in_dim(x, i, 2))(ks, 1)\n",
                "    self.assertIsInstance(ys, FooArray)\n",
                "    self.assertEqual(ys.shape, (2, 4))\n",
                "\n",
                "  def test_transpose(self):\n",
                "    ks = jax.jit(lambda: make((3, 4)))()\n",
                "    ys = jax.jit(lambda x: x.T)(ks)\n",
                "    self.assertIsInstance(ys, FooArray)\n",
                "    self.assertEqual(ys.shape, (4, 3))\n",
                "\n",
                "  def test_gather(self):\n",
                "    ks = jax.jit(lambda: make((3, 4)))()\n",
                "    ys = jax.jit(lambda x: x[1])(ks)\n",
                "    self.assertIsInstance(ys, FooArray)\n",
                "    self.assertEqual(ys.shape, (4,))\n",
                "\n",
                "    ks = jax.jit(lambda: make((3, 4, 5)))()\n",
                "\n",
                "    ys = jax.jit(lambda x: x[1])(ks)\n",
                "    self.assertIsInstance(ys, FooArray)\n",
                "    self.assertEqual(ys.shape, (4, 5))\n",
                "\n",
                "    ys = jax.jit(lambda x: x[1, 2:4])(ks)\n",
                "    self.assertIsInstance(ys, FooArray)\n",
                "    self.assertEqual(ys.shape, (2, 5))\n",
                "\n",
                "    ys = jax.jit(lambda x: x[1, 2:4, 3])(ks)\n",
                "    self.assertIsInstance(ys, FooArray)\n",
                "    self.assertEqual(ys.shape, (2,))\n",
                "\n",
                "    ys = jax.jit(lambda x: x[:, 2:4, 3:4])(ks)\n",
                "    self.assertIsInstance(ys, FooArray)\n",
                "    self.assertEqual(ys.shape, (3, 2, 1))\n",
                "\n",
                "  def test_xla_reverse_bug(self):\n",
                "    # Regression test for b/248295786\n",
                "    # This was an XLA bug related to an incorrect optimization of reverse\n",
                "    def f(x):\n",
                "      y = jnp.array([2, 5])\n",
                "      return lax.rev(x * y, (0,))\n",
                "    x = jnp.array([1, 2])\n",
                "    self.assertArraysEqual(f(x), jax.jit(f)(x))\n",
                "\n",
                "  # TODO(frostig,mattjj): more polymorphic primitives tests\n",
                "\n",
                "if __name__ == '__main__':\n",
                "  absltest.main(testLoader=jtu.JaxTestLoader())"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "bi-directional",
            "reason": "same func"
        },
        {
            "edit_hunk_pair": [
                0,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                0,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                1,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "implement and use"
        },
        {
            "edit_hunk_pair": [
                2,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        }
    ]
}