{
    "language": "python",
    "commit_url": "https://github.com/home-assistant/core/commit/153b69c971d81d7b1fb7de3d3640dfdca2f11a89",
    "commit_message": "Fix recorder setup hanging if non live schema migration fails (#122207)",
    "commit_snapshots": {
        "homeassistant/components/recorder/core.py": [
            [
                "\"\"\"Support for recording details.\"\"\"\n",
                "\n",
                "from __future__ import annotations\n",
                "\n",
                "import asyncio\n",
                "from collections.abc import Callable, Iterable\n",
                "from concurrent.futures import CancelledError\n",
                "import contextlib\n",
                "from datetime import datetime, timedelta\n",
                "from functools import cached_property\n",
                "import logging\n",
                "import queue\n",
                "import sqlite3\n",
                "import threading\n",
                "import time\n",
                "from typing import TYPE_CHECKING, Any, cast\n",
                "\n",
                "import psutil_home_assistant as ha_psutil\n",
                "from sqlalchemy import (\n",
                "    create_engine,\n",
                "    event as sqlalchemy_event,\n",
                "    exc,\n",
                "    inspect,\n",
                "    select,\n",
                "    update,\n",
                ")\n",
                "from sqlalchemy.engine import Engine\n",
                "from sqlalchemy.engine.interfaces import DBAPIConnection\n",
                "from sqlalchemy.exc import SQLAlchemyError\n",
                "from sqlalchemy.orm import scoped_session, sessionmaker\n",
                "from sqlalchemy.orm.session import Session\n",
                "\n",
                "from homeassistant.components import persistent_notification\n",
                "from homeassistant.const import (\n",
                "    ATTR_ENTITY_ID,\n",
                "    EVENT_HOMEASSISTANT_CLOSE,\n",
                "    EVENT_HOMEASSISTANT_FINAL_WRITE,\n",
                "    EVENT_STATE_CHANGED,\n",
                "    MATCH_ALL,\n",
                ")\n",
                "from homeassistant.core import (\n",
                "    CALLBACK_TYPE,\n",
                "    Event,\n",
                "    EventStateChangedData,\n",
                "    HomeAssistant,\n",
                "    callback,\n",
                ")\n",
                "from homeassistant.helpers.event import (\n",
                "    async_track_time_change,\n",
                "    async_track_time_interval,\n",
                "    async_track_utc_time_change,\n",
                ")\n",
                "from homeassistant.helpers.start import async_at_started\n",
                "from homeassistant.helpers.typing import UNDEFINED, UndefinedType\n",
                "import homeassistant.util.dt as dt_util\n",
                "from homeassistant.util.enum import try_parse_enum\n",
                "from homeassistant.util.event_type import EventType\n",
                "\n",
                "from . import migration, statistics\n",
                "from .const import (\n",
                "    DB_WORKER_PREFIX,\n",
                "    DOMAIN,\n",
                "    KEEPALIVE_TIME,\n",
                "    LAST_REPORTED_SCHEMA_VERSION,\n",
                "    LEGACY_STATES_EVENT_ID_INDEX_SCHEMA_VERSION,\n",
                "    MARIADB_PYMYSQL_URL_PREFIX,\n",
                "    MARIADB_URL_PREFIX,\n",
                "    MAX_QUEUE_BACKLOG_MIN_VALUE,\n",
                "    MIN_AVAILABLE_MEMORY_FOR_QUEUE_BACKLOG,\n",
                "    MYSQLDB_PYMYSQL_URL_PREFIX,\n",
                "    MYSQLDB_URL_PREFIX,\n",
                "    SQLITE_MAX_BIND_VARS,\n",
                "    SQLITE_URL_PREFIX,\n",
                "    STATISTICS_ROWS_SCHEMA_VERSION,\n",
                "    SupportedDialect,\n",
                ")\n",
                "from .db_schema import (\n",
                "    LEGACY_STATES_EVENT_ID_INDEX,\n",
                "    SCHEMA_VERSION,\n",
                "    TABLE_STATES,\n",
                "    Base,\n",
                "    EventData,\n",
                "    Events,\n",
                "    EventTypes,\n",
                "    StateAttributes,\n",
                "    States,\n",
                "    StatesMeta,\n",
                "    Statistics,\n",
                "    StatisticsShortTerm,\n",
                ")\n",
                "from .executor import DBInterruptibleThreadPoolExecutor\n",
                "from .migration import (\n",
                "    EntityIDMigration,\n",
                "    EventsContextIDMigration,\n",
                "    EventTypeIDMigration,\n",
                "    StatesContextIDMigration,\n",
                ")\n",
                "from .models import DatabaseEngine, StatisticData, StatisticMetaData, UnsupportedDialect\n",
                "from .pool import POOL_SIZE, MutexPool, RecorderPool\n",
                "from .queries import get_migration_changes\n",
                "from .table_managers.event_data import EventDataManager\n",
                "from .table_managers.event_types import EventTypeManager\n",
                "from .table_managers.recorder_runs import RecorderRunsManager\n",
                "from .table_managers.state_attributes import StateAttributesManager\n",
                "from .table_managers.states import StatesManager\n",
                "from .table_managers.states_meta import StatesMetaManager\n",
                "from .table_managers.statistics_meta import StatisticsMetaManager\n",
                "from .tasks import (\n",
                "    AdjustLRUSizeTask,\n",
                "    AdjustStatisticsTask,\n",
                "    ChangeStatisticsUnitTask,\n",
                "    ClearStatisticsTask,\n",
                "    CommitTask,\n",
                "    CompileMissingStatisticsTask,\n",
                "    DatabaseLockTask,\n",
                "    EventIdMigrationTask,\n",
                "    ImportStatisticsTask,\n",
                "    KeepAliveTask,\n",
                "    PerodicCleanupTask,\n",
                "    PurgeTask,\n",
                "    RecorderTask,\n",
                "    StatisticsTask,\n",
                "    StopTask,\n",
                "    SynchronizeTask,\n",
                "    UpdateStatesMetadataTask,\n",
                "    UpdateStatisticsMetadataTask,\n",
                "    WaitTask,\n",
                ")\n",
                "from .util import (\n",
                "    async_create_backup_failure_issue,\n",
                "    build_mysqldb_conv,\n",
                "    dburl_to_path,\n",
                "    end_incomplete_runs,\n",
                "    execute_stmt_lambda_element,\n",
                "    get_index_by_name,\n",
                "    is_second_sunday,\n",
                "    move_away_broken_database,\n",
                "    session_scope,\n",
                "    setup_connection_for_dialect,\n",
                "    validate_or_move_away_sqlite_database,\n",
                "    write_lock_db_sqlite,\n",
                ")\n",
                "\n",
                "_LOGGER = logging.getLogger(__name__)\n",
                "\n",
                "DEFAULT_URL = \"sqlite:///{hass_config_path}\"\n",
                "\n",
                "# Controls how often we clean up\n",
                "# States and Events objects\n",
                "EXPIRE_AFTER_COMMITS = 120\n",
                "\n",
                "SHUTDOWN_TASK = object()\n",
                "\n",
                "COMMIT_TASK = CommitTask()\n",
                "KEEP_ALIVE_TASK = KeepAliveTask()\n",
                "WAIT_TASK = WaitTask()\n",
                "ADJUST_LRU_SIZE_TASK = AdjustLRUSizeTask()\n",
                "\n",
                "DB_LOCK_TIMEOUT = 30\n",
                "DB_LOCK_QUEUE_CHECK_TIMEOUT = 10  # check every 10 seconds\n",
                "\n",
                "QUEUE_CHECK_INTERVAL = timedelta(minutes=5)\n",
                "\n",
                "INVALIDATED_ERR = \"Database connection invalidated\"\n",
                "CONNECTIVITY_ERR = \"Error in database connectivity during commit\"\n",
                "\n",
                "# Pool size must accommodate Recorder thread + All db executors\n",
                "MAX_DB_EXECUTOR_WORKERS = POOL_SIZE - 1\n",
                "\n",
                "\n",
                "class Recorder(threading.Thread):\n",
                "    \"\"\"A threaded recorder class.\"\"\"\n",
                "\n",
                "    stop_requested: bool\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        hass: HomeAssistant,\n",
                "        auto_purge: bool,\n",
                "        auto_repack: bool,\n",
                "        keep_days: int,\n",
                "        commit_interval: int,\n",
                "        uri: str,\n",
                "        db_max_retries: int,\n",
                "        db_retry_wait: int,\n",
                "        entity_filter: Callable[[str], bool] | None,\n",
                "        exclude_event_types: set[EventType[Any] | str],\n",
                "    ) -> None:\n",
                "        \"\"\"Initialize the recorder.\"\"\"\n",
                "        threading.Thread.__init__(self, name=\"Recorder\")\n",
                "\n",
                "        self.hass = hass\n",
                "        self.thread_id: int | None = None\n",
                "        self.recorder_and_worker_thread_ids: set[int] = set()\n",
                "        self.auto_purge = auto_purge\n",
                "        self.auto_repack = auto_repack\n",
                "        self.keep_days = keep_days\n",
                "        self.is_running: bool = False\n",
                "        self._hass_started: asyncio.Future[object] = hass.loop.create_future()\n",
                "        self.commit_interval = commit_interval\n",
                "        self._queue: queue.SimpleQueue[RecorderTask | Event] = queue.SimpleQueue()\n",
                "        self.db_url = uri\n",
                "        self.db_max_retries = db_max_retries\n",
                "        self.db_retry_wait = db_retry_wait\n",
                "        self.database_engine: DatabaseEngine | None = None\n",
                "        # Database connection is ready, but non-live migration may be in progress\n",
                "        db_connected: asyncio.Future[bool] = hass.data[DOMAIN].db_connected\n",
                "        self.async_db_connected: asyncio.Future[bool] = db_connected\n",
                "        # Database is ready to use but live migration may be in progress\n",
                "        self.async_db_ready: asyncio.Future[bool] = hass.loop.create_future()\n",
                "        # Database is ready to use and all migration steps completed (used by tests)\n",
                "        self.async_recorder_ready = asyncio.Event()\n",
                "        self._queue_watch = threading.Event()\n",
                "        self.engine: Engine | None = None\n",
                "        self.max_backlog: int = MAX_QUEUE_BACKLOG_MIN_VALUE\n",
                "        self._psutil: ha_psutil.PsutilWrapper | None = None\n",
                "\n",
                "        # The entity_filter is exposed on the recorder instance so that\n",
                "        # it can be used to see if an entity is being recorded and is called\n",
                "        # by is_entity_recorder and the sensor recorder.\n",
                "        self.entity_filter = entity_filter\n",
                "        self.exclude_event_types = exclude_event_types\n",
                "\n",
                "        self.schema_version = 0\n",
                "        self._commits_without_expire = 0\n",
                "        self._event_session_has_pending_writes = False\n",
                "\n",
                "        self.recorder_runs_manager = RecorderRunsManager()\n",
                "        self.states_manager = StatesManager()\n",
                "        self.event_data_manager = EventDataManager(self)\n",
                "        self.event_type_manager = EventTypeManager(self)\n",
                "        self.states_meta_manager = StatesMetaManager(self)\n",
                "        self.state_attributes_manager = StateAttributesManager(self)\n",
                "        self.statistics_meta_manager = StatisticsMetaManager(self)\n",
                "\n",
                "        self.event_session: Session | None = None\n",
                "        self._get_session: Callable[[], Session] | None = None\n",
                "        self._completed_first_database_setup: bool | None = None\n",
                "        self.async_migration_event = asyncio.Event()\n",
                "        self.migration_in_progress = False\n",
                "        self.migration_is_live = False\n",
                "        self.use_legacy_events_index = False\n",
                "        self._database_lock_task: DatabaseLockTask | None = None\n",
                "        self._db_executor: DBInterruptibleThreadPoolExecutor | None = None\n",
                "\n",
                "        self._event_listener: CALLBACK_TYPE | None = None\n",
                "        self._queue_watcher: CALLBACK_TYPE | None = None\n",
                "        self._keep_alive_listener: CALLBACK_TYPE | None = None\n",
                "        self._commit_listener: CALLBACK_TYPE | None = None\n",
                "        self._periodic_listener: CALLBACK_TYPE | None = None\n",
                "        self._nightly_listener: CALLBACK_TYPE | None = None\n",
                "        self._dialect_name: SupportedDialect | None = None\n",
                "        self.enabled = True\n",
                "\n",
                "        # For safety we default to the lowest value for max_bind_vars\n",
                "        # of all the DB types (SQLITE_MAX_BIND_VARS).\n",
                "        #\n",
                "        # We update the value once we connect to the DB\n",
                "        # and determine what is actually supported.\n",
                "        self.max_bind_vars = SQLITE_MAX_BIND_VARS\n",
                "\n",
                "    @property\n",
                "    def backlog(self) -> int:\n",
                "        \"\"\"Return the number of items in the recorder backlog.\"\"\"\n",
                "        return self._queue.qsize()\n",
                "\n",
                "    @cached_property\n",
                "    def dialect_name(self) -> SupportedDialect | None:\n",
                "        \"\"\"Return the dialect the recorder uses.\"\"\"\n",
                "        return self._dialect_name\n",
                "\n",
                "    @property\n",
                "    def _using_file_sqlite(self) -> bool:\n",
                "        \"\"\"Short version to check if we are using sqlite3 as a file.\"\"\"\n",
                "        return self.db_url != SQLITE_URL_PREFIX and self.db_url.startswith(\n",
                "            SQLITE_URL_PREFIX\n",
                "        )\n",
                "\n",
                "    @property\n",
                "    def recording(self) -> bool:\n",
                "        \"\"\"Return if the recorder is recording.\"\"\"\n",
                "        return self._event_listener is not None\n",
                "\n",
                "    def get_session(self) -> Session:\n",
                "        \"\"\"Get a new sqlalchemy session.\"\"\"\n",
                "        if self._get_session is None:\n",
                "            raise RuntimeError(\"The database connection has not been established\")\n",
                "        return self._get_session()\n",
                "\n",
                "    def queue_task(self, task: RecorderTask | Event) -> None:\n",
                "        \"\"\"Add a task to the recorder queue.\"\"\"\n",
                "        self._queue.put(task)\n",
                "\n",
                "    def set_enable(self, enable: bool) -> None:\n",
                "        \"\"\"Enable or disable recording events and states.\"\"\"\n",
                "        self.enabled = enable\n",
                "\n",
                "    @callback\n",
                "    def async_start_executor(self) -> None:\n",
                "        \"\"\"Start the executor.\"\"\"\n",
                "        self._db_executor = DBInterruptibleThreadPoolExecutor(\n",
                "            self.recorder_and_worker_thread_ids,\n",
                "            thread_name_prefix=DB_WORKER_PREFIX,\n",
                "            max_workers=MAX_DB_EXECUTOR_WORKERS,\n",
                "            shutdown_hook=self._shutdown_pool,\n",
                "        )\n",
                "\n",
                "    def _shutdown_pool(self) -> None:\n",
                "        \"\"\"Close the dbpool connections in the current thread.\"\"\"\n",
                "        if self.engine and hasattr(self.engine.pool, \"shutdown\"):\n",
                "            self.engine.pool.shutdown()\n",
                "\n",
                "    @callback\n",
                "    def async_initialize(self) -> None:\n",
                "        \"\"\"Initialize the recorder.\"\"\"\n",
                "        entity_filter = self.entity_filter\n",
                "        exclude_event_types = self.exclude_event_types\n",
                "        queue_put = self._queue.put_nowait\n",
                "\n",
                "        @callback\n",
                "        def _event_listener(event: Event) -> None:\n",
                "            \"\"\"Listen for new events and put them in the process queue.\"\"\"\n",
                "            if event.event_type in exclude_event_types:\n",
                "                return\n",
                "\n",
                "            if entity_filter is None or not (\n",
                "                entity_id := event.data.get(ATTR_ENTITY_ID)\n",
                "            ):\n",
                "                queue_put(event)\n",
                "                return\n",
                "\n",
                "            if isinstance(entity_id, str):\n",
                "                if entity_filter(entity_id):\n",
                "                    queue_put(event)\n",
                "                return\n",
                "\n",
                "            if isinstance(entity_id, list):\n",
                "                for eid in entity_id:\n",
                "                    if entity_filter(eid):\n",
                "                        queue_put(event)\n",
                "                        return\n",
                "                return\n",
                "\n",
                "            # Unknown what it is.\n",
                "            queue_put(event)\n",
                "\n",
                "        self._event_listener = self.hass.bus.async_listen(\n",
                "            MATCH_ALL,\n",
                "            _event_listener,\n",
                "        )\n",
                "        self._queue_watcher = async_track_time_interval(\n",
                "            self.hass,\n",
                "            self._async_check_queue,\n",
                "            QUEUE_CHECK_INTERVAL,\n",
                "            name=\"Recorder queue watcher\",\n",
                "        )\n",
                "\n",
                "    @callback\n",
                "    def _async_keep_alive(self, now: datetime) -> None:\n",
                "        \"\"\"Queue a keep alive.\"\"\"\n",
                "        if self._event_listener:\n",
                "            self.queue_task(KEEP_ALIVE_TASK)\n",
                "\n",
                "    @callback\n",
                "    def _async_commit(self, now: datetime) -> None:\n",
                "        \"\"\"Queue a commit.\"\"\"\n",
                "        if (\n",
                "            self._event_listener\n",
                "            and not self._database_lock_task\n",
                "            and self._event_session_has_pending_writes\n",
                "        ):\n",
                "            self.queue_task(COMMIT_TASK)\n",
                "\n",
                "    @callback\n",
                "    def async_add_executor_job[_T](\n",
                "        self, target: Callable[..., _T], *args: Any\n",
                "    ) -> asyncio.Future[_T]:\n",
                "        \"\"\"Add an executor job from within the event loop.\"\"\"\n",
                "        return self.hass.loop.run_in_executor(self._db_executor, target, *args)\n",
                "\n",
                "    def _stop_executor(self) -> None:\n",
                "        \"\"\"Stop the executor.\"\"\"\n",
                "        if self._db_executor is None:\n",
                "            return\n",
                "        self._db_executor.shutdown()\n",
                "        self._db_executor = None\n",
                "\n",
                "    @callback\n",
                "    def _async_check_queue(self, *_: Any) -> None:\n",
                "        \"\"\"Periodic check of the queue size to ensure we do not exhaust memory.\n",
                "\n",
                "        The queue grows during migration or if something really goes wrong.\n",
                "        \"\"\"\n",
                "        _LOGGER.debug(\"Recorder queue size is: %s\", self.backlog)\n",
                "        if not self._reached_max_backlog():\n",
                "            return\n",
                "        _LOGGER.error(\n",
                "            (\n",
                "                \"The recorder backlog queue reached the maximum size of %s events; \"\n",
                "                \"usually, the system is CPU bound, I/O bound, or the database \"\n",
                "                \"is corrupt due to a disk problem; The recorder will stop \"\n",
                "                \"recording events to avoid running out of memory\"\n",
                "            ),\n",
                "            self.backlog,\n",
                "        )\n",
                "        self._async_stop_queue_watcher_and_event_listener()\n",
                "\n",
                "    def _available_memory(self) -> int:\n",
                "        \"\"\"Return the available memory in bytes.\"\"\"\n",
                "        if not self._psutil:\n",
                "            self._psutil = ha_psutil.PsutilWrapper()\n",
                "        return cast(int, self._psutil.psutil.virtual_memory().available)\n",
                "\n",
                "    def _reached_max_backlog(self) -> bool:\n",
                "        \"\"\"Check if the system has reached the max queue backlog and return True if it has.\"\"\"\n",
                "        # First check the minimum value since its cheap\n",
                "        if self.backlog < MAX_QUEUE_BACKLOG_MIN_VALUE:\n",
                "            return False\n",
                "        # If they have more RAM available, keep filling the backlog\n",
                "        # since we do not want to stop recording events or give the\n",
                "        # user a bad backup when they have plenty of RAM available.\n",
                "        return self._available_memory() < MIN_AVAILABLE_MEMORY_FOR_QUEUE_BACKLOG\n",
                "\n",
                "    @callback\n",
                "    def _async_stop_queue_watcher_and_event_listener(self) -> None:\n",
                "        \"\"\"Stop watching the queue and listening for events.\"\"\"\n",
                "        if self._queue_watcher:\n",
                "            self._queue_watcher()\n",
                "            self._queue_watcher = None\n",
                "        if self._event_listener:\n",
                "            self._event_listener()\n",
                "            self._event_listener = None\n",
                "\n",
                "    @callback\n",
                "    def _async_stop_listeners(self) -> None:\n",
                "        \"\"\"Stop listeners.\"\"\"\n",
                "        self._async_stop_queue_watcher_and_event_listener()\n",
                "        if self._keep_alive_listener:\n",
                "            self._keep_alive_listener()\n",
                "            self._keep_alive_listener = None\n",
                "        if self._commit_listener:\n",
                "            self._commit_listener()\n",
                "            self._commit_listener = None\n",
                "        if self._nightly_listener:\n",
                "            self._nightly_listener()\n",
                "            self._nightly_listener = None\n",
                "        if self._periodic_listener:\n",
                "            self._periodic_listener()\n",
                "            self._periodic_listener = None\n",
                "\n",
                "    async def _async_close(self, event: Event) -> None:\n",
                "        \"\"\"Empty the queue if its still present at close.\"\"\"\n",
                "\n",
                "        # If the queue is full of events to be processed because\n",
                "        # the database is so broken that every event results in a retry\n",
                "        # we will never be able to get though the events to shutdown in time.\n",
                "        #\n",
                "        # We drain all the events in the queue and then insert\n",
                "        # an empty one to ensure the next thing the recorder sees\n",
                "        # is a request to shutdown.\n",
                "        while True:\n",
                "            try:\n",
                "                self._queue.get_nowait()\n",
                "            except queue.Empty:\n",
                "                break\n",
                "        self.queue_task(StopTask())\n",
                "        await self.hass.async_add_executor_job(self.join)\n",
                "\n",
                "    async def _async_shutdown(self, event: Event) -> None:\n",
                "        \"\"\"Shut down the Recorder at final write.\"\"\"\n",
                "        if not self._hass_started.done():\n",
                "            self._hass_started.set_result(SHUTDOWN_TASK)\n",
                "        self.queue_task(StopTask())\n",
                "        self._async_stop_listeners()\n",
                "        await self.hass.async_add_executor_job(self.join)\n",
                "\n",
                "    @callback\n",
                "    def _async_hass_started(self, hass: HomeAssistant) -> None:\n",
                "        \"\"\"Notify that hass has started.\"\"\"\n",
                "        self._hass_started.set_result(None)\n",
                "\n",
                "    @callback\n",
                "    def async_register(self) -> None:\n",
                "        \"\"\"Post connection initialize.\"\"\"\n",
                "        bus = self.hass.bus\n",
                "        bus.async_listen_once(EVENT_HOMEASSISTANT_CLOSE, self._async_close)\n",
                "        bus.async_listen_once(EVENT_HOMEASSISTANT_FINAL_WRITE, self._async_shutdown)\n",
                "        async_at_started(self.hass, self._async_hass_started)\n",
                "\n",
                "    @callback\n",
                "    def _async_startup_failed(self) -> None:\n",
                "        \"\"\"Report startup failure.\"\"\"\n",
                "        # If a live migration failed, we were able to connect (async_db_connected\n",
                "        # marked True), the database was marked ready (async_db_ready marked\n",
                "        # True), the data in the queue cannot be written to the database because\n",
                "        # the schema not in the correct format so we must stop listeners and report\n",
                "        # failure.\n",
                "        if not self.async_db_connected.done():\n",
                "            self.async_db_connected.set_result(False)\n",
                "        if not self.async_db_ready.done():\n",
                "            self.async_db_ready.set_result(False)\n",
                "        persistent_notification.async_create(\n",
                "            self.hass,\n",
                "            \"The recorder could not start, check [the logs](/config/logs)\",\n",
                "            \"Recorder\",\n",
                "        )\n",
                "        self._async_stop_listeners()\n",
                "\n",
                "    @callback\n",
                "    def async_connection_success(self) -> None:\n",
                "        \"\"\"Connect to the database succeeded, schema version and migration need known.\n",
                "\n",
                "        The database may not yet be ready for use in case of a non-live migration.\n",
                "        \"\"\"\n",
                "        self.async_db_connected.set_result(True)\n",
                "\n",
                "    @callback\n",
                "    def async_set_db_ready(self) -> None:\n",
                "        \"\"\"Database live and ready for use.\n",
                "\n",
                "        Called after non-live migration steps are finished.\n",
                "        \"\"\"\n",
                "        if self.async_db_ready.done():\n",
                "            return\n",
                "        self.async_db_ready.set_result(True)\n",
                "        self.async_start_executor()\n",
                "\n",
                "    @callback\n",
                "    def _async_set_recorder_ready_migration_done(self) -> None:\n",
                "        \"\"\"Finish start and mark recorder ready.\n",
                "\n",
                "        Called after all migration steps are finished.\n",
                "        \"\"\"\n",
                "        self._async_setup_periodic_tasks()\n",
                "        self.async_recorder_ready.set()\n",
                "\n",
                "    @callback\n",
                "    def async_nightly_tasks(self, now: datetime) -> None:\n",
                "        \"\"\"Trigger the purge.\"\"\"\n",
                "        if self.auto_purge:\n",
                "            # Purge will schedule the periodic cleanups\n",
                "            # after it completes to ensure it does not happen\n",
                "            # until after the database is vacuumed\n",
                "            repack = self.auto_repack and is_second_sunday(now)\n",
                "            purge_before = dt_util.utcnow() - timedelta(days=self.keep_days)\n",
                "            self.queue_task(PurgeTask(purge_before, repack=repack, apply_filter=False))\n",
                "        else:\n",
                "            self.queue_task(PerodicCleanupTask())\n",
                "\n",
                "    @callback\n",
                "    def _async_five_minute_tasks(self, now: datetime) -> None:\n",
                "        \"\"\"Run tasks every five minutes.\"\"\"\n",
                "        self.queue_task(ADJUST_LRU_SIZE_TASK)\n",
                "        self.async_periodic_statistics()\n",
                "\n",
                "    def _adjust_lru_size(self) -> None:\n",
                "        \"\"\"Trigger the LRU adjustment.\n",
                "\n",
                "        If the number of entities has increased, increase the size of the LRU\n",
                "        cache to avoid thrashing.\n",
                "        \"\"\"\n",
                "        if new_size := self.hass.states.async_entity_ids_count() * 2:\n",
                "            self.state_attributes_manager.adjust_lru_size(new_size)\n",
                "            self.states_meta_manager.adjust_lru_size(new_size)\n",
                "            self.statistics_meta_manager.adjust_lru_size(new_size)\n",
                "\n",
                "    @callback\n",
                "    def async_periodic_statistics(self) -> None:\n",
                "        \"\"\"Trigger the statistics run.\n",
                "\n",
                "        Short term statistics run every 5 minutes\n",
                "        \"\"\"\n",
                "        start = statistics.get_start_time()\n",
                "        self.queue_task(StatisticsTask(start, True))\n",
                "\n",
                "    @callback\n",
                "    def async_adjust_statistics(\n",
                "        self,\n",
                "        statistic_id: str,\n",
                "        start_time: datetime,\n",
                "        sum_adjustment: float,\n",
                "        adjustment_unit: str,\n",
                "    ) -> None:\n",
                "        \"\"\"Adjust statistics.\"\"\"\n",
                "        self.queue_task(\n",
                "            AdjustStatisticsTask(\n",
                "                statistic_id, start_time, sum_adjustment, adjustment_unit\n",
                "            )\n",
                "        )\n",
                "\n",
                "    @callback\n",
                "    def async_clear_statistics(self, statistic_ids: list[str]) -> None:\n",
                "        \"\"\"Clear statistics for a list of statistic_ids.\"\"\"\n",
                "        self.queue_task(ClearStatisticsTask(statistic_ids))\n",
                "\n",
                "    @callback\n",
                "    def async_update_statistics_metadata(\n",
                "        self,\n",
                "        statistic_id: str,\n",
                "        *,\n",
                "        new_statistic_id: str | UndefinedType = UNDEFINED,\n",
                "        new_unit_of_measurement: str | None | UndefinedType = UNDEFINED,\n",
                "    ) -> None:\n",
                "        \"\"\"Update statistics metadata for a statistic_id.\"\"\"\n",
                "        self.queue_task(\n",
                "            UpdateStatisticsMetadataTask(\n",
                "                statistic_id, new_statistic_id, new_unit_of_measurement\n",
                "            )\n",
                "        )\n",
                "\n",
                "    @callback\n",
                "    def async_update_states_metadata(\n",
                "        self,\n",
                "        entity_id: str,\n",
                "        new_entity_id: str,\n",
                "    ) -> None:\n",
                "        \"\"\"Update states metadata for an entity_id.\"\"\"\n",
                "        self.queue_task(UpdateStatesMetadataTask(entity_id, new_entity_id))\n",
                "\n",
                "    @callback\n",
                "    def async_change_statistics_unit(\n",
                "        self,\n",
                "        statistic_id: str,\n",
                "        *,\n",
                "        new_unit_of_measurement: str,\n",
                "        old_unit_of_measurement: str,\n",
                "    ) -> None:\n",
                "        \"\"\"Change statistics unit for a statistic_id.\"\"\"\n",
                "        self.queue_task(\n",
                "            ChangeStatisticsUnitTask(\n",
                "                statistic_id, new_unit_of_measurement, old_unit_of_measurement\n",
                "            )\n",
                "        )\n",
                "\n",
                "    @callback\n",
                "    def async_import_statistics(\n",
                "        self,\n",
                "        metadata: StatisticMetaData,\n",
                "        stats: Iterable[StatisticData],\n",
                "        table: type[Statistics | StatisticsShortTerm],\n",
                "    ) -> None:\n",
                "        \"\"\"Schedule import of statistics.\"\"\"\n",
                "        self.queue_task(ImportStatisticsTask(metadata, stats, table))\n",
                "\n",
                "    @callback\n",
                "    def _async_setup_periodic_tasks(self) -> None:\n",
                "        \"\"\"Prepare periodic tasks.\"\"\"\n",
                "        if self.hass.is_stopping or not self._get_session:\n",
                "            # Home Assistant is shutting down\n",
                "            return\n",
                "\n",
                "        # If the db is using a socket connection, we need to keep alive\n",
                "        # to prevent errors from unexpected disconnects\n",
                "        if self.dialect_name != SupportedDialect.SQLITE:\n",
                "            self._keep_alive_listener = async_track_time_interval(\n",
                "                self.hass,\n",
                "                self._async_keep_alive,\n",
                "                timedelta(seconds=KEEPALIVE_TIME),\n",
                "                name=\"Recorder keep alive\",\n",
                "            )\n",
                "\n",
                "        # If the commit interval is not 0, we need to commit periodically\n",
                "        if self.commit_interval:\n",
                "            self._commit_listener = async_track_time_interval(\n",
                "                self.hass,\n",
                "                self._async_commit,\n",
                "                timedelta(seconds=self.commit_interval),\n",
                "                name=\"Recorder commit\",\n",
                "            )\n",
                "\n",
                "        # Run nightly tasks at 4:12am\n",
                "        self._nightly_listener = async_track_time_change(\n",
                "            self.hass, self.async_nightly_tasks, hour=4, minute=12, second=0\n",
                "        )\n",
                "\n",
                "        # Compile short term statistics every 5 minutes\n",
                "        self._periodic_listener = async_track_utc_time_change(\n",
                "            self.hass, self._async_five_minute_tasks, minute=range(0, 60, 5), second=10\n",
                "        )\n",
                "\n",
                "    async def _async_wait_for_started(self) -> object | None:\n",
                "        \"\"\"Wait for the hass started future.\"\"\"\n",
                "        return await self._hass_started\n",
                "\n",
                "    def _wait_startup_or_shutdown(self) -> object | None:\n",
                "        \"\"\"Wait for startup or shutdown before starting.\"\"\"\n",
                "        try:\n",
                "            return asyncio.run_coroutine_threadsafe(\n",
                "                self._async_wait_for_started(), self.hass.loop\n",
                "            ).result()\n",
                "        except CancelledError as ex:\n",
                "            _LOGGER.warning(\n",
                "                \"Recorder startup was externally canceled before it could complete: %s\",\n",
                "                ex,\n",
                "            )\n",
                "            return SHUTDOWN_TASK\n",
                "\n",
                "    def run(self) -> None:\n",
                "        \"\"\"Run the recorder thread.\"\"\"\n",
                "        self.is_running = True\n",
                "        try:\n",
                "            self._run()\n",
                "        except Exception:\n",
                "            _LOGGER.exception(\n",
                "                \"Recorder._run threw unexpected exception, recorder shutting down\"\n",
                "            )\n",
                "        finally:\n",
                "            # Ensure shutdown happens cleanly if\n",
                "            # anything goes wrong in the run loop\n",
                "            self.is_running = False\n",
                "            self._shutdown()\n",
                "\n",
                "    def _add_to_session(self, session: Session, obj: object) -> None:\n",
                "        \"\"\"Add an object to the session.\"\"\"\n",
                "        self._event_session_has_pending_writes = True\n",
                "        session.add(obj)\n",
                "\n",
                "    def _run(self) -> None:\n",
                "        \"\"\"Start processing events to save.\"\"\"\n",
                "        thread_id = threading.get_ident()\n",
                "        self.thread_id = thread_id\n",
                "        self.recorder_and_worker_thread_ids.add(thread_id)\n",
                "\n",
                "        setup_result = self._setup_recorder()\n",
                "\n",
                "        if not setup_result:\n",
                "            # Give up if we could not connect\n",
                "            return\n",
                "\n",
                "        schema_status = migration.validate_db_schema(self.hass, self, self.get_session)\n",
                "        if schema_status is None:\n",
                "            # Give up if we could not validate the schema\n",
                "            return\n",
                "        self.schema_version = schema_status.current_version\n",
                "\n",
                "        if schema_status.valid:\n",
                "            self._setup_run()\n",
                "        else:\n",
                "            self.migration_in_progress = True\n",
                "            self.migration_is_live = migration.live_migration(schema_status)\n",
                "\n",
                "        self.hass.add_job(self.async_connection_success)\n",
                "        database_was_ready = self.migration_is_live or schema_status.valid\n",
                "\n",
                "        if database_was_ready:\n",
                "            # If the migrate is live or the schema is valid, we need to\n",
                "            # wait for startup to complete. If its not live, we need to continue\n",
                "            # on.\n",
                "            self._activate_and_set_db_ready()\n",
                "\n",
                "            # We wait to start a live migration until startup has finished\n",
                "            # since it can be cpu intensive and we do not want it to compete\n",
                "            # with startup which is also cpu intensive\n",
                "            if self._wait_startup_or_shutdown() is SHUTDOWN_TASK:\n",
                "                # Shutdown happened before Home Assistant finished starting\n",
                "                self.migration_in_progress = False\n",
                "                # Make sure we cleanly close the run if\n",
                "                # we restart before startup finishes\n",
                "                return\n",
                "\n",
                "        if not schema_status.valid:\n",
                "            if self._migrate_schema_and_setup_run(schema_status):\n",
                "                self.schema_version = SCHEMA_VERSION\n",
                "                if not self._event_listener:\n",
                "                    # If the schema migration takes so long that the end\n",
                "                    # queue watcher safety kicks in because _reached_max_backlog\n",
                "                    # was True, we need to reinitialize the listener.\n",
                "                    self.hass.add_job(self.async_initialize)\n",
                "            else:\n",
                "                persistent_notification.create(\n",
                "                    self.hass,\n",
                "                    \"The database migration failed, check [the logs](/config/logs).\",\n",
                "                    \"Database Migration Failed\",\n",
                "                    \"recorder_database_migration\",\n",
                "                )\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "                self.hass.add_job(self._async_startup_failed)\n"
                ],
                "parent_version_range": {
                    "start": 775,
                    "end": 775
                },
                "child_version_range": {
                    "start": 775,
                    "end": 776
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if not schema_status.valid:",
                        "start_line": 760,
                        "end_line": 775
                    },
                    {
                        "type": "if_statement",
                        "statement": "if self._migrate_schema_and_setup_run(schema_status):",
                        "start_line": 761,
                        "end_line": 775
                    },
                    {
                        "type": "else_clause",
                        "statement": "else:",
                        "start_line": 768,
                        "end_line": 775
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "Recorder",
                        "signature": "class Recorder(threading.Thread):",
                        "at_line": 170
                    },
                    {
                        "type": "function",
                        "name": "_run",
                        "signature": "def _run(self)->None:",
                        "at_line": 717
                    },
                    {
                        "type": "call",
                        "name": "persistent_notification.create",
                        "signature": "persistent_notification.create(\n                    self.hass,\n                    \"The database migration failed, check [the logs](/config/logs).\",\n                    \"Database Migration Failed\",\n                    \"recorder_database_migration\",\n                )",
                        "at_line": 769
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: homeassistant/components/recorder/core.py\nCode:\n           class Recorder(threading.Thread):\n               ...\n               def _run(self)->None:\n                   ...\n                   persistent_notification.create(\n                    self.hass,\n                    \"The database migration failed, check [the logs](/config/logs).\",\n                    \"Database Migration Failed\",\n                    \"recorder_database_migration\",\n                )\n                       ...\n772 772                        \"Database Migration Failed\",\n773 773                        \"recorder_database_migration\",\n774 774                    )\n    775  +                 self.hass.add_job(self._async_startup_failed)\n775 776                    return\n776 777    \n777 778            if not database_was_ready:\n         ...\n",
                "file_path": "homeassistant/components/recorder/core.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "_async_startup_failed",
                    "add_job",
                    "hass",
                    "self"
                ],
                "prefix": [
                    "                    \"Database Migration Failed\",\n",
                    "                    \"recorder_database_migration\",\n",
                    "                )\n"
                ],
                "suffix": [
                    "                return\n",
                    "\n",
                    "        if not database_was_ready:\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "                return\n",
                "\n",
                "        if not database_was_ready:\n",
                "            self._activate_and_set_db_ready()\n",
                "\n",
                "        # Catch up with missed statistics\n",
                "        self._schedule_compile_missing_statistics()\n",
                "        _LOGGER.debug(\"Recorder processing the queue\")\n",
                "        self._adjust_lru_size()\n",
                "        self.hass.add_job(self._async_set_recorder_ready_migration_done)\n",
                "        self._run_event_loop()\n",
                "\n",
                "    def _activate_and_set_db_ready(self) -> None:\n",
                "        \"\"\"Activate the table managers or schedule migrations and mark the db as ready.\"\"\"\n",
                "        with session_scope(session=self.get_session()) as session:\n",
                "            # Prime the statistics meta manager as soon as possible\n",
                "            # since we want the frontend queries to avoid a thundering\n",
                "            # herd of queries to find the statistics meta data if\n",
                "            # there are a lot of statistics graphs on the frontend.\n",
                "            schema_version = self.schema_version\n",
                "            if schema_version >= STATISTICS_ROWS_SCHEMA_VERSION:\n",
                "                self.statistics_meta_manager.load(session)\n",
                "\n",
                "            migration_changes: dict[str, int] = {\n",
                "                row[0]: row[1]\n",
                "                for row in execute_stmt_lambda_element(session, get_migration_changes())\n",
                "            }\n",
                "\n",
                "            for migrator_cls in (\n",
                "                StatesContextIDMigration,\n",
                "                EventsContextIDMigration,\n",
                "                EventTypeIDMigration,\n",
                "                EntityIDMigration,\n",
                "            ):\n",
                "                migrator = migrator_cls(schema_version, migration_changes)\n",
                "                migrator.do_migrate(self, session)\n",
                "\n",
                "            if self.schema_version > LEGACY_STATES_EVENT_ID_INDEX_SCHEMA_VERSION:\n",
                "                with contextlib.suppress(SQLAlchemyError):\n",
                "                    # If the index of event_ids on the states table is still present\n",
                "                    # or the event_id foreign key still exists we need to queue a\n",
                "                    # task to remove it.\n",
                "                    if (\n",
                "                        get_index_by_name(\n",
                "                            session, TABLE_STATES, LEGACY_STATES_EVENT_ID_INDEX\n",
                "                        )\n",
                "                        or self._legacy_event_id_foreign_key_exists()\n",
                "                    ):\n",
                "                        self.queue_task(EventIdMigrationTask())\n",
                "                        self.use_legacy_events_index = True\n",
                "\n",
                "        # We must only set the db ready after we have set the table managers\n",
                "        # to active if there is no data to migrate.\n",
                "        #\n",
                "        # This ensures that the history queries will use the new tables\n",
                "        # and not the old ones as soon as the API is available.\n",
                "        self.hass.add_job(self.async_set_db_ready)\n",
                "\n",
                "    def _run_event_loop(self) -> None:\n",
                "        \"\"\"Run the event loop for the recorder.\"\"\"\n",
                "        # Use a session for the event read loop\n",
                "        # with a commit every time the event time\n",
                "        # has changed. This reduces the disk io.\n",
                "        queue_ = self._queue\n",
                "        startup_task_or_events: list[RecorderTask | Event] = []\n",
                "        while not queue_.empty() and (task_or_event := queue_.get_nowait()):\n",
                "            startup_task_or_events.append(task_or_event)\n",
                "        self._pre_process_startup_events(startup_task_or_events)\n",
                "        for task in startup_task_or_events:\n",
                "            self._guarded_process_one_task_or_event_or_recover(task)\n",
                "\n",
                "        # Clear startup tasks since this thread runs forever\n",
                "        # and we don't want to hold them in memory\n",
                "        del startup_task_or_events\n",
                "\n",
                "        self.stop_requested = False\n",
                "        while not self.stop_requested:\n",
                "            self._guarded_process_one_task_or_event_or_recover(queue_.get())\n",
                "\n",
                "    def _pre_process_startup_events(\n",
                "        self, startup_task_or_events: list[RecorderTask | Event[Any]]\n",
                "    ) -> None:\n",
                "        \"\"\"Pre process startup events.\"\"\"\n",
                "        # Prime all the state_attributes and event_data caches\n",
                "        # before we start processing events\n",
                "        state_change_events: list[Event[EventStateChangedData]] = []\n",
                "        non_state_change_events: list[Event] = []\n",
                "\n",
                "        for task_or_event in startup_task_or_events:\n",
                "            # Event is never subclassed so we can\n",
                "            # use a fast type check\n",
                "            if type(task_or_event) is Event:\n",
                "                event_ = task_or_event\n",
                "                if event_.event_type == EVENT_STATE_CHANGED:\n",
                "                    state_change_events.append(event_)\n",
                "                else:\n",
                "                    non_state_change_events.append(event_)\n",
                "\n",
                "        assert self.event_session is not None\n",
                "        session = self.event_session\n",
                "        self.event_data_manager.load(non_state_change_events, session)\n",
                "        self.event_type_manager.load(non_state_change_events, session)\n",
                "        self.states_meta_manager.load(state_change_events, session)\n",
                "        self.state_attributes_manager.load(state_change_events, session)\n",
                "\n",
                "    def _guarded_process_one_task_or_event_or_recover(\n",
                "        self, task: RecorderTask | Event\n",
                "    ) -> None:\n",
                "        \"\"\"Process a task, guarding against exceptions to ensure the loop does not collapse.\"\"\"\n",
                "        _LOGGER.debug(\"Processing task: %s\", task)\n",
                "        try:\n",
                "            self._process_one_task_or_event_or_recover(task)\n",
                "        except Exception:\n",
                "            _LOGGER.exception(\"Error while processing event %s\", task)\n",
                "\n",
                "    def _process_one_task_or_event_or_recover(self, task: RecorderTask | Event) -> None:\n",
                "        \"\"\"Process a task or event, reconnect, or recover a malformed database.\"\"\"\n",
                "        try:\n",
                "            # Almost everything coming in via the queue\n",
                "            # is an Event so we can process it directly\n",
                "            # and since its never subclassed, we can\n",
                "            # use a fast type check\n",
                "            if type(task) is Event:\n",
                "                self._process_one_event(task)\n",
                "                return\n",
                "            # If its not an event, commit everything\n",
                "            # that is pending before running the task\n",
                "            if TYPE_CHECKING:\n",
                "                assert isinstance(task, RecorderTask)\n",
                "            if task.commit_before:\n",
                "                self._commit_event_session_or_retry()\n",
                "            task.run(self)\n",
                "        except exc.DatabaseError as err:\n",
                "            if self._handle_database_error(err):\n",
                "                return\n",
                "            _LOGGER.exception(\"Unhandled database error while processing task %s\", task)\n",
                "        except SQLAlchemyError:\n",
                "            _LOGGER.exception(\"SQLAlchemyError error processing task %s\", task)\n",
                "        else:\n",
                "            return\n",
                "\n",
                "        # Reset the session if an SQLAlchemyError (including DatabaseError)\n",
                "        # happens to rollback and recover\n",
                "        self._reopen_event_session()\n",
                "\n",
                "    def _setup_recorder(self) -> bool:\n",
                "        \"\"\"Create a connection to the database.\"\"\"\n",
                "        tries = 1\n",
                "\n",
                "        while tries <= self.db_max_retries:\n",
                "            try:\n",
                "                self._setup_connection()\n",
                "                return migration.initialize_database(self.get_session)\n",
                "            except UnsupportedDialect:\n",
                "                break\n",
                "            except Exception:\n",
                "                _LOGGER.exception(\n",
                "                    \"Error during connection setup: (retrying in %s seconds)\",\n",
                "                    self.db_retry_wait,\n",
                "                )\n",
                "            tries += 1\n",
                "\n",
                "            if tries <= self.db_max_retries:\n",
                "                self._close_connection()\n",
                "                time.sleep(self.db_retry_wait)\n",
                "\n",
                "        return False\n",
                "\n",
                "    @callback\n",
                "    def _async_migration_started(self) -> None:\n",
                "        \"\"\"Set the migration started event.\"\"\"\n",
                "        self.async_migration_event.set()\n",
                "\n",
                "    def _migrate_schema_and_setup_run(\n",
                "        self, schema_status: migration.SchemaValidationStatus\n",
                "    ) -> bool:\n",
                "        \"\"\"Migrate schema to the latest version.\"\"\"\n",
                "        persistent_notification.create(\n",
                "            self.hass,\n",
                "            (\n",
                "                \"System performance will temporarily degrade during the database\"\n",
                "                \" upgrade. Do not power down or restart the system until the upgrade\"\n",
                "                \" completes. Integrations that read the database, such as logbook,\"\n",
                "                \" history, and statistics may return inconsistent results until the \"\n",
                "                \" upgrade completes. This notification will be automatically dismissed\"\n",
                "                \" when the upgrade completes.\"\n",
                "            ),\n",
                "            \"Database upgrade in progress\",\n",
                "            \"recorder_database_migration\",\n",
                "        )\n",
                "        self.hass.add_job(self._async_migration_started)\n",
                "\n",
                "        try:\n",
                "            assert self.engine is not None\n",
                "            migration.migrate_schema(\n",
                "                self, self.hass, self.engine, self.get_session, schema_status\n",
                "            )\n",
                "        except exc.DatabaseError as err:\n",
                "            if self._handle_database_error(err):\n",
                "                return True\n",
                "            _LOGGER.exception(\"Database error during schema migration\")\n",
                "            return False\n",
                "        except Exception:\n",
                "            _LOGGER.exception(\"Error during schema migration\")\n",
                "            return False\n",
                "        else:\n",
                "            self._setup_run()\n",
                "            return True\n",
                "        finally:\n",
                "            self.migration_in_progress = False\n",
                "            persistent_notification.dismiss(self.hass, \"recorder_database_migration\")\n",
                "\n",
                "    def _lock_database(self, task: DatabaseLockTask) -> None:\n",
                "        @callback\n",
                "        def _async_set_database_locked(task: DatabaseLockTask) -> None:\n",
                "            task.database_locked.set()\n",
                "\n",
                "        local_start_time = dt_util.now()\n",
                "        hass = self.hass\n",
                "        with write_lock_db_sqlite(self):\n",
                "            # Notify that lock is being held, wait until database can be used again.\n",
                "            hass.add_job(_async_set_database_locked, task)\n",
                "            while not task.database_unlock.wait(timeout=DB_LOCK_QUEUE_CHECK_TIMEOUT):\n",
                "                if self._reached_max_backlog():\n",
                "                    _LOGGER.warning(\n",
                "                        \"Database queue backlog reached more than %s events \"\n",
                "                        \"while waiting for backup to finish; recorder will now \"\n",
                "                        \"resume writing to database. The backup cannot be trusted and \"\n",
                "                        \"must be restarted\",\n",
                "                        self.backlog,\n",
                "                    )\n",
                "                    task.queue_overflow = True\n",
                "                    hass.add_job(\n",
                "                        async_create_backup_failure_issue, self.hass, local_start_time\n",
                "                    )\n",
                "                    break\n",
                "        _LOGGER.info(\n",
                "            \"Database queue backlog reached %d entries during backup\",\n",
                "            self.backlog,\n",
                "        )\n",
                "\n",
                "    def _process_one_event(self, event: Event[Any]) -> None:\n",
                "        if not self.enabled:\n",
                "            return\n",
                "        if event.event_type == EVENT_STATE_CHANGED:\n",
                "            self._process_state_changed_event_into_session(event)\n",
                "        else:\n",
                "            self._process_non_state_changed_event_into_session(event)\n",
                "        # Commit if the commit interval is zero\n",
                "        if not self.commit_interval:\n",
                "            self._commit_event_session_or_retry()\n",
                "\n",
                "    def _process_non_state_changed_event_into_session(self, event: Event) -> None:\n",
                "        \"\"\"Process any event into the session except state changed.\"\"\"\n",
                "        session = self.event_session\n",
                "        assert session is not None\n",
                "        dbevent = Events.from_event(event)\n",
                "\n",
                "        # Map the event_type to the EventTypes table\n",
                "        event_type_manager = self.event_type_manager\n",
                "        if pending_event_types := event_type_manager.get_pending(event.event_type):\n",
                "            dbevent.event_type_rel = pending_event_types\n",
                "        elif event_type_id := event_type_manager.get(event.event_type, session, True):\n",
                "            dbevent.event_type_id = event_type_id\n",
                "        else:\n",
                "            event_types = EventTypes(event_type=event.event_type)\n",
                "            event_type_manager.add_pending(event_types)\n",
                "            self._add_to_session(session, event_types)\n",
                "            dbevent.event_type_rel = event_types\n",
                "\n",
                "        if not event.data:\n",
                "            self._add_to_session(session, dbevent)\n",
                "            return\n",
                "\n",
                "        event_data_manager = self.event_data_manager\n",
                "        if not (shared_data_bytes := event_data_manager.serialize_from_event(event)):\n",
                "            return\n",
                "\n",
                "        # Map the event data to the EventData table\n",
                "        shared_data = shared_data_bytes.decode(\"utf-8\")\n",
                "        # Matching attributes found in the pending commit\n",
                "        if pending_event_data := event_data_manager.get_pending(shared_data):\n",
                "            dbevent.event_data_rel = pending_event_data\n",
                "        # Matching attributes id found in the cache\n",
                "        elif (data_id := event_data_manager.get_from_cache(shared_data)) or (\n",
                "            (hash_ := EventData.hash_shared_data_bytes(shared_data_bytes))\n",
                "            and (data_id := event_data_manager.get(shared_data, hash_, session))\n",
                "        ):\n",
                "            dbevent.data_id = data_id\n",
                "        else:\n",
                "            # No matching attributes found, save them in the DB\n",
                "            dbevent_data = EventData(shared_data=shared_data, hash=hash_)\n",
                "            event_data_manager.add_pending(dbevent_data)\n",
                "            self._add_to_session(session, dbevent_data)\n",
                "            dbevent.event_data_rel = dbevent_data\n",
                "\n",
                "        self._add_to_session(session, dbevent)\n",
                "\n",
                "    def _process_state_changed_event_into_session(\n",
                "        self, event: Event[EventStateChangedData]\n",
                "    ) -> None:\n",
                "        \"\"\"Process a state_changed event into the session.\"\"\"\n",
                "        state_attributes_manager = self.state_attributes_manager\n",
                "        states_meta_manager = self.states_meta_manager\n",
                "        entity_removed = not event.data.get(\"new_state\")\n",
                "        entity_id = event.data[\"entity_id\"]\n",
                "\n",
                "        dbstate = States.from_event(event)\n",
                "        old_state = event.data[\"old_state\"]\n",
                "\n",
                "        assert self.event_session is not None\n",
                "        session = self.event_session\n",
                "\n",
                "        states_manager = self.states_manager\n",
                "        if pending_state := states_manager.pop_pending(entity_id):\n",
                "            dbstate.old_state = pending_state\n",
                "            if old_state:\n",
                "                pending_state.last_reported_ts = old_state.last_reported_timestamp\n",
                "        elif old_state_id := states_manager.pop_committed(entity_id):\n",
                "            dbstate.old_state_id = old_state_id\n",
                "            if old_state:\n",
                "                states_manager.update_pending_last_reported(\n",
                "                    old_state_id, old_state.last_reported_timestamp\n",
                "                )\n",
                "        if entity_removed:\n",
                "            dbstate.state = None\n",
                "        else:\n",
                "            states_manager.add_pending(entity_id, dbstate)\n",
                "\n",
                "        if states_meta_manager.active:\n",
                "            dbstate.entity_id = None\n",
                "\n",
                "        if entity_id is None or not (\n",
                "            shared_attrs_bytes := state_attributes_manager.serialize_from_event(event)\n",
                "        ):\n",
                "            return\n",
                "\n",
                "        # Map the entity_id to the StatesMeta table\n",
                "        if pending_states_meta := states_meta_manager.get_pending(entity_id):\n",
                "            dbstate.states_meta_rel = pending_states_meta\n",
                "        elif metadata_id := states_meta_manager.get(entity_id, session, True):\n",
                "            dbstate.metadata_id = metadata_id\n",
                "        elif states_meta_manager.active and entity_removed:\n",
                "            # If the entity was removed, we don't need to add it to the\n",
                "            # StatesMeta table or record it in the pending commit\n",
                "            # if it does not have a metadata_id allocated to it as\n",
                "            # it either never existed or was just renamed.\n",
                "            return\n",
                "        else:\n",
                "            states_meta = StatesMeta(entity_id=entity_id)\n",
                "            states_meta_manager.add_pending(states_meta)\n",
                "            self._add_to_session(session, states_meta)\n",
                "            dbstate.states_meta_rel = states_meta\n",
                "\n",
                "        # Map the event data to the StateAttributes table\n",
                "        shared_attrs = shared_attrs_bytes.decode(\"utf-8\")\n",
                "        dbstate.attributes = None\n",
                "        # Matching attributes found in the pending commit\n",
                "        if pending_event_data := state_attributes_manager.get_pending(shared_attrs):\n",
                "            dbstate.state_attributes = pending_event_data\n",
                "        # Matching attributes id found in the cache\n",
                "        elif (\n",
                "            attributes_id := state_attributes_manager.get_from_cache(shared_attrs)\n",
                "        ) or (\n",
                "            (hash_ := StateAttributes.hash_shared_attrs_bytes(shared_attrs_bytes))\n",
                "            and (\n",
                "                attributes_id := state_attributes_manager.get(\n",
                "                    shared_attrs, hash_, session\n",
                "                )\n",
                "            )\n",
                "        ):\n",
                "            dbstate.attributes_id = attributes_id\n",
                "        else:\n",
                "            # No matching attributes found, save them in the DB\n",
                "            dbstate_attributes = StateAttributes(shared_attrs=shared_attrs, hash=hash_)\n",
                "            state_attributes_manager.add_pending(dbstate_attributes)\n",
                "            self._add_to_session(session, dbstate_attributes)\n",
                "            dbstate.state_attributes = dbstate_attributes\n",
                "\n",
                "        self._add_to_session(session, dbstate)\n",
                "\n",
                "    def _handle_database_error(self, err: Exception) -> bool:\n",
                "        \"\"\"Handle a database error that may result in moving away the corrupt db.\"\"\"\n",
                "        if (\n",
                "            (cause := err.__cause__)\n",
                "            and isinstance(cause, sqlite3.DatabaseError)\n",
                "            and (cause_str := str(cause))\n",
                "            # Make sure we do not move away a database when its only locked\n",
                "            # externally by another process. sqlite does not give us a named\n",
                "            # exception for this so we have to check the error message.\n",
                "            and (\"malformed\" in cause_str or \"not a database\" in cause_str)\n",
                "        ):\n",
                "            _LOGGER.exception(\n",
                "                \"Unrecoverable sqlite3 database corruption detected: %s\", err\n",
                "            )\n",
                "            self._handle_sqlite_corruption()\n",
                "            return True\n",
                "        return False\n",
                "\n",
                "    def _commit_event_session_or_retry(self) -> None:\n",
                "        \"\"\"Commit the event session if there is work to do.\"\"\"\n",
                "        if not self._event_session_has_pending_writes:\n",
                "            return\n",
                "        tries = 1\n",
                "        while tries <= self.db_max_retries:\n",
                "            try:\n",
                "                self._commit_event_session()\n",
                "            except (exc.InternalError, exc.OperationalError) as err:\n",
                "                _LOGGER.error(\n",
                "                    \"%s: Error executing query: %s. (retrying in %s seconds)\",\n",
                "                    INVALIDATED_ERR if err.connection_invalidated else CONNECTIVITY_ERR,\n",
                "                    err,\n",
                "                    self.db_retry_wait,\n",
                "                )\n",
                "                if tries == self.db_max_retries:\n",
                "                    raise\n",
                "\n",
                "                tries += 1\n",
                "                time.sleep(self.db_retry_wait)\n",
                "            else:\n",
                "                return\n",
                "\n",
                "    def _commit_event_session(self) -> None:\n",
                "        assert self.event_session is not None\n",
                "        session = self.event_session\n",
                "        self._commits_without_expire += 1\n",
                "\n",
                "        if (\n",
                "            pending_last_reported\n",
                "            := self.states_manager.get_pending_last_reported_timestamp()\n",
                "        ) and self.schema_version >= LAST_REPORTED_SCHEMA_VERSION:\n",
                "            with session.no_autoflush:\n",
                "                session.execute(\n",
                "                    update(States),\n",
                "                    [\n",
                "                        {\n",
                "                            \"state_id\": state_id,\n",
                "                            \"last_reported_ts\": last_reported_timestamp,\n",
                "                        }\n",
                "                        for state_id, last_reported_timestamp in pending_last_reported.items()\n",
                "                    ],\n",
                "                )\n",
                "        session.commit()\n",
                "\n",
                "        self._event_session_has_pending_writes = False\n",
                "        # We just committed the state attributes to the database\n",
                "        # and we now know the attributes_ids.  We can save\n",
                "        # many selects for matching attributes by loading them\n",
                "        # into the LRU or committed now.\n",
                "        self.states_manager.post_commit_pending()\n",
                "        self.state_attributes_manager.post_commit_pending()\n",
                "        self.event_data_manager.post_commit_pending()\n",
                "        self.event_type_manager.post_commit_pending()\n",
                "        self.states_meta_manager.post_commit_pending()\n",
                "\n",
                "        # Expire is an expensive operation (frequently more expensive\n",
                "        # than the flush and commit itself) so we only\n",
                "        # do it after EXPIRE_AFTER_COMMITS commits\n",
                "        if self._commits_without_expire >= EXPIRE_AFTER_COMMITS:\n",
                "            self._commits_without_expire = 0\n",
                "            session.expire_all()\n",
                "\n",
                "    def _handle_sqlite_corruption(self) -> None:\n",
                "        \"\"\"Handle the sqlite3 database being corrupt.\"\"\"\n",
                "        try:\n",
                "            self._close_event_session()\n",
                "        finally:\n",
                "            self._close_connection()\n",
                "        move_away_broken_database(dburl_to_path(self.db_url))\n",
                "        self.recorder_runs_manager.reset()\n",
                "        self._setup_recorder()\n",
                "        self._setup_run()\n",
                "\n",
                "    def _close_event_session(self) -> None:\n",
                "        \"\"\"Close the event session.\"\"\"\n",
                "        self.states_manager.reset()\n",
                "        self.state_attributes_manager.reset()\n",
                "        self.event_data_manager.reset()\n",
                "        self.event_type_manager.reset()\n",
                "        self.states_meta_manager.reset()\n",
                "        self.statistics_meta_manager.reset()\n",
                "\n",
                "        if not self.event_session:\n",
                "            return\n",
                "\n",
                "        try:\n",
                "            self.event_session.rollback()\n",
                "            self.event_session.close()\n",
                "        except SQLAlchemyError:\n",
                "            _LOGGER.exception(\"Error while rolling back and closing the event session\")\n",
                "\n",
                "    def _reopen_event_session(self) -> None:\n",
                "        \"\"\"Rollback the event session and reopen it after a failure.\"\"\"\n",
                "        self._close_event_session()\n",
                "        self._open_event_session()\n",
                "\n",
                "    def _open_event_session(self) -> None:\n",
                "        \"\"\"Open the event session.\"\"\"\n",
                "        self.event_session = self.get_session()\n",
                "        self.event_session.expire_on_commit = False\n",
                "\n",
                "    def _post_schema_migration(self, old_version: int, new_version: int) -> None:\n",
                "        \"\"\"Run post schema migration tasks.\"\"\"\n",
                "        migration.post_schema_migration(self, old_version, new_version)\n",
                "\n",
                "    def _legacy_event_id_foreign_key_exists(self) -> bool:\n",
                "        \"\"\"Check if the legacy event_id foreign key exists.\"\"\"\n",
                "        engine = self.engine\n",
                "        assert engine is not None\n",
                "        return bool(\n",
                "            next(\n",
                "                (\n",
                "                    fk\n",
                "                    for fk in inspect(engine).get_foreign_keys(TABLE_STATES)\n",
                "                    if fk[\"constrained_columns\"] == [\"event_id\"]\n",
                "                ),\n",
                "                None,\n",
                "            )\n",
                "        )\n",
                "\n",
                "    def _post_migrate_entity_ids(self) -> bool:\n",
                "        \"\"\"Post migrate entity_ids if needed.\"\"\"\n",
                "        return migration.post_migrate_entity_ids(self)\n",
                "\n",
                "    def _cleanup_legacy_states_event_ids(self) -> bool:\n",
                "        \"\"\"Cleanup legacy event_ids if needed.\"\"\"\n",
                "        return migration.cleanup_legacy_states_event_ids(self)\n",
                "\n",
                "    def _send_keep_alive(self) -> None:\n",
                "        \"\"\"Send a keep alive to keep the db connection open.\"\"\"\n",
                "        assert self.event_session is not None\n",
                "        _LOGGER.debug(\"Sending keepalive\")\n",
                "        self.event_session.connection().scalar(select(1))\n",
                "\n",
                "    async def async_block_till_done(self) -> None:\n",
                "        \"\"\"Async version of block_till_done.\"\"\"\n",
                "        if self._queue.empty() and not self._event_session_has_pending_writes:\n",
                "            return\n",
                "        event = asyncio.Event()\n",
                "        self.queue_task(SynchronizeTask(event))\n",
                "        await event.wait()\n",
                "\n",
                "    def block_till_done(self) -> None:\n",
                "        \"\"\"Block till all events processed.\n",
                "\n",
                "        This is only called in tests.\n",
                "\n",
                "        This only blocks until the queue is empty\n",
                "        which does not mean the recorder is done.\n",
                "\n",
                "        Call tests.common's wait_recording_done\n",
                "        after calling this to ensure the data\n",
                "        is in the database.\n",
                "        \"\"\"\n",
                "        self._queue_watch.clear()\n",
                "        self.queue_task(WAIT_TASK)\n",
                "        self._queue_watch.wait()\n",
                "\n",
                "    async def lock_database(self) -> bool:\n",
                "        \"\"\"Lock database so it can be backed up safely.\"\"\"\n",
                "        if self.dialect_name != SupportedDialect.SQLITE:\n",
                "            _LOGGER.debug(\n",
                "                \"Not a SQLite database or not connected, locking not necessary\"\n",
                "            )\n",
                "            return True\n",
                "\n",
                "        if self._database_lock_task:\n",
                "            _LOGGER.warning(\"Database already locked\")\n",
                "            return False\n",
                "\n",
                "        database_locked = asyncio.Event()\n",
                "        task = DatabaseLockTask(database_locked, threading.Event(), False)\n",
                "        self.queue_task(task)\n",
                "        try:\n",
                "            async with asyncio.timeout(DB_LOCK_TIMEOUT):\n",
                "                await database_locked.wait()\n",
                "        except TimeoutError as err:\n",
                "            task.database_unlock.set()\n",
                "            raise TimeoutError(\n",
                "                f\"Could not lock database within {DB_LOCK_TIMEOUT} seconds.\"\n",
                "            ) from err\n",
                "        self._database_lock_task = task\n",
                "        return True\n",
                "\n",
                "    @callback\n",
                "    def unlock_database(self) -> bool:\n",
                "        \"\"\"Unlock database.\n",
                "\n",
                "        Returns true if database lock has been held throughout the process.\n",
                "        \"\"\"\n",
                "        if self.dialect_name != SupportedDialect.SQLITE:\n",
                "            _LOGGER.debug(\n",
                "                \"Not a SQLite database or not connected, unlocking not necessary\"\n",
                "            )\n",
                "            return True\n",
                "\n",
                "        if not self._database_lock_task:\n",
                "            _LOGGER.warning(\"Database currently not locked\")\n",
                "            return False\n",
                "\n",
                "        self._database_lock_task.database_unlock.set()\n",
                "        success = not self._database_lock_task.queue_overflow\n",
                "\n",
                "        self._database_lock_task = None\n",
                "\n",
                "        return success\n",
                "\n",
                "    def _setup_recorder_connection(\n",
                "        self, dbapi_connection: DBAPIConnection, connection_record: Any\n",
                "    ) -> None:\n",
                "        \"\"\"Dbapi specific connection settings.\"\"\"\n",
                "        assert self.engine is not None\n",
                "        if database_engine := setup_connection_for_dialect(\n",
                "            self,\n",
                "            self.engine.dialect.name,\n",
                "            dbapi_connection,\n",
                "            not self._completed_first_database_setup,\n",
                "        ):\n",
                "            self.database_engine = database_engine\n",
                "            self.max_bind_vars = database_engine.max_bind_vars\n",
                "        self._completed_first_database_setup = True\n",
                "\n",
                "    def _setup_connection(self) -> None:\n",
                "        \"\"\"Ensure database is ready to fly.\"\"\"\n",
                "        kwargs: dict[str, Any] = {}\n",
                "        self._completed_first_database_setup = False\n",
                "\n",
                "        if self.db_url == SQLITE_URL_PREFIX or \":memory:\" in self.db_url:\n",
                "            kwargs[\"connect_args\"] = {\"check_same_thread\": False}\n",
                "            kwargs[\"poolclass\"] = MutexPool\n",
                "            MutexPool.pool_lock = threading.RLock()\n",
                "            kwargs[\"pool_reset_on_return\"] = None\n",
                "        elif self.db_url.startswith(SQLITE_URL_PREFIX):\n",
                "            kwargs[\"poolclass\"] = RecorderPool\n",
                "            kwargs[\"recorder_and_worker_thread_ids\"] = (\n",
                "                self.recorder_and_worker_thread_ids\n",
                "            )\n",
                "        elif self.db_url.startswith(\n",
                "            (\n",
                "                MARIADB_URL_PREFIX,\n",
                "                MARIADB_PYMYSQL_URL_PREFIX,\n",
                "                MYSQLDB_URL_PREFIX,\n",
                "                MYSQLDB_PYMYSQL_URL_PREFIX,\n",
                "            )\n",
                "        ):\n",
                "            kwargs[\"connect_args\"] = {\"charset\": \"utf8mb4\"}\n",
                "            if self.db_url.startswith((MARIADB_URL_PREFIX, MYSQLDB_URL_PREFIX)):\n",
                "                # If they have configured MySQLDB but don't have\n",
                "                # the MySQLDB module installed this will throw\n",
                "                # an ImportError which we suppress here since\n",
                "                # sqlalchemy will give them a better error when\n",
                "                # it tried to import it below.\n",
                "                with contextlib.suppress(ImportError):\n",
                "                    kwargs[\"connect_args\"][\"conv\"] = build_mysqldb_conv()\n",
                "\n",
                "        # Disable extended logging for non SQLite databases\n",
                "        if not self.db_url.startswith(SQLITE_URL_PREFIX):\n",
                "            kwargs[\"echo\"] = False\n",
                "\n",
                "        if self._using_file_sqlite:\n",
                "            validate_or_move_away_sqlite_database(self.db_url)\n",
                "\n",
                "        assert not self.engine\n",
                "        self.engine = create_engine(self.db_url, **kwargs, future=True)\n",
                "        self._dialect_name = try_parse_enum(SupportedDialect, self.engine.dialect.name)\n",
                "        self.__dict__.pop(\"dialect_name\", None)\n",
                "        sqlalchemy_event.listen(self.engine, \"connect\", self._setup_recorder_connection)\n",
                "\n",
                "        migration.pre_migrate_schema(self.engine)\n",
                "        Base.metadata.create_all(self.engine)\n",
                "        self._get_session = scoped_session(sessionmaker(bind=self.engine, future=True))\n",
                "        _LOGGER.debug(\"Connected to recorder database\")\n",
                "\n",
                "    def _close_connection(self) -> None:\n",
                "        \"\"\"Close the connection.\"\"\"\n",
                "        if self.engine:\n",
                "            self.engine.dispose()\n",
                "            self.engine = None\n",
                "        self._get_session = None\n",
                "\n",
                "    def _setup_run(self) -> None:\n",
                "        \"\"\"Log the start of the current run and schedule any needed jobs.\"\"\"\n",
                "        with session_scope(session=self.get_session()) as session:\n",
                "            end_incomplete_runs(session, self.recorder_runs_manager.recording_start)\n",
                "            self.recorder_runs_manager.start(session)\n",
                "\n",
                "        self._open_event_session()\n",
                "\n",
                "    def _schedule_compile_missing_statistics(self) -> None:\n",
                "        \"\"\"Add tasks for missing statistics runs.\"\"\"\n",
                "        self.queue_task(CompileMissingStatisticsTask())\n",
                "\n",
                "    def _end_session(self) -> None:\n",
                "        \"\"\"End the recorder session.\"\"\"\n",
                "        if self.event_session is None:\n",
                "            return\n",
                "        if self.recorder_runs_manager.active:\n",
                "            # .end will add to the event session\n",
                "            self._event_session_has_pending_writes = True\n",
                "            self.recorder_runs_manager.end(self.event_session)\n",
                "        try:\n",
                "            self._commit_event_session_or_retry()\n",
                "        except Exception:\n",
                "            _LOGGER.exception(\"Error saving the event session during shutdown\")\n",
                "\n",
                "        self.event_session.close()\n",
                "        self.recorder_runs_manager.clear()\n",
                "\n",
                "    def _shutdown(self) -> None:\n",
                "        \"\"\"Save end time for current run.\"\"\"\n",
                "        _LOGGER.debug(\"Shutting down recorder\")\n",
                "        if not self.schema_version or self.schema_version != SCHEMA_VERSION:\n",
                "            # If the schema version is not set, we never had a working\n",
                "            # connection to the database or the schema never reached a\n",
                "            # good state.\n",
                "            #\n",
                "            # In either case, we want to mark startup as failed.\n",
                "            #\n",
                "            self.hass.add_job(self._async_startup_failed)\n",
                "        else:\n",
                "            self.hass.add_job(self._async_stop_listeners)\n",
                "\n",
                "        try:\n",
                "            self._end_session()\n",
                "        finally:\n",
                "            self._stop_executor()\n",
                "            self._close_connection()"
            ]
        ],
        "tests/components/recorder/test_migrate.py": [
            [
                "\"\"\"The tests for the Recorder component.\"\"\"\n",
                "\n",
                "import datetime\n",
                "import importlib\n",
                "import sqlite3\n",
                "import sys\n",
                "from unittest.mock import ANY, Mock, PropertyMock, call, patch\n",
                "\n",
                "import pytest\n",
                "from sqlalchemy import create_engine, text\n",
                "from sqlalchemy.exc import (\n",
                "    DatabaseError,\n",
                "    InternalError,\n",
                "    OperationalError,\n",
                "    ProgrammingError,\n",
                "    SQLAlchemyError,\n",
                ")\n",
                "from sqlalchemy.orm import Session, scoped_session, sessionmaker\n",
                "from sqlalchemy.pool import StaticPool\n",
                "\n",
                "from homeassistant.components import persistent_notification as pn, recorder\n",
                "from homeassistant.components.recorder import db_schema, migration\n",
                "from homeassistant.components.recorder.db_schema import (\n",
                "    SCHEMA_VERSION,\n",
                "    Events,\n",
                "    RecorderRuns,\n",
                "    States,\n",
                ")\n",
                "from homeassistant.components.recorder.util import session_scope\n",
                "from homeassistant.core import HomeAssistant\n",
                "from homeassistant.helpers import recorder as recorder_helper\n",
                "import homeassistant.util.dt as dt_util\n",
                "\n",
                "from .common import async_wait_recording_done, create_engine_test\n",
                "from .conftest import InstrumentedMigration\n",
                "\n",
                "from tests.common import async_fire_time_changed\n",
                "from tests.typing import RecorderInstanceGenerator\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def mock_recorder_before_hass(\n",
                "    async_test_recorder: RecorderInstanceGenerator,\n",
                ") -> None:\n",
                "    \"\"\"Set up recorder.\"\"\"\n",
                "\n",
                "\n",
                "def _get_native_states(hass, entity_id):\n",
                "    with session_scope(hass=hass, read_only=True) as session:\n",
                "        instance = recorder.get_instance(hass)\n",
                "        metadata_id = instance.states_meta_manager.get(entity_id, session, True)\n",
                "        states = []\n",
                "        for dbstate in session.query(States).filter(States.metadata_id == metadata_id):\n",
                "            dbstate.entity_id = entity_id\n",
                "            states.append(dbstate.to_native())\n",
                "        return states\n",
                "\n",
                "\n",
                "async def test_schema_update_calls(\n",
                "    hass: HomeAssistant, async_setup_recorder_instance: RecorderInstanceGenerator\n",
                ") -> None:\n",
                "    \"\"\"Test that schema migrations occur in correct order.\"\"\"\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.core.create_engine\",\n",
                "            new=create_engine_test,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration._apply_update\",\n",
                "            wraps=migration._apply_update,\n",
                "        ) as update,\n",
                "    ):\n",
                "        await async_setup_recorder_instance(hass)\n",
                "        await async_wait_recording_done(hass)\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "    instance = recorder.get_instance(hass)\n",
                "    engine = instance.engine\n",
                "    session_maker = instance.get_session\n",
                "    update.assert_has_calls(\n",
                "        [\n",
                "            call(instance, hass, engine, session_maker, version + 1, 0)\n",
                "            for version in range(db_schema.SCHEMA_VERSION)\n",
                "        ]\n",
                "    )\n",
                "\n",
                "\n",
                "async def test_migration_in_progress(\n",
                "    hass: HomeAssistant,\n",
                "    recorder_db_url: str,\n",
                "    async_setup_recorder_instance: RecorderInstanceGenerator,\n",
                "    instrument_migration: InstrumentedMigration,\n",
                ") -> None:\n",
                "    \"\"\"Test that we can check for migration in progress.\"\"\"\n",
                "    if recorder_db_url.startswith(\"mysql://\"):\n",
                "        # The database drop at the end of this test currently hangs on MySQL\n",
                "        # because the post migration is still in progress in the background\n",
                "        # which results in a deadlock in InnoDB. This behavior is not likely\n",
                "        # to happen in real life because the database does not get dropped\n",
                "        # in normal operation.\n",
                "        return\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.core.create_engine\",\n",
                "            new=create_engine_test,\n",
                "        ),\n",
                "    ):\n",
                "        await async_setup_recorder_instance(hass, wait_recorder=False)\n",
                "        await hass.async_add_executor_job(instrument_migration.migration_started.wait)\n",
                "        assert recorder.util.async_migration_in_progress(hass) is True\n",
                "\n",
                "        # Let migration finish\n",
                "        instrument_migration.migration_stall.set()\n",
                "        await async_wait_recording_done(hass)\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "    assert recorder.get_instance(hass).schema_version == SCHEMA_VERSION\n",
                "\n",
                "\n",
                "async def test_database_migration_failed(\n",
                "    hass: HomeAssistant, async_setup_recorder_instance: RecorderInstanceGenerator\n",
                ") -> None:\n",
                "    \"\"\"Test we notify if the migration fails.\"\"\"\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.core.create_engine\",\n",
                "            new=create_engine_test,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration._apply_update\",\n",
                "            side_effect=ValueError,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.persistent_notification.create\",\n",
                "            side_effect=pn.create,\n",
                "        ) as mock_create,\n",
                "        patch(\n",
                "            \"homeassistant.components.persistent_notification.dismiss\",\n",
                "            side_effect=pn.dismiss,\n",
                "        ) as mock_dismiss,\n",
                "    ):\n",
                "        await async_setup_recorder_instance(hass, wait_recorder=False)\n",
                "        hass.states.async_set(\"my.entity\", \"on\", {})\n",
                "        hass.states.async_set(\"my.entity\", \"off\", {})\n",
                "        await hass.async_block_till_done()\n",
                "        await hass.async_add_executor_job(recorder.get_instance(hass).join)\n",
                "        await hass.async_block_till_done()\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "    assert len(mock_create.mock_calls) == 2\n",
                "    assert len(mock_dismiss.mock_calls) == 1\n",
                "\n",
                "\n",
                "@pytest.mark.skip_on_db_engine([\"mysql\", \"postgresql\"])\n",
                "@pytest.mark.usefixtures(\"skip_by_db_engine\")\n",
                "async def test_database_migration_encounters_corruption(\n",
                "    hass: HomeAssistant,\n",
                "    recorder_db_url: str,\n",
                "    async_setup_recorder_instance: RecorderInstanceGenerator,\n",
                ") -> None:\n",
                "    \"\"\"Test we move away the database if its corrupt.\n",
                "\n",
                "    This test is specific for SQLite, wiping the database on error only happens\n",
                "    with SQLite.\n",
                "    \"\"\"\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "\n",
                "    sqlite3_exception = DatabaseError(\"statement\", {}, [])\n",
                "    sqlite3_exception.__cause__ = sqlite3.DatabaseError(\n",
                "        \"database disk image is malformed\"\n",
                "    )\n",
                "\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration._schema_is_current\",\n",
                "            side_effect=[False],\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration.migrate_schema\",\n",
                "            side_effect=sqlite3_exception,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.core.move_away_broken_database\"\n",
                "        ) as move_away,\n",
                "    ):\n",
                "        await async_setup_recorder_instance(hass)\n",
                "        hass.states.async_set(\"my.entity\", \"on\", {})\n",
                "        hass.states.async_set(\"my.entity\", \"off\", {})\n",
                "        await async_wait_recording_done(hass)\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "    assert move_away.called\n",
                "\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "@pytest.mark.parametrize(\n",
                    "    (\"live_migration\", \"expected_setup_result\"), [(True, True), (False, False)]\n",
                    ")\n"
                ],
                "parent_version_range": {
                    "start": 202,
                    "end": 202
                },
                "child_version_range": {
                    "start": 202,
                    "end": 205
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 1,
                "hunk_diff": "File: tests/components/recorder/test_migrate.py\nCode:\n  ...\n199 199        assert move_away.called\n200 200    \n201 201    \n    202  + @pytest.mark.parametrize(\n    203  +     (\"live_migration\", \"expected_setup_result\"), [(True, True), (False, False)]\n    204  + )\n202 205    async def test_database_migration_encounters_corruption_not_sqlite(\n         ...\n",
                "file_path": "tests/components/recorder/test_migrate.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "mark",
                    "parametrize",
                    "pytest"
                ],
                "prefix": [
                    "    assert move_away.called\n",
                    "\n",
                    "\n"
                ],
                "suffix": [
                    "async def test_database_migration_encounters_corruption_not_sqlite(\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "async def test_database_migration_encounters_corruption_not_sqlite(\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    hass: HomeAssistant, async_setup_recorder_instance: RecorderInstanceGenerator\n"
                ],
                "after": [
                    "    hass: HomeAssistant,\n",
                    "    async_setup_recorder_instance: RecorderInstanceGenerator,\n",
                    "    live_migration: bool,\n",
                    "    expected_setup_result: bool,\n"
                ],
                "parent_version_range": {
                    "start": 203,
                    "end": 204
                },
                "child_version_range": {
                    "start": 206,
                    "end": 210
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "test_database_migration_encounters_corruption_not_sqlite",
                        "signature": "def test_database_migration_encounters_corruption_not_sqlite(\n    hass: HomeAssistant, async_setup_recorder_instance: RecorderInstanceGenerator\n)->None:",
                        "at_line": 202
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: tests/components/recorder/test_migrate.py\nCode:\n202 205    async def test_database_migration_encounters_corruption_not_sqlite(\n203      -     hass: HomeAssistant, async_setup_recorder_instance: RecorderInstanceGenerator\n    206  +     hass: HomeAssistant,\n    207  +     async_setup_recorder_instance: RecorderInstanceGenerator,\n    208  +     live_migration: bool,\n    209  +     expected_setup_result: bool,\n204 210    ) -> None:\n205 211        \"\"\"Test we fail on database error when we cannot recover.\"\"\"\n206 212        assert recorder.util.async_migration_in_progress(hass) is False\n         ...\n",
                "file_path": "tests/components/recorder/test_migrate.py",
                "identifiers_before": [
                    "HomeAssistant",
                    "RecorderInstanceGenerator",
                    "async_setup_recorder_instance",
                    "hass"
                ],
                "identifiers_after": [
                    "HomeAssistant",
                    "RecorderInstanceGenerator",
                    "async_setup_recorder_instance",
                    "bool",
                    "expected_setup_result",
                    "hass",
                    "live_migration"
                ],
                "prefix": [
                    "async def test_database_migration_encounters_corruption_not_sqlite(\n"
                ],
                "suffix": [
                    ") -> None:\n",
                    "    \"\"\"Test we fail on database error when we cannot recover.\"\"\"\n",
                    "    assert recorder.util.async_migration_in_progress(hass) is False\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "async_setup_recorder_instance",
                            "position": {
                                "start": {
                                    "line": 203,
                                    "column": 25
                                },
                                "end": {
                                    "line": 203,
                                    "column": 54
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/components/recorder/test_migrate.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "live_migration",
                            "position": {
                                "start": {
                                    "line": 208,
                                    "column": 4
                                },
                                "end": {
                                    "line": 208,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/components/recorder/test_migrate.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 209,
                                    "column": 4
                                },
                                "end": {
                                    "line": 209,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/components/recorder/test_migrate.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "async_setup_recorder_instance",
                            "position": {
                                "start": {
                                    "line": 207,
                                    "column": 4
                                },
                                "end": {
                                    "line": 207,
                                    "column": 33
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/components/recorder/test_migrate.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                ") -> None:\n",
                "    \"\"\"Test we fail on database error when we cannot recover.\"\"\"\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration._schema_is_current\",\n",
                "            side_effect=[False],\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration.migrate_schema\",\n",
                "            side_effect=DatabaseError(\"statement\", {}, []),\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.core.move_away_broken_database\"\n",
                "        ) as move_away,\n",
                "        patch(\n",
                "            \"homeassistant.components.persistent_notification.create\",\n",
                "            side_effect=pn.create,\n",
                "        ) as mock_create,\n",
                "        patch(\n",
                "            \"homeassistant.components.persistent_notification.dismiss\",\n",
                "            side_effect=pn.dismiss,\n",
                "        ) as mock_dismiss,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        patch(\n",
                    "            \"homeassistant.components.recorder.core.migration.live_migration\",\n",
                    "            return_value=live_migration,\n",
                    "        ),\n"
                ],
                "parent_version_range": {
                    "start": 228,
                    "end": 228
                },
                "child_version_range": {
                    "start": 234,
                    "end": 238
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "with (\n        patch(\n            \"homeassistant.components.recorder.migration._schema_is_current\",\n            side_effect=[False],\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.migrate_schema\",\n            side_effect=DatabaseError(\"statement\", {}, []),\n        ),\n        patch(\n            \"homeassistant.components.recorder.core.move_away_broken_database\"\n        ) as move_away,\n        patch(\n            \"homeassistant.components.persistent_notification.create\",\n            side_effect=pn.create,\n        ) as mock_create,\n        patch(\n            \"homeassistant.components.persistent_notification.dismiss\",\n            side_effect=pn.dismiss,\n        ) as mock_dismiss,\n    ):",
                        "start_line": 208,
                        "end_line": 234
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "test_database_migration_encounters_corruption_not_sqlite",
                        "signature": "def test_database_migration_encounters_corruption_not_sqlite(\n    hass: HomeAssistant, async_setup_recorder_instance: RecorderInstanceGenerator\n)->None:",
                        "at_line": 202
                    },
                    {
                        "type": "call",
                        "name": "patch",
                        "signature": "patch(\n            \"homeassistant.components.persistent_notification.dismiss\",\n            side_effect=pn.dismiss,\n        )",
                        "at_line": 224
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: tests/components/recorder/test_migrate.py\nCode:\n           def test_database_migration_encounters_corruption_not_sqlite(\n    hass: HomeAssistant, async_setup_recorder_instance: RecorderInstanceGenerator\n)->None:\n               ...\n               patch(\n            \"homeassistant.components.persistent_notification.dismiss\",\n            side_effect=pn.dismiss,\n        )\n                   ...\n225 231                \"homeassistant.components.persistent_notification.dismiss\",\n226 232                side_effect=pn.dismiss,\n227 233            ) as mock_dismiss,\n    234  +         patch(\n    235  +             \"homeassistant.components.recorder.core.migration.live_migration\",\n    236  +             return_value=live_migration,\n    237  +         ),\n228 238        ):\n         ...\n",
                "file_path": "tests/components/recorder/test_migrate.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "live_migration",
                    "patch",
                    "return_value"
                ],
                "prefix": [
                    "            \"homeassistant.components.persistent_notification.dismiss\",\n",
                    "            side_effect=pn.dismiss,\n",
                    "        ) as mock_dismiss,\n"
                ],
                "suffix": [
                    "    ):\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "live_migration",
                            "position": {
                                "start": {
                                    "line": 236,
                                    "column": 25
                                },
                                "end": {
                                    "line": 236,
                                    "column": 39
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/components/recorder/test_migrate.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    ):\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        await async_setup_recorder_instance(hass, wait_recorder=False)\n"
                ],
                "after": [
                    "        await async_setup_recorder_instance(\n",
                    "            hass, wait_recorder=False, expected_setup_result=expected_setup_result\n",
                    "        )\n"
                ],
                "parent_version_range": {
                    "start": 229,
                    "end": 230
                },
                "child_version_range": {
                    "start": 239,
                    "end": 242
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "with (\n        patch(\n            \"homeassistant.components.recorder.migration._schema_is_current\",\n            side_effect=[False],\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.migrate_schema\",\n            side_effect=DatabaseError(\"statement\", {}, []),\n        ),\n        patch(\n            \"homeassistant.components.recorder.core.move_away_broken_database\"\n        ) as move_away,\n        patch(\n            \"homeassistant.components.persistent_notification.create\",\n            side_effect=pn.create,\n        ) as mock_create,\n        patch(\n            \"homeassistant.components.persistent_notification.dismiss\",\n            side_effect=pn.dismiss,\n        ) as mock_dismiss,\n    ):",
                        "start_line": 208,
                        "end_line": 234
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "test_database_migration_encounters_corruption_not_sqlite",
                        "signature": "def test_database_migration_encounters_corruption_not_sqlite(\n    hass: HomeAssistant, async_setup_recorder_instance: RecorderInstanceGenerator\n)->None:",
                        "at_line": 202
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: tests/components/recorder/test_migrate.py\nCode:\n           def test_database_migration_encounters_corruption_not_sqlite(\n    hass: HomeAssistant, async_setup_recorder_instance: RecorderInstanceGenerator\n)->None:\n               ...\n228 238        ):\n229      -         await async_setup_recorder_instance(hass, wait_recorder=False)\n    239  +         await async_setup_recorder_instance(\n    240  +             hass, wait_recorder=False, expected_setup_result=expected_setup_result\n    241  +         )\n230 242            hass.states.async_set(\"my.entity\", \"on\", {})\n231 243            hass.states.async_set(\"my.entity\", \"off\", {})\n232 244            await hass.async_block_till_done()\n         ...\n",
                "file_path": "tests/components/recorder/test_migrate.py",
                "identifiers_before": [
                    "async_setup_recorder_instance",
                    "hass",
                    "wait_recorder"
                ],
                "identifiers_after": [
                    "async_setup_recorder_instance",
                    "expected_setup_result",
                    "hass",
                    "wait_recorder"
                ],
                "prefix": [
                    "    ):\n"
                ],
                "suffix": [
                    "        hass.states.async_set(\"my.entity\", \"on\", {})\n",
                    "        hass.states.async_set(\"my.entity\", \"off\", {})\n",
                    "        await hass.async_block_till_done()\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "async_setup_recorder_instance",
                            "position": {
                                "start": {
                                    "line": 229,
                                    "column": 14
                                },
                                "end": {
                                    "line": 229,
                                    "column": 43
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/components/recorder/test_migrate.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 240,
                                    "column": 61
                                },
                                "end": {
                                    "line": 240,
                                    "column": 82
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/components/recorder/test_migrate.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "async_setup_recorder_instance",
                            "position": {
                                "start": {
                                    "line": 239,
                                    "column": 14
                                },
                                "end": {
                                    "line": 239,
                                    "column": 43
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/components/recorder/test_migrate.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        hass.states.async_set(\"my.entity\", \"on\", {})\n",
                "        hass.states.async_set(\"my.entity\", \"off\", {})\n",
                "        await hass.async_block_till_done()\n",
                "        await hass.async_add_executor_job(recorder.get_instance(hass).join)\n",
                "        await hass.async_block_till_done()\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "    assert not move_away.called\n",
                "    assert len(mock_create.mock_calls) == 2\n",
                "    assert len(mock_dismiss.mock_calls) == 1\n",
                "\n",
                "\n",
                "async def test_events_during_migration_are_queued(\n",
                "    hass: HomeAssistant,\n",
                "    async_setup_recorder_instance: RecorderInstanceGenerator,\n",
                "    instrument_migration: InstrumentedMigration,\n",
                ") -> None:\n",
                "    \"\"\"Test that events during migration are queued.\"\"\"\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.core.create_engine\",\n",
                "            new=create_engine_test,\n",
                "        ),\n",
                "    ):\n",
                "        await async_setup_recorder_instance(\n",
                "            hass, {\"commit_interval\": 0}, wait_recorder=False\n",
                "        )\n",
                "        await hass.async_add_executor_job(instrument_migration.migration_started.wait)\n",
                "        assert recorder.util.async_migration_in_progress(hass) is True\n",
                "        hass.states.async_set(\"my.entity\", \"on\", {})\n",
                "        hass.states.async_set(\"my.entity\", \"off\", {})\n",
                "        await hass.async_block_till_done()\n",
                "        async_fire_time_changed(hass, dt_util.utcnow() + datetime.timedelta(hours=2))\n",
                "        await hass.async_block_till_done()\n",
                "        async_fire_time_changed(hass, dt_util.utcnow() + datetime.timedelta(hours=4))\n",
                "\n",
                "        # Let migration finish\n",
                "        instrument_migration.migration_stall.set()\n",
                "        await recorder.get_instance(hass).async_recorder_ready.wait()\n",
                "        await async_wait_recording_done(hass)\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "    db_states = await recorder.get_instance(hass).async_add_executor_job(\n",
                "        _get_native_states, hass, \"my.entity\"\n",
                "    )\n",
                "    assert len(db_states) == 2\n",
                "\n",
                "\n",
                "async def test_events_during_migration_queue_exhausted(\n",
                "    hass: HomeAssistant,\n",
                "    async_setup_recorder_instance: RecorderInstanceGenerator,\n",
                "    instrument_migration: InstrumentedMigration,\n",
                ") -> None:\n",
                "    \"\"\"Test that events during migration takes so long the queue is exhausted.\"\"\"\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.core.create_engine\",\n",
                "            new=create_engine_test,\n",
                "        ),\n",
                "        patch.object(recorder.core, \"MAX_QUEUE_BACKLOG_MIN_VALUE\", 1),\n",
                "        patch.object(\n",
                "            recorder.core, \"MIN_AVAILABLE_MEMORY_FOR_QUEUE_BACKLOG\", sys.maxsize\n",
                "        ),\n",
                "    ):\n",
                "        await async_setup_recorder_instance(\n",
                "            hass, {\"commit_interval\": 0}, wait_recorder=False\n",
                "        )\n",
                "        await hass.async_add_executor_job(instrument_migration.migration_started.wait)\n",
                "        assert recorder.util.async_migration_in_progress(hass) is True\n",
                "        hass.states.async_set(\"my.entity\", \"on\", {})\n",
                "        await hass.async_block_till_done()\n",
                "        async_fire_time_changed(hass, dt_util.utcnow() + datetime.timedelta(hours=2))\n",
                "        await hass.async_block_till_done()\n",
                "        async_fire_time_changed(hass, dt_util.utcnow() + datetime.timedelta(hours=4))\n",
                "        await hass.async_block_till_done()\n",
                "        hass.states.async_set(\"my.entity\", \"off\", {})\n",
                "\n",
                "        # Let migration finish\n",
                "        instrument_migration.migration_stall.set()\n",
                "        await recorder.get_instance(hass).async_recorder_ready.wait()\n",
                "        await async_wait_recording_done(hass)\n",
                "\n",
                "    assert recorder.util.async_migration_in_progress(hass) is False\n",
                "    db_states = await recorder.get_instance(hass).async_add_executor_job(\n",
                "        _get_native_states, hass, \"my.entity\"\n",
                "    )\n",
                "    assert len(db_states) == 1\n",
                "    hass.states.async_set(\"my.entity\", \"on\", {})\n",
                "    await async_wait_recording_done(hass)\n",
                "    db_states = await recorder.get_instance(hass).async_add_executor_job(\n",
                "        _get_native_states, hass, \"my.entity\"\n",
                "    )\n",
                "    assert len(db_states) == 2\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    (\"start_version\", \"live\"),\n",
                "    [(0, True), (9, True), (16, True), (18, True), (22, True), (25, True), (43, True)],\n",
                ")\n",
                "async def test_schema_migrate(\n",
                "    hass: HomeAssistant,\n",
                "    recorder_db_url: str,\n",
                "    async_setup_recorder_instance: RecorderInstanceGenerator,\n",
                "    instrument_migration: InstrumentedMigration,\n",
                "    start_version,\n",
                "    live,\n",
                ") -> None:\n",
                "    \"\"\"Test the full schema migration logic.\n",
                "\n",
                "    We're just testing that the logic can execute successfully here without\n",
                "    throwing exceptions. Maintaining a set of assertions based on schema\n",
                "    inspection could quickly become quite cumbersome.\n",
                "    \"\"\"\n",
                "\n",
                "    real_create_index = recorder.migration._create_index\n",
                "    create_calls = 0\n",
                "\n",
                "    def _create_engine_test(*args, **kwargs):\n",
                "        \"\"\"Test version of create_engine that initializes with old schema.\n",
                "\n",
                "        This simulates an existing db with the old schema.\n",
                "        \"\"\"\n",
                "        module = f\"tests.components.recorder.db_schema_{start_version!s}\"\n",
                "        importlib.import_module(module)\n",
                "        old_models = sys.modules[module]\n",
                "        engine = create_engine(*args, **kwargs)\n",
                "        old_models.Base.metadata.create_all(engine)\n",
                "        if start_version > 0:\n",
                "            with Session(engine) as session:\n",
                "                session.add(\n",
                "                    recorder.db_schema.SchemaChanges(schema_version=start_version)\n",
                "                )\n",
                "                session.commit()\n",
                "        return engine\n",
                "\n",
                "    def _mock_setup_run(self):\n",
                "        self.run_info = RecorderRuns(\n",
                "            start=self.recorder_runs_manager.recording_start, created=dt_util.utcnow()\n",
                "        )\n",
                "\n",
                "    def _sometimes_failing_create_index(*args):\n",
                "        \"\"\"Make the first index create raise a retryable error to ensure we retry.\"\"\"\n",
                "        if recorder_db_url.startswith(\"mysql://\"):\n",
                "            nonlocal create_calls\n",
                "            if create_calls < 1:\n",
                "                create_calls += 1\n",
                "                mysql_exception = OperationalError(\"statement\", {}, [])\n",
                "                mysql_exception.orig = Exception(1205, \"retryable\")\n",
                "                raise mysql_exception\n",
                "        real_create_index(*args)\n",
                "\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.core.create_engine\",\n",
                "            new=_create_engine_test,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.Recorder._setup_run\",\n",
                "            side_effect=_mock_setup_run,\n",
                "            autospec=True,\n",
                "        ) as setup_run,\n",
                "        patch(\"homeassistant.components.recorder.util.time.sleep\"),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration._create_index\",\n",
                "            wraps=_sometimes_failing_create_index,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.Recorder._process_state_changed_event_into_session\",\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.Recorder._process_non_state_changed_event_into_session\",\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.Recorder._pre_process_startup_events\",\n",
                "        ),\n",
                "    ):\n",
                "        await async_setup_recorder_instance(hass, wait_recorder=False)\n",
                "        await hass.async_add_executor_job(instrument_migration.migration_started.wait)\n",
                "        assert recorder.util.async_migration_in_progress(hass) is True\n",
                "        await recorder_helper.async_wait_recorder(hass)\n",
                "\n",
                "        assert recorder.util.async_migration_in_progress(hass) is True\n",
                "        assert recorder.util.async_migration_is_live(hass) == live\n",
                "        instrument_migration.migration_stall.set()\n",
                "        await hass.async_block_till_done()\n",
                "        await hass.async_add_executor_job(instrument_migration.migration_done.wait)\n",
                "        await async_wait_recording_done(hass)\n",
                "        assert instrument_migration.migration_version == db_schema.SCHEMA_VERSION\n",
                "        assert setup_run.called\n",
                "        assert recorder.util.async_migration_in_progress(hass) is not True\n",
                "        assert instrument_migration.apply_update_mock.called\n",
                "\n",
                "\n",
                "def test_invalid_update(hass: HomeAssistant) -> None:\n",
                "    \"\"\"Test that an invalid new version raises an exception.\"\"\"\n",
                "    with pytest.raises(ValueError):\n",
                "        migration._apply_update(Mock(), hass, Mock(), Mock(), -1, 0)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    (\"engine_type\", \"substr\"),\n",
                "    [\n",
                "        (\"postgresql\", \"ALTER event_type TYPE VARCHAR(64)\"),\n",
                "        (\"mssql\", \"ALTER COLUMN event_type VARCHAR(64)\"),\n",
                "        (\"mysql\", \"MODIFY event_type VARCHAR(64)\"),\n",
                "        (\"sqlite\", None),\n",
                "    ],\n",
                ")\n",
                "def test_modify_column(engine_type, substr) -> None:\n",
                "    \"\"\"Test that modify column generates the expected query.\"\"\"\n",
                "    connection = Mock()\n",
                "    session = Mock()\n",
                "    session.connection = Mock(return_value=connection)\n",
                "    instance = Mock()\n",
                "    instance.get_session = Mock(return_value=session)\n",
                "    engine = Mock()\n",
                "    engine.dialect.name = engine_type\n",
                "    migration._modify_columns(\n",
                "        instance.get_session, engine, \"events\", [\"event_type VARCHAR(64)\"]\n",
                "    )\n",
                "    if substr:\n",
                "        assert substr in connection.execute.call_args[0][0].text\n",
                "    else:\n",
                "        assert not connection.execute.called\n",
                "\n",
                "\n",
                "def test_forgiving_add_column(recorder_db_url: str) -> None:\n",
                "    \"\"\"Test that add column will continue if column exists.\"\"\"\n",
                "    engine = create_engine(recorder_db_url, poolclass=StaticPool)\n",
                "    with Session(engine) as session:\n",
                "        session.execute(text(\"CREATE TABLE hello (id int)\"))\n",
                "        instance = Mock()\n",
                "        instance.get_session = Mock(return_value=session)\n",
                "        migration._add_columns(\n",
                "            instance.get_session, \"hello\", [\"context_id CHARACTER(36)\"]\n",
                "        )\n",
                "        migration._add_columns(\n",
                "            instance.get_session, \"hello\", [\"context_id CHARACTER(36)\"]\n",
                "        )\n",
                "    engine.dispose()\n",
                "\n",
                "\n",
                "def test_forgiving_add_index(recorder_db_url: str) -> None:\n",
                "    \"\"\"Test that add index will continue if index exists.\"\"\"\n",
                "    engine = create_engine(recorder_db_url, poolclass=StaticPool)\n",
                "    db_schema.Base.metadata.create_all(engine)\n",
                "    with Session(engine) as session:\n",
                "        instance = Mock()\n",
                "        instance.get_session = Mock(return_value=session)\n",
                "        migration._create_index(\n",
                "            instance.get_session, \"states\", \"ix_states_context_id_bin\"\n",
                "        )\n",
                "    engine.dispose()\n",
                "\n",
                "\n",
                "def test_forgiving_drop_index(\n",
                "    recorder_db_url: str, caplog: pytest.LogCaptureFixture\n",
                ") -> None:\n",
                "    \"\"\"Test that drop index will continue if index drop fails.\"\"\"\n",
                "    engine = create_engine(recorder_db_url, poolclass=StaticPool)\n",
                "    db_schema.Base.metadata.create_all(engine)\n",
                "    with Session(engine) as session:\n",
                "        instance = Mock()\n",
                "        instance.get_session = Mock(return_value=session)\n",
                "        migration._drop_index(\n",
                "            instance.get_session, \"states\", \"ix_states_context_id_bin\"\n",
                "        )\n",
                "        migration._drop_index(\n",
                "            instance.get_session, \"states\", \"ix_states_context_id_bin\"\n",
                "        )\n",
                "\n",
                "        with (\n",
                "            patch(\n",
                "                \"homeassistant.components.recorder.migration.get_index_by_name\",\n",
                "                return_value=\"ix_states_context_id_bin\",\n",
                "            ),\n",
                "            patch.object(\n",
                "                session, \"connection\", side_effect=SQLAlchemyError(\"connection failure\")\n",
                "            ),\n",
                "        ):\n",
                "            migration._drop_index(\n",
                "                instance.get_session, \"states\", \"ix_states_context_id_bin\"\n",
                "            )\n",
                "        assert \"Failed to drop index\" in caplog.text\n",
                "        assert \"connection failure\" in caplog.text\n",
                "        caplog.clear()\n",
                "        with (\n",
                "            patch(\n",
                "                \"homeassistant.components.recorder.migration.get_index_by_name\",\n",
                "                return_value=\"ix_states_context_id_bin\",\n",
                "            ),\n",
                "            patch.object(\n",
                "                session, \"connection\", side_effect=SQLAlchemyError(\"connection failure\")\n",
                "            ),\n",
                "        ):\n",
                "            migration._drop_index(\n",
                "                instance.get_session, \"states\", \"ix_states_context_id_bin\", quiet=True\n",
                "            )\n",
                "        assert \"Failed to drop index\" not in caplog.text\n",
                "        assert \"connection failure\" not in caplog.text\n",
                "    engine.dispose()\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"exception_type\", [OperationalError, ProgrammingError, InternalError]\n",
                ")\n",
                "def test_forgiving_add_index_with_other_db_types(\n",
                "    caplog: pytest.LogCaptureFixture, exception_type\n",
                ") -> None:\n",
                "    \"\"\"Test that add index will continue if index exists on mysql and postgres.\"\"\"\n",
                "    mocked_index = Mock()\n",
                "    type(mocked_index).name = \"ix_states_context_id\"\n",
                "    mocked_index.create = Mock(\n",
                "        side_effect=exception_type(\n",
                "            \"CREATE INDEX ix_states_old_state_id ON states (old_state_id);\",\n",
                "            [],\n",
                "            'relation \"ix_states_old_state_id\" already exists',\n",
                "        )\n",
                "    )\n",
                "\n",
                "    mocked_table = Mock()\n",
                "    type(mocked_table).indexes = PropertyMock(return_value=[mocked_index])\n",
                "\n",
                "    with patch(\n",
                "        \"homeassistant.components.recorder.migration.Table\", return_value=mocked_table\n",
                "    ):\n",
                "        migration._create_index(Mock(), \"states\", \"ix_states_context_id\")\n",
                "\n",
                "    assert \"already exists on states\" in caplog.text\n",
                "    assert \"continuing\" in caplog.text\n",
                "\n",
                "\n",
                "class MockPyODBCProgrammingError(Exception):\n",
                "    \"\"\"A mock pyodbc error.\"\"\"\n",
                "\n",
                "\n",
                "def test_raise_if_exception_missing_str() -> None:\n",
                "    \"\"\"Test we raise an exception if strings are not present.\"\"\"\n",
                "    programming_exc = ProgrammingError(\"select * from;\", Mock(), Mock())\n",
                "    programming_exc.__cause__ = MockPyODBCProgrammingError(\n",
                "        \"[42S11] [FreeTDS][SQL Server]The operation failed because an index or statistics with name 'ix_states_old_state_id' already exists on table 'states'. (1913) (SQLExecDirectW)\"\n",
                "    )\n",
                "\n",
                "    migration.raise_if_exception_missing_str(\n",
                "        programming_exc, [\"already exists\", \"duplicate\"]\n",
                "    )\n",
                "\n",
                "    with pytest.raises(ProgrammingError):\n",
                "        migration.raise_if_exception_missing_str(programming_exc, [\"not present\"])\n",
                "\n",
                "\n",
                "def test_raise_if_exception_missing_empty_cause_str() -> None:\n",
                "    \"\"\"Test we raise an exception if strings are not present with an empty cause.\"\"\"\n",
                "    programming_exc = ProgrammingError(\"select * from;\", Mock(), Mock())\n",
                "    programming_exc.__cause__ = MockPyODBCProgrammingError()\n",
                "\n",
                "    with pytest.raises(ProgrammingError):\n",
                "        migration.raise_if_exception_missing_str(\n",
                "            programming_exc, [\"already exists\", \"duplicate\"]\n",
                "        )\n",
                "\n",
                "    with pytest.raises(ProgrammingError):\n",
                "        migration.raise_if_exception_missing_str(programming_exc, [\"not present\"])\n",
                "\n",
                "\n",
                "@pytest.mark.skip_on_db_engine([\"mysql\", \"postgresql\"])\n",
                "@pytest.mark.usefixtures(\"skip_by_db_engine\")\n",
                "def test_rebuild_sqlite_states_table(recorder_db_url: str) -> None:\n",
                "    \"\"\"Test that we can rebuild the states table in SQLite.\n",
                "\n",
                "    This test is specific for SQLite.\n",
                "    \"\"\"\n",
                "    engine = create_engine(recorder_db_url)\n",
                "    session_maker = scoped_session(sessionmaker(bind=engine, future=True))\n",
                "    with session_scope(session=session_maker()) as session:\n",
                "        db_schema.Base.metadata.create_all(engine)\n",
                "    with session_scope(session=session_maker()) as session:\n",
                "        session.add(States(state=\"on\"))\n",
                "        session.commit()\n",
                "\n",
                "    migration.rebuild_sqlite_table(session_maker, engine, States)\n",
                "\n",
                "    with session_scope(session=session_maker()) as session:\n",
                "        assert session.query(States).count() == 1\n",
                "        assert session.query(States).first().state == \"on\"\n",
                "\n",
                "    engine.dispose()\n",
                "\n",
                "\n",
                "@pytest.mark.skip_on_db_engine([\"mysql\", \"postgresql\"])\n",
                "@pytest.mark.usefixtures(\"skip_by_db_engine\")\n",
                "def test_rebuild_sqlite_states_table_missing_fails(\n",
                "    recorder_db_url: str, caplog: pytest.LogCaptureFixture\n",
                ") -> None:\n",
                "    \"\"\"Test handling missing states table when attempting rebuild.\n",
                "\n",
                "    This test is specific for SQLite.\n",
                "    \"\"\"\n",
                "    engine = create_engine(recorder_db_url)\n",
                "    session_maker = scoped_session(sessionmaker(bind=engine, future=True))\n",
                "    with session_scope(session=session_maker()) as session:\n",
                "        db_schema.Base.metadata.create_all(engine)\n",
                "\n",
                "    with session_scope(session=session_maker()) as session:\n",
                "        session.add(Events(event_type=\"state_changed\", event_data=\"{}\"))\n",
                "        session.connection().execute(text(\"DROP TABLE states\"))\n",
                "        session.commit()\n",
                "\n",
                "    migration.rebuild_sqlite_table(session_maker, engine, States)\n",
                "    assert \"Error recreating SQLite table states\" in caplog.text\n",
                "    caplog.clear()\n",
                "\n",
                "    # Now rebuild the events table to make sure the database did not\n",
                "    # get corrupted\n",
                "    migration.rebuild_sqlite_table(session_maker, engine, Events)\n",
                "\n",
                "    with session_scope(session=session_maker()) as session:\n",
                "        assert session.query(Events).count() == 1\n",
                "        assert session.query(Events).first().event_type == \"state_changed\"\n",
                "        assert session.query(Events).first().event_data == \"{}\"\n",
                "\n",
                "    engine.dispose()\n",
                "\n",
                "\n",
                "@pytest.mark.skip_on_db_engine([\"mysql\", \"postgresql\"])\n",
                "@pytest.mark.usefixtures(\"skip_by_db_engine\")\n",
                "def test_rebuild_sqlite_states_table_extra_columns(\n",
                "    recorder_db_url: str, caplog: pytest.LogCaptureFixture\n",
                ") -> None:\n",
                "    \"\"\"Test handling extra columns when rebuilding the states table.\n",
                "\n",
                "    This test is specific for SQLite.\n",
                "    \"\"\"\n",
                "    engine = create_engine(recorder_db_url)\n",
                "    session_maker = scoped_session(sessionmaker(bind=engine, future=True))\n",
                "    with session_scope(session=session_maker()) as session:\n",
                "        db_schema.Base.metadata.create_all(engine)\n",
                "    with session_scope(session=session_maker()) as session:\n",
                "        session.add(States(state=\"on\"))\n",
                "        session.commit()\n",
                "        session.connection().execute(\n",
                "            text(\"ALTER TABLE states ADD COLUMN extra_column TEXT\")\n",
                "        )\n",
                "\n",
                "    migration.rebuild_sqlite_table(session_maker, engine, States)\n",
                "    assert \"Error recreating SQLite table states\" not in caplog.text\n",
                "\n",
                "    with session_scope(session=session_maker()) as session:\n",
                "        assert session.query(States).count() == 1\n",
                "        assert session.query(States).first().state == \"on\"\n",
                "\n",
                "    engine.dispose()\n",
                "\n",
                "\n",
                "@pytest.mark.skip_on_db_engine([\"sqlite\"])\n",
                "@pytest.mark.usefixtures(\"skip_by_db_engine\")\n",
                "def test_drop_restore_foreign_key_constraints(recorder_db_url: str) -> None:\n",
                "    \"\"\"Test we can drop and then restore foreign keys.\n",
                "\n",
                "    This is not supported on SQLite\n",
                "    \"\"\"\n",
                "\n",
                "    constraints_to_recreate = (\n",
                "        (\"events\", \"data_id\"),\n",
                "        (\"states\", \"event_id\"),  # This won't be found\n",
                "        (\"states\", \"old_state_id\"),\n",
                "    )\n",
                "\n",
                "    db_engine = recorder_db_url.partition(\"://\")[0]\n",
                "\n",
                "    expected_dropped_constraints = {\n",
                "        \"mysql\": [\n",
                "            (\n",
                "                \"events\",\n",
                "                \"data_id\",\n",
                "                {\n",
                "                    \"constrained_columns\": [\"data_id\"],\n",
                "                    \"name\": ANY,\n",
                "                    \"options\": {},\n",
                "                    \"referred_columns\": [\"data_id\"],\n",
                "                    \"referred_schema\": None,\n",
                "                    \"referred_table\": \"event_data\",\n",
                "                },\n",
                "            ),\n",
                "            (\n",
                "                \"states\",\n",
                "                \"old_state_id\",\n",
                "                {\n",
                "                    \"constrained_columns\": [\"old_state_id\"],\n",
                "                    \"name\": ANY,\n",
                "                    \"options\": {},\n",
                "                    \"referred_columns\": [\"state_id\"],\n",
                "                    \"referred_schema\": None,\n",
                "                    \"referred_table\": \"states\",\n",
                "                },\n",
                "            ),\n",
                "        ],\n",
                "        \"postgresql\": [\n",
                "            (\n",
                "                \"events\",\n",
                "                \"data_id\",\n",
                "                {\n",
                "                    \"comment\": None,\n",
                "                    \"constrained_columns\": [\"data_id\"],\n",
                "                    \"name\": \"events_data_id_fkey\",\n",
                "                    \"options\": {},\n",
                "                    \"referred_columns\": [\"data_id\"],\n",
                "                    \"referred_schema\": None,\n",
                "                    \"referred_table\": \"event_data\",\n",
                "                },\n",
                "            ),\n",
                "            (\n",
                "                \"states\",\n",
                "                \"old_state_id\",\n",
                "                {\n",
                "                    \"comment\": None,\n",
                "                    \"constrained_columns\": [\"old_state_id\"],\n",
                "                    \"name\": \"states_old_state_id_fkey\",\n",
                "                    \"options\": {},\n",
                "                    \"referred_columns\": [\"state_id\"],\n",
                "                    \"referred_schema\": None,\n",
                "                    \"referred_table\": \"states\",\n",
                "                },\n",
                "            ),\n",
                "        ],\n",
                "    }\n",
                "\n",
                "    engine = create_engine(recorder_db_url)\n",
                "    db_schema.Base.metadata.create_all(engine)\n",
                "\n",
                "    with Session(engine) as session:\n",
                "        session_maker = Mock(return_value=session)\n",
                "        dropped_constraints_1 = [\n",
                "            dropped_constraint\n",
                "            for table, column in constraints_to_recreate\n",
                "            for dropped_constraint in migration._drop_foreign_key_constraints(\n",
                "                session_maker, engine, table, column\n",
                "            )\n",
                "        ]\n",
                "    assert dropped_constraints_1 == expected_dropped_constraints[db_engine]\n",
                "\n",
                "    # Check we don't find the constrained columns again (they are removed)\n",
                "    with Session(engine) as session:\n",
                "        session_maker = Mock(return_value=session)\n",
                "        dropped_constraints_2 = [\n",
                "            dropped_constraint\n",
                "            for table, column in constraints_to_recreate\n",
                "            for dropped_constraint in migration._drop_foreign_key_constraints(\n",
                "                session_maker, engine, table, column\n",
                "            )\n",
                "        ]\n",
                "    assert dropped_constraints_2 == []\n",
                "\n",
                "    # Restore the constraints\n",
                "    with Session(engine) as session:\n",
                "        session_maker = Mock(return_value=session)\n",
                "        migration._restore_foreign_key_constraints(\n",
                "            session_maker, engine, dropped_constraints_1\n",
                "        )\n",
                "\n",
                "    # Check we do find the constrained columns again (they are restored)\n",
                "    with Session(engine) as session:\n",
                "        session_maker = Mock(return_value=session)\n",
                "        dropped_constraints_3 = [\n",
                "            dropped_constraint\n",
                "            for table, column in constraints_to_recreate\n",
                "            for dropped_constraint in migration._drop_foreign_key_constraints(\n",
                "                session_maker, engine, table, column\n",
                "            )\n",
                "        ]\n",
                "    assert dropped_constraints_3 == expected_dropped_constraints[db_engine]\n",
                "\n",
                "    engine.dispose()\n",
                "\n",
                "\n",
                "def test_restore_foreign_key_constraints_with_error(\n",
                "    caplog: pytest.LogCaptureFixture,\n",
                ") -> None:\n",
                "    \"\"\"Test we can drop and then restore foreign keys.\n",
                "\n",
                "    This is not supported on SQLite\n",
                "    \"\"\"\n",
                "\n",
                "    constraints_to_restore = [\n",
                "        (\n",
                "            \"events\",\n",
                "            \"data_id\",\n",
                "            {\n",
                "                \"comment\": None,\n",
                "                \"constrained_columns\": [\"data_id\"],\n",
                "                \"name\": \"events_data_id_fkey\",\n",
                "                \"options\": {},\n",
                "                \"referred_columns\": [\"data_id\"],\n",
                "                \"referred_schema\": None,\n",
                "                \"referred_table\": \"event_data\",\n",
                "            },\n",
                "        ),\n",
                "    ]\n",
                "\n",
                "    connection = Mock()\n",
                "    connection.execute = Mock(side_effect=InternalError(None, None, None))\n",
                "    session = Mock()\n",
                "    session.connection = Mock(return_value=connection)\n",
                "    instance = Mock()\n",
                "    instance.get_session = Mock(return_value=session)\n",
                "    engine = Mock()\n",
                "\n",
                "    session_maker = Mock(return_value=session)\n",
                "    migration._restore_foreign_key_constraints(\n",
                "        session_maker, engine, constraints_to_restore\n",
                "    )\n",
                "\n",
                "    assert \"Could not update foreign options in events table\" in caplog.text"
            ]
        ],
        "tests/conftest.py": [
            [
                "\"\"\"Set up some common test helper things.\"\"\"\n",
                "\n",
                "from __future__ import annotations\n",
                "\n",
                "import asyncio\n",
                "from collections.abc import AsyncGenerator, Callable, Coroutine, Generator\n",
                "from contextlib import AsyncExitStack, asynccontextmanager, contextmanager\n",
                "import datetime\n",
                "import functools\n",
                "import gc\n",
                "import itertools\n",
                "import logging\n",
                "import os\n",
                "import reprlib\n",
                "from shutil import rmtree\n",
                "import sqlite3\n",
                "import ssl\n",
                "import threading\n",
                "from typing import TYPE_CHECKING, Any, cast\n",
                "from unittest.mock import AsyncMock, MagicMock, Mock, _patch, patch\n",
                "\n",
                "from aiohttp import client\n",
                "from aiohttp.test_utils import (\n",
                "    BaseTestServer,\n",
                "    TestClient,\n",
                "    TestServer,\n",
                "    make_mocked_request,\n",
                ")\n",
                "from aiohttp.typedefs import JSONDecoder\n",
                "from aiohttp.web import Application\n",
                "import bcrypt\n",
                "import freezegun\n",
                "import multidict\n",
                "import pytest\n",
                "import pytest_socket\n",
                "import requests_mock\n",
                "from syrupy.assertion import SnapshotAssertion\n",
                "\n",
                "from homeassistant import block_async_io\n",
                "from homeassistant.exceptions import ServiceNotFound\n",
                "\n",
                "# Setup patching if dt_util time functions before any other Home Assistant imports\n",
                "from . import patch_time  # noqa: F401, isort:skip\n",
                "\n",
                "from homeassistant import core as ha, loader, runner\n",
                "from homeassistant.auth.const import GROUP_ID_ADMIN, GROUP_ID_READ_ONLY\n",
                "from homeassistant.auth.models import Credentials\n",
                "from homeassistant.auth.providers import homeassistant\n",
                "from homeassistant.components.device_tracker.legacy import Device\n",
                "from homeassistant.components.websocket_api.auth import (\n",
                "    TYPE_AUTH,\n",
                "    TYPE_AUTH_OK,\n",
                "    TYPE_AUTH_REQUIRED,\n",
                ")\n",
                "from homeassistant.components.websocket_api.http import URL\n",
                "from homeassistant.config import YAML_CONFIG_FILE\n",
                "from homeassistant.config_entries import ConfigEntries, ConfigEntry, ConfigEntryState\n",
                "from homeassistant.const import BASE_PLATFORMS, HASSIO_USER_NAME\n",
                "from homeassistant.core import (\n",
                "    Context,\n",
                "    CoreState,\n",
                "    HassJob,\n",
                "    HomeAssistant,\n",
                "    ServiceCall,\n",
                "    ServiceResponse,\n",
                ")\n",
                "from homeassistant.helpers import (\n",
                "    area_registry as ar,\n",
                "    category_registry as cr,\n",
                "    config_entry_oauth2_flow,\n",
                "    device_registry as dr,\n",
                "    entity_registry as er,\n",
                "    floor_registry as fr,\n",
                "    issue_registry as ir,\n",
                "    label_registry as lr,\n",
                "    recorder as recorder_helper,\n",
                ")\n",
                "from homeassistant.helpers.dispatcher import async_dispatcher_send\n",
                "from homeassistant.helpers.translation import _TranslationsCacheData\n",
                "from homeassistant.helpers.typing import ConfigType\n",
                "from homeassistant.setup import async_setup_component\n",
                "from homeassistant.util import dt as dt_util, location\n",
                "from homeassistant.util.async_ import create_eager_task\n",
                "from homeassistant.util.json import json_loads\n",
                "\n",
                "from .ignore_uncaught_exceptions import IGNORE_UNCAUGHT_EXCEPTIONS\n",
                "from .syrupy import HomeAssistantSnapshotExtension\n",
                "from .typing import (\n",
                "    ClientSessionGenerator,\n",
                "    MockHAClientWebSocket,\n",
                "    MqttMockHAClient,\n",
                "    MqttMockHAClientGenerator,\n",
                "    MqttMockPahoClient,\n",
                "    RecorderInstanceGenerator,\n",
                "    WebSocketGenerator,\n",
                ")\n",
                "\n",
                "if TYPE_CHECKING:\n",
                "    # Local import to avoid processing recorder and SQLite modules when running a\n",
                "    # testcase which does not use the recorder.\n",
                "    from homeassistant.components import recorder\n",
                "\n",
                "pytest.register_assert_rewrite(\"tests.common\")\n",
                "\n",
                "from .common import (  # noqa: E402, isort:skip\n",
                "    CLIENT_ID,\n",
                "    INSTANCES,\n",
                "    MockConfigEntry,\n",
                "    MockUser,\n",
                "    async_fire_mqtt_message,\n",
                "    async_test_home_assistant,\n",
                "    mock_storage,\n",
                "    patch_yaml_files,\n",
                "    extract_stack_to_frame,\n",
                ")\n",
                "from .test_util.aiohttp import (  # noqa: E402, isort:skip\n",
                "    AiohttpClientMocker,\n",
                "    mock_aiohttp_client,\n",
                ")\n",
                "\n",
                "_LOGGER = logging.getLogger(__name__)\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logging.getLogger(\"sqlalchemy.engine\").setLevel(logging.INFO)\n",
                "\n",
                "asyncio.set_event_loop_policy(runner.HassEventLoopPolicy(False))\n",
                "# Disable fixtures overriding our beautiful policy\n",
                "asyncio.set_event_loop_policy = lambda policy: None\n",
                "\n",
                "\n",
                "def pytest_addoption(parser: pytest.Parser) -> None:\n",
                "    \"\"\"Register custom pytest options.\"\"\"\n",
                "    parser.addoption(\"--dburl\", action=\"store\", default=\"sqlite://\")\n",
                "\n",
                "\n",
                "def pytest_configure(config: pytest.Config) -> None:\n",
                "    \"\"\"Register marker for tests that log exceptions.\"\"\"\n",
                "    config.addinivalue_line(\n",
                "        \"markers\", \"no_fail_on_log_exception: mark test to not fail on logged exception\"\n",
                "    )\n",
                "    if config.getoption(\"verbose\") > 0:\n",
                "        logging.getLogger().setLevel(logging.DEBUG)\n",
                "\n",
                "\n",
                "def pytest_runtest_setup() -> None:\n",
                "    \"\"\"Prepare pytest_socket and freezegun.\n",
                "\n",
                "    pytest_socket:\n",
                "    Throw if tests attempt to open sockets.\n",
                "\n",
                "    allow_unix_socket is set to True because it's needed by asyncio.\n",
                "    Important: socket_allow_hosts must be called before disable_socket, otherwise all\n",
                "    destinations will be allowed.\n",
                "\n",
                "    freezegun:\n",
                "    Modified to include https://github.com/spulec/freezegun/pull/424\n",
                "    \"\"\"\n",
                "    pytest_socket.socket_allow_hosts([\"127.0.0.1\"])\n",
                "    pytest_socket.disable_socket(allow_unix_socket=True)\n",
                "\n",
                "    freezegun.api.datetime_to_fakedatetime = ha_datetime_to_fakedatetime  # type: ignore[attr-defined]\n",
                "    freezegun.api.FakeDatetime = HAFakeDatetime  # type: ignore[attr-defined]\n",
                "\n",
                "    def adapt_datetime(val):\n",
                "        return val.isoformat(\" \")\n",
                "\n",
                "    # Setup HAFakeDatetime converter for sqlite3\n",
                "    sqlite3.register_adapter(HAFakeDatetime, adapt_datetime)\n",
                "\n",
                "    # Setup HAFakeDatetime converter for pymysql\n",
                "    try:\n",
                "        # pylint: disable-next=import-outside-toplevel\n",
                "        import MySQLdb.converters as MySQLdb_converters\n",
                "    except ImportError:\n",
                "        pass\n",
                "    else:\n",
                "        MySQLdb_converters.conversions[HAFakeDatetime] = (\n",
                "            MySQLdb_converters.DateTime2literal\n",
                "        )\n",
                "\n",
                "\n",
                "def ha_datetime_to_fakedatetime(datetime) -> freezegun.api.FakeDatetime:  # type: ignore[name-defined]\n",
                "    \"\"\"Convert datetime to FakeDatetime.\n",
                "\n",
                "    Modified to include https://github.com/spulec/freezegun/pull/424.\n",
                "    \"\"\"\n",
                "    return freezegun.api.FakeDatetime(  # type: ignore[attr-defined]\n",
                "        datetime.year,\n",
                "        datetime.month,\n",
                "        datetime.day,\n",
                "        datetime.hour,\n",
                "        datetime.minute,\n",
                "        datetime.second,\n",
                "        datetime.microsecond,\n",
                "        datetime.tzinfo,\n",
                "        fold=datetime.fold,\n",
                "    )\n",
                "\n",
                "\n",
                "class HAFakeDatetime(freezegun.api.FakeDatetime):  # type: ignore[name-defined]\n",
                "    \"\"\"Modified to include https://github.com/spulec/freezegun/pull/424.\"\"\"\n",
                "\n",
                "    @classmethod\n",
                "    def now(cls, tz=None):\n",
                "        \"\"\"Return frozen now.\"\"\"\n",
                "        now = cls._time_to_freeze() or freezegun.api.real_datetime.now()\n",
                "        if tz:\n",
                "            result = tz.fromutc(now.replace(tzinfo=tz))\n",
                "        else:\n",
                "            result = now\n",
                "\n",
                "        # Add the _tz_offset only if it's non-zero to preserve fold\n",
                "        if cls._tz_offset():\n",
                "            result += cls._tz_offset()\n",
                "\n",
                "        return ha_datetime_to_fakedatetime(result)\n",
                "\n",
                "\n",
                "def check_real[**_P, _R](func: Callable[_P, Coroutine[Any, Any, _R]]):\n",
                "    \"\"\"Force a function to require a keyword _test_real to be passed in.\"\"\"\n",
                "\n",
                "    @functools.wraps(func)\n",
                "    async def guard_func(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n",
                "        real = kwargs.pop(\"_test_real\", None)\n",
                "\n",
                "        if not real:\n",
                "            raise RuntimeError(\n",
                "                f'Forgot to mock or pass \"_test_real=True\" to {func.__name__}'\n",
                "            )\n",
                "\n",
                "        return await func(*args, **kwargs)\n",
                "\n",
                "    return guard_func\n",
                "\n",
                "\n",
                "# Guard a few functions that would make network connections\n",
                "location.async_detect_location_info = check_real(location.async_detect_location_info)\n",
                "\n",
                "\n",
                "@pytest.fixture(name=\"caplog\")\n",
                "def caplog_fixture(caplog: pytest.LogCaptureFixture) -> pytest.LogCaptureFixture:\n",
                "    \"\"\"Set log level to debug for tests using the caplog fixture.\"\"\"\n",
                "    caplog.set_level(logging.DEBUG)\n",
                "    return caplog\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True, scope=\"module\")\n",
                "def garbage_collection() -> None:\n",
                "    \"\"\"Run garbage collection at known locations.\n",
                "\n",
                "    This is to mimic the behavior of pytest-aiohttp, and is\n",
                "    required to avoid warnings during garbage collection from\n",
                "    spilling over into next test case. We run it per module which\n",
                "    handles the most common cases and let each module override\n",
                "    to run per test case if needed.\n",
                "    \"\"\"\n",
                "    gc.collect()\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True)\n",
                "def expected_lingering_tasks() -> bool:\n",
                "    \"\"\"Temporary ability to bypass test failures.\n",
                "\n",
                "    Parametrize to True to bypass the pytest failure.\n",
                "    @pytest.mark.parametrize(\"expected_lingering_tasks\", [True])\n",
                "\n",
                "    This should be removed when all lingering tasks have been cleaned up.\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True)\n",
                "def expected_lingering_timers() -> bool:\n",
                "    \"\"\"Temporary ability to bypass test failures.\n",
                "\n",
                "    Parametrize to True to bypass the pytest failure.\n",
                "    @pytest.mark.parametrize(\"expected_lingering_timers\", [True])\n",
                "\n",
                "    This should be removed when all lingering timers have been cleaned up.\n",
                "    \"\"\"\n",
                "    current_test = os.getenv(\"PYTEST_CURRENT_TEST\")\n",
                "    if (\n",
                "        current_test\n",
                "        and current_test.startswith(\"tests/components/\")\n",
                "        and current_test.split(\"/\")[2] not in BASE_PLATFORMS\n",
                "    ):\n",
                "        # As a starting point, we ignore non-platform components\n",
                "        return True\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def wait_for_stop_scripts_after_shutdown() -> bool:\n",
                "    \"\"\"Add ability to bypass _schedule_stop_scripts_after_shutdown.\n",
                "\n",
                "    _schedule_stop_scripts_after_shutdown leaves a lingering timer.\n",
                "\n",
                "    Parametrize to True to bypass the pytest failure.\n",
                "    @pytest.mark.parametrize(\"wait_for_stop_scripts_at_shutdown\", [True])\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True)\n",
                "def skip_stop_scripts(\n",
                "    wait_for_stop_scripts_after_shutdown: bool,\n",
                ") -> Generator[None]:\n",
                "    \"\"\"Add ability to bypass _schedule_stop_scripts_after_shutdown.\"\"\"\n",
                "    if wait_for_stop_scripts_after_shutdown:\n",
                "        yield\n",
                "        return\n",
                "    with patch(\n",
                "        \"homeassistant.helpers.script._schedule_stop_scripts_after_shutdown\",\n",
                "        Mock(),\n",
                "    ):\n",
                "        yield\n",
                "\n",
                "\n",
                "@contextmanager\n",
                "def long_repr_strings() -> Generator[None]:\n",
                "    \"\"\"Increase reprlib maxstring and maxother to 300.\"\"\"\n",
                "    arepr = reprlib.aRepr\n",
                "    original_maxstring = arepr.maxstring\n",
                "    original_maxother = arepr.maxother\n",
                "    arepr.maxstring = 300\n",
                "    arepr.maxother = 300\n",
                "    try:\n",
                "        yield\n",
                "    finally:\n",
                "        arepr.maxstring = original_maxstring\n",
                "        arepr.maxother = original_maxother\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True)\n",
                "def enable_event_loop_debug(event_loop: asyncio.AbstractEventLoop) -> None:\n",
                "    \"\"\"Enable event loop debug mode.\"\"\"\n",
                "    event_loop.set_debug(True)\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True)\n",
                "def verify_cleanup(\n",
                "    event_loop: asyncio.AbstractEventLoop,\n",
                "    expected_lingering_tasks: bool,\n",
                "    expected_lingering_timers: bool,\n",
                ") -> Generator[None]:\n",
                "    \"\"\"Verify that the test has cleaned up resources correctly.\"\"\"\n",
                "    threads_before = frozenset(threading.enumerate())\n",
                "    tasks_before = asyncio.all_tasks(event_loop)\n",
                "    yield\n",
                "\n",
                "    event_loop.run_until_complete(event_loop.shutdown_default_executor())\n",
                "\n",
                "    if len(INSTANCES) >= 2:\n",
                "        count = len(INSTANCES)\n",
                "        for inst in INSTANCES:\n",
                "            inst.stop()\n",
                "        pytest.exit(f\"Detected non stopped instances ({count}), aborting test run\")\n",
                "\n",
                "    # Warn and clean-up lingering tasks and timers\n",
                "    # before moving on to the next test.\n",
                "    tasks = asyncio.all_tasks(event_loop) - tasks_before\n",
                "    for task in tasks:\n",
                "        if expected_lingering_tasks:\n",
                "            _LOGGER.warning(\"Lingering task after test %r\", task)\n",
                "        else:\n",
                "            pytest.fail(f\"Lingering task after test {task!r}\")\n",
                "        task.cancel()\n",
                "    if tasks:\n",
                "        event_loop.run_until_complete(asyncio.wait(tasks))\n",
                "\n",
                "    for handle in event_loop._scheduled:  # type: ignore[attr-defined]\n",
                "        if not handle.cancelled():\n",
                "            with long_repr_strings():\n",
                "                if expected_lingering_timers:\n",
                "                    _LOGGER.warning(\"Lingering timer after test %r\", handle)\n",
                "                elif handle._args and isinstance(job := handle._args[-1], HassJob):\n",
                "                    if job.cancel_on_shutdown:\n",
                "                        continue\n",
                "                    pytest.fail(f\"Lingering timer after job {job!r}\")\n",
                "                else:\n",
                "                    pytest.fail(f\"Lingering timer after test {handle!r}\")\n",
                "                handle.cancel()\n",
                "\n",
                "    # Verify no threads where left behind.\n",
                "    threads = frozenset(threading.enumerate()) - threads_before\n",
                "    for thread in threads:\n",
                "        assert isinstance(thread, threading._DummyThread) or thread.name.startswith(\n",
                "            \"waitpid-\"\n",
                "        )\n",
                "\n",
                "    try:\n",
                "        # Verify the default time zone has been restored\n",
                "        assert dt_util.DEFAULT_TIME_ZONE is datetime.UTC\n",
                "    finally:\n",
                "        # Restore the default time zone to not break subsequent tests\n",
                "        dt_util.DEFAULT_TIME_ZONE = datetime.UTC\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True)\n",
                "def reset_hass_threading_local_object() -> Generator[None]:\n",
                "    \"\"\"Reset the _Hass threading.local object for every test case.\"\"\"\n",
                "    yield\n",
                "    ha._hass.__dict__.clear()\n",
                "\n",
                "\n",
                "@pytest.fixture(scope=\"session\", autouse=True)\n",
                "def bcrypt_cost() -> Generator[None]:\n",
                "    \"\"\"Run with reduced rounds during tests, to speed up uses.\"\"\"\n",
                "    gensalt_orig = bcrypt.gensalt\n",
                "\n",
                "    def gensalt_mock(rounds=12, prefix=b\"2b\"):\n",
                "        return gensalt_orig(4, prefix)\n",
                "\n",
                "    bcrypt.gensalt = gensalt_mock\n",
                "    yield\n",
                "    bcrypt.gensalt = gensalt_orig\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def hass_storage() -> Generator[dict[str, Any]]:\n",
                "    \"\"\"Fixture to mock storage.\"\"\"\n",
                "    with mock_storage() as stored_data:\n",
                "        yield stored_data\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def load_registries() -> bool:\n",
                "    \"\"\"Fixture to control the loading of registries when setting up the hass fixture.\n",
                "\n",
                "    To avoid loading the registries, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"load_registries\", [False])\n",
                "    \"\"\"\n",
                "    return True\n",
                "\n",
                "\n",
                "class CoalescingResponse(client.ClientWebSocketResponse):\n",
                "    \"\"\"ClientWebSocketResponse client that mimics the websocket js code.\"\"\"\n",
                "\n",
                "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
                "        \"\"\"Init the ClientWebSocketResponse.\"\"\"\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self._recv_buffer: list[Any] = []\n",
                "\n",
                "    async def receive_json(\n",
                "        self,\n",
                "        *,\n",
                "        loads: JSONDecoder = json_loads,\n",
                "        timeout: float | None = None,\n",
                "    ) -> Any:\n",
                "        \"\"\"receive_json or from buffer.\"\"\"\n",
                "        if self._recv_buffer:\n",
                "            return self._recv_buffer.pop(0)\n",
                "        data = await self.receive_str(timeout=timeout)\n",
                "        decoded = loads(data)\n",
                "        if isinstance(decoded, list):\n",
                "            self._recv_buffer = decoded\n",
                "            return self._recv_buffer.pop(0)\n",
                "        return decoded\n",
                "\n",
                "\n",
                "class CoalescingClient(TestClient):\n",
                "    \"\"\"Client that mimics the websocket js code.\"\"\"\n",
                "\n",
                "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
                "        \"\"\"Init TestClient.\"\"\"\n",
                "        super().__init__(*args, ws_response_class=CoalescingResponse, **kwargs)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def aiohttp_client_cls() -> type[CoalescingClient]:\n",
                "    \"\"\"Override the test class for aiohttp.\"\"\"\n",
                "    return CoalescingClient\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def aiohttp_client(\n",
                "    event_loop: asyncio.AbstractEventLoop,\n",
                ") -> Generator[ClientSessionGenerator]:\n",
                "    \"\"\"Override the default aiohttp_client since 3.x does not support aiohttp_client_cls.\n",
                "\n",
                "    Remove this when upgrading to 4.x as aiohttp_client_cls\n",
                "    will do the same thing\n",
                "\n",
                "    aiohttp_client(app, **kwargs)\n",
                "    aiohttp_client(server, **kwargs)\n",
                "    aiohttp_client(raw_server, **kwargs)\n",
                "    \"\"\"\n",
                "    loop = event_loop\n",
                "    clients = []\n",
                "\n",
                "    async def go(\n",
                "        __param: Application | BaseTestServer,\n",
                "        *args: Any,\n",
                "        server_kwargs: dict[str, Any] | None = None,\n",
                "        **kwargs: Any,\n",
                "    ) -> TestClient:\n",
                "        if isinstance(__param, Callable) and not isinstance(  # type: ignore[arg-type]\n",
                "            __param, (Application, BaseTestServer)\n",
                "        ):\n",
                "            __param = __param(loop, *args, **kwargs)\n",
                "            kwargs = {}\n",
                "        else:\n",
                "            assert not args, \"args should be empty\"\n",
                "\n",
                "        client: TestClient\n",
                "        if isinstance(__param, Application):\n",
                "            server_kwargs = server_kwargs or {}\n",
                "            server = TestServer(__param, loop=loop, **server_kwargs)\n",
                "            # Registering a view after starting the server should still work.\n",
                "            server.app._router.freeze = lambda: None\n",
                "            client = CoalescingClient(server, loop=loop, **kwargs)\n",
                "        elif isinstance(__param, BaseTestServer):\n",
                "            client = TestClient(__param, loop=loop, **kwargs)\n",
                "        else:\n",
                "            raise TypeError(f\"Unknown argument type: {type(__param)!r}\")\n",
                "\n",
                "        await client.start_server()\n",
                "        clients.append(client)\n",
                "        return client\n",
                "\n",
                "    yield go\n",
                "\n",
                "    async def finalize() -> None:\n",
                "        while clients:\n",
                "            await clients.pop().close()\n",
                "\n",
                "    loop.run_until_complete(finalize())\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def hass_fixture_setup() -> list[bool]:\n",
                "    \"\"\"Fixture which is truthy if the hass fixture has been setup.\"\"\"\n",
                "    return []\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def hass(\n",
                "    hass_fixture_setup: list[bool],\n",
                "    load_registries: bool,\n",
                "    hass_storage: dict[str, Any],\n",
                "    request: pytest.FixtureRequest,\n",
                "    mock_recorder_before_hass: None,\n",
                ") -> AsyncGenerator[HomeAssistant]:\n",
                "    \"\"\"Create a test instance of Home Assistant.\"\"\"\n",
                "\n",
                "    loop = asyncio.get_running_loop()\n",
                "    hass_fixture_setup.append(True)\n",
                "\n",
                "    def exc_handle(loop, context):\n",
                "        \"\"\"Handle exceptions by rethrowing them, which will fail the test.\"\"\"\n",
                "        # Most of these contexts will contain an exception, but not all.\n",
                "        # The docs note the key as \"optional\"\n",
                "        # See https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.call_exception_handler\n",
                "        if \"exception\" in context:\n",
                "            exceptions.append(context[\"exception\"])\n",
                "        else:\n",
                "            exceptions.append(\n",
                "                Exception(\n",
                "                    \"Received exception handler without exception, \"\n",
                "                    f\"but with message: {context[\"message\"]}\"\n",
                "                )\n",
                "            )\n",
                "        orig_exception_handler(loop, context)\n",
                "\n",
                "    exceptions: list[Exception] = []\n",
                "    async with async_test_home_assistant(loop, load_registries) as hass:\n",
                "        orig_exception_handler = loop.get_exception_handler()\n",
                "        loop.set_exception_handler(exc_handle)\n",
                "\n",
                "        yield hass\n",
                "\n",
                "        # Config entries are not normally unloaded on HA shutdown. They are unloaded here\n",
                "        # to ensure that they could, and to help track lingering tasks and timers.\n",
                "        loaded_entries = [\n",
                "            entry\n",
                "            for entry in hass.config_entries.async_entries()\n",
                "            if entry.state is ConfigEntryState.LOADED\n",
                "        ]\n",
                "        if loaded_entries:\n",
                "            await asyncio.gather(\n",
                "                *(\n",
                "                    create_eager_task(\n",
                "                        hass.config_entries.async_unload(config_entry.entry_id),\n",
                "                        loop=hass.loop,\n",
                "                    )\n",
                "                    for config_entry in loaded_entries\n",
                "                )\n",
                "            )\n",
                "\n",
                "        await hass.async_stop(force=True)\n",
                "\n",
                "    for ex in exceptions:\n",
                "        if (\n",
                "            request.module.__name__,\n",
                "            request.function.__name__,\n",
                "        ) in IGNORE_UNCAUGHT_EXCEPTIONS:\n",
                "            continue\n",
                "        raise ex\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def stop_hass() -> AsyncGenerator[None]:\n",
                "    \"\"\"Make sure all hass are stopped.\"\"\"\n",
                "    orig_hass = ha.HomeAssistant\n",
                "\n",
                "    event_loop = asyncio.get_running_loop()\n",
                "    created = []\n",
                "\n",
                "    def mock_hass(*args):\n",
                "        hass_inst = orig_hass(*args)\n",
                "        created.append(hass_inst)\n",
                "        return hass_inst\n",
                "\n",
                "    with patch(\"homeassistant.core.HomeAssistant\", mock_hass):\n",
                "        yield\n",
                "\n",
                "    for hass_inst in created:\n",
                "        if hass_inst.state == ha.CoreState.stopped:\n",
                "            continue\n",
                "\n",
                "        with patch.object(hass_inst.loop, \"stop\"):\n",
                "            await hass_inst.async_block_till_done()\n",
                "            await hass_inst.async_stop(force=True)\n",
                "            await event_loop.shutdown_default_executor()\n",
                "\n",
                "\n",
                "@pytest.fixture(name=\"requests_mock\")\n",
                "def requests_mock_fixture() -> Generator[requests_mock.Mocker]:\n",
                "    \"\"\"Fixture to provide a requests mocker.\"\"\"\n",
                "    with requests_mock.mock() as m:\n",
                "        yield m\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def aioclient_mock() -> Generator[AiohttpClientMocker]:\n",
                "    \"\"\"Fixture to mock aioclient calls.\"\"\"\n",
                "    with mock_aiohttp_client() as mock_session:\n",
                "        yield mock_session\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mock_device_tracker_conf() -> Generator[list[Device]]:\n",
                "    \"\"\"Prevent device tracker from reading/writing data.\"\"\"\n",
                "    devices: list[Device] = []\n",
                "\n",
                "    async def mock_update_config(path: str, dev_id: str, entity: Device) -> None:\n",
                "        devices.append(entity)\n",
                "\n",
                "    with (\n",
                "        patch(\n",
                "            (\n",
                "                \"homeassistant.components.device_tracker.legacy\"\n",
                "                \".DeviceTracker.async_update_config\"\n",
                "            ),\n",
                "            side_effect=mock_update_config,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.device_tracker.legacy.async_load_config\",\n",
                "            side_effect=lambda *args: devices,\n",
                "        ),\n",
                "    ):\n",
                "        yield devices\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def hass_admin_credential(\n",
                "    hass: HomeAssistant, local_auth: homeassistant.HassAuthProvider\n",
                ") -> Credentials:\n",
                "    \"\"\"Provide credentials for admin user.\"\"\"\n",
                "    return Credentials(\n",
                "        id=\"mock-credential-id\",\n",
                "        auth_provider_type=\"homeassistant\",\n",
                "        auth_provider_id=None,\n",
                "        data={\"username\": \"admin\"},\n",
                "        is_new=False,\n",
                "    )\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def hass_access_token(\n",
                "    hass: HomeAssistant, hass_admin_user: MockUser, hass_admin_credential: Credentials\n",
                ") -> str:\n",
                "    \"\"\"Return an access token to access Home Assistant.\"\"\"\n",
                "    await hass.auth.async_link_user(hass_admin_user, hass_admin_credential)\n",
                "\n",
                "    refresh_token = await hass.auth.async_create_refresh_token(\n",
                "        hass_admin_user, CLIENT_ID, credential=hass_admin_credential\n",
                "    )\n",
                "    return hass.auth.async_create_access_token(refresh_token)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def hass_owner_user(\n",
                "    hass: HomeAssistant, local_auth: homeassistant.HassAuthProvider\n",
                ") -> MockUser:\n",
                "    \"\"\"Return a Home Assistant admin user.\"\"\"\n",
                "    return MockUser(is_owner=True).add_to_hass(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def hass_admin_user(\n",
                "    hass: HomeAssistant, local_auth: homeassistant.HassAuthProvider\n",
                ") -> MockUser:\n",
                "    \"\"\"Return a Home Assistant admin user.\"\"\"\n",
                "    admin_group = await hass.auth.async_get_group(GROUP_ID_ADMIN)\n",
                "    return MockUser(groups=[admin_group]).add_to_hass(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def hass_read_only_user(\n",
                "    hass: HomeAssistant, local_auth: homeassistant.HassAuthProvider\n",
                ") -> MockUser:\n",
                "    \"\"\"Return a Home Assistant read only user.\"\"\"\n",
                "    read_only_group = await hass.auth.async_get_group(GROUP_ID_READ_ONLY)\n",
                "    return MockUser(groups=[read_only_group]).add_to_hass(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def hass_read_only_access_token(\n",
                "    hass: HomeAssistant,\n",
                "    hass_read_only_user: MockUser,\n",
                "    local_auth: homeassistant.HassAuthProvider,\n",
                ") -> str:\n",
                "    \"\"\"Return a Home Assistant read only user.\"\"\"\n",
                "    credential = Credentials(\n",
                "        id=\"mock-readonly-credential-id\",\n",
                "        auth_provider_type=\"homeassistant\",\n",
                "        auth_provider_id=None,\n",
                "        data={\"username\": \"readonly\"},\n",
                "        is_new=False,\n",
                "    )\n",
                "    hass_read_only_user.credentials.append(credential)\n",
                "\n",
                "    refresh_token = await hass.auth.async_create_refresh_token(\n",
                "        hass_read_only_user, CLIENT_ID, credential=credential\n",
                "    )\n",
                "    return hass.auth.async_create_access_token(refresh_token)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def hass_supervisor_user(\n",
                "    hass: HomeAssistant, local_auth: homeassistant.HassAuthProvider\n",
                ") -> MockUser:\n",
                "    \"\"\"Return the Home Assistant Supervisor user.\"\"\"\n",
                "    admin_group = await hass.auth.async_get_group(GROUP_ID_ADMIN)\n",
                "    return MockUser(\n",
                "        name=HASSIO_USER_NAME, groups=[admin_group], system_generated=True\n",
                "    ).add_to_hass(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def hass_supervisor_access_token(\n",
                "    hass: HomeAssistant,\n",
                "    hass_supervisor_user: MockUser,\n",
                "    local_auth: homeassistant.HassAuthProvider,\n",
                ") -> str:\n",
                "    \"\"\"Return a Home Assistant Supervisor access token.\"\"\"\n",
                "    refresh_token = await hass.auth.async_create_refresh_token(hass_supervisor_user)\n",
                "    return hass.auth.async_create_access_token(refresh_token)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def local_auth(hass: HomeAssistant) -> homeassistant.HassAuthProvider:\n",
                "    \"\"\"Load local auth provider.\"\"\"\n",
                "    prv = homeassistant.HassAuthProvider(\n",
                "        hass, hass.auth._store, {\"type\": \"homeassistant\"}\n",
                "    )\n",
                "    await prv.async_initialize()\n",
                "    hass.auth._providers[(prv.type, prv.id)] = prv\n",
                "    return prv\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def hass_client(\n",
                "    hass: HomeAssistant,\n",
                "    aiohttp_client: ClientSessionGenerator,\n",
                "    hass_access_token: str,\n",
                "    socket_enabled: None,\n",
                ") -> ClientSessionGenerator:\n",
                "    \"\"\"Return an authenticated HTTP client.\"\"\"\n",
                "\n",
                "    async def auth_client(access_token: str | None = hass_access_token) -> TestClient:\n",
                "        \"\"\"Return an authenticated client.\"\"\"\n",
                "        return await aiohttp_client(\n",
                "            hass.http.app, headers={\"Authorization\": f\"Bearer {access_token}\"}\n",
                "        )\n",
                "\n",
                "    return auth_client\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def hass_client_no_auth(\n",
                "    hass: HomeAssistant,\n",
                "    aiohttp_client: ClientSessionGenerator,\n",
                "    socket_enabled: None,\n",
                ") -> ClientSessionGenerator:\n",
                "    \"\"\"Return an unauthenticated HTTP client.\"\"\"\n",
                "\n",
                "    async def client() -> TestClient:\n",
                "        \"\"\"Return an authenticated client.\"\"\"\n",
                "        return await aiohttp_client(hass.http.app)\n",
                "\n",
                "    return client\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def current_request() -> Generator[MagicMock]:\n",
                "    \"\"\"Mock current request.\"\"\"\n",
                "    with patch(\"homeassistant.components.http.current_request\") as mock_request_context:\n",
                "        mocked_request = make_mocked_request(\n",
                "            \"GET\",\n",
                "            \"/some/request\",\n",
                "            headers={\"Host\": \"example.com\"},\n",
                "            sslcontext=ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT),\n",
                "        )\n",
                "        mock_request_context.get.return_value = mocked_request\n",
                "        yield mock_request_context\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def current_request_with_host(current_request: MagicMock) -> None:\n",
                "    \"\"\"Mock current request with a host header.\"\"\"\n",
                "    new_headers = multidict.CIMultiDict(current_request.get.return_value.headers)\n",
                "    new_headers[config_entry_oauth2_flow.HEADER_FRONTEND_BASE] = \"https://example.com\"\n",
                "    current_request.get.return_value = current_request.get.return_value.clone(\n",
                "        headers=new_headers\n",
                "    )\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def hass_ws_client(\n",
                "    aiohttp_client: ClientSessionGenerator,\n",
                "    hass_access_token: str,\n",
                "    hass: HomeAssistant,\n",
                "    socket_enabled: None,\n",
                ") -> WebSocketGenerator:\n",
                "    \"\"\"Websocket client fixture connected to websocket server.\"\"\"\n",
                "\n",
                "    async def create_client(\n",
                "        hass: HomeAssistant = hass, access_token: str | None = hass_access_token\n",
                "    ) -> MockHAClientWebSocket:\n",
                "        \"\"\"Create a websocket client.\"\"\"\n",
                "        assert await async_setup_component(hass, \"websocket_api\", {})\n",
                "        client = await aiohttp_client(hass.http.app)\n",
                "        websocket = await client.ws_connect(URL)\n",
                "        auth_resp = await websocket.receive_json()\n",
                "        assert auth_resp[\"type\"] == TYPE_AUTH_REQUIRED\n",
                "\n",
                "        if access_token is None:\n",
                "            await websocket.send_json({\"type\": TYPE_AUTH, \"access_token\": \"incorrect\"})\n",
                "        else:\n",
                "            await websocket.send_json({\"type\": TYPE_AUTH, \"access_token\": access_token})\n",
                "\n",
                "        auth_ok = await websocket.receive_json()\n",
                "        assert auth_ok[\"type\"] == TYPE_AUTH_OK\n",
                "\n",
                "        def _get_next_id() -> Generator[int]:\n",
                "            i = 0\n",
                "            while True:\n",
                "                yield (i := i + 1)\n",
                "\n",
                "        id_generator = _get_next_id()\n",
                "\n",
                "        def _send_json_auto_id(data: dict[str, Any]) -> Coroutine[Any, Any, None]:\n",
                "            data[\"id\"] = next(id_generator)\n",
                "            return websocket.send_json(data)\n",
                "\n",
                "        async def _remove_device(device_id: str, config_entry_id: str) -> Any:\n",
                "            await _send_json_auto_id(\n",
                "                {\n",
                "                    \"type\": \"config/device_registry/remove_config_entry\",\n",
                "                    \"config_entry_id\": config_entry_id,\n",
                "                    \"device_id\": device_id,\n",
                "                }\n",
                "            )\n",
                "            return await websocket.receive_json()\n",
                "\n",
                "        # wrap in client\n",
                "        wrapped_websocket = cast(MockHAClientWebSocket, websocket)\n",
                "        wrapped_websocket.client = client\n",
                "        wrapped_websocket.send_json_auto_id = _send_json_auto_id\n",
                "        wrapped_websocket.remove_device = _remove_device\n",
                "        return wrapped_websocket\n",
                "\n",
                "    return create_client\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True)\n",
                "def fail_on_log_exception(\n",
                "    request: pytest.FixtureRequest, monkeypatch: pytest.MonkeyPatch\n",
                ") -> None:\n",
                "    \"\"\"Fixture to fail if a callback wrapped by catch_log_exception or coroutine wrapped by async_create_catching_coro throws.\"\"\"\n",
                "    if \"no_fail_on_log_exception\" in request.keywords:\n",
                "        return\n",
                "\n",
                "    def log_exception(format_err, *args):\n",
                "        raise  # noqa: PLE0704\n",
                "\n",
                "    monkeypatch.setattr(\"homeassistant.util.logging.log_exception\", log_exception)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mqtt_config_entry_data() -> dict[str, Any] | None:\n",
                "    \"\"\"Fixture to allow overriding MQTT config.\"\"\"\n",
                "    return None\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mqtt_client_mock(hass: HomeAssistant) -> Generator[MqttMockPahoClient]:\n",
                "    \"\"\"Fixture to mock MQTT client.\"\"\"\n",
                "\n",
                "    mid: int = 0\n",
                "\n",
                "    def get_mid() -> int:\n",
                "        nonlocal mid\n",
                "        mid += 1\n",
                "        return mid\n",
                "\n",
                "    class FakeInfo:\n",
                "        \"\"\"Class to fake MQTT info.\"\"\"\n",
                "\n",
                "        def __init__(self, mid: int) -> None:\n",
                "            self.mid = mid\n",
                "            self.rc = 0\n",
                "\n",
                "    with patch(\n",
                "        \"homeassistant.components.mqtt.async_client.AsyncMQTTClient\"\n",
                "    ) as mock_client:\n",
                "        # The below use a call_soon for the on_publish/on_subscribe/on_unsubscribe\n",
                "        # callbacks to simulate the behavior of the real MQTT client which will\n",
                "        # not be synchronous.\n",
                "\n",
                "        @ha.callback\n",
                "        def _async_fire_mqtt_message(topic, payload, qos, retain):\n",
                "            async_fire_mqtt_message(hass, topic, payload or b\"\", qos, retain)\n",
                "            mid = get_mid()\n",
                "            hass.loop.call_soon(mock_client.on_publish, 0, 0, mid)\n",
                "            return FakeInfo(mid)\n",
                "\n",
                "        def _subscribe(topic, qos=0):\n",
                "            mid = get_mid()\n",
                "            hass.loop.call_soon(mock_client.on_subscribe, 0, 0, mid)\n",
                "            return (0, mid)\n",
                "\n",
                "        def _unsubscribe(topic):\n",
                "            mid = get_mid()\n",
                "            hass.loop.call_soon(mock_client.on_unsubscribe, 0, 0, mid)\n",
                "            return (0, mid)\n",
                "\n",
                "        def _connect(*args, **kwargs):\n",
                "            # Connect always calls reconnect once, but we\n",
                "            # mock it out so we call reconnect to simulate\n",
                "            # the behavior.\n",
                "            mock_client.reconnect()\n",
                "            hass.loop.call_soon_threadsafe(\n",
                "                mock_client.on_connect, mock_client, None, 0, 0, 0\n",
                "            )\n",
                "            mock_client.on_socket_open(\n",
                "                mock_client, None, Mock(fileno=Mock(return_value=-1))\n",
                "            )\n",
                "            mock_client.on_socket_register_write(\n",
                "                mock_client, None, Mock(fileno=Mock(return_value=-1))\n",
                "            )\n",
                "            return 0\n",
                "\n",
                "        mock_client = mock_client.return_value\n",
                "        mock_client.connect.side_effect = _connect\n",
                "        mock_client.subscribe.side_effect = _subscribe\n",
                "        mock_client.unsubscribe.side_effect = _unsubscribe\n",
                "        mock_client.publish.side_effect = _async_fire_mqtt_message\n",
                "        mock_client.loop_read.return_value = 0\n",
                "        yield mock_client\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def mqtt_mock(\n",
                "    hass: HomeAssistant,\n",
                "    mock_hass_config: None,\n",
                "    mqtt_client_mock: MqttMockPahoClient,\n",
                "    mqtt_config_entry_data: dict[str, Any] | None,\n",
                "    mqtt_mock_entry: MqttMockHAClientGenerator,\n",
                ") -> AsyncGenerator[MqttMockHAClient]:\n",
                "    \"\"\"Fixture to mock MQTT component.\"\"\"\n",
                "    return await mqtt_mock_entry()\n",
                "\n",
                "\n",
                "@asynccontextmanager\n",
                "async def _mqtt_mock_entry(\n",
                "    hass: HomeAssistant,\n",
                "    mqtt_client_mock: MqttMockPahoClient,\n",
                "    mqtt_config_entry_data: dict[str, Any] | None,\n",
                ") -> AsyncGenerator[MqttMockHAClientGenerator]:\n",
                "    \"\"\"Fixture to mock a delayed setup of the MQTT config entry.\"\"\"\n",
                "    # Local import to avoid processing MQTT modules when running a testcase\n",
                "    # which does not use MQTT.\n",
                "    from homeassistant.components import mqtt  # pylint: disable=import-outside-toplevel\n",
                "\n",
                "    if mqtt_config_entry_data is None:\n",
                "        mqtt_config_entry_data = {\n",
                "            mqtt.CONF_BROKER: \"mock-broker\",\n",
                "            mqtt.CONF_BIRTH_MESSAGE: {},\n",
                "        }\n",
                "\n",
                "    await hass.async_block_till_done()\n",
                "\n",
                "    entry = MockConfigEntry(\n",
                "        data=mqtt_config_entry_data,\n",
                "        domain=mqtt.DOMAIN,\n",
                "        title=\"MQTT\",\n",
                "    )\n",
                "    entry.add_to_hass(hass)\n",
                "\n",
                "    real_mqtt = mqtt.MQTT\n",
                "    real_mqtt_instance = None\n",
                "    mock_mqtt_instance = None\n",
                "\n",
                "    async def _setup_mqtt_entry(\n",
                "        setup_entry: Callable[[HomeAssistant, ConfigEntry], Coroutine[Any, Any, bool]],\n",
                "    ) -> MagicMock:\n",
                "        \"\"\"Set up the MQTT config entry.\"\"\"\n",
                "        assert await setup_entry(hass, entry)\n",
                "\n",
                "        # Assert that MQTT is setup\n",
                "        assert real_mqtt_instance is not None, \"MQTT was not setup correctly\"\n",
                "        mock_mqtt_instance.conf = real_mqtt_instance.conf  # For diagnostics\n",
                "        mock_mqtt_instance._mqttc = mqtt_client_mock\n",
                "\n",
                "        # connected set to True to get a more realistic behavior when subscribing\n",
                "        mock_mqtt_instance.connected = True\n",
                "        mqtt_client_mock.on_connect(mqtt_client_mock, None, 0, 0, 0)\n",
                "\n",
                "        async_dispatcher_send(hass, mqtt.MQTT_CONNECTION_STATE, True)\n",
                "        await hass.async_block_till_done()\n",
                "\n",
                "        return mock_mqtt_instance\n",
                "\n",
                "    def create_mock_mqtt(*args, **kwargs) -> MqttMockHAClient:\n",
                "        \"\"\"Create a mock based on mqtt.MQTT.\"\"\"\n",
                "        nonlocal mock_mqtt_instance\n",
                "        nonlocal real_mqtt_instance\n",
                "        real_mqtt_instance = real_mqtt(*args, **kwargs)\n",
                "        spec = [*dir(real_mqtt_instance), \"_mqttc\"]\n",
                "        mock_mqtt_instance = MagicMock(\n",
                "            return_value=real_mqtt_instance,\n",
                "            spec_set=spec,\n",
                "            wraps=real_mqtt_instance,\n",
                "        )\n",
                "        return mock_mqtt_instance\n",
                "\n",
                "    with patch(\"homeassistant.components.mqtt.MQTT\", side_effect=create_mock_mqtt):\n",
                "        yield _setup_mqtt_entry\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def hass_config() -> ConfigType:\n",
                "    \"\"\"Fixture to parametrize the content of main configuration using mock_hass_config.\n",
                "\n",
                "    To set a configuration, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"hass_config\", [{integration: {...}}])\n",
                "    Add the `mock_hass_config: None` fixture to the test.\n",
                "    \"\"\"\n",
                "    return {}\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mock_hass_config(hass: HomeAssistant, hass_config: ConfigType) -> Generator[None]:\n",
                "    \"\"\"Fixture to mock the content of main configuration.\n",
                "\n",
                "    Patches homeassistant.config.load_yaml_config_file and hass.config_entries\n",
                "    with `hass_config` as parameterized.\n",
                "    \"\"\"\n",
                "    if hass_config:\n",
                "        hass.config_entries = ConfigEntries(hass, hass_config)\n",
                "    with patch(\"homeassistant.config.load_yaml_config_file\", return_value=hass_config):\n",
                "        yield\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def hass_config_yaml() -> str:\n",
                "    \"\"\"Fixture to parametrize the content of configuration.yaml file.\n",
                "\n",
                "    To set yaml content, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"hass_config_yaml\", [\"...\"])\n",
                "    Add the `mock_hass_config_yaml: None` fixture to the test.\n",
                "    \"\"\"\n",
                "    return \"\"\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def hass_config_yaml_files(hass_config_yaml: str) -> dict[str, str]:\n",
                "    \"\"\"Fixture to parametrize multiple yaml configuration files.\n",
                "\n",
                "    To set the YAML files to patch, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\n",
                "        \"hass_config_yaml_files\", [{\"configuration.yaml\": \"...\"}]\n",
                "    )\n",
                "    Add the `mock_hass_config_yaml: None` fixture to the test.\n",
                "    \"\"\"\n",
                "    return {YAML_CONFIG_FILE: hass_config_yaml}\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mock_hass_config_yaml(\n",
                "    hass: HomeAssistant, hass_config_yaml_files: dict[str, str]\n",
                ") -> Generator[None]:\n",
                "    \"\"\"Fixture to mock the content of the yaml configuration files.\n",
                "\n",
                "    Patches yaml configuration files using the `hass_config_yaml`\n",
                "    and `hass_config_yaml_files` fixtures.\n",
                "    \"\"\"\n",
                "    with patch_yaml_files(hass_config_yaml_files):\n",
                "        yield\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def mqtt_mock_entry(\n",
                "    hass: HomeAssistant,\n",
                "    mqtt_client_mock: MqttMockPahoClient,\n",
                "    mqtt_config_entry_data: dict[str, Any] | None,\n",
                ") -> AsyncGenerator[MqttMockHAClientGenerator]:\n",
                "    \"\"\"Set up an MQTT config entry.\"\"\"\n",
                "\n",
                "    async def _async_setup_config_entry(\n",
                "        hass: HomeAssistant, entry: ConfigEntry\n",
                "    ) -> bool:\n",
                "        \"\"\"Help set up the config entry.\"\"\"\n",
                "        assert await hass.config_entries.async_setup(entry.entry_id)\n",
                "        await hass.async_block_till_done()\n",
                "        return True\n",
                "\n",
                "    async def _setup_mqtt_entry() -> MqttMockHAClient:\n",
                "        \"\"\"Set up the MQTT config entry.\"\"\"\n",
                "        return await mqtt_mock_entry(_async_setup_config_entry)\n",
                "\n",
                "    async with _mqtt_mock_entry(\n",
                "        hass, mqtt_client_mock, mqtt_config_entry_data\n",
                "    ) as mqtt_mock_entry:\n",
                "        yield _setup_mqtt_entry\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True, scope=\"session\")\n",
                "def mock_network() -> Generator[None]:\n",
                "    \"\"\"Mock network.\"\"\"\n",
                "    with patch(\n",
                "        \"homeassistant.components.network.util.ifaddr.get_adapters\",\n",
                "        return_value=[\n",
                "            Mock(\n",
                "                nice_name=\"eth0\",\n",
                "                ips=[Mock(is_IPv6=False, ip=\"10.10.10.10\", network_prefix=24)],\n",
                "                index=0,\n",
                "            )\n",
                "        ],\n",
                "    ):\n",
                "        yield\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True, scope=\"session\")\n",
                "def mock_get_source_ip() -> Generator[_patch]:\n",
                "    \"\"\"Mock network util's async_get_source_ip.\"\"\"\n",
                "    patcher = patch(\n",
                "        \"homeassistant.components.network.util.async_get_source_ip\",\n",
                "        return_value=\"10.10.10.10\",\n",
                "    )\n",
                "    patcher.start()\n",
                "    try:\n",
                "        yield patcher\n",
                "    finally:\n",
                "        patcher.stop()\n",
                "\n",
                "\n",
                "@pytest.fixture(autouse=True, scope=\"session\")\n",
                "def translations_once() -> Generator[_patch]:\n",
                "    \"\"\"Only load translations once per session.\"\"\"\n",
                "    cache = _TranslationsCacheData({}, {})\n",
                "    patcher = patch(\n",
                "        \"homeassistant.helpers.translation._TranslationsCacheData\",\n",
                "        return_value=cache,\n",
                "    )\n",
                "    patcher.start()\n",
                "    try:\n",
                "        yield patcher\n",
                "    finally:\n",
                "        patcher.stop()\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def disable_translations_once(\n",
                "    translations_once: _patch,\n",
                ") -> Generator[None]:\n",
                "    \"\"\"Override loading translations once.\"\"\"\n",
                "    translations_once.stop()\n",
                "    yield\n",
                "    translations_once.start()\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mock_zeroconf() -> Generator[MagicMock]:\n",
                "    \"\"\"Mock zeroconf.\"\"\"\n",
                "    from zeroconf import DNSCache  # pylint: disable=import-outside-toplevel\n",
                "\n",
                "    with (\n",
                "        patch(\"homeassistant.components.zeroconf.HaZeroconf\", autospec=True) as mock_zc,\n",
                "        patch(\"homeassistant.components.zeroconf.AsyncServiceBrowser\", autospec=True),\n",
                "    ):\n",
                "        zc = mock_zc.return_value\n",
                "        # DNSCache has strong Cython type checks, and MagicMock does not work\n",
                "        # so we must mock the class directly\n",
                "        zc.cache = DNSCache()\n",
                "        yield mock_zc\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mock_async_zeroconf(mock_zeroconf: MagicMock) -> Generator[MagicMock]:\n",
                "    \"\"\"Mock AsyncZeroconf.\"\"\"\n",
                "    from zeroconf import DNSCache, Zeroconf  # pylint: disable=import-outside-toplevel\n",
                "    from zeroconf.asyncio import (  # pylint: disable=import-outside-toplevel\n",
                "        AsyncZeroconf,\n",
                "    )\n",
                "\n",
                "    with patch(\n",
                "        \"homeassistant.components.zeroconf.HaAsyncZeroconf\", spec=AsyncZeroconf\n",
                "    ) as mock_aiozc:\n",
                "        zc = mock_aiozc.return_value\n",
                "        zc.async_unregister_service = AsyncMock()\n",
                "        zc.async_register_service = AsyncMock()\n",
                "        zc.async_update_service = AsyncMock()\n",
                "        zc.zeroconf = Mock(spec=Zeroconf)\n",
                "        zc.zeroconf.async_wait_for_start = AsyncMock()\n",
                "        # DNSCache has strong Cython type checks, and MagicMock does not work\n",
                "        # so we must mock the class directly\n",
                "        zc.zeroconf.cache = DNSCache()\n",
                "        zc.zeroconf.done = False\n",
                "        zc.async_close = AsyncMock()\n",
                "        zc.ha_async_close = AsyncMock()\n",
                "        yield zc\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def enable_custom_integrations(hass: HomeAssistant) -> None:\n",
                "    \"\"\"Enable custom integrations defined in the test dir.\"\"\"\n",
                "    hass.data.pop(loader.DATA_CUSTOM_COMPONENTS)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def enable_statistics() -> bool:\n",
                "    \"\"\"Fixture to control enabling of recorder's statistics compilation.\n",
                "\n",
                "    To enable statistics, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"enable_statistics\", [True])\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def enable_schema_validation() -> bool:\n",
                "    \"\"\"Fixture to control enabling of recorder's statistics table validation.\n",
                "\n",
                "    To enable statistics table validation, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"enable_schema_validation\", [True])\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def enable_nightly_purge() -> bool:\n",
                "    \"\"\"Fixture to control enabling of recorder's nightly purge job.\n",
                "\n",
                "    To enable nightly purging, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"enable_nightly_purge\", [True])\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def enable_migrate_context_ids() -> bool:\n",
                "    \"\"\"Fixture to control enabling of recorder's context id migration.\n",
                "\n",
                "    To enable context id migration, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"enable_migrate_context_ids\", [True])\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def enable_migrate_event_type_ids() -> bool:\n",
                "    \"\"\"Fixture to control enabling of recorder's event type id migration.\n",
                "\n",
                "    To enable context id migration, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"enable_migrate_event_type_ids\", [True])\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def enable_migrate_entity_ids() -> bool:\n",
                "    \"\"\"Fixture to control enabling of recorder's entity_id migration.\n",
                "\n",
                "    To enable context id migration, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"enable_migrate_entity_ids\", [True])\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def enable_migrate_event_ids() -> bool:\n",
                "    \"\"\"Fixture to control enabling of recorder's event id migration.\n",
                "\n",
                "    To enable context id migration, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"enable_migrate_event_ids\", [True])\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def recorder_config() -> dict[str, Any] | None:\n",
                "    \"\"\"Fixture to override recorder config.\n",
                "\n",
                "    To override the config, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"recorder_config\", [{...}])\n",
                "    \"\"\"\n",
                "    return None\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def persistent_database() -> bool:\n",
                "    \"\"\"Fixture to control if database should persist when recorder is shut down in test.\n",
                "\n",
                "    When using sqlite, this uses on disk database instead of in memory database.\n",
                "    This does nothing when using mysql or postgresql.\n",
                "\n",
                "    Note that the database is always destroyed in between tests.\n",
                "\n",
                "    To use a persistent database, tests can be marked with:\n",
                "    @pytest.mark.parametrize(\"persistent_database\", [True])\n",
                "    \"\"\"\n",
                "    return False\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def recorder_db_url(\n",
                "    pytestconfig: pytest.Config,\n",
                "    hass_fixture_setup: list[bool],\n",
                "    persistent_database: str,\n",
                "    tmp_path_factory: pytest.TempPathFactory,\n",
                ") -> Generator[str]:\n",
                "    \"\"\"Prepare a default database for tests and return a connection URL.\"\"\"\n",
                "    assert not hass_fixture_setup\n",
                "\n",
                "    db_url = cast(str, pytestconfig.getoption(\"dburl\"))\n",
                "    if db_url == \"sqlite://\" and persistent_database:\n",
                "        tmp_path = tmp_path_factory.mktemp(\"recorder\")\n",
                "        db_url = \"sqlite:///\" + str(tmp_path / \"pytest.db\")\n",
                "    elif db_url.startswith(\"mysql://\"):\n",
                "        # pylint: disable-next=import-outside-toplevel\n",
                "        import sqlalchemy_utils\n",
                "\n",
                "        charset = \"utf8mb4' COLLATE = 'utf8mb4_unicode_ci\"\n",
                "        assert not sqlalchemy_utils.database_exists(db_url)\n",
                "        sqlalchemy_utils.create_database(db_url, encoding=charset)\n",
                "    elif db_url.startswith(\"postgresql://\"):\n",
                "        # pylint: disable-next=import-outside-toplevel\n",
                "        import sqlalchemy_utils\n",
                "\n",
                "        assert not sqlalchemy_utils.database_exists(db_url)\n",
                "        sqlalchemy_utils.create_database(db_url, encoding=\"utf8\")\n",
                "    yield db_url\n",
                "    if db_url == \"sqlite://\" and persistent_database:\n",
                "        rmtree(tmp_path, ignore_errors=True)\n",
                "    elif db_url.startswith(\"mysql://\"):\n",
                "        # pylint: disable-next=import-outside-toplevel\n",
                "        import sqlalchemy as sa\n",
                "\n",
                "        made_url = sa.make_url(db_url)\n",
                "        db = made_url.database\n",
                "        engine = sa.create_engine(db_url)\n",
                "        # Check for any open connections to the database before dropping it\n",
                "        # to ensure that InnoDB does not deadlock.\n",
                "        with engine.begin() as connection:\n",
                "            query = sa.text(\n",
                "                \"select id FROM information_schema.processlist WHERE db=:db and id != CONNECTION_ID()\"\n",
                "            )\n",
                "            rows = connection.execute(query, parameters={\"db\": db}).fetchall()\n",
                "            if rows:\n",
                "                raise RuntimeError(\n",
                "                    f\"Unable to drop database {db} because it is in use by {rows}\"\n",
                "                )\n",
                "        engine.dispose()\n",
                "        sqlalchemy_utils.drop_database(db_url)\n",
                "    elif db_url.startswith(\"postgresql://\"):\n",
                "        sqlalchemy_utils.drop_database(db_url)\n",
                "\n",
                "\n",
                "async def _async_init_recorder_component(\n",
                "    hass: HomeAssistant,\n",
                "    add_config: dict[str, Any] | None = None,\n",
                "    db_url: str | None = None,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "    *,\n",
                    "    expected_setup_result: bool,\n"
                ],
                "parent_version_range": {
                    "start": 1396,
                    "end": 1396
                },
                "child_version_range": {
                    "start": 1396,
                    "end": 1398
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "_async_init_recorder_component",
                        "signature": "def _async_init_recorder_component(\n    hass: HomeAssistant,\n    add_config: dict[str, Any] | None = None,\n    db_url: str | None = None,\n)->None:",
                        "at_line": 1392
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: tests/conftest.py\nCode:\n             def _async_init_recorder_component(\n    hass: HomeAssistant,\n    add_config: dict[str, Any] | None = None,\n    db_url: str | None = None,\n)->None:\n                 ...\n1393 1393        hass: HomeAssistant,\n1394 1394        add_config: dict[str, Any] | None = None,\n1395 1395        db_url: str | None = None,\n     1396  +     *,\n     1397  +     expected_setup_result: bool,\n1396 1398    ) -> None:\n1397 1399        \"\"\"Initialize the recorder asynchronously.\"\"\"\n1398 1400        # pylint: disable-next=import-outside-toplevel\n           ...\n",
                "file_path": "tests/conftest.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "bool",
                    "expected_setup_result"
                ],
                "prefix": [
                    "    hass: HomeAssistant,\n",
                    "    add_config: dict[str, Any] | None = None,\n",
                    "    db_url: str | None = None,\n"
                ],
                "suffix": [
                    ") -> None:\n",
                    "    \"\"\"Initialize the recorder asynchronously.\"\"\"\n",
                    "    # pylint: disable-next=import-outside-toplevel\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1397,
                                    "column": 4
                                },
                                "end": {
                                    "line": 1397,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1397,
                                    "column": 4
                                },
                                "end": {
                                    "line": 1397,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 9,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1397,
                                    "column": 4
                                },
                                "end": {
                                    "line": 1397,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                ") -> None:\n",
                "    \"\"\"Initialize the recorder asynchronously.\"\"\"\n",
                "    # pylint: disable-next=import-outside-toplevel\n",
                "    from homeassistant.components import recorder\n",
                "\n",
                "    config = dict(add_config) if add_config else {}\n",
                "    if recorder.CONF_DB_URL not in config:\n",
                "        config[recorder.CONF_DB_URL] = db_url\n",
                "        if recorder.CONF_COMMIT_INTERVAL not in config:\n",
                "            config[recorder.CONF_COMMIT_INTERVAL] = 0\n",
                "\n",
                "    with patch(\"homeassistant.components.recorder.ALLOW_IN_MEMORY_DB\", True):\n",
                "        if recorder.DOMAIN not in hass.data:\n",
                "            recorder_helper.async_initialize_recorder(hass)\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        assert await async_setup_component(\n",
                    "            hass, recorder.DOMAIN, {recorder.DOMAIN: config}\n"
                ],
                "after": [
                    "        setup_task = asyncio.ensure_future(\n",
                    "            async_setup_component(hass, recorder.DOMAIN, {recorder.DOMAIN: config})\n"
                ],
                "parent_version_range": {
                    "start": 1410,
                    "end": 1412
                },
                "child_version_range": {
                    "start": 1412,
                    "end": 1414
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "with patch(\"homeassistant.components.recorder.ALLOW_IN_MEMORY_DB\", True):",
                        "start_line": 1407,
                        "end_line": 1413
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "_async_init_recorder_component",
                        "signature": "def _async_init_recorder_component(\n    hass: HomeAssistant,\n    add_config: dict[str, Any] | None = None,\n    db_url: str | None = None,\n)->None:",
                        "at_line": 1392
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: tests/conftest.py\nCode:\n             def _async_init_recorder_component(\n    hass: HomeAssistant,\n    add_config: dict[str, Any] | None = None,\n    db_url: str | None = None,\n)->None:\n                 ...\n1407 1409        with patch(\"homeassistant.components.recorder.ALLOW_IN_MEMORY_DB\", True):\n1408 1410            if recorder.DOMAIN not in hass.data:\n1409 1411                recorder_helper.async_initialize_recorder(hass)\n1410       -         assert await async_setup_component(\n1411       -             hass, recorder.DOMAIN, {recorder.DOMAIN: config}\n     1412  +         setup_task = asyncio.ensure_future(\n     1413  +             async_setup_component(hass, recorder.DOMAIN, {recorder.DOMAIN: config})\n1412 1414            )\n           ...\n",
                "file_path": "tests/conftest.py",
                "identifiers_before": [
                    "DOMAIN",
                    "async_setup_component",
                    "config",
                    "hass",
                    "recorder"
                ],
                "identifiers_after": [
                    "DOMAIN",
                    "async_setup_component",
                    "asyncio",
                    "config",
                    "ensure_future",
                    "hass",
                    "recorder",
                    "setup_task"
                ],
                "prefix": [
                    "    with patch(\"homeassistant.components.recorder.ALLOW_IN_MEMORY_DB\", True):\n",
                    "        if recorder.DOMAIN not in hass.data:\n",
                    "            recorder_helper.async_initialize_recorder(hass)\n"
                ],
                "suffix": [
                    "        )\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "setup_task",
                            "position": {
                                "start": {
                                    "line": 1412,
                                    "column": 8
                                },
                                "end": {
                                    "line": 1412,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "        )\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        assert recorder.DOMAIN in hass.config.components\n"
                ],
                "after": [
                    "        # Wait for recorder integration to setup\n",
                    "        setup_result = await setup_task\n",
                    "        assert setup_result == expected_setup_result\n",
                    "        assert (recorder.DOMAIN in hass.config.components) == expected_setup_result\n"
                ],
                "parent_version_range": {
                    "start": 1413,
                    "end": 1414
                },
                "child_version_range": {
                    "start": 1415,
                    "end": 1419
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "with patch(\"homeassistant.components.recorder.ALLOW_IN_MEMORY_DB\", True):",
                        "start_line": 1407,
                        "end_line": 1413
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "_async_init_recorder_component",
                        "signature": "def _async_init_recorder_component(\n    hass: HomeAssistant,\n    add_config: dict[str, Any] | None = None,\n    db_url: str | None = None,\n)->None:",
                        "at_line": 1392
                    }
                ],
                "idx": 7,
                "hunk_diff": "File: tests/conftest.py\nCode:\n             def _async_init_recorder_component(\n    hass: HomeAssistant,\n    add_config: dict[str, Any] | None = None,\n    db_url: str | None = None,\n)->None:\n                 ...\n1412 1414            )\n1413       -         assert recorder.DOMAIN in hass.config.components\n     1415  +         # Wait for recorder integration to setup\n     1416  +         setup_result = await setup_task\n     1417  +         assert setup_result == expected_setup_result\n     1418  +         assert (recorder.DOMAIN in hass.config.components) == expected_setup_result\n1414 1419        _LOGGER.info(\n1415 1420            \"Test recorder successfully started, database location: %s\",\n1416 1421            config[recorder.CONF_DB_URL],\n           ...\n",
                "file_path": "tests/conftest.py",
                "identifiers_before": [
                    "DOMAIN",
                    "components",
                    "config",
                    "hass",
                    "recorder"
                ],
                "identifiers_after": [
                    "DOMAIN",
                    "components",
                    "config",
                    "expected_setup_result",
                    "hass",
                    "recorder",
                    "setup_result",
                    "setup_task"
                ],
                "prefix": [
                    "        )\n"
                ],
                "suffix": [
                    "    _LOGGER.info(\n",
                    "        \"Test recorder successfully started, database location: %s\",\n",
                    "        config[recorder.CONF_DB_URL],\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1417,
                                    "column": 31
                                },
                                "end": {
                                    "line": 1417,
                                    "column": 52
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1418,
                                    "column": 62
                                },
                                "end": {
                                    "line": 1418,
                                    "column": 83
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "setup_task",
                            "position": {
                                "start": {
                                    "line": 1416,
                                    "column": 29
                                },
                                "end": {
                                    "line": 1416,
                                    "column": 39
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    _LOGGER.info(\n",
                "        \"Test recorder successfully started, database location: %s\",\n",
                "        config[recorder.CONF_DB_URL],\n",
                "    )\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def async_test_recorder(\n",
                "    recorder_db_url: str,\n",
                "    enable_nightly_purge: bool,\n",
                "    enable_statistics: bool,\n",
                "    enable_schema_validation: bool,\n",
                "    enable_migrate_context_ids: bool,\n",
                "    enable_migrate_event_type_ids: bool,\n",
                "    enable_migrate_entity_ids: bool,\n",
                "    enable_migrate_event_ids: bool,\n",
                ") -> AsyncGenerator[RecorderInstanceGenerator]:\n",
                "    \"\"\"Yield context manager to setup recorder instance.\"\"\"\n",
                "    # pylint: disable-next=import-outside-toplevel\n",
                "    from homeassistant.components import recorder\n",
                "\n",
                "    # pylint: disable-next=import-outside-toplevel\n",
                "    from homeassistant.components.recorder import migration\n",
                "\n",
                "    # pylint: disable-next=import-outside-toplevel\n",
                "    from .components.recorder.common import async_recorder_block_till_done\n",
                "\n",
                "    nightly = recorder.Recorder.async_nightly_tasks if enable_nightly_purge else None\n",
                "    stats = recorder.Recorder.async_periodic_statistics if enable_statistics else None\n",
                "    schema_validate = (\n",
                "        migration._find_schema_errors\n",
                "        if enable_schema_validation\n",
                "        else itertools.repeat(set())\n",
                "    )\n",
                "    compile_missing = (\n",
                "        recorder.Recorder._schedule_compile_missing_statistics\n",
                "        if enable_statistics\n",
                "        else None\n",
                "    )\n",
                "    migrate_states_context_ids = (\n",
                "        migration.StatesContextIDMigration.migrate_data\n",
                "        if enable_migrate_context_ids\n",
                "        else None\n",
                "    )\n",
                "    migrate_events_context_ids = (\n",
                "        migration.EventsContextIDMigration.migrate_data\n",
                "        if enable_migrate_context_ids\n",
                "        else None\n",
                "    )\n",
                "    migrate_event_type_ids = (\n",
                "        migration.EventTypeIDMigration.migrate_data\n",
                "        if enable_migrate_event_type_ids\n",
                "        else None\n",
                "    )\n",
                "    migrate_entity_ids = (\n",
                "        migration.EntityIDMigration.migrate_data if enable_migrate_entity_ids else None\n",
                "    )\n",
                "    legacy_event_id_foreign_key_exists = (\n",
                "        recorder.Recorder._legacy_event_id_foreign_key_exists\n",
                "        if enable_migrate_event_ids\n",
                "        else None\n",
                "    )\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.Recorder.async_nightly_tasks\",\n",
                "            side_effect=nightly,\n",
                "            autospec=True,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.Recorder.async_periodic_statistics\",\n",
                "            side_effect=stats,\n",
                "            autospec=True,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration._find_schema_errors\",\n",
                "            side_effect=schema_validate,\n",
                "            autospec=True,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration.EventsContextIDMigration.migrate_data\",\n",
                "            side_effect=migrate_events_context_ids,\n",
                "            autospec=True,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration.StatesContextIDMigration.migrate_data\",\n",
                "            side_effect=migrate_states_context_ids,\n",
                "            autospec=True,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration.EventTypeIDMigration.migrate_data\",\n",
                "            side_effect=migrate_event_type_ids,\n",
                "            autospec=True,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.migration.EntityIDMigration.migrate_data\",\n",
                "            side_effect=migrate_entity_ids,\n",
                "            autospec=True,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.Recorder._legacy_event_id_foreign_key_exists\",\n",
                "            side_effect=legacy_event_id_foreign_key_exists,\n",
                "            autospec=True,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.components.recorder.Recorder._schedule_compile_missing_statistics\",\n",
                "            side_effect=compile_missing,\n",
                "            autospec=True,\n",
                "        ),\n",
                "    ):\n",
                "\n",
                "        @asynccontextmanager\n",
                "        async def async_test_recorder(\n",
                "            hass: HomeAssistant,\n",
                "            config: ConfigType | None = None,\n",
                "            *,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "            expected_setup_result: bool = True,\n"
                ],
                "parent_version_range": {
                    "start": 1529,
                    "end": 1529
                },
                "child_version_range": {
                    "start": 1534,
                    "end": 1535
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "with (\n        patch(\n            \"homeassistant.components.recorder.Recorder.async_nightly_tasks\",\n            side_effect=nightly,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.Recorder.async_periodic_statistics\",\n            side_effect=stats,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration._find_schema_errors\",\n            side_effect=schema_validate,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.EventsContextIDMigration.migrate_data\",\n            side_effect=migrate_events_context_ids,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.StatesContextIDMigration.migrate_data\",\n            side_effect=migrate_states_context_ids,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.EventTypeIDMigration.migrate_data\",\n            side_effect=migrate_event_type_ids,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.EntityIDMigration.migrate_data\",\n            side_effect=migrate_entity_ids,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.Recorder._legacy_event_id_foreign_key_exists\",\n            side_effect=legacy_event_id_foreign_key_exists,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.Recorder._schedule_compile_missing_statistics\",\n            side_effect=compile_missing,\n            autospec=True,\n        ),\n    ):",
                        "start_line": 1476,
                        "end_line": 1544
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "async_test_recorder",
                        "signature": "def async_test_recorder(\n    recorder_db_url: str,\n    enable_nightly_purge: bool,\n    enable_statistics: bool,\n    enable_schema_validation: bool,\n    enable_migrate_context_ids: bool,\n    enable_migrate_event_type_ids: bool,\n    enable_migrate_entity_ids: bool,\n    enable_migrate_event_ids: bool,\n)->AsyncGenerator[RecorderInstanceGenerator]:",
                        "at_line": 1421
                    },
                    {
                        "type": "function",
                        "name": "async_test_recorder",
                        "signature": "def async_test_recorder(\n            hass: HomeAssistant,\n            config: ConfigType | None = None,\n            *,\n            wait_recorder: bool = True,\n        )->AsyncGenerator[recorder.Recorder]:",
                        "at_line": 1525
                    }
                ],
                "idx": 8,
                "hunk_diff": "File: tests/conftest.py\nCode:\n             def async_test_recorder(\n    recorder_db_url: str,\n    enable_nightly_purge: bool,\n    enable_statistics: bool,\n    enable_schema_validation: bool,\n    enable_migrate_context_ids: bool,\n    enable_migrate_event_type_ids: bool,\n    enable_migrate_entity_ids: bool,\n    enable_migrate_event_ids: bool,\n)->AsyncGenerator[RecorderInstanceGenerator]:\n                 ...\n                 def async_test_recorder(\n            hass: HomeAssistant,\n            config: ConfigType | None = None,\n            *,\n            wait_recorder: bool = True,\n        )->AsyncGenerator[recorder.Recorder]:\n                     ...\n1526 1531                hass: HomeAssistant,\n1527 1532                config: ConfigType | None = None,\n1528 1533                *,\n     1534  +             expected_setup_result: bool = True,\n1529 1535                wait_recorder: bool = True,\n1530 1536            ) -> AsyncGenerator[recorder.Recorder]:\n1531 1537                \"\"\"Setup and return recorder instance.\"\"\"  # noqa: D401\n           ...\n",
                "file_path": "tests/conftest.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "bool",
                    "expected_setup_result"
                ],
                "prefix": [
                    "            hass: HomeAssistant,\n",
                    "            config: ConfigType | None = None,\n",
                    "            *,\n"
                ],
                "suffix": [
                    "            wait_recorder: bool = True,\n",
                    "        ) -> AsyncGenerator[recorder.Recorder]:\n",
                    "            \"\"\"Setup and return recorder instance.\"\"\"  # noqa: D401\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 9,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1534,
                                    "column": 12
                                },
                                "end": {
                                    "line": 1534,
                                    "column": 33
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 8,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    10
                ]
            },
            [
                "            wait_recorder: bool = True,\n",
                "        ) -> AsyncGenerator[recorder.Recorder]:\n",
                "            \"\"\"Setup and return recorder instance.\"\"\"  # noqa: D401\n"
            ],
            {
                "type": "replace",
                "before": [
                    "            await _async_init_recorder_component(hass, config, recorder_db_url)\n"
                ],
                "after": [
                    "            await _async_init_recorder_component(\n",
                    "                hass,\n",
                    "                config,\n",
                    "                recorder_db_url,\n",
                    "                expected_setup_result=expected_setup_result,\n",
                    "            )\n"
                ],
                "parent_version_range": {
                    "start": 1532,
                    "end": 1533
                },
                "child_version_range": {
                    "start": 1538,
                    "end": 1544
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "with (\n        patch(\n            \"homeassistant.components.recorder.Recorder.async_nightly_tasks\",\n            side_effect=nightly,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.Recorder.async_periodic_statistics\",\n            side_effect=stats,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration._find_schema_errors\",\n            side_effect=schema_validate,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.EventsContextIDMigration.migrate_data\",\n            side_effect=migrate_events_context_ids,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.StatesContextIDMigration.migrate_data\",\n            side_effect=migrate_states_context_ids,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.EventTypeIDMigration.migrate_data\",\n            side_effect=migrate_event_type_ids,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.migration.EntityIDMigration.migrate_data\",\n            side_effect=migrate_entity_ids,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.Recorder._legacy_event_id_foreign_key_exists\",\n            side_effect=legacy_event_id_foreign_key_exists,\n            autospec=True,\n        ),\n        patch(\n            \"homeassistant.components.recorder.Recorder._schedule_compile_missing_statistics\",\n            side_effect=compile_missing,\n            autospec=True,\n        ),\n    ):",
                        "start_line": 1476,
                        "end_line": 1544
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "async_test_recorder",
                        "signature": "def async_test_recorder(\n    recorder_db_url: str,\n    enable_nightly_purge: bool,\n    enable_statistics: bool,\n    enable_schema_validation: bool,\n    enable_migrate_context_ids: bool,\n    enable_migrate_event_type_ids: bool,\n    enable_migrate_entity_ids: bool,\n    enable_migrate_event_ids: bool,\n)->AsyncGenerator[RecorderInstanceGenerator]:",
                        "at_line": 1421
                    },
                    {
                        "type": "function",
                        "name": "async_test_recorder",
                        "signature": "def async_test_recorder(\n            hass: HomeAssistant,\n            config: ConfigType | None = None,\n            *,\n            wait_recorder: bool = True,\n        )->AsyncGenerator[recorder.Recorder]:",
                        "at_line": 1525
                    }
                ],
                "idx": 9,
                "hunk_diff": "File: tests/conftest.py\nCode:\n             def async_test_recorder(\n    recorder_db_url: str,\n    enable_nightly_purge: bool,\n    enable_statistics: bool,\n    enable_schema_validation: bool,\n    enable_migrate_context_ids: bool,\n    enable_migrate_event_type_ids: bool,\n    enable_migrate_entity_ids: bool,\n    enable_migrate_event_ids: bool,\n)->AsyncGenerator[RecorderInstanceGenerator]:\n                 ...\n                 def async_test_recorder(\n            hass: HomeAssistant,\n            config: ConfigType | None = None,\n            *,\n            wait_recorder: bool = True,\n        )->AsyncGenerator[recorder.Recorder]:\n                     ...\n1529 1535                wait_recorder: bool = True,\n1530 1536            ) -> AsyncGenerator[recorder.Recorder]:\n1531 1537                \"\"\"Setup and return recorder instance.\"\"\"  # noqa: D401\n1532       -             await _async_init_recorder_component(hass, config, recorder_db_url)\n     1538  +             await _async_init_recorder_component(\n     1539  +                 hass,\n     1540  +                 config,\n     1541  +                 recorder_db_url,\n     1542  +                 expected_setup_result=expected_setup_result,\n     1543  +             )\n1533 1544                await hass.async_block_till_done()\n1534 1545                instance = hass.data[recorder.DATA_INSTANCE]\n1535 1546                # The recorder's worker is not started until Home Assistant is running\n           ...\n",
                "file_path": "tests/conftest.py",
                "identifiers_before": [
                    "_async_init_recorder_component",
                    "config",
                    "hass",
                    "recorder_db_url"
                ],
                "identifiers_after": [
                    "_async_init_recorder_component",
                    "config",
                    "expected_setup_result",
                    "hass",
                    "recorder_db_url"
                ],
                "prefix": [
                    "            wait_recorder: bool = True,\n",
                    "        ) -> AsyncGenerator[recorder.Recorder]:\n",
                    "            \"\"\"Setup and return recorder instance.\"\"\"  # noqa: D401\n"
                ],
                "suffix": [
                    "            await hass.async_block_till_done()\n",
                    "            instance = hass.data[recorder.DATA_INSTANCE]\n",
                    "            # The recorder's worker is not started until Home Assistant is running\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1542,
                                    "column": 16
                                },
                                "end": {
                                    "line": 1542,
                                    "column": 37
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 9,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 8,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1542,
                                    "column": 38
                                },
                                "end": {
                                    "line": 1542,
                                    "column": 59
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 9,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "            await hass.async_block_till_done()\n",
                "            instance = hass.data[recorder.DATA_INSTANCE]\n",
                "            # The recorder's worker is not started until Home Assistant is running\n",
                "            if hass.state is CoreState.running and wait_recorder:\n",
                "                await async_recorder_block_till_done(hass)\n",
                "            try:\n",
                "                yield instance\n",
                "            finally:\n",
                "                if instance.is_alive():\n",
                "                    await instance._async_shutdown(None)\n",
                "\n",
                "        yield async_test_recorder\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def async_setup_recorder_instance(\n",
                "    async_test_recorder: RecorderInstanceGenerator,\n",
                ") -> AsyncGenerator[RecorderInstanceGenerator]:\n",
                "    \"\"\"Yield callable to setup recorder instance.\"\"\"\n",
                "\n",
                "    async with AsyncExitStack() as stack:\n",
                "\n",
                "        async def async_setup_recorder(\n",
                "            hass: HomeAssistant,\n",
                "            config: ConfigType | None = None,\n",
                "            *,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "            expected_setup_result: bool = True,\n"
                ],
                "parent_version_range": {
                    "start": 1559,
                    "end": 1559
                },
                "child_version_range": {
                    "start": 1570,
                    "end": 1571
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "async with AsyncExitStack() as stack:",
                        "start_line": 1553,
                        "end_line": 1567
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "async_setup_recorder_instance",
                        "signature": "def async_setup_recorder_instance(\n    async_test_recorder: RecorderInstanceGenerator,\n)->AsyncGenerator[RecorderInstanceGenerator]:",
                        "at_line": 1548
                    },
                    {
                        "type": "function",
                        "name": "async_setup_recorder",
                        "signature": "def async_setup_recorder(\n            hass: HomeAssistant,\n            config: ConfigType | None = None,\n            *,\n            wait_recorder: bool = True,\n        )->AsyncGenerator[recorder.Recorder]:",
                        "at_line": 1555
                    }
                ],
                "idx": 10,
                "hunk_diff": "File: tests/conftest.py\nCode:\n             def async_setup_recorder_instance(\n    async_test_recorder: RecorderInstanceGenerator,\n)->AsyncGenerator[RecorderInstanceGenerator]:\n                 ...\n                 def async_setup_recorder(\n            hass: HomeAssistant,\n            config: ConfigType | None = None,\n            *,\n            wait_recorder: bool = True,\n        )->AsyncGenerator[recorder.Recorder]:\n                     ...\n1556 1567                hass: HomeAssistant,\n1557 1568                config: ConfigType | None = None,\n1558 1569                *,\n     1570  +             expected_setup_result: bool = True,\n1559 1571                wait_recorder: bool = True,\n1560 1572            ) -> AsyncGenerator[recorder.Recorder]:\n1561 1573                \"\"\"Set up and return recorder instance.\"\"\"\n           ...\n",
                "file_path": "tests/conftest.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "bool",
                    "expected_setup_result"
                ],
                "prefix": [
                    "            hass: HomeAssistant,\n",
                    "            config: ConfigType | None = None,\n",
                    "            *,\n"
                ],
                "suffix": [
                    "            wait_recorder: bool = True,\n",
                    "        ) -> AsyncGenerator[recorder.Recorder]:\n",
                    "            \"\"\"Set up and return recorder instance.\"\"\"\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 11,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1570,
                                    "column": 12
                                },
                                "end": {
                                    "line": 1570,
                                    "column": 33
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 10,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    8
                ]
            },
            [
                "            wait_recorder: bool = True,\n",
                "        ) -> AsyncGenerator[recorder.Recorder]:\n",
                "            \"\"\"Set up and return recorder instance.\"\"\"\n",
                "\n",
                "            return await stack.enter_async_context(\n"
            ],
            {
                "type": "replace",
                "before": [
                    "                async_test_recorder(hass, config, wait_recorder=wait_recorder)\n"
                ],
                "after": [
                    "                async_test_recorder(\n",
                    "                    hass,\n",
                    "                    config,\n",
                    "                    expected_setup_result=expected_setup_result,\n",
                    "                    wait_recorder=wait_recorder,\n",
                    "                )\n"
                ],
                "parent_version_range": {
                    "start": 1564,
                    "end": 1565
                },
                "child_version_range": {
                    "start": 1576,
                    "end": 1582
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "async with AsyncExitStack() as stack:",
                        "start_line": 1553,
                        "end_line": 1567
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "async_setup_recorder_instance",
                        "signature": "def async_setup_recorder_instance(\n    async_test_recorder: RecorderInstanceGenerator,\n)->AsyncGenerator[RecorderInstanceGenerator]:",
                        "at_line": 1548
                    },
                    {
                        "type": "function",
                        "name": "async_setup_recorder",
                        "signature": "def async_setup_recorder(\n            hass: HomeAssistant,\n            config: ConfigType | None = None,\n            *,\n            wait_recorder: bool = True,\n        )->AsyncGenerator[recorder.Recorder]:",
                        "at_line": 1555
                    },
                    {
                        "type": "call",
                        "name": "stack.enter_async_context",
                        "signature": "stack.enter_async_context(\n                async_test_recorder(hass, config, wait_recorder=wait_recorder)\n            )",
                        "at_line": 1563,
                        "argument": "async_test_recorder(hass, conf..."
                    },
                    {
                        "type": "call",
                        "name": "async_test_recorder",
                        "signature": "async_test_recorder(hass, config, wait_recorder=wait_recorder)",
                        "at_line": 1564,
                        "argument": "hass"
                    }
                ],
                "idx": 11,
                "hunk_diff": "File: tests/conftest.py\nCode:\n             def async_setup_recorder_instance(\n    async_test_recorder: RecorderInstanceGenerator,\n)->AsyncGenerator[RecorderInstanceGenerator]:\n                 ...\n                 def async_setup_recorder(\n            hass: HomeAssistant,\n            config: ConfigType | None = None,\n            *,\n            wait_recorder: bool = True,\n        )->AsyncGenerator[recorder.Recorder]:\n                     ...\n1561 1573                \"\"\"Set up and return recorder instance.\"\"\"\n1562 1574    \n1563 1575                return await stack.enter_async_context(\n1564       -                 async_test_recorder(hass, config, wait_recorder=wait_recorder)\n     1576  +                 async_test_recorder(\n     1577  +                     hass,\n     1578  +                     config,\n     1579  +                     expected_setup_result=expected_setup_result,\n     1580  +                     wait_recorder=wait_recorder,\n     1581  +                 )\n1565 1582                )\n1566 1583    \n1567 1584            yield async_setup_recorder\n           ...\n",
                "file_path": "tests/conftest.py",
                "identifiers_before": [
                    "async_test_recorder",
                    "config",
                    "hass",
                    "wait_recorder"
                ],
                "identifiers_after": [
                    "async_test_recorder",
                    "config",
                    "expected_setup_result",
                    "hass",
                    "wait_recorder"
                ],
                "prefix": [
                    "            \"\"\"Set up and return recorder instance.\"\"\"\n",
                    "\n",
                    "            return await stack.enter_async_context(\n"
                ],
                "suffix": [
                    "            )\n",
                    "\n",
                    "        yield async_setup_recorder\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 10,
                        "detail": {
                            "identifier": "expected_setup_result",
                            "position": {
                                "start": {
                                    "line": 1579,
                                    "column": 42
                                },
                                "end": {
                                    "line": 1579,
                                    "column": 63
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/core/tests/conftest.py",
                            "hunk_idx": 11,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "            )\n",
                "\n",
                "        yield async_setup_recorder\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "async def recorder_mock(\n",
                "    recorder_config: dict[str, Any] | None,\n",
                "    async_test_recorder: RecorderInstanceGenerator,\n",
                "    hass: HomeAssistant,\n",
                ") -> AsyncGenerator[recorder.Recorder]:\n",
                "    \"\"\"Fixture with in-memory recorder.\"\"\"\n",
                "    async with async_test_recorder(hass, recorder_config) as instance:\n",
                "        yield instance\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mock_recorder_before_hass() -> None:\n",
                "    \"\"\"Mock the recorder.\n",
                "\n",
                "    Override or parametrize this fixture with a fixture that mocks the recorder,\n",
                "    in the tests that need to test the recorder.\n",
                "    \"\"\"\n",
                "\n",
                "\n",
                "@pytest.fixture(name=\"enable_bluetooth\")\n",
                "async def mock_enable_bluetooth(\n",
                "    hass: HomeAssistant,\n",
                "    mock_bleak_scanner_start: MagicMock,\n",
                "    mock_bluetooth_adapters: None,\n",
                ") -> AsyncGenerator[None]:\n",
                "    \"\"\"Fixture to mock starting the bleak scanner.\"\"\"\n",
                "    entry = MockConfigEntry(domain=\"bluetooth\", unique_id=\"00:00:00:00:00:01\")\n",
                "    entry.add_to_hass(hass)\n",
                "    await hass.config_entries.async_setup(entry.entry_id)\n",
                "    await hass.async_block_till_done()\n",
                "    yield\n",
                "    await hass.config_entries.async_unload(entry.entry_id)\n",
                "    await hass.async_block_till_done()\n",
                "\n",
                "\n",
                "@pytest.fixture(scope=\"session\")\n",
                "def mock_bluetooth_adapters() -> Generator[None]:\n",
                "    \"\"\"Fixture to mock bluetooth adapters.\"\"\"\n",
                "    with (\n",
                "        patch(\"bluetooth_auto_recovery.recover_adapter\"),\n",
                "        patch(\"bluetooth_adapters.systems.platform.system\", return_value=\"Linux\"),\n",
                "        patch(\"bluetooth_adapters.systems.linux.LinuxAdapters.refresh\"),\n",
                "        patch(\n",
                "            \"bluetooth_adapters.systems.linux.LinuxAdapters.adapters\",\n",
                "            {\n",
                "                \"hci0\": {\n",
                "                    \"address\": \"00:00:00:00:00:01\",\n",
                "                    \"hw_version\": \"usb:v1D6Bp0246d053F\",\n",
                "                    \"passive_scan\": False,\n",
                "                    \"sw_version\": \"homeassistant\",\n",
                "                    \"manufacturer\": \"ACME\",\n",
                "                    \"product\": \"Bluetooth Adapter 5.0\",\n",
                "                    \"product_id\": \"aa01\",\n",
                "                    \"vendor_id\": \"cc01\",\n",
                "                },\n",
                "            },\n",
                "        ),\n",
                "    ):\n",
                "        yield\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mock_bleak_scanner_start() -> Generator[MagicMock]:\n",
                "    \"\"\"Fixture to mock starting the bleak scanner.\"\"\"\n",
                "\n",
                "    # Late imports to avoid loading bleak unless we need it\n",
                "\n",
                "    # pylint: disable-next=import-outside-toplevel\n",
                "    from habluetooth import scanner as bluetooth_scanner\n",
                "\n",
                "    # We need to drop the stop method from the object since we patched\n",
                "    # out start and this fixture will expire before the stop method is called\n",
                "    # when EVENT_HOMEASSISTANT_STOP is fired.\n",
                "    # pylint: disable-next=c-extension-no-member\n",
                "    bluetooth_scanner.OriginalBleakScanner.stop = AsyncMock()  # type: ignore[assignment]\n",
                "    with (\n",
                "        patch.object(\n",
                "            bluetooth_scanner.OriginalBleakScanner,  # pylint: disable=c-extension-no-member\n",
                "            \"start\",\n",
                "        ) as mock_bleak_scanner_start,\n",
                "        patch.object(bluetooth_scanner, \"HaScanner\"),\n",
                "    ):\n",
                "        yield mock_bleak_scanner_start\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mock_integration_frame() -> Generator[Mock]:\n",
                "    \"\"\"Mock as if we're calling code from inside an integration.\"\"\"\n",
                "    correct_frame = Mock(\n",
                "        filename=\"/home/paulus/homeassistant/components/hue/light.py\",\n",
                "        lineno=\"23\",\n",
                "        line=\"self.light.is_on\",\n",
                "    )\n",
                "    with (\n",
                "        patch(\n",
                "            \"homeassistant.helpers.frame.linecache.getline\",\n",
                "            return_value=correct_frame.line,\n",
                "        ),\n",
                "        patch(\n",
                "            \"homeassistant.helpers.frame.get_current_frame\",\n",
                "            return_value=extract_stack_to_frame(\n",
                "                [\n",
                "                    Mock(\n",
                "                        filename=\"/home/paulus/homeassistant/core.py\",\n",
                "                        lineno=\"23\",\n",
                "                        line=\"do_something()\",\n",
                "                    ),\n",
                "                    correct_frame,\n",
                "                    Mock(\n",
                "                        filename=\"/home/paulus/aiohue/lights.py\",\n",
                "                        lineno=\"2\",\n",
                "                        line=\"something()\",\n",
                "                    ),\n",
                "                ]\n",
                "            ),\n",
                "        ),\n",
                "    ):\n",
                "        yield correct_frame\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def mock_bluetooth(\n",
                "    mock_bleak_scanner_start: MagicMock, mock_bluetooth_adapters: None\n",
                ") -> None:\n",
                "    \"\"\"Mock out bluetooth from starting.\"\"\"\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def category_registry(hass: HomeAssistant) -> cr.CategoryRegistry:\n",
                "    \"\"\"Return the category registry from the current hass instance.\"\"\"\n",
                "    return cr.async_get(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def area_registry(hass: HomeAssistant) -> ar.AreaRegistry:\n",
                "    \"\"\"Return the area registry from the current hass instance.\"\"\"\n",
                "    return ar.async_get(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def device_registry(hass: HomeAssistant) -> dr.DeviceRegistry:\n",
                "    \"\"\"Return the device registry from the current hass instance.\"\"\"\n",
                "    return dr.async_get(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def entity_registry(hass: HomeAssistant) -> er.EntityRegistry:\n",
                "    \"\"\"Return the entity registry from the current hass instance.\"\"\"\n",
                "    return er.async_get(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def floor_registry(hass: HomeAssistant) -> fr.FloorRegistry:\n",
                "    \"\"\"Return the floor registry from the current hass instance.\"\"\"\n",
                "    return fr.async_get(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def issue_registry(hass: HomeAssistant) -> ir.IssueRegistry:\n",
                "    \"\"\"Return the issue registry from the current hass instance.\"\"\"\n",
                "    return ir.async_get(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def label_registry(hass: HomeAssistant) -> lr.LabelRegistry:\n",
                "    \"\"\"Return the label registry from the current hass instance.\"\"\"\n",
                "    return lr.async_get(hass)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def service_calls(hass: HomeAssistant) -> Generator[list[ServiceCall]]:\n",
                "    \"\"\"Track all service calls.\"\"\"\n",
                "    calls = []\n",
                "\n",
                "    _original_async_call = hass.services.async_call\n",
                "\n",
                "    async def _async_call(\n",
                "        self,\n",
                "        domain: str,\n",
                "        service: str,\n",
                "        service_data: dict[str, Any] | None = None,\n",
                "        blocking: bool = False,\n",
                "        context: Context | None = None,\n",
                "        target: dict[str, Any] | None = None,\n",
                "        return_response: bool = False,\n",
                "    ) -> ServiceResponse:\n",
                "        calls.append(\n",
                "            ServiceCall(domain, service, service_data, context, return_response)\n",
                "        )\n",
                "        try:\n",
                "            return await _original_async_call(\n",
                "                domain,\n",
                "                service,\n",
                "                service_data,\n",
                "                blocking,\n",
                "                context,\n",
                "                target,\n",
                "                return_response,\n",
                "            )\n",
                "        except ServiceNotFound:\n",
                "            _LOGGER.debug(\"Ignoring unknown service call to %s.%s\", domain, service)\n",
                "        return None\n",
                "\n",
                "    with patch(\"homeassistant.core.ServiceRegistry.async_call\", _async_call):\n",
                "        yield calls\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def snapshot(snapshot: SnapshotAssertion) -> SnapshotAssertion:\n",
                "    \"\"\"Return snapshot assertion fixture with the Home Assistant extension.\"\"\"\n",
                "    return snapshot.use_extension(HomeAssistantSnapshotExtension)\n",
                "\n",
                "\n",
                "@pytest.fixture\n",
                "def disable_block_async_io() -> Generator[None]:\n",
                "    \"\"\"Fixture to disable the loop protection from block_async_io.\"\"\"\n",
                "    yield\n",
                "    calls = block_async_io._BLOCKED_CALLS.calls\n",
                "    for blocking_call in calls:\n",
                "        setattr(\n",
                "            blocking_call.object, blocking_call.function, blocking_call.original_func\n",
                "        )\n",
                "    calls.clear()"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                1,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                1,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                1,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                2,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                2,
                10
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                2,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                4,
                10
            ],
            "edit_order": "bi-directional",
            "reason": "def and use about @pytest.fixture, anyway its quite complicated..."
        },
        {
            "edit_hunk_pair": [
                4,
                11
            ],
            "edit_order": "bi-directional",
            "reason": "use and implement about @pytest.fixture, anyway its quite complicated..."
        },
        {
            "edit_hunk_pair": [
                5,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                5,
                9
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                6,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                7,
                9
            ],
            "edit_order": "bi-directional",
            "reason": "implement and use"
        },
        {
            "edit_hunk_pair": [
                8,
                9
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                8,
                10
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        },
        {
            "edit_hunk_pair": [
                8,
                11
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                10,
                11
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        }
    ]
}