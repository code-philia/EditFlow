{
    "language": "python",
    "commit_url": "https://github.com/Lightning-AI/pytorch-lightning/commit/ab4c838ba394f7b92f28676d8bf758ecf499d7e8",
    "commit_message": "Remove ModelSummary validation from train loop on_trainer_init (#6610)",
    "commit_snapshots": {
        "pytorch_lightning/trainer/trainer.py": [
            [
                "# Copyright The PyTorch Lightning team.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     http://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\"\"\"Trainer to automate the training.\"\"\"\n",
                "import logging\n",
                "import warnings\n",
                "from itertools import count\n",
                "from pathlib import Path\n",
                "from traceback import print_exc\n",
                "from typing import Any, Dict, Iterable, List, Optional, Union\n",
                "\n",
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "from pytorch_lightning.accelerators import Accelerator\n",
                "from pytorch_lightning.callbacks import Callback\n",
                "from pytorch_lightning.core.datamodule import LightningDataModule\n",
                "from pytorch_lightning.core.lightning import LightningModule\n",
                "from pytorch_lightning.core.memory import ModelSummary\n",
                "from pytorch_lightning.core.step_result import Result\n",
                "from pytorch_lightning.loggers import LightningLoggerBase\n",
                "from pytorch_lightning.plugins import Plugin\n",
                "from pytorch_lightning.profiler import BaseProfiler\n",
                "from pytorch_lightning.trainer.callback_hook import TrainerCallbackHookMixin\n",
                "from pytorch_lightning.trainer.configuration_validator import ConfigValidator\n",
                "from pytorch_lightning.trainer.connectors.accelerator_connector import AcceleratorConnector\n",
                "from pytorch_lightning.trainer.connectors.callback_connector import CallbackConnector\n",
                "from pytorch_lightning.trainer.connectors.checkpoint_connector import CheckpointConnector\n",
                "from pytorch_lightning.trainer.connectors.data_connector import DataConnector\n",
                "from pytorch_lightning.trainer.connectors.debugging_connector import DebuggingConnector\n",
                "from pytorch_lightning.trainer.connectors.env_vars_connector import _defaults_from_env_vars\n",
                "from pytorch_lightning.trainer.connectors.logger_connector import LoggerConnector\n",
                "from pytorch_lightning.trainer.connectors.model_connector import ModelConnector\n",
                "from pytorch_lightning.trainer.connectors.optimizer_connector import OptimizerConnector\n",
                "from pytorch_lightning.trainer.connectors.profiler_connector import ProfilerConnector\n",
                "from pytorch_lightning.trainer.connectors.slurm_connector import SLURMConnector\n",
                "from pytorch_lightning.trainer.connectors.training_trick_connector import TrainingTricksConnector\n",
                "from pytorch_lightning.trainer.data_loading import TrainerDataLoadingMixin\n",
                "from pytorch_lightning.trainer.deprecated_api import DeprecatedDistDeviceAttributes, DeprecatedTrainerAttributes\n",
                "from pytorch_lightning.trainer.evaluation_loop import EvaluationLoop\n",
                "from pytorch_lightning.trainer.logging import TrainerLoggingMixin\n",
                "from pytorch_lightning.trainer.model_hooks import TrainerModelHooksMixin\n",
                "from pytorch_lightning.trainer.optimizers import TrainerOptimizersMixin\n",
                "from pytorch_lightning.trainer.predict_loop import PredictLoop\n",
                "from pytorch_lightning.trainer.properties import TrainerProperties\n",
                "from pytorch_lightning.trainer.states import TrainerState\n",
                "from pytorch_lightning.trainer.training_loop import TrainLoop\n",
                "from pytorch_lightning.trainer.training_tricks import TrainerTrainingTricksMixin\n",
                "from pytorch_lightning.tuner.tuning import Tuner\n",
                "from pytorch_lightning.utilities import rank_zero_warn\n",
                "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
                "from pytorch_lightning.utilities.debugging import InternalDebugger\n",
                "from pytorch_lightning.utilities.exceptions import MisconfigurationException\n",
                "from pytorch_lightning.utilities.memory import recursive_detach\n",
                "from pytorch_lightning.utilities.model_helpers import is_overridden\n",
                "\n",
                "log = logging.getLogger(__name__)\n",
                "# warnings to ignore in trainer\n",
                "warnings.filterwarnings(\n",
                "    'ignore', message='torch.distributed.reduce_op is deprecated, '\n",
                "    'please use torch.distributed.ReduceOp instead'\n",
                ")\n",
                "\n",
                "\n",
                "class Trainer(\n",
                "    TrainerProperties,\n",
                "    TrainerCallbackHookMixin,\n",
                "    TrainerModelHooksMixin,\n",
                "    TrainerOptimizersMixin,\n",
                "    TrainerLoggingMixin,\n",
                "    TrainerTrainingTricksMixin,\n",
                "    TrainerDataLoadingMixin,\n",
                "    DeprecatedDistDeviceAttributes,\n",
                "    DeprecatedTrainerAttributes,\n",
                "):\n",
                "\n",
                "    @_defaults_from_env_vars\n",
                "    def __init__(\n",
                "        self,\n",
                "        logger: Union[LightningLoggerBase, Iterable[LightningLoggerBase], bool] = True,\n",
                "        checkpoint_callback: bool = True,\n",
                "        callbacks: Optional[Union[List[Callback], Callback]] = None,\n",
                "        default_root_dir: Optional[str] = None,\n",
                "        gradient_clip_val: float = 0,\n",
                "        process_position: int = 0,\n",
                "        num_nodes: int = 1,\n",
                "        num_processes: int = 1,\n",
                "        gpus: Optional[Union[List[int], str, int]] = None,\n",
                "        auto_select_gpus: bool = False,\n",
                "        tpu_cores: Optional[Union[List[int], str, int]] = None,\n",
                "        log_gpu_memory: Optional[str] = None,\n",
                "        progress_bar_refresh_rate: Optional[int] = None,\n",
                "        overfit_batches: Union[int, float] = 0.0,\n",
                "        track_grad_norm: Union[int, float, str] = -1,\n",
                "        check_val_every_n_epoch: int = 1,\n",
                "        fast_dev_run: Union[int, bool] = False,\n",
                "        accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1,\n",
                "        max_epochs: Optional[int] = None,\n",
                "        min_epochs: Optional[int] = None,\n",
                "        max_steps: Optional[int] = None,\n",
                "        min_steps: Optional[int] = None,\n",
                "        limit_train_batches: Union[int, float] = 1.0,\n",
                "        limit_val_batches: Union[int, float] = 1.0,\n",
                "        limit_test_batches: Union[int, float] = 1.0,\n",
                "        limit_predict_batches: Union[int, float] = 1.0,\n",
                "        val_check_interval: Union[int, float] = 1.0,\n",
                "        flush_logs_every_n_steps: int = 100,\n",
                "        log_every_n_steps: int = 50,\n",
                "        accelerator: Optional[Union[str, Accelerator]] = None,\n",
                "        sync_batchnorm: bool = False,\n",
                "        precision: int = 32,\n",
                "        weights_summary: Optional[str] = 'top',\n",
                "        weights_save_path: Optional[str] = None,\n",
                "        num_sanity_val_steps: int = 2,\n",
                "        truncated_bptt_steps: Optional[int] = None,\n",
                "        resume_from_checkpoint: Optional[Union[Path, str]] = None,\n",
                "        profiler: Optional[Union[BaseProfiler, str]] = None,\n",
                "        benchmark: bool = False,\n",
                "        deterministic: bool = False,\n",
                "        reload_dataloaders_every_epoch: bool = False,\n",
                "        auto_lr_find: Union[bool, str] = False,\n",
                "        replace_sampler_ddp: bool = True,\n",
                "        terminate_on_nan: bool = False,\n",
                "        auto_scale_batch_size: Union[str, bool] = False,\n",
                "        prepare_data_per_node: bool = True,\n",
                "        plugins: Optional[Union[Plugin, str, list]] = None,\n",
                "        amp_backend: str = 'native',\n",
                "        amp_level: str = 'O2',\n",
                "        distributed_backend: Optional[str] = None,\n",
                "        move_metrics_to_cpu: bool = False,\n",
                "        multiple_trainloader_mode: str = 'max_size_cycle',\n",
                "        stochastic_weight_avg: bool = False\n",
                "    ):\n",
                "        r\"\"\"\n",
                "        Customize every aspect of training via flags\n",
                "\n",
                "        Args:\n",
                "\n",
                "            accelerator: Previously known as distributed_backend (dp, ddp, ddp2, etc...).\n",
                "                Can also take in an accelerator object for custom hardware.\n",
                "\n",
                "            accumulate_grad_batches: Accumulates grads every k batches or as set up in the dict.\n",
                "\n",
                "            amp_backend: The mixed precision backend to use (\"native\" or \"apex\")\n",
                "\n",
                "            amp_level: The optimization level to use (O1, O2, etc...).\n",
                "\n",
                "            auto_lr_find: If set to True, will make trainer.tune() run a learning rate finder,\n",
                "                trying to optimize initial learning for faster convergence. trainer.tune() method will\n",
                "                set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\n",
                "                To use a different key set a string instead of True with the key name.\n",
                "\n",
                "            auto_scale_batch_size: If set to True, will `initially` run a batch size\n",
                "                finder trying to find the largest batch size that fits into memory.\n",
                "                The result will be stored in self.batch_size in the LightningModule.\n",
                "                Additionally, can be set to either `power` that estimates the batch size through\n",
                "                a power search or `binsearch` that estimates the batch size through a binary search.\n",
                "\n",
                "            auto_select_gpus: If enabled and `gpus` is an integer, pick available\n",
                "                gpus automatically. This is especially useful when\n",
                "                GPUs are configured to be in \"exclusive mode\", such\n",
                "                that only one process at a time can access them.\n",
                "\n",
                "            benchmark: If true enables cudnn.benchmark.\n",
                "\n",
                "            callbacks: Add a callback or list of callbacks.\n",
                "\n",
                "            checkpoint_callback: If ``True``, enable checkpointing.\n",
                "                It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
                "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n",
                "\n",
                "            check_val_every_n_epoch: Check val every n train epochs.\n",
                "\n",
                "            default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
                "                Default: ``os.getcwd()``.\n",
                "                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
                "\n",
                "            deterministic: If true enables cudnn.deterministic.\n",
                "\n",
                "            distributed_backend: deprecated. Please use 'accelerator'\n",
                "\n",
                "            fast_dev_run: runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
                "                of train, val and test to find any bugs (ie: a sort of unit test).\n",
                "\n",
                "            flush_logs_every_n_steps: How often to flush logs to disk (defaults to every 100 steps).\n",
                "\n",
                "            gpus: number of gpus to train on (int) or which GPUs to train on (list or str) applied per node\n",
                "\n",
                "            gradient_clip_val: 0 means don't clip.\n",
                "\n",
                "            limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches)\n",
                "\n",
                "            limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches)\n",
                "\n",
                "            limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches)\n",
                "\n",
                "            limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches)\n",
                "\n",
                "            logger: Logger (or iterable collection of loggers) for experiment tracking.\n",
                "\n",
                "            log_gpu_memory: None, 'min_max', 'all'. Might slow performance\n",
                "\n",
                "            log_every_n_steps: How often to log within steps (defaults to every 50 steps).\n",
                "\n",
                "            prepare_data_per_node: If True, each LOCAL_RANK=0 will call prepare data.\n",
                "                Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data\n",
                "\n",
                "            process_position: orders the progress bar when running multiple models on same machine.\n",
                "\n",
                "            progress_bar_refresh_rate: How often to refresh progress bar (in steps). Value ``0`` disables progress bar.\n",
                "                Ignored when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means\n",
                "                a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).\n",
                "\n",
                "            profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
                "\n",
                "            overfit_batches: Overfit a fraction of training data (float) or a set number of batches (int).\n",
                "\n",
                "            plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
                "\n",
                "            precision: Double precision (64), full precision (32) or half precision (16). Can be used on CPU, GPU or\n",
                "                TPUs.\n",
                "\n",
                "            max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
                "                If both max_epochs and max_steps are not specified, defaults to ``max_epochs`` = 1000.\n",
                "\n",
                "            min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
                "                If both min_epochs and min_steps are not specified, defaults to ``min_epochs`` = 1.\n",
                "\n",
                "            max_steps: Stop training after this number of steps. Disabled by default (None).\n",
                "\n",
                "            min_steps: Force training for at least these number of steps. Disabled by default (None).\n",
                "\n",
                "            num_nodes: number of GPU nodes for distributed training.\n",
                "\n",
                "            num_processes: number of processes for distributed training with distributed_backend=\"ddp_cpu\"\n",
                "\n",
                "            num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
                "                Set it to `-1` to run all batches in all validation dataloaders.\n",
                "\n",
                "            reload_dataloaders_every_epoch: Set to True to reload dataloaders every epoch.\n",
                "\n",
                "            replace_sampler_ddp: Explicitly enables or disables sampler replacement. If not specified this\n",
                "                will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for\n",
                "                train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,\n",
                "                you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.\n",
                "\n",
                "            resume_from_checkpoint: Path/URL of the checkpoint from which training is resumed. If there is\n",
                "                no checkpoint file at the path, start from scratch. If resuming from mid-epoch checkpoint,\n",
                "                training will start from the beginning of the next epoch.\n",
                "\n",
                "            sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
                "\n",
                "            terminate_on_nan: If set to True, will terminate training (by raising a `ValueError`) at the\n",
                "                end of each training batch, if any of the parameters or the loss are NaN or +/-inf.\n",
                "\n",
                "            tpu_cores: How many TPU cores to train on (1 or 8) / Single TPU to train on [1]\n",
                "\n",
                "            track_grad_norm: -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm.\n",
                "\n",
                "            truncated_bptt_steps: Truncated back prop breaks performs backprop every k steps of much longer\n",
                "                sequence.\n",
                "\n",
                "            val_check_interval: How often to check the validation set. Use float to check within a training epoch,\n",
                "                use int to check every n steps (batches).\n",
                "\n",
                "            weights_summary: Prints a summary of the weights when training begins.\n",
                "\n",
                "            weights_save_path: Where to save weights if specified. Will override default_root_dir\n",
                "                for checkpoints only. Use this if for whatever reason you need the checkpoints\n",
                "                stored in a different place than the logs written in `default_root_dir`.\n",
                "                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
                "                Defaults to `default_root_dir`.\n",
                "\n",
                "            move_metrics_to_cpu: Whether to force internal logged metrics to be moved to cpu.\n",
                "                This can save some gpu memory, but can make training slower. Use with attention.\n",
                "\n",
                "            multiple_trainloader_mode: How to loop over the datasets when there are multiple train loaders.\n",
                "                In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\n",
                "                and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\n",
                "                reload when reaching the minimum length of datasets.\n",
                "\n",
                "            stochastic_weight_avg: Whether to use `Stochastic Weight Averaging (SWA)\n",
                "                <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>_`\n",
                "\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "\n",
                "        distributed_backend = distributed_backend or accelerator\n",
                "\n",
                "        # init connectors\n",
                "        self.dev_debugger = InternalDebugger(self)\n",
                "        self.config_validator = ConfigValidator(self)\n",
                "        self.data_connector = DataConnector(self)\n",
                "        self.optimizer_connector = OptimizerConnector(self)\n",
                "\n",
                "        self.accelerator_connector = AcceleratorConnector(\n",
                "            num_processes, tpu_cores, distributed_backend, auto_select_gpus, gpus, num_nodes, sync_batchnorm, benchmark,\n",
                "            replace_sampler_ddp, deterministic, precision, amp_backend, amp_level, plugins\n",
                "        )\n",
                "        self.logger_connector = LoggerConnector(self, log_gpu_memory)\n",
                "        self.model_connector = ModelConnector(self)\n",
                "        self.callback_connector = CallbackConnector(self)\n",
                "        self.debugging_connector = DebuggingConnector(self)\n",
                "        self.training_tricks_connector = TrainingTricksConnector(self)\n",
                "        self.profile_connector = ProfilerConnector(self)\n",
                "        self.checkpoint_connector = CheckpointConnector(self)\n",
                "        self.slurm_connector = SLURMConnector(self)\n",
                "        self.tuner = Tuner(self)\n",
                "        self.train_loop = TrainLoop(self, multiple_trainloader_mode)\n",
                "        self.evaluation_loop = EvaluationLoop(self)\n",
                "        self.predict_loop = PredictLoop(self)\n",
                "\n",
                "        # training state\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        if weights_summary is not None and weights_summary not in ModelSummary.MODES:\n",
                    "            raise MisconfigurationException(\n",
                    "                f\"`weights_summary` can be None, {', '.join(ModelSummary.MODES)}, but got {weights_summary}\"\n",
                    "            )\n"
                ],
                "parent_version_range": {
                    "start": 323,
                    "end": 323
                },
                "child_version_range": {
                    "start": 323,
                    "end": 327
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "Trainer",
                        "signature": "class Trainer(\n    TrainerProperties,\n    TrainerCallbackHookMixin,\n    TrainerModelHooksMixin,\n    TrainerOptimizersMixin,\n    TrainerLoggingMixin,\n    TrainerTrainingTricksMixin,\n    TrainerDataLoadingMixin,\n    DeprecatedDistDeviceAttributes,\n    DeprecatedTrainerAttributes,\n):",
                        "at_line": 74
                    },
                    {
                        "type": "function",
                        "name": "__init__",
                        "signature": "def __init__(\n        self,\n        logger: Union[LightningLoggerBase, Iterable[LightningLoggerBase], bool] = True,\n        checkpoint_callback: bool = True,\n        callbacks: Optional[Union[List[Callback], Callback]] = None,\n        default_root_dir: Optional[str] = None,\n        gradient_clip_val: float = 0,\n        process_position: int = 0,\n        num_nodes: int = 1,\n        num_processes: int = 1,\n        gpus: Optional[Union[List[int], str, int]] = None,\n        auto_select_gpus: bool = False,\n        tpu_cores: Optional[Union[List[int], str, int]] = None,\n        log_gpu_memory: Optional[str] = None,\n        progress_bar_refresh_rate: Optional[int] = None,\n        overfit_batches: Union[int, float] = 0.0,\n        track_grad_norm: Union[int, float, str] = -1,\n        check_val_every_n_epoch: int = 1,\n        fast_dev_run: Union[int, bool] = False,\n        accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1,\n        max_epochs: Optional[int] = None,\n        min_epochs: Optional[int] = None,\n        max_steps: Optional[int] = None,\n        min_steps: Optional[int] = None,\n        limit_train_batches: Union[int, float] = 1.0,\n        limit_val_batches: Union[int, float] = 1.0,\n        limit_test_batches: Union[int, float] = 1.0,\n        limit_predict_batches: Union[int, float] = 1.0,\n        val_check_interval: Union[int, float] = 1.0,\n        flush_logs_every_n_steps: int = 100,\n        log_every_n_steps: int = 50,\n        accelerator: Optional[Union[str, Accelerator]] = None,\n        sync_batchnorm: bool = False,\n        precision: int = 32,\n        weights_summary: Optional[str] = 'top',\n        weights_save_path: Optional[str] = None,\n        num_sanity_val_steps: int = 2,\n        truncated_bptt_steps: Optional[int] = None,\n        resume_from_checkpoint: Optional[Union[Path, str]] = None,\n        profiler: Optional[Union[BaseProfiler, str]] = None,\n        benchmark: bool = False,\n        deterministic: bool = False,\n        reload_dataloaders_every_epoch: bool = False,\n        auto_lr_find: Union[bool, str] = False,\n        replace_sampler_ddp: bool = True,\n        terminate_on_nan: bool = False,\n        auto_scale_batch_size: Union[str, bool] = False,\n        prepare_data_per_node: bool = True,\n        plugins: Optional[Union[Plugin, str, list]] = None,\n        amp_backend: str = 'native',\n        amp_level: str = 'O2',\n        distributed_backend: Optional[str] = None,\n        move_metrics_to_cpu: bool = False,\n        multiple_trainloader_mode: str = 'max_size_cycle',\n        stochastic_weight_avg: bool = False\n    ):",
                        "at_line": 87
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: pytorch_lightning/trainer/trainer.py\nCode:\n           class Trainer(\n    TrainerProperties,\n    TrainerCallbackHookMixin,\n    TrainerModelHooksMixin,\n    TrainerOptimizersMixin,\n    TrainerLoggingMixin,\n    TrainerTrainingTricksMixin,\n    TrainerDataLoadingMixin,\n    DeprecatedDistDeviceAttributes,\n    DeprecatedTrainerAttributes,\n):\n               ...\n               def __init__(\n        self,\n        logger: Union[LightningLoggerBase, Iterable[LightningLoggerBase], bool] = True,\n        checkpoint_callback: bool = True,\n        callbacks: Optional[Union[List[Callback], Callback]] = None,\n        default_root_dir: Optional[str] = None,\n        gradient_clip_val: float = 0,\n        process_position: int = 0,\n        num_nodes: int = 1,\n        num_processes: int = 1,\n        gpus: Optional[Union[List[int], str, int]] = None,\n        auto_select_gpus: bool = False,\n        tpu_cores: Optional[Union[List[int], str, int]] = None,\n        log_gpu_memory: Optional[str] = None,\n        progress_bar_refresh_rate: Optional[int] = None,\n        overfit_batches: Union[int, float] = 0.0,\n        track_grad_norm: Union[int, float, str] = -1,\n        check_val_every_n_epoch: int = 1,\n        fast_dev_run: Union[int, bool] = False,\n        accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1,\n        max_epochs: Optional[int] = None,\n        min_epochs: Optional[int] = None,\n        max_steps: Optional[int] = None,\n        min_steps: Optional[int] = None,\n        limit_train_batches: Union[int, float] = 1.0,\n        limit_val_batches: Union[int, float] = 1.0,\n        limit_test_batches: Union[int, float] = 1.0,\n        limit_predict_batches: Union[int, float] = 1.0,\n        val_check_interval: Union[int, float] = 1.0,\n        flush_logs_every_n_steps: int = 100,\n        log_every_n_steps: int = 50,\n        accelerator: Optional[Union[str, Accelerator]] = None,\n        sync_batchnorm: bool = False,\n        precision: int = 32,\n        weights_summary: Optional[str] = 'top',\n        weights_save_path: Optional[str] = None,\n        num_sanity_val_steps: int = 2,\n        truncated_bptt_steps: Optional[int] = None,\n        resume_from_checkpoint: Optional[Union[Path, str]] = None,\n        profiler: Optional[Union[BaseProfiler, str]] = None,\n        benchmark: bool = False,\n        deterministic: bool = False,\n        reload_dataloaders_every_epoch: bool = False,\n        auto_lr_find: Union[bool, str] = False,\n        replace_sampler_ddp: bool = True,\n        terminate_on_nan: bool = False,\n        auto_scale_batch_size: Union[str, bool] = False,\n        prepare_data_per_node: bool = True,\n        plugins: Optional[Union[Plugin, str, list]] = None,\n        amp_backend: str = 'native',\n        amp_level: str = 'O2',\n        distributed_backend: Optional[str] = None,\n        move_metrics_to_cpu: bool = False,\n        multiple_trainloader_mode: str = 'max_size_cycle',\n        stochastic_weight_avg: bool = False\n    ):\n                   ...\n320 320            self.predict_loop = PredictLoop(self)\n321 321    \n322 322            # training state\n    323  +         if weights_summary is not None and weights_summary not in ModelSummary.MODES:\n    324  +             raise MisconfigurationException(\n    325  +                 f\"`weights_summary` can be None, {', '.join(ModelSummary.MODES)}, but got {weights_summary}\"\n    326  +             )\n323 327            self.weights_summary = weights_summary\n324 328            self.shown_warnings = set()\n325 329    \n         ...\n",
                "file_path": "pytorch_lightning/trainer/trainer.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "MODES",
                    "MisconfigurationException",
                    "ModelSummary",
                    "join",
                    "weights_summary"
                ],
                "prefix": [
                    "        self.predict_loop = PredictLoop(self)\n",
                    "\n",
                    "        # training state\n"
                ],
                "suffix": [
                    "        self.weights_summary = weights_summary\n",
                    "        self.shown_warnings = set()\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        self.weights_summary = weights_summary\n",
                "        self.shown_warnings = set()\n",
                "\n",
                "        # init callbacks\n",
                "        # Declare attributes to be set in callback_connector on_trainer_init\n",
                "        self.callback_connector.on_trainer_init(\n",
                "            callbacks, checkpoint_callback, progress_bar_refresh_rate, process_position, default_root_dir,\n",
                "            weights_save_path, resume_from_checkpoint, stochastic_weight_avg\n",
                "        )\n",
                "\n",
                "        # hook\n",
                "        self.on_init_start()\n",
                "\n",
                "        # init optimizer + lr scheduler related flags\n",
                "        self.optimizer_connector.on_trainer_init()\n",
                "\n",
                "        # init data flags\n",
                "        self.data_connector.on_trainer_init(\n",
                "            check_val_every_n_epoch, reload_dataloaders_every_epoch, prepare_data_per_node\n",
                "        )\n",
                "\n",
                "        # init training tricks\n",
                "        self.training_tricks_connector.on_trainer_init(\n",
                "            gradient_clip_val, track_grad_norm, accumulate_grad_batches, truncated_bptt_steps, terminate_on_nan\n",
                "        )\n",
                "        self.train_loop.on_trainer_init(\n",
                "            max_epochs,\n",
                "            min_epochs,\n",
                "            max_steps,\n",
                "            min_steps,\n",
                "            num_sanity_val_steps,\n"
            ],
            {
                "type": "delete",
                "before": [
                    "            weights_summary,\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 354,
                    "end": 355
                },
                "child_version_range": {
                    "start": 358,
                    "end": 358
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "Trainer",
                        "signature": "class Trainer(\n    TrainerProperties,\n    TrainerCallbackHookMixin,\n    TrainerModelHooksMixin,\n    TrainerOptimizersMixin,\n    TrainerLoggingMixin,\n    TrainerTrainingTricksMixin,\n    TrainerDataLoadingMixin,\n    DeprecatedDistDeviceAttributes,\n    DeprecatedTrainerAttributes,\n):",
                        "at_line": 74
                    },
                    {
                        "type": "function",
                        "name": "__init__",
                        "signature": "def __init__(\n        self,\n        logger: Union[LightningLoggerBase, Iterable[LightningLoggerBase], bool] = True,\n        checkpoint_callback: bool = True,\n        callbacks: Optional[Union[List[Callback], Callback]] = None,\n        default_root_dir: Optional[str] = None,\n        gradient_clip_val: float = 0,\n        process_position: int = 0,\n        num_nodes: int = 1,\n        num_processes: int = 1,\n        gpus: Optional[Union[List[int], str, int]] = None,\n        auto_select_gpus: bool = False,\n        tpu_cores: Optional[Union[List[int], str, int]] = None,\n        log_gpu_memory: Optional[str] = None,\n        progress_bar_refresh_rate: Optional[int] = None,\n        overfit_batches: Union[int, float] = 0.0,\n        track_grad_norm: Union[int, float, str] = -1,\n        check_val_every_n_epoch: int = 1,\n        fast_dev_run: Union[int, bool] = False,\n        accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1,\n        max_epochs: Optional[int] = None,\n        min_epochs: Optional[int] = None,\n        max_steps: Optional[int] = None,\n        min_steps: Optional[int] = None,\n        limit_train_batches: Union[int, float] = 1.0,\n        limit_val_batches: Union[int, float] = 1.0,\n        limit_test_batches: Union[int, float] = 1.0,\n        limit_predict_batches: Union[int, float] = 1.0,\n        val_check_interval: Union[int, float] = 1.0,\n        flush_logs_every_n_steps: int = 100,\n        log_every_n_steps: int = 50,\n        accelerator: Optional[Union[str, Accelerator]] = None,\n        sync_batchnorm: bool = False,\n        precision: int = 32,\n        weights_summary: Optional[str] = 'top',\n        weights_save_path: Optional[str] = None,\n        num_sanity_val_steps: int = 2,\n        truncated_bptt_steps: Optional[int] = None,\n        resume_from_checkpoint: Optional[Union[Path, str]] = None,\n        profiler: Optional[Union[BaseProfiler, str]] = None,\n        benchmark: bool = False,\n        deterministic: bool = False,\n        reload_dataloaders_every_epoch: bool = False,\n        auto_lr_find: Union[bool, str] = False,\n        replace_sampler_ddp: bool = True,\n        terminate_on_nan: bool = False,\n        auto_scale_batch_size: Union[str, bool] = False,\n        prepare_data_per_node: bool = True,\n        plugins: Optional[Union[Plugin, str, list]] = None,\n        amp_backend: str = 'native',\n        amp_level: str = 'O2',\n        distributed_backend: Optional[str] = None,\n        move_metrics_to_cpu: bool = False,\n        multiple_trainloader_mode: str = 'max_size_cycle',\n        stochastic_weight_avg: bool = False\n    ):",
                        "at_line": 87
                    },
                    {
                        "type": "call",
                        "name": "self.train_loop.on_trainer_init",
                        "signature": "self.train_loop.on_trainer_init(\n            max_epochs,\n            min_epochs,\n            max_steps,\n            min_steps,\n            num_sanity_val_steps,\n            weights_summary,\n        )",
                        "at_line": 348,
                        "argument": "weights_summary"
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: pytorch_lightning/trainer/trainer.py\nCode:\n           class Trainer(\n    TrainerProperties,\n    TrainerCallbackHookMixin,\n    TrainerModelHooksMixin,\n    TrainerOptimizersMixin,\n    TrainerLoggingMixin,\n    TrainerTrainingTricksMixin,\n    TrainerDataLoadingMixin,\n    DeprecatedDistDeviceAttributes,\n    DeprecatedTrainerAttributes,\n):\n               ...\n               def __init__(\n        self,\n        logger: Union[LightningLoggerBase, Iterable[LightningLoggerBase], bool] = True,\n        checkpoint_callback: bool = True,\n        callbacks: Optional[Union[List[Callback], Callback]] = None,\n        default_root_dir: Optional[str] = None,\n        gradient_clip_val: float = 0,\n        process_position: int = 0,\n        num_nodes: int = 1,\n        num_processes: int = 1,\n        gpus: Optional[Union[List[int], str, int]] = None,\n        auto_select_gpus: bool = False,\n        tpu_cores: Optional[Union[List[int], str, int]] = None,\n        log_gpu_memory: Optional[str] = None,\n        progress_bar_refresh_rate: Optional[int] = None,\n        overfit_batches: Union[int, float] = 0.0,\n        track_grad_norm: Union[int, float, str] = -1,\n        check_val_every_n_epoch: int = 1,\n        fast_dev_run: Union[int, bool] = False,\n        accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1,\n        max_epochs: Optional[int] = None,\n        min_epochs: Optional[int] = None,\n        max_steps: Optional[int] = None,\n        min_steps: Optional[int] = None,\n        limit_train_batches: Union[int, float] = 1.0,\n        limit_val_batches: Union[int, float] = 1.0,\n        limit_test_batches: Union[int, float] = 1.0,\n        limit_predict_batches: Union[int, float] = 1.0,\n        val_check_interval: Union[int, float] = 1.0,\n        flush_logs_every_n_steps: int = 100,\n        log_every_n_steps: int = 50,\n        accelerator: Optional[Union[str, Accelerator]] = None,\n        sync_batchnorm: bool = False,\n        precision: int = 32,\n        weights_summary: Optional[str] = 'top',\n        weights_save_path: Optional[str] = None,\n        num_sanity_val_steps: int = 2,\n        truncated_bptt_steps: Optional[int] = None,\n        resume_from_checkpoint: Optional[Union[Path, str]] = None,\n        profiler: Optional[Union[BaseProfiler, str]] = None,\n        benchmark: bool = False,\n        deterministic: bool = False,\n        reload_dataloaders_every_epoch: bool = False,\n        auto_lr_find: Union[bool, str] = False,\n        replace_sampler_ddp: bool = True,\n        terminate_on_nan: bool = False,\n        auto_scale_batch_size: Union[str, bool] = False,\n        prepare_data_per_node: bool = True,\n        plugins: Optional[Union[Plugin, str, list]] = None,\n        amp_backend: str = 'native',\n        amp_level: str = 'O2',\n        distributed_backend: Optional[str] = None,\n        move_metrics_to_cpu: bool = False,\n        multiple_trainloader_mode: str = 'max_size_cycle',\n        stochastic_weight_avg: bool = False\n    ):\n                   ...\n                   self.train_loop.on_trainer_init(\n            max_epochs,\n            min_epochs,\n            max_steps,\n            min_steps,\n            num_sanity_val_steps,\n            weights_summary,\n        )\n                       ...\n351 355                max_steps,\n352 356                min_steps,\n353 357                num_sanity_val_steps,\n354      -             weights_summary,\n355 358            )\n356 359            self.evaluation_loop.on_trainer_init()\n357 360    \n         ...\n",
                "file_path": "pytorch_lightning/trainer/trainer.py",
                "identifiers_before": [
                    "weights_summary"
                ],
                "identifiers_after": [],
                "prefix": [
                    "            max_steps,\n",
                    "            min_steps,\n",
                    "            num_sanity_val_steps,\n"
                ],
                "suffix": [
                    "        )\n",
                    "        self.evaluation_loop.on_trainer_init()\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        )\n",
                "        self.evaluation_loop.on_trainer_init()\n",
                "\n",
                "        # configure tuner\n",
                "        self.tuner.on_trainer_init(auto_lr_find, auto_scale_batch_size)\n",
                "\n",
                "        # configure profiler\n",
                "        self.profile_connector.on_trainer_init(profiler)\n",
                "\n",
                "        # init logger flags\n",
                "        self.logger_connector.on_trainer_init(\n",
                "            logger,\n",
                "            flush_logs_every_n_steps,\n",
                "            log_every_n_steps,\n",
                "            move_metrics_to_cpu,\n",
                "        )\n",
                "\n",
                "        # init debugging flags\n",
                "        self.debugging_connector.on_init_start(\n",
                "            limit_train_batches,\n",
                "            limit_val_batches,\n",
                "            limit_test_batches,\n",
                "            limit_predict_batches,\n",
                "            val_check_interval,\n",
                "            overfit_batches,\n",
                "            fast_dev_run,\n",
                "        )\n",
                "\n",
                "        # Callback system\n",
                "        self.on_init_end()\n",
                "\n",
                "    def fit(\n",
                "        self,\n",
                "        model: LightningModule,\n",
                "        train_dataloader: Any = None,\n",
                "        val_dataloaders: Optional[Union[DataLoader, List[DataLoader]]] = None,\n",
                "        datamodule: Optional[LightningDataModule] = None,\n",
                "    ):\n",
                "        r\"\"\"\n",
                "        Runs the full optimization routine.\n",
                "\n",
                "        Args:\n",
                "            datamodule: A instance of :class:`LightningDataModule`.\n",
                "\n",
                "            model: Model to fit.\n",
                "\n",
                "            train_dataloader: Either a single PyTorch DataLoader or a collection of these\n",
                "                (list, dict, nested lists and dicts). In the case of multiple dataloaders, please\n",
                "                see this :ref:`page <multiple-training-dataloaders>`\n",
                "\n",
                "            val_dataloaders: Either a single Pytorch Dataloader or a list of them, specifying validation samples.\n",
                "                If the model has a predefined val_dataloaders method this will be skipped\n",
                "\n",
                "        \"\"\"\n",
                "        # we reuse fit for other functions. When already set, it shouldn't be modified.\n",
                "        if not self.state.running:\n",
                "            self.state = TrainerState.FITTING\n",
                "        if self._running_stage is None:\n",
                "            self.training = True\n",
                "\n",
                "        # set local properties on the model\n",
                "        self.model_connector.copy_trainer_model_properties(model)\n",
                "\n",
                "        # ----------------------------\n",
                "        # LINK DATA\n",
                "        # ----------------------------\n",
                "        # setup data, etc...\n",
                "        self.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\n",
                "\n",
                "        # hook\n",
                "        self.data_connector.prepare_data(model)\n",
                "        self.callback_connector._attach_model_callbacks(model, self)\n",
                "\n",
                "        # ----------------------------\n",
                "        # SET UP TRAINING\n",
                "        # ----------------------------\n",
                "        self.call_hook(\"on_before_accelerator_backend_setup\", model)\n",
                "        self.accelerator.connect(model)\n",
                "        self.accelerator.setup_environment()\n",
                "        self.call_setup_hook(model)  # allow user to setup lightning_module in accelerator environment\n",
                "        self.accelerator.setup(self, model)  # note: this sets up self.lightning_module\n",
                "\n",
                "        # ----------------------------\n",
                "        # INSPECT THE CORE LOOPS\n",
                "        # ----------------------------\n",
                "        f\"\"\"\n",
                "             Lightning internal flow looks like this:\n",
                "        {Trainer.fit} or {Trainer.test} or {Trainer.predict}  ||\n",
                "                                |                             ||\n",
                "                        create accelerator                    ||\n",
                "                                |                             ||\n",
                "                         {self.dispatch}                      ||\n",
                "                                |                             ||  LIGHTNING\n",
                "                  {self.accelerator.start_training}           ||\n",
                "                or {self.accelerator.start_evaluating}        ||\n",
                "                or {self.accelerator.start_predicting}        ||  FLOW\n",
                "                                |                             ||\n",
                "                         {self.run_stage}                     ||\n",
                "                                |                             ||  DIRECTION\n",
                "                        {self.run_train}                      ||\n",
                "                     or {self.run_evaluation}                 ||\n",
                "                     or {self.run_predict}                    ||\n",
                "                                |                             ||\n",
                "                             results                          \\/\n",
                "        This is used to guide readers to the core loops: train, test, predict.\n",
                "        {self.run_predict} is the simplest to understand, use `Go to Definition` to read it :)\n",
                "        Search for `start_training` or `start_evaluating` or `start_predicting` in\n",
                "        `pytorch_lightning/plugins/training_type_plugin` to find accelerator dispatch functions.\n",
                "        \"\"\"  # noqa: W605\n",
                "\n",
                "        # ----------------------------\n",
                "        # TRAIN\n",
                "        # ----------------------------\n",
                "        # hook\n",
                "        if self.state == TrainerState.FITTING:\n",
                "            self.call_hook(\"on_fit_start\")\n",
                "\n",
                "        # plugin will setup fitting (e.g. ddp will launch child processes)\n",
                "        self.pre_dispatch()\n",
                "\n",
                "        # dispatch `start_training` or `start_evaluating` or `start_predicting`\n",
                "        self.dispatch()\n",
                "\n",
                "        # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\n",
                "        self.post_dispatch()\n",
                "\n",
                "        # ----------------------------\n",
                "        # POST-Training CLEAN UP\n",
                "        # ----------------------------\n",
                "        # hook\n",
                "        if self.state == TrainerState.FITTING:\n",
                "            self.call_hook('on_fit_end')\n",
                "\n",
                "        # teardown\n",
                "        self.call_teardown_hook(model)\n",
                "\n",
                "        if self.state != TrainerState.INTERRUPTED:\n",
                "            self.state = TrainerState.FINISHED\n",
                "        self._running_stage = None\n",
                "\n",
                "        # return 1 when finished\n",
                "        # used for testing or when we need to know that training succeeded\n",
                "        return self.accelerator.results or 1\n",
                "\n",
                "    def pre_dispatch(self):\n",
                "        self.accelerator.pre_dispatch(self)\n",
                "\n",
                "        # log hyper-parameters\n",
                "        if self.logger is not None:\n",
                "            # save exp to get started (this is where the first experiment logs are written)\n",
                "            self.logger.log_hyperparams(self.lightning_module.hparams_initial)\n",
                "            self.logger.log_graph(self.lightning_module)\n",
                "            self.logger.save()\n",
                "\n",
                "    def post_dispatch(self):\n",
                "        self.accelerator.post_dispatch(self)\n",
                "        self.accelerator.teardown()\n",
                "\n",
                "    def dispatch(self):\n",
                "        if self.evaluating:\n",
                "            self.accelerator.start_evaluating(self)\n",
                "        elif self.predicting:\n",
                "            self.accelerator.start_predicting(self)\n",
                "        else:\n",
                "            self.accelerator.start_training(self)\n",
                "\n",
                "    def run_stage(self):\n",
                "        results = None\n",
                "\n",
                "        self.profile_connector.setup()\n",
                "\n",
                "        if self.evaluating:\n",
                "            results = self.run_evaluate()\n",
                "        elif self.predicting:\n",
                "            results = self.run_predict()\n",
                "        else:\n",
                "            self.run_train()\n",
                "        return results\n",
                "\n",
                "    def _pre_training_routine(self):\n",
                "        # wait for all to join if on distributed\n",
                "        self.accelerator.barrier(\"setup_training\")\n",
                "\n",
                "        # register auto-resubmit when on SLURM\n",
                "        self.slurm_connector.register_slurm_signal_handlers()\n",
                "\n",
                "        # --------------------------\n",
                "        # Pre-train\n",
                "        # --------------------------\n",
                "        # on pretrain routine start\n",
                "        ref_model = self.lightning_module\n",
                "\n",
                "        self.on_pretrain_routine_start()\n",
                "        ref_model.on_pretrain_routine_start()\n",
                "\n",
                "        # print model summary\n",
                "        if self.is_global_zero and self.weights_summary is not None and not self.testing:\n"
            ],
            {
                "type": "replace",
                "before": [
                    "            if self.weights_summary in ModelSummary.MODES:\n",
                    "                ref_model.summarize(mode=self.weights_summary)\n",
                    "            else:\n",
                    "                raise MisconfigurationException(\"weights_summary can be None, \" + \", \".join(ModelSummary.MODES))\n"
                ],
                "after": [
                    "            ref_model.summarize(mode=self.weights_summary)\n"
                ],
                "parent_version_range": {
                    "start": 552,
                    "end": 556
                },
                "child_version_range": {
                    "start": 555,
                    "end": 556
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if self.is_global_zero and self.weights_summary is not None and not self.testing:",
                        "start_line": 551,
                        "end_line": 555
                    },
                    {
                        "type": "if_statement",
                        "statement": "if self.weights_summary in ModelSummary.MODES:",
                        "start_line": 552,
                        "end_line": 555
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "Trainer",
                        "signature": "class Trainer(\n    TrainerProperties,\n    TrainerCallbackHookMixin,\n    TrainerModelHooksMixin,\n    TrainerOptimizersMixin,\n    TrainerLoggingMixin,\n    TrainerTrainingTricksMixin,\n    TrainerDataLoadingMixin,\n    DeprecatedDistDeviceAttributes,\n    DeprecatedTrainerAttributes,\n):",
                        "at_line": 74
                    },
                    {
                        "type": "function",
                        "name": "_pre_training_routine",
                        "signature": "def _pre_training_routine(self):",
                        "at_line": 534
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: pytorch_lightning/trainer/trainer.py\nCode:\n           class Trainer(\n    TrainerProperties,\n    TrainerCallbackHookMixin,\n    TrainerModelHooksMixin,\n    TrainerOptimizersMixin,\n    TrainerLoggingMixin,\n    TrainerTrainingTricksMixin,\n    TrainerDataLoadingMixin,\n    DeprecatedDistDeviceAttributes,\n    DeprecatedTrainerAttributes,\n):\n               ...\n               def _pre_training_routine(self):\n                   ...\n549 552    \n550 553            # print model summary\n551 554            if self.is_global_zero and self.weights_summary is not None and not self.testing:\n552      -             if self.weights_summary in ModelSummary.MODES:\n553      -                 ref_model.summarize(mode=self.weights_summary)\n554      -             else:\n555      -                 raise MisconfigurationException(\"weights_summary can be None, \" + \", \".join(ModelSummary.MODES))\n    555  +             ref_model.summarize(mode=self.weights_summary)\n556 556    \n557 557            # restore training and model before hpc is called\n558 558            self.checkpoint_connector.restore_weights()\n         ...\n",
                "file_path": "pytorch_lightning/trainer/trainer.py",
                "identifiers_before": [
                    "MODES",
                    "MisconfigurationException",
                    "ModelSummary",
                    "join",
                    "mode",
                    "ref_model",
                    "self",
                    "summarize",
                    "weights_summary"
                ],
                "identifiers_after": [
                    "mode",
                    "ref_model",
                    "self",
                    "summarize",
                    "weights_summary"
                ],
                "prefix": [
                    "\n",
                    "        # print model summary\n",
                    "        if self.is_global_zero and self.weights_summary is not None and not self.testing:\n"
                ],
                "suffix": [
                    "\n",
                    "        # restore training and model before hpc is called\n",
                    "        self.checkpoint_connector.restore_weights()\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "\n",
                "        # restore training and model before hpc is called\n",
                "        self.checkpoint_connector.restore_weights()\n",
                "\n",
                "        # on pretrain routine end\n",
                "        self.on_pretrain_routine_end()\n",
                "        ref_model.on_pretrain_routine_end()\n",
                "\n",
                "    def run_train(self) -> None:\n",
                "\n",
                "        self._pre_training_routine()\n",
                "\n",
                "        if not self.is_global_zero and self.progress_bar_callback is not None:\n",
                "            self.progress_bar_callback.disable()\n",
                "\n",
                "        self.run_sanity_check(self.lightning_module)\n",
                "\n",
                "        self.checkpoint_connector.has_trained = False\n",
                "\n",
                "        # enable train mode\n",
                "        model = self.lightning_module\n",
                "        model.train()\n",
                "        torch.set_grad_enabled(True)\n",
                "\n",
                "        # reload data when needed\n",
                "        self.train_loop.reset_train_val_dataloaders(model)\n",
                "\n",
                "        # hook\n",
                "        self.train_loop.on_train_start()\n",
                "\n",
                "        try:\n",
                "            if self.train_loop.should_skip_training():\n",
                "                return\n",
                "            # run all epochs\n",
                "            epochs = range(self.current_epoch, self.max_epochs) if self.max_epochs else count(self.current_epoch)\n",
                "            for epoch in epochs:\n",
                "\n",
                "                # hook\n",
                "                self.train_loop.on_train_epoch_start(epoch)\n",
                "\n",
                "                with self.profiler.profile(\"run_training_epoch\"):\n",
                "                    # run train epoch\n",
                "                    self.train_loop.run_training_epoch()\n",
                "\n",
                "                if self.max_steps and self.max_steps <= self.global_step:\n",
                "                    return\n",
                "\n",
                "                # early stopping\n",
                "                met_min_epochs = (epoch >= self.min_epochs - 1) if self.min_epochs else True\n",
                "                met_min_steps = self.global_step >= self.min_steps if self.min_steps else True\n",
                "\n",
                "                if self.should_stop:\n",
                "                    if met_min_epochs and met_min_steps:\n",
                "                        return\n",
                "                    else:\n",
                "                        log.info(\n",
                "                            'Trainer was signaled to stop but required minimum epochs'\n",
                "                            f' ({self.min_epochs}) or minimum steps ({self.min_steps}) has'\n",
                "                            ' not been met. Training will continue...'\n",
                "                        )\n",
                "\n",
                "            # hook\n",
                "            self.train_loop.on_train_end()\n",
                "\n",
                "        except KeyboardInterrupt:\n",
                "            rank_zero_warn('Detected KeyboardInterrupt, attempting graceful shutdown...')\n",
                "            # user could press Ctrl+c many times... only shutdown once\n",
                "            if not self.interrupted:\n",
                "                self.state = TrainerState.INTERRUPTED\n",
                "                self.on_keyboard_interrupt()\n",
                "        except (RuntimeError, AssertionError):\n",
                "            # if an exception is raised, the finally block is executed and can hide the actual exception\n",
                "            # that was initially raised if `on_train_end` also raises an exception. we want to avoid that\n",
                "            # for assertions and other runtime errors so we aren't misled while debugging\n",
                "            print_exc()\n",
                "        finally:\n",
                "            # hook\n",
                "            self.train_loop.on_train_end()\n",
                "\n",
                "    def run_evaluation(self, on_epoch=False):\n",
                "        if not (self.evaluating or self.sanity_checking):\n",
                "            rank_zero_warn(\n",
                "                f\"`trainer.run_evaluation()` was called but the running stage is set to {self._running_stage}.\"\n",
                "                \" This should not happen normally. Setting it to `RunningStage.VALIDATING`\", RuntimeWarning\n",
                "            )\n",
                "            self.validating = True\n",
                "\n",
                "        # reset cached results\n",
                "        self.logger_connector.reset()\n",
                "\n",
                "        # prepare dataloaders\n",
                "        dataloaders, max_batches = self.evaluation_loop.get_evaluation_dataloaders()\n",
                "\n",
                "        # check if we want to skip this evaluation\n",
                "        if self.evaluation_loop.should_skip_evaluation(max_batches):\n",
                "            return [], []\n",
                "\n",
                "        # enable eval mode + no grads\n",
                "        self.evaluation_loop.on_evaluation_model_eval()\n",
                "        # ref model\n",
                "        model = self.lightning_module\n",
                "        model.zero_grad()\n",
                "        torch.set_grad_enabled(False)\n",
                "\n",
                "        # hook\n",
                "        self.evaluation_loop.on_evaluation_start()\n",
                "\n",
                "        # set up the eval loop\n",
                "        self.evaluation_loop.setup(model, max_batches, dataloaders)\n",
                "\n",
                "        # hook\n",
                "        self.evaluation_loop.on_evaluation_epoch_start()\n",
                "\n",
                "        # run validation/testing\n",
                "        for dataloader_idx, dataloader in enumerate(dataloaders):\n",
                "            # bookkeeping\n",
                "            dl_outputs = []\n",
                "            dataloader = self.accelerator.process_dataloader(dataloader)\n",
                "            dl_max_batches = self.evaluation_loop.max_batches[dataloader_idx]\n",
                "\n",
                "            for batch_idx, batch in enumerate(dataloader):\n",
                "                if batch is None:\n",
                "                    continue\n",
                "\n",
                "                # stop short when running on limited batches\n",
                "                if batch_idx >= dl_max_batches:\n",
                "                    break\n",
                "\n",
                "                # hook\n",
                "                self.evaluation_loop.on_evaluation_batch_start(batch, batch_idx, dataloader_idx)\n",
                "\n",
                "                # lightning module methods\n",
                "                with self.profiler.profile(\"evaluation_step_and_end\"):\n",
                "                    output = self.evaluation_loop.evaluation_step(batch, batch_idx, dataloader_idx)\n",
                "                    output = self.evaluation_loop.evaluation_step_end(output)\n",
                "\n",
                "                # hook + store predictions\n",
                "                self.evaluation_loop.on_evaluation_batch_end(output, batch, batch_idx, dataloader_idx)\n",
                "\n",
                "                # log batch metrics\n",
                "                self.evaluation_loop.log_evaluation_step_metrics(output, batch_idx)\n",
                "\n",
                "                # track epoch level outputs\n",
                "                dl_outputs = self.track_output_for_epoch_end(dl_outputs, output)\n",
                "\n",
                "            # store batch level output per dataloader\n",
                "            self.evaluation_loop.outputs.append(dl_outputs)\n",
                "\n",
                "        # lightning module method\n",
                "        deprecated_eval_results = self.evaluation_loop.evaluation_epoch_end()\n",
                "\n",
                "        # hook\n",
                "        self.evaluation_loop.on_evaluation_epoch_end()\n",
                "\n",
                "        # update epoch-level lr_schedulers\n",
                "        if on_epoch:\n",
                "            self.optimizer_connector.update_learning_rates(interval='epoch')\n",
                "\n",
                "        # hook\n",
                "        self.evaluation_loop.on_evaluation_end()\n",
                "\n",
                "        # log epoch metrics\n",
                "        eval_loop_results = self.evaluation_loop.log_epoch_metrics_on_evaluation_end()\n",
                "\n",
                "        # save predictions to disk\n",
                "        self.evaluation_loop.predictions.to_disk()\n",
                "\n",
                "        # enable train mode again\n",
                "        self.evaluation_loop.on_evaluation_model_train()\n",
                "\n",
                "        torch.set_grad_enabled(True)\n",
                "\n",
                "        return eval_loop_results, deprecated_eval_results\n",
                "\n",
                "    def track_output_for_epoch_end(self, outputs, output):\n",
                "        if output is not None:\n",
                "            if isinstance(output, Result):\n",
                "                output = output.detach()\n",
                "                if self.move_metrics_to_cpu:\n",
                "                    output = output.cpu()\n",
                "            elif isinstance(output, dict):\n",
                "                output = recursive_detach(output, to_cpu=self.move_metrics_to_cpu)\n",
                "            elif isinstance(output, torch.Tensor) and output.is_cuda and self.move_metrics_to_cpu:\n",
                "                output = output.cpu()\n",
                "            outputs.append(output)\n",
                "        return outputs\n",
                "\n",
                "    def run_evaluate(self):\n",
                "        if not self.is_global_zero and self.progress_bar_callback is not None:\n",
                "            self.progress_bar_callback.disable()\n",
                "\n",
                "        assert self.evaluating\n",
                "\n",
                "        with self.profiler.profile(f\"run_{self._running_stage}_evaluation\"):\n",
                "            eval_loop_results, _ = self.run_evaluation()\n",
                "\n",
                "        if len(eval_loop_results) == 0:\n",
                "            return 1\n",
                "\n",
                "        # remove the tensors from the eval results\n",
                "        for i, result in enumerate(eval_loop_results):\n",
                "            if isinstance(result, dict):\n",
                "                for k, v in result.items():\n",
                "                    if isinstance(v, torch.Tensor):\n",
                "                        result[k] = v.cpu().item()\n",
                "\n",
                "        return eval_loop_results\n",
                "\n",
                "    def run_predict(self):\n",
                "        self.predict_loop.on_predict_start()\n",
                "\n",
                "        # prepare dataloaders\n",
                "        dataloaders, max_batches = self.predict_loop.get_predict_dataloaders()\n",
                "\n",
                "        # check if we want to skip this evaluation\n",
                "        if self.predict_loop.should_skip_predict(max_batches):\n",
                "            return []\n",
                "\n",
                "        # ref model\n",
                "        model = self.lightning_module\n",
                "\n",
                "        # enable eval mode + no grads\n",
                "        self.predict_loop.on_predict_model_eval()\n",
                "        model.zero_grad()\n",
                "        torch.set_grad_enabled(False)\n",
                "\n",
                "        # set up the eval loop\n",
                "        self.predict_loop.setup(model, max_batches, dataloaders)\n",
                "\n",
                "        # run validation/testing\n",
                "        for dataloader_idx, dataloader in enumerate(dataloaders):\n",
                "            dataloader = self.accelerator.process_dataloader(dataloader)\n",
                "            dl_max_batches = self.predict_loop.max_batches[dataloader_idx]\n",
                "            for batch_idx, batch in enumerate(dataloader):\n",
                "                if batch is None:\n",
                "                    continue\n",
                "\n",
                "                # stop short when running on limited batches\n",
                "                if batch_idx >= dl_max_batches:\n",
                "                    break\n",
                "\n",
                "                # lightning module methods\n",
                "                with self.profiler.profile(\"predict_step\"):\n",
                "                    self.predict_loop.predict_step(batch, batch_idx, dataloader_idx)\n",
                "\n",
                "        results = self.predict_loop.on_predict_epoch_end()\n",
                "        self.predict_loop.on_predict_end()\n",
                "\n",
                "        # re-enable grads\n",
                "        torch.set_grad_enabled(True)\n",
                "\n",
                "        return results\n",
                "\n",
                "    def run_sanity_check(self, ref_model):\n",
                "        using_val_step = ref_model.val_dataloader is not None and is_overridden('validation_step', ref_model)\n",
                "        should_sanity_check = using_val_step and self.num_sanity_val_steps > 0 and self.limit_val_batches > 0\n",
                "\n",
                "        # run tiny validation (if validation defined)\n",
                "        # to make sure program won't crash during val\n",
                "        if should_sanity_check:\n",
                "            stage = self._running_stage\n",
                "            self.sanity_checking = True\n",
                "\n",
                "            # hook and callback\n",
                "            self.on_sanity_check_start()\n",
                "\n",
                "            # run eval step\n",
                "            _, eval_results = self.run_evaluation()\n",
                "\n",
                "            # allow no returns from eval\n",
                "            if eval_results is not None and len(eval_results) > 0:\n",
                "                # when we get a list back, used only the last item\n",
                "                if isinstance(eval_results, list):\n",
                "                    eval_results = eval_results[-1]\n",
                "\n",
                "                _, _, _, callback_metrics, _ = self.process_dict_result(eval_results)\n",
                "                self.logger_connector.callback_metrics = callback_metrics\n",
                "\n",
                "            self.on_sanity_check_end()\n",
                "\n",
                "            self._running_stage = stage\n",
                "\n",
                "    def validate(\n",
                "        self,\n",
                "        model: Optional[LightningModule] = None,\n",
                "        val_dataloaders: Optional[Union[DataLoader, List[DataLoader]]] = None,\n",
                "        ckpt_path: Optional[str] = 'best',\n",
                "        verbose: bool = True,\n",
                "        datamodule: Optional[LightningDataModule] = None,\n",
                "    ):\n",
                "        r\"\"\"\n",
                "        Perform one evaluation epoch over the validation set.\n",
                "\n",
                "        Args:\n",
                "            model: The model to validate.\n",
                "\n",
                "            val_dataloaders: Either a single PyTorch DataLoader or a list of them,\n",
                "                specifying validation samples.\n",
                "\n",
                "            ckpt_path: Either ``best`` or path to the checkpoint you wish to validate.\n",
                "                If ``None``, use the current weights of the model.\n",
                "                When the model is given as argument, this parameter will not apply.\n",
                "\n",
                "            verbose: If True, prints the validation results.\n",
                "\n",
                "            datamodule: A instance of :class:`LightningDataModule`.\n",
                "\n",
                "        Returns:\n",
                "            The dictionary with final validation results returned by validation_epoch_end.\n",
                "            If validation_epoch_end is not defined, the output is a list of the dictionaries\n",
                "            returned by validation_step.\n",
                "        \"\"\"\n",
                "        # --------------------\n",
                "        # SETUP HOOK\n",
                "        # --------------------\n",
                "        self.verbose_evaluate = verbose\n",
                "\n",
                "        self.state = TrainerState.VALIDATING\n",
                "        self.validating = True\n",
                "\n",
                "        # If you supply a datamodule you can't supply val_dataloaders\n",
                "        if val_dataloaders and datamodule:\n",
                "            raise MisconfigurationException(\n",
                "                'You cannot pass both `trainer.validate(val_dataloaders=..., datamodule=...)`'\n",
                "            )\n",
                "\n",
                "        model_provided = model is not None\n",
                "        model = model or self.lightning_module\n",
                "\n",
                "        # Attach datamodule to get setup/prepare_data added to model before the call to it below\n",
                "        self.data_connector.attach_datamodule(model, datamodule)\n",
                "        #  Attach dataloaders (if given)\n",
                "        self.data_connector.attach_dataloaders(model, val_dataloaders=val_dataloaders)\n",
                "\n",
                "        if not model_provided:\n",
                "            self.validated_ckpt_path = self.__load_ckpt_weights(model, ckpt_path=ckpt_path)\n",
                "\n",
                "        # run validate\n",
                "        results = self.fit(model)\n",
                "\n",
                "        assert self.state.stopped\n",
                "        self.validating = False\n",
                "\n",
                "        return results\n",
                "\n",
                "    def test(\n",
                "        self,\n",
                "        model: Optional[LightningModule] = None,\n",
                "        test_dataloaders: Optional[Union[DataLoader, List[DataLoader]]] = None,\n",
                "        ckpt_path: Optional[str] = 'best',\n",
                "        verbose: bool = True,\n",
                "        datamodule: Optional[LightningDataModule] = None,\n",
                "    ):\n",
                "        r\"\"\"\n",
                "        Perform one evaluation epoch over the test set. It's separated from\n",
                "        fit to make sure you never run on your test set until you want to.\n",
                "\n",
                "        Args:\n",
                "            model: The model to test.\n",
                "\n",
                "            test_dataloaders: Either a single PyTorch DataLoader or a list of them,\n",
                "                specifying test samples.\n",
                "\n",
                "            ckpt_path: Either ``best`` or path to the checkpoint you wish to test.\n",
                "                If ``None``, use the current weights of the model.\n",
                "                When the model is given as argument, this parameter will not apply.\n",
                "\n",
                "            verbose: If True, prints the test results.\n",
                "\n",
                "            datamodule: A instance of :class:`LightningDataModule`.\n",
                "\n",
                "        Returns:\n",
                "            Returns a list of dictionaries, one for each test dataloader containing their respective metrics.\n",
                "        \"\"\"\n",
                "        # --------------------\n",
                "        # SETUP HOOK\n",
                "        # --------------------\n",
                "        self.verbose_evaluate = verbose\n",
                "\n",
                "        self.state = TrainerState.TESTING\n",
                "        self.testing = True\n",
                "\n",
                "        # If you supply a datamodule you can't supply test_dataloaders\n",
                "        if test_dataloaders and datamodule:\n",
                "            raise MisconfigurationException('You cannot pass both `trainer.test(test_dataloaders=..., datamodule=...)`')\n",
                "\n",
                "        model_provided = model is not None\n",
                "        model = model or self.lightning_module\n",
                "\n",
                "        # Attach datamodule to get setup/prepare_data added to model before the call to it below\n",
                "        self.data_connector.attach_datamodule(model, datamodule)\n",
                "        #  Attach dataloaders (if given)\n",
                "        self.data_connector.attach_dataloaders(model, test_dataloaders=test_dataloaders)\n",
                "\n",
                "        if not model_provided:\n",
                "            self.tested_ckpt_path = self.__load_ckpt_weights(model, ckpt_path=ckpt_path)\n",
                "\n",
                "        # run test\n",
                "        results = self.fit(model)\n",
                "\n",
                "        assert self.state.stopped\n",
                "        self.testing = False\n",
                "\n",
                "        return results\n",
                "\n",
                "    def __load_ckpt_weights(\n",
                "        self,\n",
                "        model,\n",
                "        ckpt_path: Optional[str] = None,\n",
                "    ) -> Optional[str]:\n",
                "        # if user requests the best checkpoint but we don't have it, error\n",
                "        if ckpt_path == 'best' and not self.checkpoint_callback.best_model_path:\n",
                "            raise MisconfigurationException(\n",
                "                'ckpt_path is \"best\", but `ModelCheckpoint` is not configured to save the best model.'\n",
                "            )\n",
                "\n",
                "        # load best weights\n",
                "        if ckpt_path is not None:\n",
                "            # ckpt_path is 'best' so load the best model\n",
                "            if ckpt_path == 'best':\n",
                "                ckpt_path = self.checkpoint_callback.best_model_path\n",
                "\n",
                "            if not ckpt_path:\n",
                "                fn = self.state.value\n",
                "                raise MisconfigurationException(\n",
                "                    f'`.{fn}()` found no path for the best weights: \"{ckpt_path}\". Please'\n",
                "                    ' specify a path for a checkpoint `.{fn}(ckpt_path=PATH)`'\n",
                "                )\n",
                "\n",
                "            self.training_type_plugin.barrier()\n",
                "\n",
                "            ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)\n",
                "            model.load_state_dict(ckpt['state_dict'])\n",
                "        return ckpt_path\n",
                "\n",
                "    def predict(\n",
                "        self,\n",
                "        model: Optional[LightningModule] = None,\n",
                "        dataloaders: Optional[Union[DataLoader, List[DataLoader]]] = None,\n",
                "        datamodule: Optional[LightningDataModule] = None,\n",
                "    ):\n",
                "        r\"\"\"\n",
                "\n",
                "        Separates from fit to make sure you never run on your predictions set until you want to.\n",
                "\n",
                "        This will call the model forward function to compute predictions.\n",
                "\n",
                "        Args:\n",
                "            model: The model to predict on.\n",
                "\n",
                "            dataloaders: Either a single\n",
                "                Pytorch Dataloader or a list of them, specifying inference samples.\n",
                "\n",
                "            datamodule: A instance of :class:`LightningDataModule`.\n",
                "\n",
                "        Returns:\n",
                "            Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\n",
                "        \"\"\"\n",
                "\n",
                "        # --------------------\n",
                "        # SETUP HOOK\n",
                "        # --------------------\n",
                "        # If you supply a datamodule you can't supply dataloaders\n",
                "\n",
                "        model = model or self.lightning_module\n",
                "\n",
                "        self.state = TrainerState.PREDICTING\n",
                "        self.predicting = True\n",
                "\n",
                "        if dataloaders and datamodule:\n",
                "            raise MisconfigurationException(\n",
                "                'You cannot pass dataloaders to trainer.predict if you supply a datamodule.'\n",
                "            )\n",
                "\n",
                "        # Attach datamodule to get setup/prepare_data added to model before the call to it below\n",
                "        self.data_connector.attach_datamodule(model, datamodule)\n",
                "        #  Attach dataloaders (if given)\n",
                "        self.data_connector.attach_dataloaders(model, predict_dataloaders=dataloaders)\n",
                "\n",
                "        results = self.fit(model)\n",
                "\n",
                "        assert self.state.stopped\n",
                "        self.predicting = False\n",
                "\n",
                "        return results\n",
                "\n",
                "    def tune(\n",
                "        self,\n",
                "        model: LightningModule,\n",
                "        train_dataloader: Optional[DataLoader] = None,\n",
                "        val_dataloaders: Optional[Union[DataLoader, List[DataLoader]]] = None,\n",
                "        datamodule: Optional[LightningDataModule] = None,\n",
                "    ):\n",
                "        r\"\"\"\n",
                "        Runs routines to tune hyperparameters before training.\n",
                "\n",
                "        Args:\n",
                "            datamodule: A instance of :class:`LightningDataModule`.\n",
                "\n",
                "            model: Model to tune.\n",
                "\n",
                "            train_dataloader: A Pytorch DataLoader with training samples. If the model has\n",
                "                a predefined train_dataloader method this will be skipped.\n",
                "\n",
                "            val_dataloaders: Either a single Pytorch Dataloader or a list of them, specifying validation samples.\n",
                "                If the model has a predefined val_dataloaders method this will be skipped\n",
                "\n",
                "        \"\"\"\n",
                "        self.state = TrainerState.TUNING\n",
                "        self.tuning = True\n",
                "\n",
                "        self.tuner.tune(model, train_dataloader, val_dataloaders, datamodule)\n",
                "\n",
                "        assert self.state.stopped\n",
                "        self.tuning = False\n",
                "\n",
                "    def call_setup_hook(self, model: LightningModule) -> None:\n",
                "        assert self.state.running, f\"TrainerState: {self.state}\"\n",
                "        state = self._setup_state\n",
                "\n",
                "        if self.datamodule is not None:\n",
                "            called = getattr(self.datamodule, f'has_setup_{state}')\n",
                "            if not called:\n",
                "                self.datamodule.setup(stage=state)\n",
                "\n",
                "        self.setup(model, stage=state)\n",
                "        model.setup(stage=state)\n",
                "\n",
                "    def call_teardown_hook(self, model: LightningModule) -> None:\n",
                "        state = self._teardown_state\n",
                "        self.profiler.teardown(stage=state)\n",
                "        self.teardown(stage=state)\n",
                "        model.teardown(stage=state)\n",
                "\n",
                "    def _reset_result_and_set_hook_fx_name(self, hook_name):\n",
                "        # on_before_zero_grad is called within training_step\n",
                "        if \"batch_start\" in hook_name or \"on_before_zero_grad\" in hook_name:\n",
                "            return True\n",
                "        model_ref = self.lightning_module\n",
                "        if model_ref is not None:\n",
                "            # used to track current hook name called\n",
                "            model_ref._results = Result()\n",
                "            model_ref._current_hook_fx_name = hook_name\n",
                "        return False\n",
                "\n",
                "    def _cache_logged_metrics(self):\n",
                "        model_ref = self.lightning_module\n",
                "        if model_ref is not None:\n",
                "            # capture logging for this hook\n",
                "            self.logger_connector.cache_logged_metrics()\n",
                "\n",
                "    def call_hook(self, hook_name, *args, **kwargs):\n",
                "        # set hook_name to model + reset Result obj\n",
                "        skip = self._reset_result_and_set_hook_fx_name(hook_name)\n",
                "\n",
                "        # always profile hooks\n",
                "        with self.profiler.profile(hook_name):\n",
                "\n",
                "            # first call trainer hook\n",
                "            if hasattr(self, hook_name):\n",
                "                trainer_hook = getattr(self, hook_name)\n",
                "                trainer_hook(*args, **kwargs)\n",
                "\n",
                "            # next call hook in lightningModule\n",
                "            output = None\n",
                "            model_ref = self.lightning_module\n",
                "            if is_overridden(hook_name, model_ref):\n",
                "                hook_fx = getattr(model_ref, hook_name)\n",
                "                output = hook_fx(*args, **kwargs)\n",
                "\n",
                "            # if the PL module doesn't have the hook then call the accelerator\n",
                "            # used to auto-reduce things for the user with Results obj\n",
                "            elif hasattr(self.accelerator, hook_name):\n",
                "                accelerator_hook = getattr(self.accelerator, hook_name)\n",
                "                output = accelerator_hook(*args, **kwargs)\n",
                "\n",
                "        if not skip:\n",
                "            self._cache_logged_metrics()\n",
                "        return output"
            ]
        ],
        "pytorch_lightning/trainer/training_loop.py": [
            [
                "# Copyright The PyTorch Lightning team.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     http://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\n",
                "from contextlib import contextmanager, suppress\n",
                "from copy import copy, deepcopy\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "from typing import Optional\n"
                ],
                "parent_version_range": {
                    "start": 16,
                    "end": 16
                },
                "child_version_range": {
                    "start": 16,
                    "end": 17
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 3,
                "hunk_diff": "File: pytorch_lightning/trainer/training_loop.py\nCode:\n  ...\n13 13    \n14 14    from contextlib import contextmanager, suppress\n15 15    from copy import copy, deepcopy\n   16  + from typing import Optional\n16 17    \n17 18    import numpy as np\n18 19    import torch\n       ...\n",
                "file_path": "pytorch_lightning/trainer/training_loop.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "Optional",
                    "typing"
                ],
                "prefix": [
                    "\n",
                    "from contextlib import contextmanager, suppress\n",
                    "from copy import copy, deepcopy\n"
                ],
                "suffix": [
                    "\n",
                    "import numpy as np\n",
                    "import torch\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "Optional",
                            "position": {
                                "start": {
                                    "line": 16,
                                    "column": 19
                                },
                                "end": {
                                    "line": 16,
                                    "column": 27
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "Optional",
                            "position": {
                                "start": {
                                    "line": 16,
                                    "column": 19
                                },
                                "end": {
                                    "line": 16,
                                    "column": 27
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "Optional",
                            "position": {
                                "start": {
                                    "line": 16,
                                    "column": 19
                                },
                                "end": {
                                    "line": 16,
                                    "column": 27
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "Optional",
                            "position": {
                                "start": {
                                    "line": 16,
                                    "column": 19
                                },
                                "end": {
                                    "line": 16,
                                    "column": 27
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "from pytorch_lightning.callbacks import EarlyStopping\n"
            ],
            {
                "type": "delete",
                "before": [
                    "from pytorch_lightning.core.memory import ModelSummary\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 21,
                    "end": 22
                },
                "child_version_range": {
                    "start": 22,
                    "end": 22
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 4,
                "hunk_diff": "File: pytorch_lightning/trainer/training_loop.py\nCode:\n  ...\n18 19    import torch\n19 20    \n20 21    from pytorch_lightning.callbacks import EarlyStopping\n21     - from pytorch_lightning.core.memory import ModelSummary\n22 22    from pytorch_lightning.core.optimizer import LightningOptimizer\n23 23    from pytorch_lightning.core.step_result import Result\n24 24    from pytorch_lightning.plugins import ParallelPlugin\n       ...\n",
                "file_path": "pytorch_lightning/trainer/training_loop.py",
                "identifiers_before": [
                    "ModelSummary",
                    "core",
                    "memory",
                    "pytorch_lightning"
                ],
                "identifiers_after": [],
                "prefix": [
                    "import torch\n",
                    "\n",
                    "from pytorch_lightning.callbacks import EarlyStopping\n"
                ],
                "suffix": [
                    "from pytorch_lightning.core.optimizer import LightningOptimizer\n",
                    "from pytorch_lightning.core.step_result import Result\n",
                    "from pytorch_lightning.plugins import ParallelPlugin\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "ModelSummary",
                            "position": {
                                "start": {
                                    "line": 21,
                                    "column": 42
                                },
                                "end": {
                                    "line": 21,
                                    "column": 54
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "ModelSummary",
                            "position": {
                                "start": {
                                    "line": 21,
                                    "column": 42
                                },
                                "end": {
                                    "line": 21,
                                    "column": 54
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "from pytorch_lightning.core.optimizer import LightningOptimizer\n",
                "from pytorch_lightning.core.step_result import Result\n",
                "from pytorch_lightning.plugins import ParallelPlugin\n",
                "from pytorch_lightning.trainer.states import TrainerState\n",
                "from pytorch_lightning.trainer.supporters import Accumulator, TensorRunningAccum\n",
                "from pytorch_lightning.utilities import _TPU_AVAILABLE, AMPType, DeviceType, parsing\n",
                "from pytorch_lightning.utilities.distributed import rank_zero_info\n",
                "from pytorch_lightning.utilities.exceptions import MisconfigurationException\n",
                "from pytorch_lightning.utilities.memory import recursive_detach\n",
                "from pytorch_lightning.utilities.model_helpers import is_overridden\n",
                "from pytorch_lightning.utilities.parsing import AttributeDict\n",
                "from pytorch_lightning.utilities.warnings import WarningCache\n",
                "\n",
                "\n",
                "class TrainLoop:\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    def __init__(self, trainer, multiple_trainloader_mode):\n"
                ],
                "after": [
                    "    def __init__(self, trainer, multiple_trainloader_mode: str):\n"
                ],
                "parent_version_range": {
                    "start": 38,
                    "end": 39
                },
                "child_version_range": {
                    "start": 38,
                    "end": 39
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "TrainLoop",
                        "signature": "class TrainLoop:",
                        "at_line": 36
                    },
                    {
                        "type": "function",
                        "name": "__init__",
                        "signature": "def __init__(self, trainer, multiple_trainloader_mode):",
                        "at_line": 38
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: pytorch_lightning/trainer/training_loop.py\nCode:\n35 35    \n36 36    class TrainLoop:\n37 37    \n38     -     def __init__(self, trainer, multiple_trainloader_mode):\n   38  +     def __init__(self, trainer, multiple_trainloader_mode: str):\n39 39            self.trainer = trainer\n40 40            self.early_stopping_accumulator = None\n41 41            self.checkpoint_accumulator = None\n       ...\n",
                "file_path": "pytorch_lightning/trainer/training_loop.py",
                "identifiers_before": [
                    "__init__",
                    "multiple_trainloader_mode",
                    "self",
                    "trainer"
                ],
                "identifiers_after": [
                    "__init__",
                    "multiple_trainloader_mode",
                    "self",
                    "str",
                    "trainer"
                ],
                "prefix": [
                    "\n",
                    "class TrainLoop:\n",
                    "\n"
                ],
                "suffix": [
                    "        self.trainer = trainer\n",
                    "        self.early_stopping_accumulator = None\n",
                    "        self.checkpoint_accumulator = None\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        self.trainer = trainer\n",
                "        self.early_stopping_accumulator = None\n",
                "        self.checkpoint_accumulator = None\n",
                "        self.accumulated_loss = None\n",
                "        self.warning_cache = WarningCache()\n",
                "        self._teardown_already_run = False\n",
                "        self.running_loss = TensorRunningAccum(window_length=20)\n",
                "        self.automatic_optimization = True\n",
                "        self._curr_step_result = None\n",
                "        self._cur_grad_norm_dict = None\n",
                "        self._multiple_trainloader_mode = multiple_trainloader_mode\n",
                "        self._skip_backward = False\n",
                "        self.trainer._multiple_trainloader_mode = multiple_trainloader_mode\n",
                "\n",
                "    def on_trainer_init(\n",
                "        self,\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        max_epochs,\n",
                    "        min_epochs,\n",
                    "        max_steps,\n",
                    "        min_steps,\n",
                    "        num_sanity_val_steps,\n",
                    "        weights_summary,\n",
                    "    ):\n"
                ],
                "after": [
                    "        max_epochs: Optional[int],\n",
                    "        min_epochs: Optional[int],\n",
                    "        max_steps: Optional[int],\n",
                    "        min_steps: Optional[int],\n",
                    "        num_sanity_val_steps: int,\n",
                    "    ) -> None:\n"
                ],
                "parent_version_range": {
                    "start": 55,
                    "end": 62
                },
                "child_version_range": {
                    "start": 55,
                    "end": 61
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "TrainLoop",
                        "signature": "class TrainLoop:",
                        "at_line": 36
                    },
                    {
                        "type": "function",
                        "name": "on_trainer_init",
                        "signature": "def on_trainer_init(\n        self,\n        max_epochs,\n        min_epochs,\n        max_steps,\n        min_steps,\n        num_sanity_val_steps,\n        weights_summary,\n    ):",
                        "at_line": 53
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: pytorch_lightning/trainer/training_loop.py\nCode:\n         class TrainLoop:\n             ...\n52 52    \n53 53        def on_trainer_init(\n54 54            self,\n55     -         max_epochs,\n56     -         min_epochs,\n57     -         max_steps,\n58     -         min_steps,\n59     -         num_sanity_val_steps,\n60     -         weights_summary,\n61     -     ):\n   55  +         max_epochs: Optional[int],\n   56  +         min_epochs: Optional[int],\n   57  +         max_steps: Optional[int],\n   58  +         min_steps: Optional[int],\n   59  +         num_sanity_val_steps: int,\n   60  +     ) -> None:\n62 61            self.trainer.global_step = 0\n63 62            self.trainer.current_epoch = 0\n64 63            self.trainer.should_stop = False\n       ...\n",
                "file_path": "pytorch_lightning/trainer/training_loop.py",
                "identifiers_before": [
                    "max_epochs",
                    "max_steps",
                    "min_epochs",
                    "min_steps",
                    "num_sanity_val_steps",
                    "weights_summary"
                ],
                "identifiers_after": [
                    "Optional",
                    "int",
                    "max_epochs",
                    "max_steps",
                    "min_epochs",
                    "min_steps",
                    "num_sanity_val_steps"
                ],
                "prefix": [
                    "\n",
                    "    def on_trainer_init(\n",
                    "        self,\n"
                ],
                "suffix": [
                    "        self.trainer.global_step = 0\n",
                    "        self.trainer.current_epoch = 0\n",
                    "        self.trainer.should_stop = False\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "weights_summary",
                            "position": {
                                "start": {
                                    "line": 60,
                                    "column": 8
                                },
                                "end": {
                                    "line": 60,
                                    "column": 23
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "weights_summary",
                            "position": {
                                "start": {
                                    "line": 60,
                                    "column": 8
                                },
                                "end": {
                                    "line": 60,
                                    "column": 23
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "weights_summary",
                            "position": {
                                "start": {
                                    "line": 60,
                                    "column": 8
                                },
                                "end": {
                                    "line": 60,
                                    "column": 23
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 7,
                        "detail": {
                            "identifier": "weights_summary",
                            "position": {
                                "start": {
                                    "line": 60,
                                    "column": 8
                                },
                                "end": {
                                    "line": 60,
                                    "column": 23
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "Optional",
                            "position": {
                                "start": {
                                    "line": 55,
                                    "column": 20
                                },
                                "end": {
                                    "line": 55,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "Optional",
                            "position": {
                                "start": {
                                    "line": 56,
                                    "column": 20
                                },
                                "end": {
                                    "line": 56,
                                    "column": 28
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "Optional",
                            "position": {
                                "start": {
                                    "line": 57,
                                    "column": 19
                                },
                                "end": {
                                    "line": 57,
                                    "column": 27
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "Optional",
                            "position": {
                                "start": {
                                    "line": 58,
                                    "column": 19
                                },
                                "end": {
                                    "line": 58,
                                    "column": 27
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        self.trainer.global_step = 0\n",
                "        self.trainer.current_epoch = 0\n",
                "        self.trainer.should_stop = False\n",
                "        self.trainer._state = TrainerState.INITIALIZING\n",
                "\n",
                "        self.trainer.total_batch_idx = 0\n",
                "        self.trainer.batch_idx = 0\n",
                "        self.trainer.num_training_batches = 0\n",
                "        self.trainer.train_dataloader = None\n",
                "\n",
                "        # If neither max_epochs or max_steps is set, then use existing default of max_epochs = 1000\n",
                "        self.trainer.max_epochs = 1000 if (max_epochs is None and max_steps is None) else max_epochs\n",
                "        # If neither min_epochs or min_steps is set, then use existing default of min_epochs = 1\n",
                "        self.trainer.min_epochs = 1 if (min_epochs is None and min_steps is None) else min_epochs\n",
                "        self.trainer.max_steps = max_steps\n",
                "        self.trainer.min_steps = min_steps\n",
                "\n",
                "        if num_sanity_val_steps == -1:\n",
                "            self.trainer.num_sanity_val_steps = float(\"inf\")\n",
                "        else:\n",
                "            self.trainer.num_sanity_val_steps = num_sanity_val_steps\n",
                "\n"
            ],
            {
                "type": "delete",
                "before": [
                    "        self.trainer.weights_summary = weights_summary\n",
                    "        if weights_summary is not None and weights_summary not in ModelSummary.MODES:\n",
                    "            raise MisconfigurationException(\n",
                    "                f\"`weights_summary` can be None, {', '.join(ModelSummary.MODES)}, got {weights_summary}\"\n",
                    "            )\n",
                    "\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 84,
                    "end": 90
                },
                "child_version_range": {
                    "start": 83,
                    "end": 83
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "TrainLoop",
                        "signature": "class TrainLoop:",
                        "at_line": 36
                    },
                    {
                        "type": "function",
                        "name": "on_trainer_init",
                        "signature": "def on_trainer_init(\n        self,\n        max_epochs,\n        min_epochs,\n        max_steps,\n        min_steps,\n        num_sanity_val_steps,\n        weights_summary,\n    ):",
                        "at_line": 53
                    }
                ],
                "idx": 7,
                "hunk_diff": "File: pytorch_lightning/trainer/training_loop.py\nCode:\n         class TrainLoop:\n             ...\n             def on_trainer_init(\n        self,\n        max_epochs,\n        min_epochs,\n        max_steps,\n        min_steps,\n        num_sanity_val_steps,\n        weights_summary,\n    ):\n                 ...\n81 80            else:\n82 81                self.trainer.num_sanity_val_steps = num_sanity_val_steps\n83 82    \n84     -         self.trainer.weights_summary = weights_summary\n85     -         if weights_summary is not None and weights_summary not in ModelSummary.MODES:\n86     -             raise MisconfigurationException(\n87     -                 f\"`weights_summary` can be None, {', '.join(ModelSummary.MODES)}, got {weights_summary}\"\n88     -             )\n89     - \n90 83        @property\n91 84        def num_optimizers(self):\n92 85            num_optimizers = len(self.get_optimizers_iterable())\n       ...\n",
                "file_path": "pytorch_lightning/trainer/training_loop.py",
                "identifiers_before": [
                    "MODES",
                    "MisconfigurationException",
                    "ModelSummary",
                    "join",
                    "self",
                    "trainer",
                    "weights_summary"
                ],
                "identifiers_after": [],
                "prefix": [
                    "        else:\n",
                    "            self.trainer.num_sanity_val_steps = num_sanity_val_steps\n",
                    "\n"
                ],
                "suffix": [
                    "    @property\n",
                    "    def num_optimizers(self):\n",
                    "        num_optimizers = len(self.get_optimizers_iterable())\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "ModelSummary",
                            "position": {
                                "start": {
                                    "line": 85,
                                    "column": 66
                                },
                                "end": {
                                    "line": 85,
                                    "column": 78
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "ModelSummary",
                            "position": {
                                "start": {
                                    "line": 87,
                                    "column": 60
                                },
                                "end": {
                                    "line": 87,
                                    "column": 72
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "weights_summary",
                            "position": {
                                "start": {
                                    "line": 84,
                                    "column": 39
                                },
                                "end": {
                                    "line": 84,
                                    "column": 54
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "weights_summary",
                            "position": {
                                "start": {
                                    "line": 85,
                                    "column": 11
                                },
                                "end": {
                                    "line": 85,
                                    "column": 26
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "weights_summary",
                            "position": {
                                "start": {
                                    "line": 85,
                                    "column": 43
                                },
                                "end": {
                                    "line": 85,
                                    "column": 58
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "weights_summary",
                            "position": {
                                "start": {
                                    "line": 87,
                                    "column": 87
                                },
                                "end": {
                                    "line": 87,
                                    "column": 102
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py",
                            "hunk_idx": 7,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    @property\n",
                "    def num_optimizers(self):\n",
                "        num_optimizers = len(self.get_optimizers_iterable())\n",
                "        return num_optimizers\n",
                "\n",
                "    def should_skip_training(self):\n",
                "        should_by_max_steps = self.trainer.max_steps is not None and self.trainer.global_step >= self.trainer.max_steps\n",
                "        should_by_epoch = self.trainer.max_epochs is not None and self.trainer.current_epoch >= self.trainer.max_epochs\n",
                "        return should_by_max_steps or should_by_epoch or self.trainer.num_training_batches == 0\n",
                "\n",
                "    def on_train_start(self):\n",
                "        # hook\n",
                "        self.trainer.call_hook(\"on_train_start\")\n",
                "\n",
                "    def setup_fit(self, model, train_dataloader=None, val_dataloaders=None, datamodule=None):\n",
                "        # clean hparams\n",
                "        if hasattr(model, \"hparams\"):\n",
                "            parsing.clean_namespace(model.hparams)\n",
                "\n",
                "        # links data to the trainer\n",
                "        self.trainer.data_connector.attach_data(model, train_dataloader, val_dataloaders, datamodule)\n",
                "\n",
                "        # check that model is configured correctly\n",
                "        self.trainer.config_validator.verify_loop_configurations(model)\n",
                "\n",
                "        # attach model log function to callback\n",
                "        self.trainer.callback_connector.attach_model_logging_functions(model)\n",
                "\n",
                "    def on_train_end(self):\n",
                "        if self._teardown_already_run:\n",
                "            return\n",
                "        self._teardown_already_run = True\n",
                "\n",
                "        # trigger checkpoint check. need to temporarily decrease the global step to avoid saving duplicates\n",
                "        # when a checkpoint was saved at the last step\n",
                "        self.trainer.global_step -= 1\n",
                "        self.check_checkpoint_callback(should_update=True, is_last=True)\n",
                "        self.trainer.global_step += 1\n",
                "\n",
                "        # hook\n",
                "        self.trainer.call_hook(\"on_train_end\")\n",
                "\n",
                "        # todo: TPU 8 cores hangs in flush with TensorBoard. Might do for all loggers.\n",
                "        # It might be related to xla tensors blocked when moving the cpu\n",
                "        # kill loggers\n",
                "        if self.trainer.logger is not None:\n",
                "            self.trainer.logger.finalize(\"success\")\n",
                "\n",
                "        # summarize profile results\n",
                "        self.trainer.profiler.describe()\n",
                "\n",
                "        # give accelerators a chance to finish\n",
                "        self.trainer.accelerator.on_train_end()\n",
                "\n",
                "        # reset bookkeeping\n",
                "        self.trainer._running_stage = None\n",
                "\n",
                "    def check_checkpoint_callback(self, should_update, is_last=False):\n",
                "        # TODO bake this logic into the ModelCheckpoint callback\n",
                "        if should_update and self.trainer.checkpoint_connector.has_trained:\n",
                "            callbacks = self.trainer.checkpoint_callbacks\n",
                "\n",
                "            if is_last and any(cb.save_last and cb.verbose for cb in callbacks):\n",
                "                rank_zero_info(\"Saving latest checkpoint...\")\n",
                "\n",
                "            model = self.trainer.lightning_module\n",
                "\n",
                "            for cb in callbacks:\n",
                "                cb.on_validation_end(self.trainer, model)\n",
                "\n",
                "    def check_early_stopping_callback(self, should_update):\n",
                "        # TODO bake this logic into the EarlyStopping callback\n",
                "        if should_update and self.trainer.checkpoint_connector.has_trained:\n",
                "            callbacks = [c for c in self.trainer.callbacks if isinstance(c, EarlyStopping)]\n",
                "            model = self.trainer.lightning_module\n",
                "\n",
                "            for cb in callbacks:\n",
                "                cb.on_validation_end(self.trainer, model)\n",
                "\n",
                "    def on_train_epoch_start(self, epoch):\n",
                "\n",
                "        # update training progress in trainer\n",
                "        self.trainer.current_epoch = epoch\n",
                "\n",
                "        model = self.trainer.lightning_module\n",
                "\n",
                "        # reset train dataloader\n",
                "        if epoch != 0 and self.trainer.reload_dataloaders_every_epoch:\n",
                "            self.trainer.reset_train_dataloader(model)\n",
                "\n",
                "        # todo: specify the possible exception\n",
                "        with suppress(Exception):\n",
                "            # set seed for distributed sampler (enables shuffling for each epoch)\n",
                "            self.trainer.train_dataloader.sampler.set_epoch(epoch)\n",
                "\n",
                "        # changing gradient according accumulation_scheduler\n",
                "        self.trainer.accumulation_scheduler.on_epoch_start(self.trainer, self.trainer.lightning_module)\n",
                "\n",
                "        # stores accumulated grad fractions per batch\n",
                "        self.accumulated_loss = TensorRunningAccum(window_length=self.trainer.accumulate_grad_batches)\n",
                "\n",
                "        # structured result accumulators for callbacks\n",
                "        self.early_stopping_accumulator = Accumulator()\n",
                "        self.checkpoint_accumulator = Accumulator()\n",
                "\n",
                "        # hook\n",
                "        self.trainer.call_hook(\"on_epoch_start\")\n",
                "        self.trainer.call_hook(\"on_train_epoch_start\")\n",
                "\n",
                "    def on_train_batch_end(self, epoch_output, batch_end_outputs, batch, batch_idx, dataloader_idx):\n",
                "        # hook\n",
                "        self.trainer.call_hook('on_train_batch_end', batch_end_outputs, batch, batch_idx, dataloader_idx)\n",
                "        self.trainer.call_hook('on_batch_end')\n",
                "\n",
                "        # figure out what to track for epoch end\n",
                "        self.track_epoch_end_reduce_metrics(epoch_output, batch_end_outputs)\n",
                "\n",
                "        # reset batch logger internals\n",
                "        self.trainer.logger_connector.on_train_batch_end()\n",
                "\n",
                "    def reset_train_val_dataloaders(self, model):\n",
                "        if self.trainer.train_dataloader is None or not self.trainer.reload_dataloaders_every_epoch:\n",
                "            self.trainer.reset_train_dataloader(model)\n",
                "\n",
                "        if self.trainer.val_dataloaders is None and not self.trainer.reload_dataloaders_every_epoch:\n",
                "            self.trainer.reset_val_dataloader(model)\n",
                "\n",
                "    def track_epoch_end_reduce_metrics(self, epoch_output, batch_end_outputs):\n",
                "\n",
                "        # track the outputs to reduce at the end of the epoch\n",
                "        for opt_idx, opt_outputs in enumerate(batch_end_outputs):\n",
                "            sample_output = opt_outputs[-1]\n",
                "\n",
                "            # decide if we need to reduce at the end of the epoch automatically\n",
                "            auto_reduce_tng_result = isinstance(sample_output, Result) and sample_output.should_reduce_on_epoch_end\n",
                "            hook_overridden = (\n",
                "                is_overridden(\"training_epoch_end\", model=self.trainer.lightning_module)\n",
                "                or is_overridden(\"on_train_epoch_end\", model=self.trainer.lightning_module)\n",
                "            )\n",
                "\n",
                "            # only track when a) it needs to be autoreduced OR b) the user wants to manually reduce on epoch end\n",
                "            if not (hook_overridden or auto_reduce_tng_result):\n",
                "                continue\n",
                "\n",
                "            # with 1 step (no tbptt) don't use a sequence at epoch end\n",
                "            if isinstance(opt_outputs, list) and len(opt_outputs) == 1 and not isinstance(opt_outputs[0], Result):\n",
                "                opt_outputs = opt_outputs[0]\n",
                "\n",
                "            epoch_output[opt_idx].append(opt_outputs)\n",
                "\n",
                "    def get_optimizers_iterable(self):\n",
                "        \"\"\"\n",
                "        Generates an iterable with (idx, optimizer) for each optimizer.\n",
                "        \"\"\"\n",
                "        if not self.trainer.optimizer_frequencies:\n",
                "            # call training_step once per optimizer\n",
                "            return list(enumerate(self.trainer.optimizers))\n",
                "\n",
                "        optimizer_freq_cumsum = np.cumsum(self.trainer.optimizer_frequencies)\n",
                "        optimizers_loop_length = optimizer_freq_cumsum[-1]\n",
                "        current_place_in_loop = self.trainer.total_batch_idx % optimizers_loop_length\n",
                "\n",
                "        # find optimzier index by looking for the first {item > current_place} in the cumsum list\n",
                "        opt_idx = np.argmax(optimizer_freq_cumsum > current_place_in_loop)\n",
                "        return [[opt_idx, self.trainer.optimizers[opt_idx]]]\n",
                "\n",
                "    def on_after_backward(self, training_step_output, batch_idx, untouched_loss):\n",
                "        is_result_obj = isinstance(training_step_output, Result)\n",
                "\n",
                "        if is_result_obj:\n",
                "            training_step_output = training_step_output.detach()\n",
                "        else:\n",
                "            training_step_output.batch_loss = training_step_output.batch_loss.detach()\n",
                "\n",
                "        # insert after step hook\n",
                "        self.trainer.call_hook(\"on_after_backward\")\n",
                "\n",
                "        # when in dev debugging track the losses\n",
                "        self.trainer.dev_debugger.track_train_loss_history(batch_idx, untouched_loss.detach())\n",
                "\n",
                "    def _check_training_step_output(self, training_step_output):\n",
                "        if isinstance(training_step_output, torch.Tensor) and not self.automatic_optimization:\n",
                "            if training_step_output.grad_fn is None:\n",
                "                # TODO: Find why - RuntimeError: Expected to mark a variable ready only once ...\n",
                "                raise MisconfigurationException(\"In manual optimization, `training_step` should not return a Tensor\")\n",
                "\n",
                "    def training_step(self, split_batch, batch_idx, opt_idx, hiddens):\n",
                "        # give the PL module a result for logging\n",
                "        model_ref = self.trainer.lightning_module\n",
                "\n",
                "        with self.trainer.profiler.profile(\"model_forward\"):\n",
                "            args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens)\n",
                "\n",
                "            # manually capture logged metrics\n",
                "            model_ref._current_fx_name = 'training_step'\n",
                "            model_ref._results = Result()\n",
                "            with self.trainer.profiler.profile(\"training_step\"):\n",
                "                training_step_output = self.trainer.accelerator.training_step(args)\n",
                "                self.trainer.accelerator.post_training_step()\n",
                "\n",
                "            self.trainer.logger_connector.cache_logged_metrics()\n",
                "\n",
                "            self._check_training_step_output(training_step_output)\n",
                "\n",
                "            training_step_output = self.trainer.call_hook(\"training_step_end\", training_step_output)\n",
                "\n",
                "            training_step_output_for_epoch_end, training_step_output = self._process_training_step_output(\n",
                "                training_step_output, split_batch\n",
                "            )\n",
                "            is_result_obj = isinstance(training_step_output, Result)\n",
                "\n",
                "            if training_step_output_for_epoch_end is None:\n",
                "                return None\n",
                "\n",
                "        # enable empty loss when using manual opt\n",
                "        closure_loss = None\n",
                "        untouched_loss = None\n",
                "\n",
                "        if self.automatic_optimization:\n",
                "            # accumulate loss\n",
                "            # (if accumulate_grad_batches = 1 no effect)\n",
                "            if is_result_obj:\n",
                "                closure_loss = training_step_output.minimize\n",
                "            else:\n",
                "                closure_loss = training_step_output.batch_loss\n",
                "\n",
                "            closure_loss = closure_loss / self.trainer.accumulate_grad_batches\n",
                "\n",
                "            # the loss will get scaled for amp. avoid any modifications to it\n",
                "            untouched_loss = closure_loss.detach().clone()\n",
                "\n",
                "        # result\n",
                "        result = AttributeDict(\n",
                "            closure_loss=closure_loss,\n",
                "            loss=untouched_loss,\n",
                "            training_step_output=training_step_output,\n",
                "            training_step_output_for_epoch_end=training_step_output_for_epoch_end,\n",
                "            hiddens=training_step_output.hiddens,\n",
                "        )\n",
                "        return result\n",
                "\n",
                "    def _process_training_step_output(self, training_step_output, split_batch):\n",
                "        training_step_output_for_epoch_end = training_step_output\n",
                "\n",
                "        # enable validation_step return None\n",
                "        if training_step_output_for_epoch_end is None:\n",
                "            return None, None\n",
                "\n",
                "        # -----------------------------------------\n",
                "        # process hybrid (1.0)\n",
                "        # -----------------------------------------\n",
                "        # no need for these checks in 1.0.0\n",
                "        # TODO: remove checks in 1.0.0\n",
                "        is_tensor = isinstance(training_step_output_for_epoch_end, torch.Tensor)\n",
                "        is_1_0_output = is_tensor or (\"log\" not in training_step_output and \"progress_bar\" not in training_step_output)\n",
                "        if is_1_0_output:\n",
                "            return self._process_training_step_output_1_0(training_step_output, split_batch)\n",
                "\n",
                "        # -----------------------------------------\n",
                "        # process old dict (deprecate 1.0)\n",
                "        # -----------------------------------------\n",
                "        training_step_output = self.trainer.process_dict_result(training_step_output, train=True)\n",
                "\n",
                "        training_step_output = AttributeDict(\n",
                "            batch_loss=training_step_output[0],\n",
                "            pbar_on_batch_end=training_step_output[1],\n",
                "            log_metrics=training_step_output[2],\n",
                "            callback_metrics=training_step_output[3],\n",
                "            hiddens=training_step_output[4],\n",
                "        )\n",
                "        # if the user decides to finally reduce things in epoch_end, save raw output without graphs\n",
                "        if isinstance(training_step_output_for_epoch_end, torch.Tensor):\n",
                "            training_step_output_for_epoch_end = training_step_output_for_epoch_end.detach()\n",
                "        else:\n",
                "            training_step_output_for_epoch_end = recursive_detach(training_step_output_for_epoch_end)\n",
                "\n",
                "        return training_step_output_for_epoch_end, training_step_output\n",
                "\n",
                "    def _process_training_step_output_1_0(self, training_step_output, split_batch):\n",
                "        result = self.trainer.lightning_module._results\n",
                "\n",
                "        loss = None\n",
                "        hiddens = None\n",
                "\n",
                "        # handle dict return\n",
                "        if isinstance(training_step_output, dict):\n",
                "            loss = training_step_output.pop(\"loss\", None)\n",
                "            hiddens = training_step_output.pop(\"hiddens\", None)\n",
                "            result[\"extra\"] = training_step_output\n",
                "\n",
                "        # handle scalar return\n",
                "        elif isinstance(training_step_output, torch.Tensor):\n",
                "            loss = training_step_output\n",
                "            result[\"extra\"] = {}\n",
                "\n",
                "        # map to results under the hood\n",
                "        result.minimize = loss\n",
                "        result.hiddens = hiddens\n",
                "\n",
                "        # track batch for manual reduction with result\n",
                "        result.track_batch_size(len(split_batch))\n",
                "\n",
                "        # track metrics without grads for epoch reduction\n",
                "        training_step_output_for_epoch_end = copy(result)\n",
                "        training_step_output_for_epoch_end = training_step_output_for_epoch_end.detach()\n",
                "        if self.trainer.move_metrics_to_cpu:\n",
                "            training_step_output_for_epoch_end = training_step_output_for_epoch_end.cpu()\n",
                "\n",
                "        # what flows back into the system\n",
                "        training_step_output = result\n",
                "\n",
                "        return training_step_output_for_epoch_end, training_step_output\n",
                "\n",
                "    def optimizer_step(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure):\n",
                "        model_ref = self.trainer.lightning_module\n",
                "\n",
                "        is_lbfgs = isinstance(optimizer, torch.optim.LBFGS)\n",
                "        using_native_amp = self.trainer.amp_backend == AMPType.NATIVE\n",
                "\n",
                "        # native amp + lbfgs is a no go right now\n",
                "        if using_native_amp and is_lbfgs:\n",
                "            raise MisconfigurationException(\n",
                "                'native PyTorch amp and lbfgs are not compatible.'\n",
                "                ' To request, please file a Github issue in PyTorch and tag @mcarilli'\n",
                "            )\n",
                "\n",
                "        # wraps into LightningOptimizer only for running step\n",
                "        optimizer = LightningOptimizer._to_lightning_optimizer(optimizer, self.trainer, opt_idx)\n",
                "\n",
                "        # model hook\n",
                "        model_ref.optimizer_step(\n",
                "            self.trainer.current_epoch,\n",
                "            batch_idx,\n",
                "            optimizer,\n",
                "            opt_idx,\n",
                "            train_step_and_backward_closure,\n",
                "            on_tpu=self.trainer._device_type == DeviceType.TPU and _TPU_AVAILABLE,\n",
                "            using_native_amp=using_native_amp,\n",
                "            using_lbfgs=is_lbfgs,\n",
                "        )\n",
                "\n",
                "    def on_before_zero_grad(self, optimizer):\n",
                "        self.trainer.call_hook('on_before_zero_grad', optimizer)\n",
                "\n",
                "    def optimizer_zero_grad(self, batch_idx, optimizer, opt_idx):\n",
                "        self.trainer.accelerator.optimizer_zero_grad(self.trainer.current_epoch, batch_idx, optimizer, opt_idx)\n",
                "\n",
                "    def track_and_norm_grad(self, optimizer):\n",
                "        # track gradient norms\n",
                "        grad_norm_dic = self._track_gradient_norm()\n",
                "\n",
                "        # clip gradients\n",
                "        self.trainer.accelerator.clip_gradients(optimizer, self.trainer.gradient_clip_val)\n",
                "        self._cur_grad_norm_dict = grad_norm_dic\n",
                "\n",
                "    def _track_gradient_norm(self):\n",
                "        grad_norm_dict = {}\n",
                "        if (self.trainer.global_step + 1) % self.trainer.log_every_n_steps == 0:\n",
                "            if float(self.trainer.track_grad_norm) > 0:\n",
                "                model = self.trainer.lightning_module\n",
                "                grad_norm_dict = model.grad_norm(self.trainer.track_grad_norm)\n",
                "        return grad_norm_dict\n",
                "\n",
                "    def process_hiddens(self, opt_closure_result):\n",
                "        hiddens = opt_closure_result.hiddens\n",
                "        if isinstance(opt_closure_result.training_step_output, Result):\n",
                "            opt_closure_result.training_step_output_for_epoch_end.drop_hiddens()\n",
                "        return hiddens\n",
                "\n",
                "    def tbptt_split_batch(self, batch):\n",
                "        splits = [batch]\n",
                "        if self.trainer.truncated_bptt_steps is not None:\n",
                "            model_ref = self.trainer.lightning_module\n",
                "            with self.trainer.profiler.profile(\"tbptt_split_batch\"):\n",
                "                splits = model_ref.tbptt_split_batch(batch, self.trainer.truncated_bptt_steps)\n",
                "        return splits\n",
                "\n",
                "    def run_training_epoch(self):\n",
                "        # modify dataloader if needed (ddp, etc...)\n",
                "        train_dataloader = self.trainer.accelerator.process_dataloader(self.trainer.train_dataloader)\n",
                "\n",
                "        # track epoch output\n",
                "        epoch_output = [[] for _ in range(self.num_optimizers)]\n",
                "\n",
                "        train_dataloader = self.trainer.data_connector.get_profiled_train_dataloader(train_dataloader)\n",
                "        dataloader_idx = 0\n",
                "        val_loop_called = False\n",
                "\n",
                "        for batch_idx, (batch, is_last_batch) in train_dataloader:\n",
                "\n",
                "            self.trainer.batch_idx = batch_idx\n",
                "\n",
                "            # ------------------------------------\n",
                "            # TRAINING_STEP + TRAINING_STEP_END\n",
                "            # ------------------------------------\n",
                "            with self.trainer.profiler.profile(\"run_training_batch\"):\n",
                "                batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
                "\n",
                "            # when returning -1 from train_step, we end epoch early\n",
                "            if batch_output.signal == -1:\n",
                "                break\n",
                "\n",
                "            batch_end_outputs = self.process_train_step_outputs(\n",
                "                batch_output.training_step_output_for_epoch_end,\n",
                "                self.early_stopping_accumulator,\n",
                "                self.checkpoint_accumulator,\n",
                "            )\n",
                "            # hook\n",
                "            # TODO: add outputs to batches\n",
                "            self.on_train_batch_end(epoch_output, batch_end_outputs, batch, batch_idx, dataloader_idx)\n",
                "\n",
                "            # -----------------------------------------\n",
                "            # SAVE METRICS TO LOGGERS\n",
                "            # -----------------------------------------\n",
                "            self.trainer.logger_connector.log_train_step_metrics(batch_output)\n",
                "\n",
                "            # -----------------------------------------\n",
                "            # VALIDATE IF NEEDED + CHECKPOINT CALLBACK\n",
                "            # -----------------------------------------\n",
                "            should_check_val = self.should_check_val_fx(batch_idx, is_last_batch)\n",
                "            if should_check_val:\n",
                "                self.trainer.validating = True\n",
                "                self.trainer.run_evaluation()\n",
                "                self.trainer.training = True\n",
                "                val_loop_called = True\n",
                "\n",
                "            # -----------------------------------------\n",
                "            # SAVE LOGGERS (ie: Tensorboard, etc...)\n",
                "            # -----------------------------------------\n",
                "            self.save_loggers_on_train_batch_end()\n",
                "\n",
                "            # update LR schedulers\n",
                "            monitor_metrics = deepcopy(self.trainer.logger_connector.callback_metrics)\n",
                "            self.update_train_loop_lr_schedulers(monitor_metrics=monitor_metrics)\n",
                "            self.trainer.checkpoint_connector.has_trained = True\n",
                "\n",
                "            # max steps reached, end training\n",
                "            if (\n",
                "                self.trainer.max_steps is not None and self.trainer.max_steps == self.trainer.global_step + 1\n",
                "                and self._accumulated_batches_reached()\n",
                "            ):\n",
                "                break\n",
                "\n",
                "            # end epoch early\n",
                "            # stop when the flag is changed or we've gone past the amount\n",
                "            # requested in the batches\n",
                "            if self.trainer.should_stop:\n",
                "                break\n",
                "\n",
                "            self.trainer.total_batch_idx += 1\n",
                "\n",
                "            # stop epoch if we limited the number of training batches\n",
                "            if self._num_training_batches_reached(is_last_batch):\n",
                "                break\n",
                "\n",
                "            # progress global step according to grads progress\n",
                "            self.increment_accumulated_grad_global_step()\n",
                "\n",
                "        # epoch end hook\n",
                "        self.run_on_epoch_end_hook(epoch_output)\n",
                "\n",
                "        # log epoch metrics\n",
                "        self.trainer.logger_connector.log_train_epoch_end_metrics(\n",
                "            epoch_output, self.checkpoint_accumulator, self.early_stopping_accumulator, self.num_optimizers\n",
                "        )\n",
                "\n",
                "        should_check_val = self.should_check_val_fx(batch_idx, is_last_batch, on_epoch=True)\n",
                "        should_skip_eval = self.trainer.evaluation_loop.should_skip_evaluation(self.trainer.num_val_batches)\n",
                "        should_train_only = self.trainer.disable_validation or should_skip_eval\n",
                "\n",
                "        # update epoch level lr_schedulers if no val loop outside train loop is triggered\n",
                "        if (val_loop_called and not should_check_val) or should_train_only:\n",
                "            self.trainer.optimizer_connector.update_learning_rates(interval='epoch')\n",
                "\n",
                "        if should_train_only:\n",
                "            self.check_checkpoint_callback(True)\n",
                "            self.check_early_stopping_callback(True)\n",
                "\n",
                "        if should_check_val:\n",
                "            self.trainer.validating = True\n",
                "            self.trainer.run_evaluation(on_epoch=True)\n",
                "            self.trainer.training = True\n",
                "\n",
                "        # increment the global step once\n",
                "        # progress global step according to grads progress\n",
                "        self.increment_accumulated_grad_global_step()\n",
                "\n",
                "    def run_training_batch(self, batch, batch_idx, dataloader_idx):\n",
                "        # track grad norms\n",
                "        grad_norm_dic = {}\n",
                "\n",
                "        # bookkeeping\n",
                "        self.trainer.hiddens = None\n",
                "\n",
                "        # track all outputs across time and num of optimizers\n",
                "        batch_outputs = [[] for _ in range(len(self.get_optimizers_iterable()))]\n",
                "\n",
                "        if batch is None:\n",
                "            return AttributeDict(signal=0, grad_norm_dic=grad_norm_dic)\n",
                "\n",
                "        # hook\n",
                "        response = self.trainer.call_hook(\"on_batch_start\")\n",
                "        if response == -1:\n",
                "            return AttributeDict(signal=-1, grad_norm_dic=grad_norm_dic)\n",
                "\n",
                "        # hook\n",
                "        response = self.trainer.call_hook(\"on_train_batch_start\", batch, batch_idx, dataloader_idx)\n",
                "        if response == -1:\n",
                "            return AttributeDict(signal=-1, grad_norm_dic=grad_norm_dic)\n",
                "\n",
                "        # lightning module hook\n",
                "        splits = self.tbptt_split_batch(batch)\n",
                "\n",
                "        for split_idx, split_batch in enumerate(splits):\n",
                "\n",
                "            # create an iterable for optimizers and loop over them\n",
                "            for opt_idx, optimizer in self.prepare_optimizers():\n",
                "\n",
                "                # toggle model params + set info to logger_connector\n",
                "                self.run_train_split_start(split_idx, split_batch, opt_idx, optimizer)\n",
                "\n",
                "                if self.should_accumulate():\n",
                "                    # For gradient accumulation\n",
                "\n",
                "                    # -------------------\n",
                "                    # calculate loss (train step + train step end)\n",
                "                    # -------------------\n",
                "\n",
                "                    # automatic_optimization=True: perform dpp sync only when performing optimizer_step\n",
                "                    # automatic_optimization=False: don't block synchronization here\n",
                "                    with self.block_ddp_sync_behaviour():\n",
                "                        self.training_step_and_backward(\n",
                "                            split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
                "                        )\n",
                "\n",
                "                    batch_outputs = self._process_closure_result(\n",
                "                        batch_outputs=batch_outputs,\n",
                "                        opt_idx=opt_idx,\n",
                "                    )\n",
                "\n",
                "                # ------------------------------\n",
                "                # BACKWARD PASS\n",
                "                # ------------------------------\n",
                "                # gradient update with accumulated gradients\n",
                "\n",
                "                else:\n",
                "                    if self.automatic_optimization:\n",
                "\n",
                "                        def train_step_and_backward_closure():\n",
                "                            result = self.training_step_and_backward(\n",
                "                                split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
                "                            )\n",
                "                            return None if result is None else result.loss\n",
                "\n",
                "                        # optimizer step\n",
                "                        self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
                "\n",
                "                    else:\n",
                "                        self._curr_step_result = self.training_step(\n",
                "                            split_batch, batch_idx, opt_idx, self.trainer.hiddens\n",
                "                        )\n",
                "\n",
                "                    if self._curr_step_result is None:\n",
                "                        # user decided to skip optimization\n",
                "                        # make sure to zero grad.\n",
                "                        continue\n",
                "\n",
                "                    batch_outputs = self._process_closure_result(\n",
                "                        batch_outputs=batch_outputs,\n",
                "                        opt_idx=opt_idx,\n",
                "                    )\n",
                "\n",
                "                    # todo: Properly aggregate grad_norm accros opt_idx and split_idx\n",
                "                    grad_norm_dic = self._cur_grad_norm_dict\n",
                "                    self._cur_grad_norm_dict = None\n",
                "\n",
                "                    # update running loss + reset accumulated loss\n",
                "                    self.update_running_loss()\n",
                "\n",
                "        result = AttributeDict(\n",
                "            signal=0,\n",
                "            grad_norm_dic=grad_norm_dic,\n",
                "            training_step_output_for_epoch_end=batch_outputs,\n",
                "        )\n",
                "        return result\n",
                "\n",
                "    @contextmanager\n",
                "    def block_ddp_sync_behaviour(self, should_block_sync: bool = False):\n",
                "        \"\"\"\n",
                "        automatic_optimization = True\n",
                "        Blocks ddp sync gradients behaviour on backwards pass.\n",
                "        This is useful for skipping sync when accumulating gradients, reducing communication overhead\n",
                "\n",
                "        automatic_optimization = False\n",
                "        do not block ddp gradient sync when using manual optimization\n",
                "        as gradients are needed within the training step\n",
                "\n",
                "        Returns:\n",
                "            context manager with sync behaviour off\n",
                "\n",
                "        \"\"\"\n",
                "        if (\n",
                "            isinstance(self.trainer.training_type_plugin, ParallelPlugin)\n",
                "            and (self.automatic_optimization or should_block_sync)\n",
                "        ):\n",
                "            with self.trainer.training_type_plugin.block_backward_sync():\n",
                "                yield None\n",
                "        else:\n",
                "            yield None\n",
                "\n",
                "    def _process_closure_result(self, batch_outputs: list, opt_idx: int) -> list:\n",
                "        opt_closure_result = self._curr_step_result\n",
                "\n",
                "        if opt_closure_result is not None:\n",
                "\n",
                "            # cache metrics\n",
                "            self.trainer.logger_connector.cache_training_step_metrics(opt_closure_result)\n",
                "\n",
                "            # track hiddens\n",
                "            self.trainer.hiddens = self.process_hiddens(opt_closure_result)\n",
                "\n",
                "            # check if loss or model weights are nan\n",
                "            if self.trainer.terminate_on_nan:\n",
                "                self.trainer.detect_nan_tensors(opt_closure_result.loss)\n",
                "\n",
                "            # track all the outputs across all steps\n",
                "            batch_opt_idx = opt_idx if len(batch_outputs) > 1 else 0\n",
                "            batch_outputs[batch_opt_idx].append(opt_closure_result.training_step_output_for_epoch_end)\n",
                "\n",
                "            if self.automatic_optimization:\n",
                "                # track total loss for logging (avoid mem leaks)\n",
                "                self.accumulated_loss.append(opt_closure_result.loss)\n",
                "\n",
                "        self._curr_step_result = None\n",
                "\n",
                "        return batch_outputs\n",
                "\n",
                "    def training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens):\n",
                "        \"\"\"\n",
                "        wrap the forward step in a closure so second order methods work\n",
                "        \"\"\"\n",
                "        with self.trainer.profiler.profile(\"training_step_and_backward\"):\n",
                "            # lightning module hook\n",
                "            result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
                "            self._curr_step_result = result\n",
                "\n",
                "            if not self._skip_backward and self.automatic_optimization:\n",
                "                is_first_batch_to_accumulate = batch_idx % self.trainer.accumulate_grad_batches == 0\n",
                "\n",
                "                if is_first_batch_to_accumulate:\n",
                "                    self.on_before_zero_grad(optimizer)\n",
                "                    self.optimizer_zero_grad(batch_idx, optimizer, opt_idx)\n",
                "\n",
                "                # backward pass\n",
                "                if result is not None:\n",
                "                    with self.trainer.profiler.profile(\"backward\"):\n",
                "                        self.backward(result, optimizer, opt_idx)\n",
                "\n",
                "                    # hook - call this hook only\n",
                "                    # when gradients have finished to accumulate\n",
                "                    if not self.should_accumulate():\n",
                "                        self.on_after_backward(result.training_step_output, batch_idx, result.loss)\n",
                "\n",
                "                    # check if loss or model weights are nan\n",
                "                    if self.trainer.terminate_on_nan:\n",
                "                        self.trainer.detect_nan_tensors(result.loss)\n",
                "\n",
                "                else:\n",
                "                    self.warning_cache.warn(\"training_step returned None if it was on purpose, ignore this warning...\")\n",
                "\n",
                "                if len(self.trainer.optimizers) > 1:\n",
                "                    # revert back to previous state\n",
                "                    self.trainer.lightning_module.untoggle_optimizer(opt_idx)\n",
                "\n",
                "        return result\n",
                "\n",
                "    def backward(self, result, optimizer, opt_idx, *args, **kwargs):\n",
                "        self.trainer.dev_debugger.track_event(\"backward_call\")\n",
                "\n",
                "        should_accumulate = self.should_accumulate()\n",
                "\n",
                "        # backward can be called manually in the training loop\n",
                "        if isinstance(result, torch.Tensor):\n",
                "            self.trainer.accelerator.backward(result, optimizer, opt_idx, should_accumulate, *args, **kwargs)\n",
                "        else:\n",
                "            result.closure_loss = self.trainer.accelerator.backward(\n",
                "                result.closure_loss, optimizer, opt_idx, should_accumulate, *args, **kwargs\n",
                "            )\n",
                "\n",
                "        if not self.should_accumulate():\n",
                "            # track gradients\n",
                "            self.track_and_norm_grad(optimizer=optimizer)\n",
                "\n",
                "    def update_train_loop_lr_schedulers(self, monitor_metrics=None):\n",
                "        num_accumulated_batches_reached = self._accumulated_batches_reached()\n",
                "        num_training_batches_reached = self._num_training_batches_reached()\n",
                "\n",
                "        if num_accumulated_batches_reached or num_training_batches_reached:\n",
                "            # update lr\n",
                "            self.trainer.optimizer_connector.update_learning_rates(interval=\"step\", monitor_metrics=monitor_metrics)\n",
                "\n",
                "    def run_on_epoch_end_hook(self, epoch_output):\n",
                "        # inform logger the batch loop has finished\n",
                "        self.trainer.logger_connector.on_train_epoch_end()\n",
                "\n",
                "        self.trainer.call_hook('on_train_epoch_end', epoch_output)\n",
                "        self.trainer.call_hook('on_epoch_end')\n",
                "\n",
                "    def increment_accumulated_grad_global_step(self):\n",
                "        num_accumulated_batches_reached = self._accumulated_batches_reached()\n",
                "        num_training_batches_reached = self._num_training_batches_reached()\n",
                "\n",
                "        # progress global step according to grads progress\n",
                "        if num_accumulated_batches_reached or num_training_batches_reached:\n",
                "            self.trainer.global_step += 1\n",
                "\n",
                "    def _accumulated_batches_reached(self):\n",
                "        return (self.trainer.batch_idx + 1) % self.trainer.accumulate_grad_batches == 0\n",
                "\n",
                "    def _num_training_batches_reached(self, is_last_batch=False):\n",
                "        return (self.trainer.batch_idx + 1) == self.trainer.num_training_batches or is_last_batch\n",
                "\n",
                "    def should_accumulate(self):\n",
                "        # checks if backward or backward + optimizer step (via closure)\n",
                "        accumulation_done = self._accumulated_batches_reached()\n",
                "        is_final_batch = self._num_training_batches_reached()\n",
                "        return not (accumulation_done or is_final_batch)\n",
                "\n",
                "    def should_check_val_fx(self, batch_idx, is_last_batch, on_epoch=False):\n",
                "        # decide if we should run validation\n",
                "        is_val_check_batch = (batch_idx + 1) % self.trainer.val_check_batch == 0\n",
                "        is_val_check_epoch = (self.trainer.current_epoch + 1) % self.trainer.check_val_every_n_epoch == 0\n",
                "        can_check_val = self.trainer.enable_validation and is_val_check_epoch\n",
                "        is_last_batch_for_infinite_dataset = is_last_batch and self.trainer.val_check_batch == float(\"inf\")\n",
                "        epoch_end_val_check = (batch_idx + 1) % self.trainer.num_training_batches == 0\n",
                "\n",
                "        should_check_val = ((is_val_check_batch and epoch_end_val_check) or self.trainer.should_stop\n",
                "                            or is_last_batch_for_infinite_dataset\n",
                "                            ) if on_epoch else (is_val_check_batch and not epoch_end_val_check)\n",
                "\n",
                "        return should_check_val and can_check_val\n",
                "\n",
                "    def build_train_args(self, batch, batch_idx, opt_idx, hiddens):\n",
                "        # enable not needing to add opt_idx to training_step\n",
                "        args = [batch, batch_idx]\n",
                "\n",
                "        if len(self.trainer.optimizers) > 1:\n",
                "            if self.trainer.has_arg(\"training_step\", \"optimizer_idx\"):\n",
                "                if not self.automatic_optimization:\n",
                "                    self.warning_cache.warn(\n",
                "                        \"`training_step` hook signature has changed in v1.3.\"\n",
                "                        \" `optimizer_idx` argument has been removed in case of manual optimization. Support for\"\n",
                "                        \" the old signature will be removed in v1.5\", DeprecationWarning\n",
                "                    )\n",
                "                args.append(opt_idx)\n",
                "            elif not self.trainer.has_arg(\"training_step\", \"optimizer_idx\") and self.automatic_optimization:\n",
                "                raise ValueError(\n",
                "                    f\"Your LightningModule defines {len(self.trainer.optimizers)} optimizers but\"\n",
                "                    ' `training_step` is missing the `optimizer_idx` argument.'\n",
                "                )\n",
                "\n",
                "        # pass hiddens if using tbptt\n",
                "        if self.trainer.truncated_bptt_steps is not None:\n",
                "            args.append(hiddens)\n",
                "\n",
                "        return args\n",
                "\n",
                "    def save_loggers_on_train_batch_end(self):\n",
                "        # when loggers should save to disk\n",
                "        should_flush_logs = self.trainer.logger_connector.should_flush_logs\n",
                "        if should_flush_logs and self.trainer.is_global_zero and self.trainer.logger is not None:\n",
                "            self.trainer.logger.save()\n",
                "\n",
                "    def process_train_step_outputs(self, all_train_step_outputs, early_stopping_accumulator, checkpoint_accumulator):\n",
                "        \"\"\"\n",
                "        Figure out what needs to be tracked/logged at the end of the epoch\n",
                "        \"\"\"\n",
                "\n",
                "        # the training step outputs a list per optimizer. The list contains the outputs at each time step\n",
                "        # when no TBPTT is used, then the list has 1 item per batch\n",
                "        # when TBPTT IS used, then the list has n items (1 per time step)\n",
                "        batch_end_outputs = []\n",
                "        for optimizer_idx_outputs in all_train_step_outputs:\n",
                "            # extract one representative sample from each time step (1 if no tbptt) and 0th optimizer\n",
                "            if len(optimizer_idx_outputs) == 0:\n",
                "                continue\n",
                "\n",
                "            sample_output = optimizer_idx_outputs[-1]\n",
                "\n",
                "            # pull out callback info if available (ie: Results object)\n",
                "            if isinstance(sample_output, dict) and \"early_stop_on\" in sample_output:\n",
                "                early_stopping_accumulator.accumulate(sample_output[\"early_stop_on\"])\n",
                "\n",
                "            if isinstance(sample_output, dict) and \"checkpoint_on\" in sample_output:\n",
                "                checkpoint_accumulator.accumulate(sample_output[\"checkpoint_on\"])\n",
                "\n",
                "            batch_end_outputs.append(optimizer_idx_outputs)\n",
                "\n",
                "        return batch_end_outputs\n",
                "\n",
                "    def prepare_optimizers(self):\n",
                "        # in manual optimization we loop over all optimizers at once\n",
                "        optimizers = self.get_optimizers_iterable()\n",
                "        if not self.automatic_optimization:\n",
                "            optimizers = [optimizers[0]]\n",
                "        return optimizers\n",
                "\n",
                "    def run_train_split_start(self, split_idx, split_batch, opt_idx, optimizer):\n",
                "        # set split_idx to trainer for tracking\n",
                "        self.trainer.split_idx = split_idx\n",
                "\n",
                "        # make sure only the gradients of the current optimizer's parameters are calculated\n",
                "        # in the training step to prevent dangling gradients in multiple-optimizer setup.\n",
                "        if self.automatic_optimization and len(self.trainer.optimizers) > 1:\n",
                "            model = self.trainer.lightning_module\n",
                "            model.toggle_optimizer(optimizer, opt_idx)\n",
                "\n",
                "        # use to track metrics internally\n",
                "        self.trainer.logger_connector.on_train_split_start(split_idx, opt_idx, split_batch)\n",
                "\n",
                "    def update_running_loss(self):\n",
                "        accumulated_loss = self.accumulated_loss.mean()\n",
                "\n",
                "        if accumulated_loss is not None:\n",
                "            # calculate running loss for display\n",
                "            self.running_loss.append(self.accumulated_loss.mean() * self.trainer.accumulate_grad_batches)\n",
                "\n",
                "        # reset for next set of accumulated grads\n",
                "        self.accumulated_loss.reset()"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "logic update"
        },
        {
            "edit_hunk_pair": [
                0,
                7
            ],
            "edit_order": "1 before 0",
            "reason": "move"
        },
        {
            "edit_hunk_pair": [
                1,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                1,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "call(1) and implement(7)"
        },
        {
            "edit_hunk_pair": [
                3,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "import and use"
        },
        {
            "edit_hunk_pair": [
                4,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "import and use"
        },
        {
            "edit_hunk_pair": [
                6,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "data flow"
        }
    ]
}