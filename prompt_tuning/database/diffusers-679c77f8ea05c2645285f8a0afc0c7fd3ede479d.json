{
    "language": "python",
    "commit_url": "https://github.com/huggingface/diffusers/commit/679c77f8ea05c2645285f8a0afc0c7fd3ede479d",
    "commit_message": "Add diffusers version and pipeline class to the Hub UA",
    "commit_snapshots": {
        "src/diffusers/modeling_flax_utils.py": [
            [
                "# coding=utf-8\n",
                "# Copyright 2022 The HuggingFace Inc. team.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     http://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\n",
                "import os\n",
                "from pickle import UnpicklingError\n",
                "from typing import Any, Dict, Union\n",
                "\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "import msgpack.exceptions\n",
                "from flax.core.frozen_dict import FrozenDict, unfreeze\n",
                "from flax.serialization import from_bytes, to_bytes\n",
                "from flax.traverse_util import flatten_dict, unflatten_dict\n",
                "from huggingface_hub import hf_hub_download\n",
                "from huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError\n",
                "from requests import HTTPError\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "from . import is_torch_available\n"
                ],
                "after": [
                    "from . import __version__, is_torch_available\n"
                ],
                "parent_version_range": {
                    "start": 29,
                    "end": 30
                },
                "child_version_range": {
                    "start": 29,
                    "end": 30
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 0,
                "hunk_diff": "File: src/diffusers/modeling_flax_utils.py\nCode:\n  ...\n26 26    from huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError\n27 27    from requests import HTTPError\n28 28    \n29     - from . import is_torch_available\n   29  + from . import __version__, is_torch_available\n30 30    from .modeling_flax_pytorch_utils import convert_pytorch_state_dict_to_flax\n31 31    from .utils import (\n32 32        CONFIG_NAME,\n       ...\n",
                "file_path": "src/diffusers/modeling_flax_utils.py",
                "identifiers_before": [
                    "is_torch_available"
                ],
                "identifiers_after": [
                    "__version__",
                    "is_torch_available"
                ],
                "prefix": [
                    "from huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError\n",
                    "from requests import HTTPError\n",
                    "\n"
                ],
                "suffix": [
                    "from .modeling_flax_pytorch_utils import convert_pytorch_state_dict_to_flax\n",
                    "from .utils import (\n",
                    "    CONFIG_NAME,\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 1,
                        "detail": {
                            "identifier": "__version__",
                            "position": {
                                "start": {
                                    "line": 29,
                                    "column": 14
                                },
                                "end": {
                                    "line": 29,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/diffusers/src/diffusers/modeling_flax_utils.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "from .modeling_flax_pytorch_utils import convert_pytorch_state_dict_to_flax\n",
                "from .utils import (\n",
                "    CONFIG_NAME,\n",
                "    DIFFUSERS_CACHE,\n",
                "    FLAX_WEIGHTS_NAME,\n",
                "    HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n",
                "    WEIGHTS_NAME,\n",
                "    logging,\n",
                ")\n",
                "\n",
                "\n",
                "logger = logging.get_logger(__name__)\n",
                "\n",
                "\n",
                "class FlaxModelMixin:\n",
                "    r\"\"\"\n",
                "    Base class for all flax models.\n",
                "\n",
                "    [`FlaxModelMixin`] takes care of storing the configuration of the models and handles methods for loading,\n",
                "    downloading and saving models.\n",
                "    \"\"\"\n",
                "    config_name = CONFIG_NAME\n",
                "    _automatically_saved_args = [\"_diffusers_version\", \"_class_name\", \"_name_or_path\"]\n",
                "    _flax_internal_args = [\"name\", \"parent\", \"dtype\"]\n",
                "\n",
                "    @classmethod\n",
                "    def _from_config(cls, config, **kwargs):\n",
                "        \"\"\"\n",
                "        All context managers that the model should be initialized under go here.\n",
                "        \"\"\"\n",
                "        return cls(config, **kwargs)\n",
                "\n",
                "    def _cast_floating_to(self, params: Union[Dict, FrozenDict], dtype: jnp.dtype, mask: Any = None) -> Any:\n",
                "        \"\"\"\n",
                "        Helper method to cast floating-point values of given parameter `PyTree` to given `dtype`.\n",
                "        \"\"\"\n",
                "\n",
                "        # taken from https://github.com/deepmind/jmp/blob/3a8318abc3292be38582794dbf7b094e6583b192/jmp/_src/policy.py#L27\n",
                "        def conditional_cast(param):\n",
                "            if isinstance(param, jnp.ndarray) and jnp.issubdtype(param.dtype, jnp.floating):\n",
                "                param = param.astype(dtype)\n",
                "            return param\n",
                "\n",
                "        if mask is None:\n",
                "            return jax.tree_map(conditional_cast, params)\n",
                "\n",
                "        flat_params = flatten_dict(params)\n",
                "        flat_mask, _ = jax.tree_flatten(mask)\n",
                "\n",
                "        for masked, key in zip(flat_mask, flat_params.keys()):\n",
                "            if masked:\n",
                "                param = flat_params[key]\n",
                "                flat_params[key] = conditional_cast(param)\n",
                "\n",
                "        return unflatten_dict(flat_params)\n",
                "\n",
                "    def to_bf16(self, params: Union[Dict, FrozenDict], mask: Any = None):\n",
                "        r\"\"\"\n",
                "        Cast the floating-point `params` to `jax.numpy.bfloat16`. This returns a new `params` tree and does not cast\n",
                "        the `params` in place.\n",
                "\n",
                "        This method can be used on TPU to explicitly convert the model parameters to bfloat16 precision to do full\n",
                "        half-precision training or to save weights in bfloat16 for inference in order to save memory and improve speed.\n",
                "\n",
                "        Arguments:\n",
                "            params (`Union[Dict, FrozenDict]`):\n",
                "                A `PyTree` of model parameters.\n",
                "            mask (`Union[Dict, FrozenDict]`):\n",
                "                A `PyTree` with same structure as the `params` tree. The leaves should be booleans, `True` for params\n",
                "                you want to cast, and should be `False` for those you want to skip.\n",
                "\n",
                "        Examples:\n",
                "\n",
                "        ```python\n",
                "        >>> from diffusers import FlaxUNet2DConditionModel\n",
                "\n",
                "        >>> # load model\n",
                "        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
                "        >>> # By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision\n",
                "        >>> params = model.to_bf16(params)\n",
                "        >>> # If you don't want to cast certain parameters (for example layer norm bias and scale)\n",
                "        >>> # then pass the mask as follows\n",
                "        >>> from flax import traverse_util\n",
                "\n",
                "        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
                "        >>> flat_params = traverse_util.flatten_dict(params)\n",
                "        >>> mask = {\n",
                "        ...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n",
                "        ...     for path in flat_params\n",
                "        ... }\n",
                "        >>> mask = traverse_util.unflatten_dict(mask)\n",
                "        >>> params = model.to_bf16(params, mask)\n",
                "        ```\"\"\"\n",
                "        return self._cast_floating_to(params, jnp.bfloat16, mask)\n",
                "\n",
                "    def to_fp32(self, params: Union[Dict, FrozenDict], mask: Any = None):\n",
                "        r\"\"\"\n",
                "        Cast the floating-point `params` to `jax.numpy.float32`. This method can be used to explicitly convert the\n",
                "        model parameters to fp32 precision. This returns a new `params` tree and does not cast the `params` in place.\n",
                "\n",
                "        Arguments:\n",
                "            params (`Union[Dict, FrozenDict]`):\n",
                "                A `PyTree` of model parameters.\n",
                "            mask (`Union[Dict, FrozenDict]`):\n",
                "                A `PyTree` with same structure as the `params` tree. The leaves should be booleans, `True` for params\n",
                "                you want to cast, and should be `False` for those you want to skip\n",
                "\n",
                "        Examples:\n",
                "\n",
                "        ```python\n",
                "        >>> from diffusers import FlaxUNet2DConditionModel\n",
                "\n",
                "        >>> # Download model and configuration from huggingface.co\n",
                "        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
                "        >>> # By default, the model params will be in fp32, to illustrate the use of this method,\n",
                "        >>> # we'll first cast to fp16 and back to fp32\n",
                "        >>> params = model.to_f16(params)\n",
                "        >>> # now cast back to fp32\n",
                "        >>> params = model.to_fp32(params)\n",
                "        ```\"\"\"\n",
                "        return self._cast_floating_to(params, jnp.float32, mask)\n",
                "\n",
                "    def to_fp16(self, params: Union[Dict, FrozenDict], mask: Any = None):\n",
                "        r\"\"\"\n",
                "        Cast the floating-point `params` to `jax.numpy.float16`. This returns a new `params` tree and does not cast the\n",
                "        `params` in place.\n",
                "\n",
                "        This method can be used on GPU to explicitly convert the model parameters to float16 precision to do full\n",
                "        half-precision training or to save weights in float16 for inference in order to save memory and improve speed.\n",
                "\n",
                "        Arguments:\n",
                "            params (`Union[Dict, FrozenDict]`):\n",
                "                A `PyTree` of model parameters.\n",
                "            mask (`Union[Dict, FrozenDict]`):\n",
                "                A `PyTree` with same structure as the `params` tree. The leaves should be booleans, `True` for params\n",
                "                you want to cast, and should be `False` for those you want to skip\n",
                "\n",
                "        Examples:\n",
                "\n",
                "        ```python\n",
                "        >>> from diffusers import FlaxUNet2DConditionModel\n",
                "\n",
                "        >>> # load model\n",
                "        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
                "        >>> # By default, the model params will be in fp32, to cast these to float16\n",
                "        >>> params = model.to_fp16(params)\n",
                "        >>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)\n",
                "        >>> # then pass the mask as follows\n",
                "        >>> from flax import traverse_util\n",
                "\n",
                "        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
                "        >>> flat_params = traverse_util.flatten_dict(params)\n",
                "        >>> mask = {\n",
                "        ...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n",
                "        ...     for path in flat_params\n",
                "        ... }\n",
                "        >>> mask = traverse_util.unflatten_dict(mask)\n",
                "        >>> params = model.to_fp16(params, mask)\n",
                "        ```\"\"\"\n",
                "        return self._cast_floating_to(params, jnp.float16, mask)\n",
                "\n",
                "    def init_weights(self, rng: jax.random.PRNGKey) -> Dict:\n",
                "        raise NotImplementedError(f\"init_weights method has to be implemented for {self}\")\n",
                "\n",
                "    @classmethod\n",
                "    def from_pretrained(\n",
                "        cls,\n",
                "        pretrained_model_name_or_path: Union[str, os.PathLike],\n",
                "        dtype: jnp.dtype = jnp.float32,\n",
                "        *model_args,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        r\"\"\"\n",
                "        Instantiate a pretrained flax model from a pre-trained model configuration.\n",
                "\n",
                "        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
                "        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
                "        task.\n",
                "\n",
                "        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
                "        weights are discarded.\n",
                "\n",
                "        Parameters:\n",
                "            pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
                "                Can be either:\n",
                "\n",
                "                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
                "                      Valid model ids are namespaced under a user or organization name, like\n",
                "                      `CompVis/stable-diffusion-v1-4`.\n",
                "                    - A path to a *directory* containing model weights saved using [`~ModelMixin.save_pretrained`],\n",
                "                      e.g., `./my_model_directory/`.\n",
                "            dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):\n",
                "                The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n",
                "                `jax.numpy.bfloat16` (on TPUs).\n",
                "\n",
                "                This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If\n",
                "                specified all the computation will be performed with the given `dtype`.\n",
                "\n",
                "                **Note that this only specifies the dtype of the computation and does not influence the dtype of model\n",
                "                parameters.**\n",
                "\n",
                "                If you wish to change the dtype of the model parameters, see [`~ModelMixin.to_fp16`] and\n",
                "                [`~ModelMixin.to_bf16`].\n",
                "            model_args (sequence of positional arguments, *optional*):\n",
                "                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n",
                "            cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
                "                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
                "                standard cache should not be used.\n",
                "            force_download (`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
                "                cached versions if they exist.\n",
                "            resume_download (`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
                "                file exists.\n",
                "            proxies (`Dict[str, str]`, *optional*):\n",
                "                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
                "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
                "            local_files_only(`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to only look at local files (i.e., do not try to download the model).\n",
                "            revision (`str`, *optional*, defaults to `\"main\"`):\n",
                "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
                "                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
                "                identifier allowed by git.\n",
                "            from_pt (`bool`, *optional*, defaults to `False`):\n",
                "                Load the model weights from a PyTorch checkpoint save file.\n",
                "            kwargs (remaining dictionary of keyword arguments, *optional*):\n",
                "                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
                "                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
                "                automatically loaded:\n",
                "\n",
                "                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
                "                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
                "                      already been done)\n",
                "                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
                "                      initialization function ([`~ConfigMixin.from_config`]). Each key of `kwargs` that corresponds to\n",
                "                      a configuration attribute will be used to override said attribute with the supplied `kwargs`\n",
                "                      value. Remaining keys that do not correspond to any configuration attribute will be passed to the\n",
                "                      underlying model's `__init__` function.\n",
                "\n",
                "        Examples:\n",
                "\n",
                "        ```python\n",
                "        >>> from diffusers import FlaxUNet2DConditionModel\n",
                "\n",
                "        >>> # Download model and configuration from huggingface.co and cache.\n",
                "        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
                "        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n",
                "        >>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"./test/saved_model/\")\n",
                "        ```\"\"\"\n",
                "        config = kwargs.pop(\"config\", None)\n",
                "        cache_dir = kwargs.pop(\"cache_dir\", DIFFUSERS_CACHE)\n",
                "        force_download = kwargs.pop(\"force_download\", False)\n",
                "        from_pt = kwargs.pop(\"from_pt\", False)\n",
                "        resume_download = kwargs.pop(\"resume_download\", False)\n",
                "        proxies = kwargs.pop(\"proxies\", None)\n",
                "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
                "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
                "        revision = kwargs.pop(\"revision\", None)\n",
                "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
                "        subfolder = kwargs.pop(\"subfolder\", None)\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        user_agent = {\"file_type\": \"model\", \"framework\": \"flax\", \"from_auto_class\": from_auto_class}\n"
                ],
                "after": [
                    "        user_agent = {\n",
                    "            \"diffusers\": __version__,\n",
                    "            \"file_type\": \"model\",\n",
                    "            \"framework\": \"flax\",\n",
                    "            \"from_auto_class\": from_auto_class,\n",
                    "        }\n"
                ],
                "parent_version_range": {
                    "start": 291,
                    "end": 292
                },
                "child_version_range": {
                    "start": 291,
                    "end": 297
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "FlaxModelMixin",
                        "signature": "class FlaxModelMixin:",
                        "at_line": 44
                    },
                    {
                        "type": "function",
                        "name": "from_pretrained",
                        "signature": "def from_pretrained(\n        cls,\n        pretrained_model_name_or_path: Union[str, os.PathLike],\n        dtype: jnp.dtype = jnp.float32,\n        *model_args,\n        **kwargs,\n    ):",
                        "at_line": 195
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: src/diffusers/modeling_flax_utils.py\nCode:\n           class FlaxModelMixin:\n               ...\n               def from_pretrained(\n        cls,\n        pretrained_model_name_or_path: Union[str, os.PathLike],\n        dtype: jnp.dtype = jnp.float32,\n        *model_args,\n        **kwargs,\n    ):\n                   ...\n288 288            from_auto_class = kwargs.pop(\"_from_auto\", False)\n289 289            subfolder = kwargs.pop(\"subfolder\", None)\n290 290    \n291      -         user_agent = {\"file_type\": \"model\", \"framework\": \"flax\", \"from_auto_class\": from_auto_class}\n    291  +         user_agent = {\n    292  +             \"diffusers\": __version__,\n    293  +             \"file_type\": \"model\",\n    294  +             \"framework\": \"flax\",\n    295  +             \"from_auto_class\": from_auto_class,\n    296  +         }\n292 297    \n293 298            # Load config if we don't provide a configuration\n294 299            config_path = config if config is not None else pretrained_model_name_or_path\n         ...\n",
                "file_path": "src/diffusers/modeling_flax_utils.py",
                "identifiers_before": [
                    "from_auto_class",
                    "user_agent"
                ],
                "identifiers_after": [
                    "__version__",
                    "from_auto_class",
                    "user_agent"
                ],
                "prefix": [
                    "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
                    "        subfolder = kwargs.pop(\"subfolder\", None)\n",
                    "\n"
                ],
                "suffix": [
                    "\n",
                    "        # Load config if we don't provide a configuration\n",
                    "        config_path = config if config is not None else pretrained_model_name_or_path\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "__version__",
                            "position": {
                                "start": {
                                    "line": 292,
                                    "column": 25
                                },
                                "end": {
                                    "line": 292,
                                    "column": 36
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/diffusers/src/diffusers/modeling_flax_utils.py",
                            "hunk_idx": 1,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    3
                ]
            },
            [
                "\n",
                "        # Load config if we don't provide a configuration\n",
                "        config_path = config if config is not None else pretrained_model_name_or_path\n",
                "        model, model_kwargs = cls.from_config(\n",
                "            config_path,\n",
                "            cache_dir=cache_dir,\n",
                "            return_unused_kwargs=True,\n",
                "            force_download=force_download,\n",
                "            resume_download=resume_download,\n",
                "            proxies=proxies,\n",
                "            local_files_only=local_files_only,\n",
                "            use_auth_token=use_auth_token,\n",
                "            revision=revision,\n",
                "            subfolder=subfolder,\n",
                "            # model args\n",
                "            dtype=dtype,\n",
                "            **kwargs,\n",
                "        )\n",
                "\n",
                "        # Load model\n",
                "        pretrained_path_with_subfolder = (\n",
                "            pretrained_model_name_or_path\n",
                "            if subfolder is None\n",
                "            else os.path.join(pretrained_model_name_or_path, subfolder)\n",
                "        )\n",
                "        if os.path.isdir(pretrained_path_with_subfolder):\n",
                "            if from_pt:\n",
                "                if not os.path.isfile(os.path.join(pretrained_path_with_subfolder, WEIGHTS_NAME)):\n",
                "                    raise EnvironmentError(\n",
                "                        f\"Error no file named {WEIGHTS_NAME} found in directory {pretrained_path_with_subfolder} \"\n",
                "                    )\n",
                "                model_file = os.path.join(pretrained_path_with_subfolder, WEIGHTS_NAME)\n",
                "            elif os.path.isfile(os.path.join(pretrained_path_with_subfolder, FLAX_WEIGHTS_NAME)):\n",
                "                # Load from a Flax checkpoint\n",
                "                model_file = os.path.join(pretrained_path_with_subfolder, FLAX_WEIGHTS_NAME)\n",
                "            # Check if pytorch weights exist instead\n",
                "            elif os.path.isfile(os.path.join(pretrained_path_with_subfolder, WEIGHTS_NAME)):\n",
                "                raise EnvironmentError(\n",
                "                    f\"{WEIGHTS_NAME} file found in directory {pretrained_path_with_subfolder}. Please load the model\"\n",
                "                    \" using  `from_pt=True`.\"\n",
                "                )\n",
                "            else:\n",
                "                raise EnvironmentError(\n",
                "                    f\"Error no file named {FLAX_WEIGHTS_NAME} or {WEIGHTS_NAME} found in directory \"\n",
                "                    f\"{pretrained_path_with_subfolder}.\"\n",
                "                )\n",
                "        else:\n",
                "            try:\n",
                "                model_file = hf_hub_download(\n",
                "                    pretrained_model_name_or_path,\n",
                "                    filename=FLAX_WEIGHTS_NAME if not from_pt else WEIGHTS_NAME,\n",
                "                    cache_dir=cache_dir,\n",
                "                    force_download=force_download,\n",
                "                    proxies=proxies,\n",
                "                    resume_download=resume_download,\n",
                "                    local_files_only=local_files_only,\n",
                "                    use_auth_token=use_auth_token,\n",
                "                    user_agent=user_agent,\n",
                "                    subfolder=subfolder,\n",
                "                    revision=revision,\n",
                "                )\n",
                "\n",
                "            except RepositoryNotFoundError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier \"\n",
                "                    \"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a \"\n",
                "                    \"token having permission to this repo with `use_auth_token` or log in with `huggingface-cli \"\n",
                "                    \"login`.\"\n",
                "                )\n",
                "            except RevisionNotFoundError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for \"\n",
                "                    \"this model name. Check the model page at \"\n",
                "                    f\"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions.\"\n",
                "                )\n",
                "            except EntryNotFoundError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"{pretrained_model_name_or_path} does not appear to have a file named {FLAX_WEIGHTS_NAME}.\"\n",
                "                )\n",
                "            except HTTPError as err:\n",
                "                raise EnvironmentError(\n",
                "                    f\"There was a specific connection error when trying to load {pretrained_model_name_or_path}:\\n\"\n",
                "                    f\"{err}\"\n",
                "                )\n",
                "            except ValueError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it\"\n",
                "                    f\" in the cached files and it looks like {pretrained_model_name_or_path} is not the path to a\"\n",
                "                    f\" directory containing a file named {FLAX_WEIGHTS_NAME} or {WEIGHTS_NAME}.\\nCheckout your\"\n",
                "                    \" internet connection or see how to run the library in offline mode at\"\n",
                "                    \" 'https://huggingface.co/docs/transformers/installation#offline-mode'.\"\n",
                "                )\n",
                "            except EnvironmentError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\n",
                "                    \"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\n",
                "                    f\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\n",
                "                    f\"containing a file named {FLAX_WEIGHTS_NAME} or {WEIGHTS_NAME}.\"\n",
                "                )\n",
                "\n",
                "        if from_pt:\n",
                "            if is_torch_available():\n",
                "                from .modeling_utils import load_state_dict\n",
                "            else:\n",
                "                raise EnvironmentError(\n",
                "                    \"Can't load the model in PyTorch format because PyTorch is not installed. \"\n",
                "                    \"Please, install PyTorch or use native Flax weights.\"\n",
                "                )\n",
                "\n",
                "            # Step 1: Get the pytorch file\n",
                "            pytorch_model_file = load_state_dict(model_file)\n",
                "\n",
                "            # Step 2: Convert the weights\n",
                "            state = convert_pytorch_state_dict_to_flax(pytorch_model_file, model)\n",
                "        else:\n",
                "            try:\n",
                "                with open(model_file, \"rb\") as state_f:\n",
                "                    state = from_bytes(cls, state_f.read())\n",
                "            except (UnpicklingError, msgpack.exceptions.ExtraData) as e:\n",
                "                try:\n",
                "                    with open(model_file) as f:\n",
                "                        if f.read().startswith(\"version\"):\n",
                "                            raise OSError(\n",
                "                                \"You seem to have cloned a repository without having git-lfs installed. Please\"\n",
                "                                \" install git-lfs and run `git lfs install` followed by `git lfs pull` in the\"\n",
                "                                \" folder you cloned.\"\n",
                "                            )\n",
                "                        else:\n",
                "                            raise ValueError from e\n",
                "                except (UnicodeDecodeError, ValueError):\n",
                "                    raise EnvironmentError(f\"Unable to convert {model_file} to Flax deserializable object. \")\n",
                "            # make sure all arrays are stored as jnp.ndarray\n",
                "            # NOTE: This is to prevent a bug this will be fixed in Flax >= v0.3.4:\n",
                "            # https://github.com/google/flax/issues/1261\n",
                "        state = jax.tree_util.tree_map(lambda x: jax.device_put(x, jax.devices(\"cpu\")[0]), state)\n",
                "\n",
                "        # flatten dicts\n",
                "        state = flatten_dict(state)\n",
                "\n",
                "        params_shape_tree = jax.eval_shape(model.init_weights, rng=jax.random.PRNGKey(0))\n",
                "        required_params = set(flatten_dict(unfreeze(params_shape_tree)).keys())\n",
                "\n",
                "        shape_state = flatten_dict(unfreeze(params_shape_tree))\n",
                "\n",
                "        missing_keys = required_params - set(state.keys())\n",
                "        unexpected_keys = set(state.keys()) - required_params\n",
                "\n",
                "        if missing_keys:\n",
                "            logger.warning(\n",
                "                f\"The checkpoint {pretrained_model_name_or_path} is missing required keys: {missing_keys}. \"\n",
                "                \"Make sure to call model.init_weights to initialize the missing weights.\"\n",
                "            )\n",
                "            cls._missing_keys = missing_keys\n",
                "\n",
                "        for key in state.keys():\n",
                "            if key in shape_state and state[key].shape != shape_state[key].shape:\n",
                "                raise ValueError(\n",
                "                    f\"Trying to load the pretrained weight for {key} failed: checkpoint has shape \"\n",
                "                    f\"{state[key].shape} which is incompatible with the model shape {shape_state[key].shape}. \"\n",
                "                )\n",
                "\n",
                "        # remove unexpected keys to not be saved again\n",
                "        for unexpected_key in unexpected_keys:\n",
                "            del state[unexpected_key]\n",
                "\n",
                "        if len(unexpected_keys) > 0:\n",
                "            logger.warning(\n",
                "                f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when\"\n",
                "                f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n",
                "                f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n",
                "                \" with another architecture.\"\n",
                "            )\n",
                "        else:\n",
                "            logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n",
                "\n",
                "        if len(missing_keys) > 0:\n",
                "            logger.warning(\n",
                "                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n",
                "                f\" {pretrained_model_name_or_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n",
                "                \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n",
                "            )\n",
                "        else:\n",
                "            logger.info(\n",
                "                f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n",
                "                f\" {pretrained_model_name_or_path}.\\nIf your task is similar to the task the model of the checkpoint\"\n",
                "                f\" was trained on, you can already use {model.__class__.__name__} for predictions without further\"\n",
                "                \" training.\"\n",
                "            )\n",
                "\n",
                "        # dictionary of key: dtypes for the model params\n",
                "        param_dtypes = jax.tree_map(lambda x: x.dtype, state)\n",
                "        # extract keys of parameters not in jnp.float32\n",
                "        fp16_params = [k for k in param_dtypes if param_dtypes[k] == jnp.float16]\n",
                "        bf16_params = [k for k in param_dtypes if param_dtypes[k] == jnp.bfloat16]\n",
                "\n",
                "        # raise a warning if any of the parameters are not in jnp.float32\n",
                "        if len(fp16_params) > 0:\n",
                "            logger.warning(\n",
                "                f\"Some of the weights of {model.__class__.__name__} were initialized in float16 precision from \"\n",
                "                f\"the model checkpoint at {pretrained_model_name_or_path}:\\n{fp16_params}\\n\"\n",
                "                \"You should probably UPCAST the model weights to float32 if this was not intended. \"\n",
                "                \"See [`~ModelMixin.to_fp32`] for further information on how to do this.\"\n",
                "            )\n",
                "\n",
                "        if len(bf16_params) > 0:\n",
                "            logger.warning(\n",
                "                f\"Some of the weights of {model.__class__.__name__} were initialized in bfloat16 precision from \"\n",
                "                f\"the model checkpoint at {pretrained_model_name_or_path}:\\n{bf16_params}\\n\"\n",
                "                \"You should probably UPCAST the model weights to float32 if this was not intended. \"\n",
                "                \"See [`~ModelMixin.to_fp32`] for further information on how to do this.\"\n",
                "            )\n",
                "\n",
                "        return model, unflatten_dict(state)\n",
                "\n",
                "    def save_pretrained(\n",
                "        self,\n",
                "        save_directory: Union[str, os.PathLike],\n",
                "        params: Union[Dict, FrozenDict],\n",
                "        is_main_process: bool = True,\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
                "        `[`~FlaxModelMixin.from_pretrained`]` class method\n",
                "\n",
                "        Arguments:\n",
                "            save_directory (`str` or `os.PathLike`):\n",
                "                Directory to which to save. Will be created if it doesn't exist.\n",
                "            params (`Union[Dict, FrozenDict]`):\n",
                "                A `PyTree` of model parameters.\n",
                "            is_main_process (`bool`, *optional*, defaults to `True`):\n",
                "                Whether the process calling this is the main process or not. Useful when in distributed training like\n",
                "                TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n",
                "                the main process to avoid race conditions.\n",
                "        \"\"\"\n",
                "        if os.path.isfile(save_directory):\n",
                "            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n",
                "            return\n",
                "\n",
                "        os.makedirs(save_directory, exist_ok=True)\n",
                "\n",
                "        model_to_save = self\n",
                "\n",
                "        # Attach architecture to the config\n",
                "        # Save the config\n",
                "        if is_main_process:\n",
                "            model_to_save.save_config(save_directory)\n",
                "\n",
                "        # save model\n",
                "        output_model_file = os.path.join(save_directory, FLAX_WEIGHTS_NAME)\n",
                "        with open(output_model_file, \"wb\") as f:\n",
                "            model_bytes = to_bytes(params)\n",
                "            f.write(model_bytes)\n",
                "\n",
                "        logger.info(f\"Model weights saved in {output_model_file}\")"
            ]
        ],
        "src/diffusers/modeling_utils.py": [
            [
                "# coding=utf-8\n",
                "# Copyright 2022 The HuggingFace Inc. team.\n",
                "# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     http://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\n",
                "import os\n",
                "from functools import partial\n",
                "from typing import Callable, List, Optional, Tuple, Union\n",
                "\n",
                "import torch\n",
                "from torch import Tensor, device\n",
                "\n",
                "from diffusers.utils import is_accelerate_available\n",
                "from huggingface_hub import hf_hub_download\n",
                "from huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError\n",
                "from requests import HTTPError\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "from . import __version__\n"
                ],
                "parent_version_range": {
                    "start": 28,
                    "end": 28
                },
                "child_version_range": {
                    "start": 28,
                    "end": 29
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 2,
                "hunk_diff": "File: src/diffusers/modeling_utils.py\nCode:\n  ...\n25 25    from huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError\n26 26    from requests import HTTPError\n27 27    \n   28  + from . import __version__\n28 29    from .utils import CONFIG_NAME, DIFFUSERS_CACHE, HUGGINGFACE_CO_RESOLVE_ENDPOINT, WEIGHTS_NAME, logging\n29 30    \n30 31    \n       ...\n",
                "file_path": "src/diffusers/modeling_utils.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "__version__"
                ],
                "prefix": [
                    "from huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError\n",
                    "from requests import HTTPError\n",
                    "\n"
                ],
                "suffix": [
                    "from .utils import CONFIG_NAME, DIFFUSERS_CACHE, HUGGINGFACE_CO_RESOLVE_ENDPOINT, WEIGHTS_NAME, logging\n",
                    "\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "__version__",
                            "position": {
                                "start": {
                                    "line": 28,
                                    "column": 14
                                },
                                "end": {
                                    "line": 28,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/diffusers/src/diffusers/modeling_utils.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    4
                ]
            },
            [
                "from .utils import CONFIG_NAME, DIFFUSERS_CACHE, HUGGINGFACE_CO_RESOLVE_ENDPOINT, WEIGHTS_NAME, logging\n",
                "\n",
                "\n",
                "logger = logging.get_logger(__name__)\n",
                "\n",
                "\n",
                "def get_parameter_device(parameter: torch.nn.Module):\n",
                "    try:\n",
                "        return next(parameter.parameters()).device\n",
                "    except StopIteration:\n",
                "        # For torch.nn.DataParallel compatibility in PyTorch 1.5\n",
                "\n",
                "        def find_tensor_attributes(module: torch.nn.Module) -> List[Tuple[str, Tensor]]:\n",
                "            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
                "            return tuples\n",
                "\n",
                "        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n",
                "        first_tuple = next(gen)\n",
                "        return first_tuple[1].device\n",
                "\n",
                "\n",
                "def get_parameter_dtype(parameter: torch.nn.Module):\n",
                "    try:\n",
                "        return next(parameter.parameters()).dtype\n",
                "    except StopIteration:\n",
                "        # For torch.nn.DataParallel compatibility in PyTorch 1.5\n",
                "\n",
                "        def find_tensor_attributes(module: torch.nn.Module) -> List[Tuple[str, Tensor]]:\n",
                "            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
                "            return tuples\n",
                "\n",
                "        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n",
                "        first_tuple = next(gen)\n",
                "        return first_tuple[1].dtype\n",
                "\n",
                "\n",
                "def load_state_dict(checkpoint_file: Union[str, os.PathLike]):\n",
                "    \"\"\"\n",
                "    Reads a PyTorch checkpoint file, returning properly formatted errors if they arise.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        return torch.load(checkpoint_file, map_location=\"cpu\")\n",
                "    except Exception as e:\n",
                "        try:\n",
                "            with open(checkpoint_file) as f:\n",
                "                if f.read().startswith(\"version\"):\n",
                "                    raise OSError(\n",
                "                        \"You seem to have cloned a repository without having git-lfs installed. Please install \"\n",
                "                        \"git-lfs and run `git lfs install` followed by `git lfs pull` in the folder \"\n",
                "                        \"you cloned.\"\n",
                "                    )\n",
                "                else:\n",
                "                    raise ValueError(\n",
                "                        f\"Unable to locate the file {checkpoint_file} which is necessary to load this pretrained \"\n",
                "                        \"model. Make sure you have saved the model properly.\"\n",
                "                    ) from e\n",
                "        except (UnicodeDecodeError, ValueError):\n",
                "            raise OSError(\n",
                "                f\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\n",
                "                f\"at '{checkpoint_file}'. \"\n",
                "                \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\"\n",
                "            )\n",
                "\n",
                "\n",
                "def _load_state_dict_into_model(model_to_load, state_dict):\n",
                "    # Convert old format to new format if needed from a PyTorch state_dict\n",
                "    # copy state_dict so _load_from_state_dict can modify it\n",
                "    state_dict = state_dict.copy()\n",
                "    error_msgs = []\n",
                "\n",
                "    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n",
                "    # so we need to apply the function recursively.\n",
                "    def load(module: torch.nn.Module, prefix=\"\"):\n",
                "        args = (state_dict, prefix, {}, True, [], [], error_msgs)\n",
                "        module._load_from_state_dict(*args)\n",
                "\n",
                "        for name, child in module._modules.items():\n",
                "            if child is not None:\n",
                "                load(child, prefix + name + \".\")\n",
                "\n",
                "    load(model_to_load)\n",
                "\n",
                "    return error_msgs\n",
                "\n",
                "\n",
                "class ModelMixin(torch.nn.Module):\n",
                "    r\"\"\"\n",
                "    Base class for all models.\n",
                "\n",
                "    [`ModelMixin`] takes care of storing the configuration of the models and handles methods for loading, downloading\n",
                "    and saving models.\n",
                "\n",
                "        - **config_name** ([`str`]) -- A filename under which the model should be stored when calling\n",
                "          [`~modeling_utils.ModelMixin.save_pretrained`].\n",
                "    \"\"\"\n",
                "    config_name = CONFIG_NAME\n",
                "    _automatically_saved_args = [\"_diffusers_version\", \"_class_name\", \"_name_or_path\"]\n",
                "    _supports_gradient_checkpointing = False\n",
                "\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "\n",
                "    @property\n",
                "    def is_gradient_checkpointing(self) -> bool:\n",
                "        \"\"\"\n",
                "        Whether gradient checkpointing is activated for this model or not.\n",
                "\n",
                "        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
                "        activations\".\n",
                "        \"\"\"\n",
                "        return any(hasattr(m, \"gradient_checkpointing\") and m.gradient_checkpointing for m in self.modules())\n",
                "\n",
                "    def enable_gradient_checkpointing(self):\n",
                "        \"\"\"\n",
                "        Activates gradient checkpointing for the current model.\n",
                "\n",
                "        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
                "        activations\".\n",
                "        \"\"\"\n",
                "        if not self._supports_gradient_checkpointing:\n",
                "            raise ValueError(f\"{self.__class__.__name__} does not support gradient checkpointing.\")\n",
                "        self.apply(partial(self._set_gradient_checkpointing, value=True))\n",
                "\n",
                "    def disable_gradient_checkpointing(self):\n",
                "        \"\"\"\n",
                "        Deactivates gradient checkpointing for the current model.\n",
                "\n",
                "        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
                "        activations\".\n",
                "        \"\"\"\n",
                "        if self._supports_gradient_checkpointing:\n",
                "            self.apply(partial(self._set_gradient_checkpointing, value=False))\n",
                "\n",
                "    def save_pretrained(\n",
                "        self,\n",
                "        save_directory: Union[str, os.PathLike],\n",
                "        is_main_process: bool = True,\n",
                "        save_function: Callable = torch.save,\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
                "        `[`~modeling_utils.ModelMixin.from_pretrained`]` class method.\n",
                "\n",
                "        Arguments:\n",
                "            save_directory (`str` or `os.PathLike`):\n",
                "                Directory to which to save. Will be created if it doesn't exist.\n",
                "            is_main_process (`bool`, *optional*, defaults to `True`):\n",
                "                Whether the process calling this is the main process or not. Useful when in distributed training like\n",
                "                TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n",
                "                the main process to avoid race conditions.\n",
                "            save_function (`Callable`):\n",
                "                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
                "                need to replace `torch.save` by another method.\n",
                "        \"\"\"\n",
                "        if os.path.isfile(save_directory):\n",
                "            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n",
                "            return\n",
                "\n",
                "        os.makedirs(save_directory, exist_ok=True)\n",
                "\n",
                "        model_to_save = self\n",
                "\n",
                "        # Attach architecture to the config\n",
                "        # Save the config\n",
                "        if is_main_process:\n",
                "            model_to_save.save_config(save_directory)\n",
                "\n",
                "        # Save the model\n",
                "        state_dict = model_to_save.state_dict()\n",
                "\n",
                "        # Clean the folder from a previous save\n",
                "        for filename in os.listdir(save_directory):\n",
                "            full_filename = os.path.join(save_directory, filename)\n",
                "            # If we have a shard file that is not going to be replaced, we delete it, but only from the main process\n",
                "            # in distributed settings to avoid race conditions.\n",
                "            if filename.startswith(WEIGHTS_NAME[:-4]) and os.path.isfile(full_filename) and is_main_process:\n",
                "                os.remove(full_filename)\n",
                "\n",
                "        # Save the model\n",
                "        save_function(state_dict, os.path.join(save_directory, WEIGHTS_NAME))\n",
                "\n",
                "        logger.info(f\"Model weights saved in {os.path.join(save_directory, WEIGHTS_NAME)}\")\n",
                "\n",
                "    @classmethod\n",
                "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
                "        r\"\"\"\n",
                "        Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
                "\n",
                "        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n",
                "        the model, you should first set it back in training mode with `model.train()`.\n",
                "\n",
                "        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
                "        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
                "        task.\n",
                "\n",
                "        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
                "        weights are discarded.\n",
                "\n",
                "        Parameters:\n",
                "            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
                "                Can be either:\n",
                "\n",
                "                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
                "                      Valid model ids should have an organization name, like `google/ddpm-celebahq-256`.\n",
                "                    - A path to a *directory* containing model weights saved using [`~ModelMixin.save_config`], e.g.,\n",
                "                      `./my_model_directory/`.\n",
                "\n",
                "            cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
                "                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
                "                standard cache should not be used.\n",
                "            torch_dtype (`str` or `torch.dtype`, *optional*):\n",
                "                Override the default `torch.dtype` and load the model under this dtype. If `\"auto\"` is passed the dtype\n",
                "                will be automatically derived from the model's weights.\n",
                "            force_download (`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
                "                cached versions if they exist.\n",
                "            resume_download (`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
                "                file exists.\n",
                "            proxies (`Dict[str, str]`, *optional*):\n",
                "                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
                "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
                "            output_loading_info(`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
                "            local_files_only(`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to only look at local files (i.e., do not try to download the model).\n",
                "            use_auth_token (`str` or *bool*, *optional*):\n",
                "                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
                "                when running `diffusers-cli login` (stored in `~/.huggingface`).\n",
                "            revision (`str`, *optional*, defaults to `\"main\"`):\n",
                "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
                "                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
                "                identifier allowed by git.\n",
                "            subfolder (`str`, *optional*, defaults to `\"\"`):\n",
                "                In case the relevant files are located inside a subfolder of the model repo (either remote in\n",
                "                huggingface.co or downloaded locally), you can specify the folder name here.\n",
                "\n",
                "            mirror (`str`, *optional*):\n",
                "                Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
                "                problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
                "                Please refer to the mirror site for more information.\n",
                "\n",
                "        <Tip>\n",
                "\n",
                "         It is required to be logged in (`huggingface-cli login`) when you want to use private or [gated\n",
                "         models](https://huggingface.co/docs/hub/models-gated#gated-models).\n",
                "\n",
                "        </Tip>\n",
                "\n",
                "        <Tip>\n",
                "\n",
                "        Activate the special [\"offline-mode\"](https://huggingface.co/diffusers/installation.html#offline-mode) to use\n",
                "        this method in a firewalled environment.\n",
                "\n",
                "        </Tip>\n",
                "\n",
                "        \"\"\"\n",
                "        cache_dir = kwargs.pop(\"cache_dir\", DIFFUSERS_CACHE)\n",
                "        ignore_mismatched_sizes = kwargs.pop(\"ignore_mismatched_sizes\", False)\n",
                "        force_download = kwargs.pop(\"force_download\", False)\n",
                "        resume_download = kwargs.pop(\"resume_download\", False)\n",
                "        proxies = kwargs.pop(\"proxies\", None)\n",
                "        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n",
                "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
                "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
                "        revision = kwargs.pop(\"revision\", None)\n",
                "        from_auto_class = kwargs.pop(\"_from_auto\", False)\n",
                "        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n",
                "        subfolder = kwargs.pop(\"subfolder\", None)\n",
                "        device_map = kwargs.pop(\"device_map\", None)\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n"
                ],
                "after": [
                    "        user_agent = {\n",
                    "            \"diffusers\": __version__,\n",
                    "            \"file_type\": \"model\",\n",
                    "            \"framework\": \"pytorch\",\n",
                    "            \"from_auto_class\": from_auto_class,\n",
                    "        }\n"
                ],
                "parent_version_range": {
                    "start": 299,
                    "end": 300
                },
                "child_version_range": {
                    "start": 300,
                    "end": 306
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "ModelMixin",
                        "signature": "class ModelMixin(torch.nn.Module):",
                        "at_line": 113
                    },
                    {
                        "type": "function",
                        "name": "from_pretrained",
                        "signature": "def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):",
                        "at_line": 212
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: src/diffusers/modeling_utils.py\nCode:\n           class ModelMixin(torch.nn.Module):\n               ...\n               def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n                   ...\n296 297            subfolder = kwargs.pop(\"subfolder\", None)\n297 298            device_map = kwargs.pop(\"device_map\", None)\n298 299    \n299      -         user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n    300  +         user_agent = {\n    301  +             \"diffusers\": __version__,\n    302  +             \"file_type\": \"model\",\n    303  +             \"framework\": \"pytorch\",\n    304  +             \"from_auto_class\": from_auto_class,\n    305  +         }\n300 306    \n301 307            # Load config if we don't provide a configuration\n302 308            config_path = pretrained_model_name_or_path\n         ...\n",
                "file_path": "src/diffusers/modeling_utils.py",
                "identifiers_before": [
                    "from_auto_class",
                    "user_agent"
                ],
                "identifiers_after": [
                    "__version__",
                    "from_auto_class",
                    "user_agent"
                ],
                "prefix": [
                    "        subfolder = kwargs.pop(\"subfolder\", None)\n",
                    "        device_map = kwargs.pop(\"device_map\", None)\n",
                    "\n"
                ],
                "suffix": [
                    "\n",
                    "        # Load config if we don't provide a configuration\n",
                    "        config_path = pretrained_model_name_or_path\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "__version__",
                            "position": {
                                "start": {
                                    "line": 301,
                                    "column": 25
                                },
                                "end": {
                                    "line": 301,
                                    "column": 36
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/diffusers/src/diffusers/modeling_utils.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    1
                ]
            },
            [
                "\n",
                "        # Load config if we don't provide a configuration\n",
                "        config_path = pretrained_model_name_or_path\n",
                "\n",
                "        # This variable will flag if we're loading a sharded checkpoint. In this case the archive file is just the\n",
                "        # Load model\n",
                "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
                "        if os.path.isdir(pretrained_model_name_or_path):\n",
                "            if os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n",
                "                # Load from a PyTorch checkpoint\n",
                "                model_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n",
                "            elif subfolder is not None and os.path.isfile(\n",
                "                os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_NAME)\n",
                "            ):\n",
                "                model_file = os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_NAME)\n",
                "            else:\n",
                "                raise EnvironmentError(\n",
                "                    f\"Error no file named {WEIGHTS_NAME} found in directory {pretrained_model_name_or_path}.\"\n",
                "                )\n",
                "        else:\n",
                "            try:\n",
                "                # Load from URL or cache if already cached\n",
                "                model_file = hf_hub_download(\n",
                "                    pretrained_model_name_or_path,\n",
                "                    filename=WEIGHTS_NAME,\n",
                "                    cache_dir=cache_dir,\n",
                "                    force_download=force_download,\n",
                "                    proxies=proxies,\n",
                "                    resume_download=resume_download,\n",
                "                    local_files_only=local_files_only,\n",
                "                    use_auth_token=use_auth_token,\n",
                "                    user_agent=user_agent,\n",
                "                    subfolder=subfolder,\n",
                "                    revision=revision,\n",
                "                )\n",
                "\n",
                "            except RepositoryNotFoundError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier \"\n",
                "                    \"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a \"\n",
                "                    \"token having permission to this repo with `use_auth_token` or log in with `huggingface-cli \"\n",
                "                    \"login`.\"\n",
                "                )\n",
                "            except RevisionNotFoundError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for \"\n",
                "                    \"this model name. Check the model page at \"\n",
                "                    f\"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions.\"\n",
                "                )\n",
                "            except EntryNotFoundError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"{pretrained_model_name_or_path} does not appear to have a file named {WEIGHTS_NAME}.\"\n",
                "                )\n",
                "            except HTTPError as err:\n",
                "                raise EnvironmentError(\n",
                "                    \"There was a specific connection error when trying to load\"\n",
                "                    f\" {pretrained_model_name_or_path}:\\n{err}\"\n",
                "                )\n",
                "            except ValueError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it\"\n",
                "                    f\" in the cached files and it looks like {pretrained_model_name_or_path} is not the path to a\"\n",
                "                    f\" directory containing a file named {WEIGHTS_NAME} or\"\n",
                "                    \" \\nCheckout your internet connection or see how to run the library in\"\n",
                "                    \" offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.\"\n",
                "                )\n",
                "            except EnvironmentError:\n",
                "                raise EnvironmentError(\n",
                "                    f\"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\n",
                "                    \"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\n",
                "                    f\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\n",
                "                    f\"containing a file named {WEIGHTS_NAME}\"\n",
                "                )\n",
                "\n",
                "            # restore default dtype\n",
                "\n",
                "        if device_map == \"auto\":\n",
                "            if is_accelerate_available():\n",
                "                import accelerate\n",
                "            else:\n",
                "                raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n",
                "\n",
                "            with accelerate.init_empty_weights():\n",
                "                model, unused_kwargs = cls.from_config(\n",
                "                    config_path,\n",
                "                    cache_dir=cache_dir,\n",
                "                    return_unused_kwargs=True,\n",
                "                    force_download=force_download,\n",
                "                    resume_download=resume_download,\n",
                "                    proxies=proxies,\n",
                "                    local_files_only=local_files_only,\n",
                "                    use_auth_token=use_auth_token,\n",
                "                    revision=revision,\n",
                "                    subfolder=subfolder,\n",
                "                    device_map=device_map,\n",
                "                    **kwargs,\n",
                "                )\n",
                "\n",
                "            accelerate.load_checkpoint_and_dispatch(model, model_file, device_map)\n",
                "\n",
                "            loading_info = {\n",
                "                \"missing_keys\": [],\n",
                "                \"unexpected_keys\": [],\n",
                "                \"mismatched_keys\": [],\n",
                "                \"error_msgs\": [],\n",
                "            }\n",
                "        else:\n",
                "            model, unused_kwargs = cls.from_config(\n",
                "                config_path,\n",
                "                cache_dir=cache_dir,\n",
                "                return_unused_kwargs=True,\n",
                "                force_download=force_download,\n",
                "                resume_download=resume_download,\n",
                "                proxies=proxies,\n",
                "                local_files_only=local_files_only,\n",
                "                use_auth_token=use_auth_token,\n",
                "                revision=revision,\n",
                "                subfolder=subfolder,\n",
                "                device_map=device_map,\n",
                "                **kwargs,\n",
                "            )\n",
                "\n",
                "            state_dict = load_state_dict(model_file)\n",
                "            model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(\n",
                "                model,\n",
                "                state_dict,\n",
                "                model_file,\n",
                "                pretrained_model_name_or_path,\n",
                "                ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
                "            )\n",
                "\n",
                "            loading_info = {\n",
                "                \"missing_keys\": missing_keys,\n",
                "                \"unexpected_keys\": unexpected_keys,\n",
                "                \"mismatched_keys\": mismatched_keys,\n",
                "                \"error_msgs\": error_msgs,\n",
                "            }\n",
                "\n",
                "        if torch_dtype is not None and not isinstance(torch_dtype, torch.dtype):\n",
                "            raise ValueError(\n",
                "                f\"{torch_dtype} needs to be of type `torch.dtype`, e.g. `torch.float16`, but is {type(torch_dtype)}.\"\n",
                "            )\n",
                "        elif torch_dtype is not None:\n",
                "            model = model.to(torch_dtype)\n",
                "\n",
                "        model.register_to_config(_name_or_path=pretrained_model_name_or_path)\n",
                "\n",
                "        # Set model in evaluation mode to deactivate DropOut modules by default\n",
                "        model.eval()\n",
                "        if output_loading_info:\n",
                "            return model, loading_info\n",
                "\n",
                "        return model\n",
                "\n",
                "    @classmethod\n",
                "    def _load_pretrained_model(\n",
                "        cls,\n",
                "        model,\n",
                "        state_dict,\n",
                "        resolved_archive_file,\n",
                "        pretrained_model_name_or_path,\n",
                "        ignore_mismatched_sizes=False,\n",
                "    ):\n",
                "        # Retrieve missing & unexpected_keys\n",
                "        model_state_dict = model.state_dict()\n",
                "        loaded_keys = [k for k in state_dict.keys()]\n",
                "\n",
                "        expected_keys = list(model_state_dict.keys())\n",
                "\n",
                "        original_loaded_keys = loaded_keys\n",
                "\n",
                "        missing_keys = list(set(expected_keys) - set(loaded_keys))\n",
                "        unexpected_keys = list(set(loaded_keys) - set(expected_keys))\n",
                "\n",
                "        # Make sure we are able to load base models as well as derived models (with heads)\n",
                "        model_to_load = model\n",
                "\n",
                "        def _find_mismatched_keys(\n",
                "            state_dict,\n",
                "            model_state_dict,\n",
                "            loaded_keys,\n",
                "            ignore_mismatched_sizes,\n",
                "        ):\n",
                "            mismatched_keys = []\n",
                "            if ignore_mismatched_sizes:\n",
                "                for checkpoint_key in loaded_keys:\n",
                "                    model_key = checkpoint_key\n",
                "\n",
                "                    if (\n",
                "                        model_key in model_state_dict\n",
                "                        and state_dict[checkpoint_key].shape != model_state_dict[model_key].shape\n",
                "                    ):\n",
                "                        mismatched_keys.append(\n",
                "                            (checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape)\n",
                "                        )\n",
                "                        del state_dict[checkpoint_key]\n",
                "            return mismatched_keys\n",
                "\n",
                "        if state_dict is not None:\n",
                "            # Whole checkpoint\n",
                "            mismatched_keys = _find_mismatched_keys(\n",
                "                state_dict,\n",
                "                model_state_dict,\n",
                "                original_loaded_keys,\n",
                "                ignore_mismatched_sizes,\n",
                "            )\n",
                "            error_msgs = _load_state_dict_into_model(model_to_load, state_dict)\n",
                "\n",
                "        if len(error_msgs) > 0:\n",
                "            error_msg = \"\\n\\t\".join(error_msgs)\n",
                "            if \"size mismatch\" in error_msg:\n",
                "                error_msg += (\n",
                "                    \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n",
                "                )\n",
                "            raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n",
                "\n",
                "        if len(unexpected_keys) > 0:\n",
                "            logger.warning(\n",
                "                f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when\"\n",
                "                f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n",
                "                f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task\"\n",
                "                \" or with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n",
                "                \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n",
                "                f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly\"\n",
                "                \" identical (initializing a BertForSequenceClassification model from a\"\n",
                "                \" BertForSequenceClassification model).\"\n",
                "            )\n",
                "        else:\n",
                "            logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n",
                "        if len(missing_keys) > 0:\n",
                "            logger.warning(\n",
                "                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n",
                "                f\" {pretrained_model_name_or_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n",
                "                \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n",
                "            )\n",
                "        elif len(mismatched_keys) == 0:\n",
                "            logger.info(\n",
                "                f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n",
                "                f\" {pretrained_model_name_or_path}.\\nIf your task is similar to the task the model of the\"\n",
                "                f\" checkpoint was trained on, you can already use {model.__class__.__name__} for predictions\"\n",
                "                \" without further training.\"\n",
                "            )\n",
                "        if len(mismatched_keys) > 0:\n",
                "            mismatched_warning = \"\\n\".join(\n",
                "                [\n",
                "                    f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n",
                "                    for key, shape1, shape2 in mismatched_keys\n",
                "                ]\n",
                "            )\n",
                "            logger.warning(\n",
                "                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n",
                "                f\" {pretrained_model_name_or_path} and are newly initialized because the shapes did not\"\n",
                "                f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be\"\n",
                "                \" able to use it for predictions and inference.\"\n",
                "            )\n",
                "\n",
                "        return model, missing_keys, unexpected_keys, mismatched_keys, error_msgs\n",
                "\n",
                "    @property\n",
                "    def device(self) -> device:\n",
                "        \"\"\"\n",
                "        `torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n",
                "        device).\n",
                "        \"\"\"\n",
                "        return get_parameter_device(self)\n",
                "\n",
                "    @property\n",
                "    def dtype(self) -> torch.dtype:\n",
                "        \"\"\"\n",
                "        `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n",
                "        \"\"\"\n",
                "        return get_parameter_dtype(self)\n",
                "\n",
                "    def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n",
                "        \"\"\"\n",
                "        Get number of (optionally, trainable or non-embeddings) parameters in the module.\n",
                "\n",
                "        Args:\n",
                "            only_trainable (`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to return only the number of trainable parameters\n",
                "\n",
                "            exclude_embeddings (`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to return only the number of non-embeddings parameters\n",
                "\n",
                "        Returns:\n",
                "            `int`: The number of parameters.\n",
                "        \"\"\"\n",
                "\n",
                "        if exclude_embeddings:\n",
                "            embedding_param_names = [\n",
                "                f\"{name}.weight\"\n",
                "                for name, module_type in self.named_modules()\n",
                "                if isinstance(module_type, torch.nn.Embedding)\n",
                "            ]\n",
                "            non_embedding_parameters = [\n",
                "                parameter for name, parameter in self.named_parameters() if name not in embedding_param_names\n",
                "            ]\n",
                "            return sum(p.numel() for p in non_embedding_parameters if p.requires_grad or not only_trainable)\n",
                "        else:\n",
                "            return sum(p.numel() for p in self.parameters() if p.requires_grad or not only_trainable)\n",
                "\n",
                "\n",
                "def unwrap_model(model: torch.nn.Module) -> torch.nn.Module:\n",
                "    \"\"\"\n",
                "    Recursively unwraps a model from potential containers (as used in distributed training).\n",
                "\n",
                "    Args:\n",
                "        model (`torch.nn.Module`): The model to unwrap.\n",
                "    \"\"\"\n",
                "    # since there could be multiple levels of wrapping, unwrap recursively\n",
                "    if hasattr(model, \"module\"):\n",
                "        return unwrap_model(model.module)\n",
                "    else:\n",
                "        return model"
            ]
        ],
        "src/diffusers/pipeline_utils.py": [
            [
                "# coding=utf-8\n",
                "# Copyright 2022 The HuggingFace Inc. team.\n",
                "# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     http://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "\n",
                "import importlib\n",
                "import inspect\n",
                "import os\n",
                "from dataclasses import dataclass\n",
                "from typing import List, Optional, Union\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "import diffusers\n",
                "import PIL\n",
                "from huggingface_hub import snapshot_download\n",
                "from PIL import Image\n",
                "from tqdm.auto import tqdm\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "from . import __version__\n"
                ],
                "parent_version_range": {
                    "start": 31,
                    "end": 31
                },
                "child_version_range": {
                    "start": 31,
                    "end": 32
                },
                "control_flow": [],
                "structural_path": [],
                "idx": 4,
                "hunk_diff": "File: src/diffusers/pipeline_utils.py\nCode:\n  ...\n28 28    from PIL import Image\n29 29    from tqdm.auto import tqdm\n30 30    \n   31  + from . import __version__\n31 32    from .configuration_utils import ConfigMixin\n32 33    from .dynamic_modules_utils import get_class_from_dynamic_module\n33 34    from .schedulers.scheduling_utils import SCHEDULER_CONFIG_NAME\n       ...\n",
                "file_path": "src/diffusers/pipeline_utils.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "__version__"
                ],
                "prefix": [
                    "from PIL import Image\n",
                    "from tqdm.auto import tqdm\n",
                    "\n"
                ],
                "suffix": [
                    "from .configuration_utils import ConfigMixin\n",
                    "from .dynamic_modules_utils import get_class_from_dynamic_module\n",
                    "from .schedulers.scheduling_utils import SCHEDULER_CONFIG_NAME\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "__version__",
                            "position": {
                                "start": {
                                    "line": 31,
                                    "column": 14
                                },
                                "end": {
                                    "line": 31,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "import",
                            "abs_file_path": "/data2/chenyan/repos/diffusers/src/diffusers/pipeline_utils.py",
                            "hunk_idx": 4,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    2
                ]
            },
            [
                "from .configuration_utils import ConfigMixin\n",
                "from .dynamic_modules_utils import get_class_from_dynamic_module\n",
                "from .schedulers.scheduling_utils import SCHEDULER_CONFIG_NAME\n",
                "from .utils import (\n",
                "    CONFIG_NAME,\n",
                "    DIFFUSERS_CACHE,\n",
                "    ONNX_WEIGHTS_NAME,\n",
                "    WEIGHTS_NAME,\n",
                "    BaseOutput,\n",
                "    is_transformers_available,\n",
                "    logging,\n",
                ")\n",
                "\n",
                "\n",
                "if is_transformers_available():\n",
                "    from transformers import PreTrainedModel\n",
                "\n",
                "\n",
                "INDEX_FILE = \"diffusion_pytorch_model.bin\"\n",
                "CUSTOM_PIPELINE_FILE_NAME = \"pipeline.py\"\n",
                "\n",
                "\n",
                "logger = logging.get_logger(__name__)\n",
                "\n",
                "\n",
                "LOADABLE_CLASSES = {\n",
                "    \"diffusers\": {\n",
                "        \"ModelMixin\": [\"save_pretrained\", \"from_pretrained\"],\n",
                "        \"SchedulerMixin\": [\"save_config\", \"from_config\"],\n",
                "        \"DiffusionPipeline\": [\"save_pretrained\", \"from_pretrained\"],\n",
                "        \"OnnxRuntimeModel\": [\"save_pretrained\", \"from_pretrained\"],\n",
                "    },\n",
                "    \"transformers\": {\n",
                "        \"PreTrainedTokenizer\": [\"save_pretrained\", \"from_pretrained\"],\n",
                "        \"PreTrainedTokenizerFast\": [\"save_pretrained\", \"from_pretrained\"],\n",
                "        \"PreTrainedModel\": [\"save_pretrained\", \"from_pretrained\"],\n",
                "        \"FeatureExtractionMixin\": [\"save_pretrained\", \"from_pretrained\"],\n",
                "    },\n",
                "}\n",
                "\n",
                "ALL_IMPORTABLE_CLASSES = {}\n",
                "for library in LOADABLE_CLASSES:\n",
                "    ALL_IMPORTABLE_CLASSES.update(LOADABLE_CLASSES[library])\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class ImagePipelineOutput(BaseOutput):\n",
                "    \"\"\"\n",
                "    Output class for image pipelines.\n",
                "\n",
                "    Args:\n",
                "        images (`List[PIL.Image.Image]` or `np.ndarray`)\n",
                "            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n",
                "            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n",
                "    \"\"\"\n",
                "\n",
                "    images: Union[List[PIL.Image.Image], np.ndarray]\n",
                "\n",
                "\n",
                "class DiffusionPipeline(ConfigMixin):\n",
                "    r\"\"\"\n",
                "    Base class for all models.\n",
                "\n",
                "    [`DiffusionPipeline`] takes care of storing all components (models, schedulers, processors) for diffusion pipelines\n",
                "    and handles methods for loading, downloading and saving models as well as a few methods common to all pipelines to:\n",
                "\n",
                "        - move all PyTorch modules to the device of your choice\n",
                "        - enabling/disabling the progress bar for the denoising iteration\n",
                "\n",
                "    Class attributes:\n",
                "\n",
                "        - **config_name** ([`str`]) -- name of the config file that will store the class and module names of all\n",
                "          components of the diffusion pipeline.\n",
                "    \"\"\"\n",
                "    config_name = \"model_index.json\"\n",
                "\n",
                "    def register_modules(self, **kwargs):\n",
                "        # import it here to avoid circular import\n",
                "        from diffusers import pipelines\n",
                "\n",
                "        for name, module in kwargs.items():\n",
                "            # retrieve library\n",
                "            library = module.__module__.split(\".\")[0]\n",
                "\n",
                "            # check if the module is a pipeline module\n",
                "            pipeline_dir = module.__module__.split(\".\")[-2]\n",
                "            path = module.__module__.split(\".\")\n",
                "            is_pipeline_module = pipeline_dir in path and hasattr(pipelines, pipeline_dir)\n",
                "\n",
                "            # if library is not in LOADABLE_CLASSES, then it is a custom module.\n",
                "            # Or if it's a pipeline module, then the module is inside the pipeline\n",
                "            # folder so we set the library to module name.\n",
                "            if library not in LOADABLE_CLASSES or is_pipeline_module:\n",
                "                library = pipeline_dir\n",
                "\n",
                "            # retrieve class_name\n",
                "            class_name = module.__class__.__name__\n",
                "\n",
                "            register_dict = {name: (library, class_name)}\n",
                "\n",
                "            # save model index config\n",
                "            self.register_to_config(**register_dict)\n",
                "\n",
                "            # set models\n",
                "            setattr(self, name, module)\n",
                "\n",
                "    def save_pretrained(self, save_directory: Union[str, os.PathLike]):\n",
                "        \"\"\"\n",
                "        Save all variables of the pipeline that can be saved and loaded as well as the pipelines configuration file to\n",
                "        a directory. A pipeline variable can be saved and loaded if its class implements both a save and loading\n",
                "        method. The pipeline can easily be re-loaded using the `[`~DiffusionPipeline.from_pretrained`]` class method.\n",
                "\n",
                "        Arguments:\n",
                "            save_directory (`str` or `os.PathLike`):\n",
                "                Directory to which to save. Will be created if it doesn't exist.\n",
                "        \"\"\"\n",
                "        self.save_config(save_directory)\n",
                "\n",
                "        model_index_dict = dict(self.config)\n",
                "        model_index_dict.pop(\"_class_name\")\n",
                "        model_index_dict.pop(\"_diffusers_version\")\n",
                "        model_index_dict.pop(\"_module\", None)\n",
                "\n",
                "        for pipeline_component_name in model_index_dict.keys():\n",
                "            sub_model = getattr(self, pipeline_component_name)\n",
                "            model_cls = sub_model.__class__\n",
                "\n",
                "            save_method_name = None\n",
                "            # search for the model's base class in LOADABLE_CLASSES\n",
                "            for library_name, library_classes in LOADABLE_CLASSES.items():\n",
                "                library = importlib.import_module(library_name)\n",
                "                for base_class, save_load_methods in library_classes.items():\n",
                "                    class_candidate = getattr(library, base_class)\n",
                "                    if issubclass(model_cls, class_candidate):\n",
                "                        # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n",
                "                        save_method_name = save_load_methods[0]\n",
                "                        break\n",
                "                if save_method_name is not None:\n",
                "                    break\n",
                "\n",
                "            save_method = getattr(sub_model, save_method_name)\n",
                "            save_method(os.path.join(save_directory, pipeline_component_name))\n",
                "\n",
                "    def to(self, torch_device: Optional[Union[str, torch.device]] = None):\n",
                "        if torch_device is None:\n",
                "            return self\n",
                "\n",
                "        module_names, _ = self.extract_init_dict(dict(self.config))\n",
                "        for name in module_names.keys():\n",
                "            module = getattr(self, name)\n",
                "            if isinstance(module, torch.nn.Module):\n",
                "                if module.dtype == torch.float16 and str(torch_device) in [\"cpu\", \"mps\"]:\n",
                "                    logger.warning(\n",
                "                        \"Pipelines loaded with `torch_dtype=torch.float16` cannot run with `cpu` or `mps` device. It\"\n",
                "                        \" is not recommended to move them to `cpu` or `mps` as running them will fail. Please make\"\n",
                "                        \" sure to use a `cuda` device to run the pipeline in inference. due to the lack of support for\"\n",
                "                        \" `float16` operations on those devices in PyTorch. Please remove the\"\n",
                "                        \" `torch_dtype=torch.float16` argument, or use a `cuda` device to run inference.\"\n",
                "                    )\n",
                "                module.to(torch_device)\n",
                "        return self\n",
                "\n",
                "    @property\n",
                "    def device(self) -> torch.device:\n",
                "        r\"\"\"\n",
                "        Returns:\n",
                "            `torch.device`: The torch device on which the pipeline is located.\n",
                "        \"\"\"\n",
                "        module_names, _ = self.extract_init_dict(dict(self.config))\n",
                "        for name in module_names.keys():\n",
                "            module = getattr(self, name)\n",
                "            if isinstance(module, torch.nn.Module):\n",
                "                return module.device\n",
                "        return torch.device(\"cpu\")\n",
                "\n",
                "    @classmethod\n",
                "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
                "        r\"\"\"\n",
                "        Instantiate a PyTorch diffusion pipeline from pre-trained pipeline weights.\n",
                "\n",
                "        The pipeline is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated).\n",
                "\n",
                "        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
                "        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
                "        task.\n",
                "\n",
                "        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
                "        weights are discarded.\n",
                "\n",
                "        Parameters:\n",
                "            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
                "                Can be either:\n",
                "\n",
                "                    - A string, the *repo id* of a pretrained pipeline hosted inside a model repo on\n",
                "                      https://huggingface.co/ Valid repo ids have to be located under a user or organization name, like\n",
                "                      `CompVis/ldm-text2im-large-256`.\n",
                "                    - A path to a *directory* containing pipeline weights saved using\n",
                "                      [`~DiffusionPipeline.save_pretrained`], e.g., `./my_pipeline_directory/`.\n",
                "            torch_dtype (`str` or `torch.dtype`, *optional*):\n",
                "                Override the default `torch.dtype` and load the model under this dtype. If `\"auto\"` is passed the dtype\n",
                "                will be automatically derived from the model's weights.\n",
                "            custom_pipeline (`str`, *optional*):\n",
                "\n",
                "                <Tip warning={true}>\n",
                "\n",
                "                    This is an experimental feature and is likely to change in the future.\n",
                "\n",
                "                </Tip>\n",
                "\n",
                "                Can be either:\n",
                "\n",
                "                    - A string, the *repo id* of a custom pipeline hosted inside a model repo on\n",
                "                      https://huggingface.co/. Valid repo ids have to be located under a user or organization name,\n",
                "                      like `hf-internal-testing/diffusers-dummy-pipeline`.\n",
                "\n",
                "                        <Tip>\n",
                "\n",
                "                         It is required that the model repo has a file, called `pipeline.py` that defines the custom\n",
                "                         pipeline.\n",
                "\n",
                "                        </Tip>\n",
                "\n",
                "                    - A string, the *file name* of a community pipeline hosted on GitHub under\n",
                "                      https://github.com/huggingface/diffusers/tree/main/examples/community. Valid file names have to\n",
                "                      match exactly the file name without `.py` located under the above link, *e.g.*\n",
                "                      `clip_guided_stable_diffusion`.\n",
                "\n",
                "                        <Tip>\n",
                "\n",
                "                         Community pipelines are always loaded from the current `main` branch of GitHub.\n",
                "\n",
                "                        </Tip>\n",
                "\n",
                "                    - A path to a *directory* containing a custom pipeline, e.g., `./my_pipeline_directory/`.\n",
                "\n",
                "                        <Tip>\n",
                "\n",
                "                         It is required that the directory has a file, called `pipeline.py` that defines the custom\n",
                "                         pipeline.\n",
                "\n",
                "                        </Tip>\n",
                "\n",
                "                For more information on how to load and create custom pipelines, please have a look at [Loading and\n",
                "                Creating Custom\n",
                "                Pipelines](https://huggingface.co/docs/diffusers/main/en/using-diffusers/custom_pipelines)\n",
                "\n",
                "            torch_dtype (`str` or `torch.dtype`, *optional*):\n",
                "            force_download (`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
                "                cached versions if they exist.\n",
                "            resume_download (`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
                "                file exists.\n",
                "            proxies (`Dict[str, str]`, *optional*):\n",
                "                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
                "                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
                "            output_loading_info(`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
                "            local_files_only(`bool`, *optional*, defaults to `False`):\n",
                "                Whether or not to only look at local files (i.e., do not try to download the model).\n",
                "            use_auth_token (`str` or *bool*, *optional*):\n",
                "                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
                "                when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
                "            revision (`str`, *optional*, defaults to `\"main\"`):\n",
                "                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
                "                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
                "                identifier allowed by git.\n",
                "            mirror (`str`, *optional*):\n",
                "                Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
                "                problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
                "                Please refer to the mirror site for more information. specify the folder name here.\n",
                "\n",
                "            kwargs (remaining dictionary of keyword arguments, *optional*):\n",
                "                Can be used to overwrite load - and saveable variables - *i.e.* the pipeline components - of the\n",
                "                specific pipeline class. The overwritten components are then directly passed to the pipelines\n",
                "                `__init__` method. See example below for more information.\n",
                "\n",
                "        <Tip>\n",
                "\n",
                "         It is required to be logged in (`huggingface-cli login`) when you want to use private or [gated\n",
                "         models](https://huggingface.co/docs/hub/models-gated#gated-models), *e.g.* `\"CompVis/stable-diffusion-v1-4\"`\n",
                "\n",
                "        </Tip>\n",
                "\n",
                "        <Tip>\n",
                "\n",
                "        Activate the special [\"offline-mode\"](https://huggingface.co/diffusers/installation.html#offline-mode) to use\n",
                "        this method in a firewalled environment.\n",
                "\n",
                "        </Tip>\n",
                "\n",
                "        Examples:\n",
                "\n",
                "        ```py\n",
                "        >>> from diffusers import DiffusionPipeline\n",
                "\n",
                "        >>> # Download pipeline from huggingface.co and cache.\n",
                "        >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\")\n",
                "\n",
                "        >>> # Download pipeline that requires an authorization token\n",
                "        >>> # For more information on access tokens, please refer to this section\n",
                "        >>> # of the documentation](https://huggingface.co/docs/hub/security-tokens)\n",
                "        >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
                "\n",
                "        >>> # Download pipeline, but overwrite scheduler\n",
                "        >>> from diffusers import LMSDiscreteScheduler\n",
                "\n",
                "        >>> scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
                "        >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", scheduler=scheduler)\n",
                "        ```\n",
                "        \"\"\"\n",
                "        cache_dir = kwargs.pop(\"cache_dir\", DIFFUSERS_CACHE)\n",
                "        resume_download = kwargs.pop(\"resume_download\", False)\n",
                "        proxies = kwargs.pop(\"proxies\", None)\n",
                "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
                "        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n",
                "        revision = kwargs.pop(\"revision\", None)\n",
                "        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n",
                "        custom_pipeline = kwargs.pop(\"custom_pipeline\", None)\n",
                "        provider = kwargs.pop(\"provider\", None)\n",
                "        sess_options = kwargs.pop(\"sess_options\", None)\n",
                "        device_map = kwargs.pop(\"device_map\", None)\n",
                "\n",
                "        # 1. Download the checkpoints and configs\n",
                "        # use snapshot download here to get it working from from_pretrained\n",
                "        if not os.path.isdir(pretrained_model_name_or_path):\n",
                "            config_dict = cls.get_config_dict(\n",
                "                pretrained_model_name_or_path,\n",
                "                cache_dir=cache_dir,\n",
                "                resume_download=resume_download,\n",
                "                proxies=proxies,\n",
                "                local_files_only=local_files_only,\n",
                "                use_auth_token=use_auth_token,\n",
                "                revision=revision,\n",
                "            )\n",
                "            # make sure we only download sub-folders and `diffusers` filenames\n",
                "            folder_names = [k for k in config_dict.keys() if not k.startswith(\"_\")]\n",
                "            allow_patterns = [os.path.join(k, \"*\") for k in folder_names]\n",
                "            allow_patterns += [WEIGHTS_NAME, SCHEDULER_CONFIG_NAME, CONFIG_NAME, ONNX_WEIGHTS_NAME, cls.config_name]\n",
                "\n",
                "            if custom_pipeline is not None:\n",
                "                allow_patterns += [CUSTOM_PIPELINE_FILE_NAME]\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "            user_agent = {\"diffusers\": __version__, \"pipeline_class\": config_dict[\"_class_name\"]}\n",
                    "            if custom_pipeline is not None:\n",
                    "                user_agent[\"custom_pipeline\"] = custom_pipeline\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 374,
                    "end": 374
                },
                "child_version_range": {
                    "start": 375,
                    "end": 379
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if not os.path.isdir(pretrained_model_name_or_path):",
                        "start_line": 356,
                        "end_line": 386
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "DiffusionPipeline",
                        "signature": "class DiffusionPipeline(ConfigMixin):",
                        "at_line": 90
                    },
                    {
                        "type": "function",
                        "name": "from_pretrained",
                        "signature": "def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):",
                        "at_line": 207
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: src/diffusers/pipeline_utils.py\nCode:\n           class DiffusionPipeline(ConfigMixin):\n               ...\n               def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n                   ...\n371 372                if custom_pipeline is not None:\n372 373                    allow_patterns += [CUSTOM_PIPELINE_FILE_NAME]\n373 374    \n    375  +             user_agent = {\"diffusers\": __version__, \"pipeline_class\": config_dict[\"_class_name\"]}\n    376  +             if custom_pipeline is not None:\n    377  +                 user_agent[\"custom_pipeline\"] = custom_pipeline\n    378  + \n374 379                # download all allow_patterns\n375 380                cached_folder = snapshot_download(\n376 381                    pretrained_model_name_or_path,\n         ...\n",
                "file_path": "src/diffusers/pipeline_utils.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "__version__",
                    "config_dict",
                    "custom_pipeline",
                    "user_agent"
                ],
                "prefix": [
                    "            if custom_pipeline is not None:\n",
                    "                allow_patterns += [CUSTOM_PIPELINE_FILE_NAME]\n",
                    "\n"
                ],
                "suffix": [
                    "            # download all allow_patterns\n",
                    "            cached_folder = snapshot_download(\n",
                    "                pretrained_model_name_or_path,\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 4,
                        "detail": {
                            "identifier": "__version__",
                            "position": {
                                "start": {
                                    "line": 375,
                                    "column": 39
                                },
                                "end": {
                                    "line": 375,
                                    "column": 50
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/diffusers/src/diffusers/pipeline_utils.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "user_agent",
                            "position": {
                                "start": {
                                    "line": 375,
                                    "column": 12
                                },
                                "end": {
                                    "line": 375,
                                    "column": 22
                                }
                            },
                            "type": "identifier",
                            "kind": "variable",
                            "abs_file_path": "/data2/chenyan/repos/diffusers/src/diffusers/pipeline_utils.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": []
            },
            [
                "            # download all allow_patterns\n",
                "            cached_folder = snapshot_download(\n",
                "                pretrained_model_name_or_path,\n",
                "                cache_dir=cache_dir,\n",
                "                resume_download=resume_download,\n",
                "                proxies=proxies,\n",
                "                local_files_only=local_files_only,\n",
                "                use_auth_token=use_auth_token,\n",
                "                revision=revision,\n",
                "                allow_patterns=allow_patterns,\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "                user_agent=user_agent,\n"
                ],
                "parent_version_range": {
                    "start": 384,
                    "end": 384
                },
                "child_version_range": {
                    "start": 389,
                    "end": 390
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if not os.path.isdir(pretrained_model_name_or_path):",
                        "start_line": 356,
                        "end_line": 386
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "DiffusionPipeline",
                        "signature": "class DiffusionPipeline(ConfigMixin):",
                        "at_line": 90
                    },
                    {
                        "type": "function",
                        "name": "from_pretrained",
                        "signature": "def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):",
                        "at_line": 207
                    },
                    {
                        "type": "call",
                        "name": "snapshot_download",
                        "signature": "snapshot_download(\n                pretrained_model_name_or_path,\n                cache_dir=cache_dir,\n                resume_download=resume_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                use_auth_token=use_auth_token,\n                revision=revision,\n                allow_patterns=allow_patterns,\n            )",
                        "at_line": 375,
                        "argument": "allow_patterns=..."
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: src/diffusers/pipeline_utils.py\nCode:\n           class DiffusionPipeline(ConfigMixin):\n               ...\n               def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n                   ...\n                   snapshot_download(\n                pretrained_model_name_or_path,\n                cache_dir=cache_dir,\n                resume_download=resume_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                use_auth_token=use_auth_token,\n                revision=revision,\n                allow_patterns=allow_patterns,\n            )\n                       ...\n381 386                    use_auth_token=use_auth_token,\n382 387                    revision=revision,\n383 388                    allow_patterns=allow_patterns,\n    389  +                 user_agent=user_agent,\n384 390                )\n385 391            else:\n386 392                cached_folder = pretrained_model_name_or_path\n         ...\n",
                "file_path": "src/diffusers/pipeline_utils.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "user_agent"
                ],
                "prefix": [
                    "                use_auth_token=use_auth_token,\n",
                    "                revision=revision,\n",
                    "                allow_patterns=allow_patterns,\n"
                ],
                "suffix": [
                    "            )\n",
                    "        else:\n",
                    "            cached_folder = pretrained_model_name_or_path\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "user_agent",
                            "position": {
                                "start": {
                                    "line": 389,
                                    "column": 27
                                },
                                "end": {
                                    "line": 389,
                                    "column": 37
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/diffusers/src/diffusers/pipeline_utils.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "            )\n",
                "        else:\n",
                "            cached_folder = pretrained_model_name_or_path\n",
                "\n",
                "        config_dict = cls.get_config_dict(cached_folder)\n",
                "\n",
                "        # 2. Load the pipeline class, if using custom module then load it from the hub\n",
                "        # if we load from explicit class, let's use it\n",
                "        if custom_pipeline is not None:\n",
                "            pipeline_class = get_class_from_dynamic_module(\n",
                "                custom_pipeline, module_file=CUSTOM_PIPELINE_FILE_NAME, cache_dir=custom_pipeline\n",
                "            )\n",
                "        elif cls != DiffusionPipeline:\n",
                "            pipeline_class = cls\n",
                "        else:\n",
                "            diffusers_module = importlib.import_module(cls.__module__.split(\".\")[0])\n",
                "            pipeline_class = getattr(diffusers_module, config_dict[\"_class_name\"])\n",
                "\n",
                "        # some modules can be passed directly to the init\n",
                "        # in this case they are already instantiated in `kwargs`\n",
                "        # extract them here\n",
                "        expected_modules = set(inspect.signature(pipeline_class.__init__).parameters.keys()) - set([\"self\"])\n",
                "        passed_class_obj = {k: kwargs.pop(k) for k in expected_modules if k in kwargs}\n",
                "\n",
                "        init_dict, _ = pipeline_class.extract_init_dict(config_dict, **kwargs)\n",
                "\n",
                "        init_kwargs = {}\n",
                "\n",
                "        # import it here to avoid circular import\n",
                "        from diffusers import pipelines\n",
                "\n",
                "        # 3. Load each module in the pipeline\n",
                "        for name, (library_name, class_name) in init_dict.items():\n",
                "            # 3.1 - now that JAX/Flax is an official framework of the library, we might load from Flax names\n",
                "            if class_name.startswith(\"Flax\"):\n",
                "                class_name = class_name[4:]\n",
                "\n",
                "            is_pipeline_module = hasattr(pipelines, library_name)\n",
                "            loaded_sub_model = None\n",
                "\n",
                "            # if the model is in a pipeline module, then we load it from the pipeline\n",
                "            if name in passed_class_obj:\n",
                "                # 1. check that passed_class_obj has correct parent class\n",
                "                if not is_pipeline_module:\n",
                "                    library = importlib.import_module(library_name)\n",
                "                    class_obj = getattr(library, class_name)\n",
                "                    importable_classes = LOADABLE_CLASSES[library_name]\n",
                "                    class_candidates = {c: getattr(library, c) for c in importable_classes.keys()}\n",
                "\n",
                "                    expected_class_obj = None\n",
                "                    for class_name, class_candidate in class_candidates.items():\n",
                "                        if issubclass(class_obj, class_candidate):\n",
                "                            expected_class_obj = class_candidate\n",
                "\n",
                "                    if not issubclass(passed_class_obj[name].__class__, expected_class_obj):\n",
                "                        raise ValueError(\n",
                "                            f\"{passed_class_obj[name]} is of type: {type(passed_class_obj[name])}, but should be\"\n",
                "                            f\" {expected_class_obj}\"\n",
                "                        )\n",
                "                else:\n",
                "                    logger.warn(\n",
                "                        f\"You have passed a non-standard module {passed_class_obj[name]}. We cannot verify whether it\"\n",
                "                        \" has the correct type\"\n",
                "                    )\n",
                "\n",
                "                # set passed class object\n",
                "                loaded_sub_model = passed_class_obj[name]\n",
                "            elif is_pipeline_module:\n",
                "                pipeline_module = getattr(pipelines, library_name)\n",
                "                class_obj = getattr(pipeline_module, class_name)\n",
                "                importable_classes = ALL_IMPORTABLE_CLASSES\n",
                "                class_candidates = {c: class_obj for c in importable_classes.keys()}\n",
                "            else:\n",
                "                # else we just import it from the library.\n",
                "                library = importlib.import_module(library_name)\n",
                "                class_obj = getattr(library, class_name)\n",
                "                importable_classes = LOADABLE_CLASSES[library_name]\n",
                "                class_candidates = {c: getattr(library, c) for c in importable_classes.keys()}\n",
                "\n",
                "            if loaded_sub_model is None:\n",
                "                load_method_name = None\n",
                "                for class_name, class_candidate in class_candidates.items():\n",
                "                    if issubclass(class_obj, class_candidate):\n",
                "                        load_method_name = importable_classes[class_name][1]\n",
                "\n",
                "                load_method = getattr(class_obj, load_method_name)\n",
                "\n",
                "                loading_kwargs = {}\n",
                "                if issubclass(class_obj, torch.nn.Module):\n",
                "                    loading_kwargs[\"torch_dtype\"] = torch_dtype\n",
                "                if issubclass(class_obj, diffusers.OnnxRuntimeModel):\n",
                "                    loading_kwargs[\"provider\"] = provider\n",
                "                    loading_kwargs[\"sess_options\"] = sess_options\n",
                "\n",
                "                if (\n",
                "                    issubclass(class_obj, diffusers.ModelMixin)\n",
                "                    or is_transformers_available()\n",
                "                    and issubclass(class_obj, PreTrainedModel)\n",
                "                ):\n",
                "                    loading_kwargs[\"device_map\"] = device_map\n",
                "\n",
                "                # check if the module is in a subdirectory\n",
                "                if os.path.isdir(os.path.join(cached_folder, name)):\n",
                "                    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)\n",
                "                else:\n",
                "                    # else load from the root directory\n",
                "                    loaded_sub_model = load_method(cached_folder, **loading_kwargs)\n",
                "\n",
                "            init_kwargs[name] = loaded_sub_model  # UNet(...), # DiffusionSchedule(...)\n",
                "\n",
                "        # 4. Potentially add passed objects if expected\n",
                "        missing_modules = set(expected_modules) - set(init_kwargs.keys())\n",
                "        if len(missing_modules) > 0 and missing_modules <= set(passed_class_obj.keys()):\n",
                "            for module in missing_modules:\n",
                "                init_kwargs[module] = passed_class_obj[module]\n",
                "        elif len(missing_modules) > 0:\n",
                "            passed_modules = set(list(init_kwargs.keys()) + list(passed_class_obj.keys()))\n",
                "            raise ValueError(\n",
                "                f\"Pipeline {pipeline_class} expected {expected_modules}, but only {passed_modules} were passed.\"\n",
                "            )\n",
                "\n",
                "        # 5. Instantiate the pipeline\n",
                "        model = pipeline_class(**init_kwargs)\n",
                "        return model\n",
                "\n",
                "    @staticmethod\n",
                "    def numpy_to_pil(images):\n",
                "        \"\"\"\n",
                "        Convert a numpy image or a batch of images to a PIL image.\n",
                "        \"\"\"\n",
                "        if images.ndim == 3:\n",
                "            images = images[None, ...]\n",
                "        images = (images * 255).round().astype(\"uint8\")\n",
                "        pil_images = [Image.fromarray(image) for image in images]\n",
                "\n",
                "        return pil_images\n",
                "\n",
                "    def progress_bar(self, iterable):\n",
                "        if not hasattr(self, \"_progress_bar_config\"):\n",
                "            self._progress_bar_config = {}\n",
                "        elif not isinstance(self._progress_bar_config, dict):\n",
                "            raise ValueError(\n",
                "                f\"`self._progress_bar_config` should be of type `dict`, but is {type(self._progress_bar_config)}.\"\n",
                "            )\n",
                "\n",
                "        return tqdm(iterable, **self._progress_bar_config)\n",
                "\n",
                "    def set_progress_bar_config(self, **kwargs):\n",
                "        self._progress_bar_config = kwargs"
            ]
        ]
    },
    "edit_order": [
        [
            0,
            1,
            2,
            3,
            4,
            5,
            6
        ],
        [
            2,
            3,
            0,
            1,
            4,
            5,
            6
        ]
    ],
    "partial_orders": [
        {
            "edit_hunk_pair": [
                1,
                0
            ],
            "edit_order": "bi-directional",
            "reason": "Exist import-use relationship, hence bi-directional",
            "scenario of 0 -> 1": "edit 0 imports `__version__` first, and then used it in edit 1.",
            "scenario of 1 -> 0": "edit 1 used the `__version__` first, and then edit 0 imports it."
        },
        {
            "edit_hunk_pair": [
                3,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "Exist import-use relationship, hence bi-directional",
            "scenario of 0 -> 1": "edit 0 imports `__version__` first, and then used it in edit 1.",
            "scenario of 1 -> 0": "edit 1 used the `__version__` first, and then edit 0 imports it."
        },
        {
            "edit_hunk_pair": [
                5,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "Exist import-use relationship, hence bi-directional",
            "scenario of 0 -> 1": "edit 0 imports `__version__` first, and then used it in edit 1.",
            "scenario of 1 -> 0": "edit 1 used the `__version__` first, and then edit 0 imports it."
        },
        {
            "edit_hunk_pair": [
                5,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "share a dataflow for variable `user_agent`",
            "scenario of 0 -> 1": "edit 0 defines the variable `user_agent` first, and then used it in edit 1.",
            "scenario of 1 -> 0": "edit 1 used the variable `user_agent` first, then defines it in edit 0."
        },
        {
            "edit_hunk_pair": [
                1,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "user may under the mental flow of search-replace",
            "scenario of 0 -> 1": "edit 0 adds the code logic first, then copy paste it to edit 1.",
            "scenario of 1 -> 0": "edit 1 adds the code logic first, then copy paste it to edit 0."
        },
        {
            "edit_hunk_pair": [
                0,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "user may under the mental flow of search-replace",
            "scenario of 0 -> 1": "edit 0 adds the code logic first, then copy paste it to edit 1.",
            "scenario of 1 -> 0": "edit 1 adds the code logic first, then copy paste it to edit 0."
        },
        {
            "edit_hunk_pair": [
                2,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "user may under the mental flow of search-replace",
            "scenario of 0 -> 1": "edit 0 adds the code logic first, then copy paste it to edit 1.",
            "scenario of 1 -> 0": "edit 1 adds the code logic first, then copy paste it to edit 0."
        },
        {
            "edit_hunk_pair": [
                0,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "user may under the mental flow of search-replace",
            "scenario of 0 -> 1": "edit 0 adds the code logic first, then copy paste it to edit 1.",
            "scenario of 1 -> 0": "edit 1 adds the code logic first, then copy paste it to edit 0."
        }
    ]
}