{
    "language": "python",
    "commit_url": "https://github.com/scikit-learn/scikit-learn/commit/5aeeeefe31acd5444148fd89f3a8df9443b04889",
    "commit_message": "MAINT remove deprecated 'full' and 'auto' option from KMeans (#28115)",
    "commit_snapshots": {
        "sklearn/cluster/_kmeans.py": [
            [
                "\"\"\"K-means clustering.\"\"\"\n",
                "\n",
                "# Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>\n",
                "#          Thomas Rueckstiess <ruecksti@in.tum.de>\n",
                "#          James Bergstra <james.bergstra@umontreal.ca>\n",
                "#          Jan Schlueter <scikit-learn@jan-schlueter.de>\n",
                "#          Nelle Varoquaux\n",
                "#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
                "#          Olivier Grisel <olivier.grisel@ensta.org>\n",
                "#          Mathieu Blondel <mathieu@mblondel.org>\n",
                "#          Robert Layton <robertlayton@gmail.com>\n",
                "# License: BSD 3 clause\n",
                "\n",
                "import warnings\n",
                "from abc import ABC, abstractmethod\n",
                "from numbers import Integral, Real\n",
                "\n",
                "import numpy as np\n",
                "import scipy.sparse as sp\n",
                "\n",
                "from ..base import (\n",
                "    BaseEstimator,\n",
                "    ClassNamePrefixFeaturesOutMixin,\n",
                "    ClusterMixin,\n",
                "    TransformerMixin,\n",
                "    _fit_context,\n",
                ")\n",
                "from ..exceptions import ConvergenceWarning\n",
                "from ..metrics.pairwise import _euclidean_distances, euclidean_distances\n",
                "from ..utils import check_array, check_random_state\n",
                "from ..utils._openmp_helpers import _openmp_effective_n_threads\n",
                "from ..utils._param_validation import Interval, StrOptions, validate_params\n",
                "from ..utils.extmath import row_norms, stable_cumsum\n",
                "from ..utils.fixes import threadpool_info, threadpool_limits\n",
                "from ..utils.sparsefuncs import mean_variance_axis\n",
                "from ..utils.sparsefuncs_fast import assign_rows_csr\n",
                "from ..utils.validation import (\n",
                "    _check_sample_weight,\n",
                "    _is_arraylike_not_scalar,\n",
                "    check_is_fitted,\n",
                ")\n",
                "from ._k_means_common import (\n",
                "    CHUNK_SIZE,\n",
                "    _inertia_dense,\n",
                "    _inertia_sparse,\n",
                "    _is_same_clustering,\n",
                ")\n",
                "from ._k_means_elkan import (\n",
                "    elkan_iter_chunked_dense,\n",
                "    elkan_iter_chunked_sparse,\n",
                "    init_bounds_dense,\n",
                "    init_bounds_sparse,\n",
                ")\n",
                "from ._k_means_lloyd import lloyd_iter_chunked_dense, lloyd_iter_chunked_sparse\n",
                "from ._k_means_minibatch import _minibatch_update_dense, _minibatch_update_sparse\n",
                "\n",
                "###############################################################################\n",
                "# Initialization heuristic\n",
                "\n",
                "\n",
                "@validate_params(\n",
                "    {\n",
                "        \"X\": [\"array-like\", \"sparse matrix\"],\n",
                "        \"n_clusters\": [Interval(Integral, 1, None, closed=\"left\")],\n",
                "        \"sample_weight\": [\"array-like\", None],\n",
                "        \"x_squared_norms\": [\"array-like\", None],\n",
                "        \"random_state\": [\"random_state\"],\n",
                "        \"n_local_trials\": [Interval(Integral, 1, None, closed=\"left\"), None],\n",
                "    },\n",
                "    prefer_skip_nested_validation=True,\n",
                ")\n",
                "def kmeans_plusplus(\n",
                "    X,\n",
                "    n_clusters,\n",
                "    *,\n",
                "    sample_weight=None,\n",
                "    x_squared_norms=None,\n",
                "    random_state=None,\n",
                "    n_local_trials=None,\n",
                "):\n",
                "    \"\"\"Init n_clusters seeds according to k-means++.\n",
                "\n",
                "    .. versionadded:: 0.24\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "        The data to pick seeds from.\n",
                "\n",
                "    n_clusters : int\n",
                "        The number of centroids to initialize.\n",
                "\n",
                "    sample_weight : array-like of shape (n_samples,), default=None\n",
                "        The weights for each observation in `X`. If `None`, all observations\n",
                "        are assigned equal weight. `sample_weight` is ignored if `init`\n",
                "        is a callable or a user provided array.\n",
                "\n",
                "        .. versionadded:: 1.3\n",
                "\n",
                "    x_squared_norms : array-like of shape (n_samples,), default=None\n",
                "        Squared Euclidean norm of each data point.\n",
                "\n",
                "    random_state : int or RandomState instance, default=None\n",
                "        Determines random number generation for centroid initialization. Pass\n",
                "        an int for reproducible output across multiple function calls.\n",
                "        See :term:`Glossary <random_state>`.\n",
                "\n",
                "    n_local_trials : int, default=None\n",
                "        The number of seeding trials for each center (except the first),\n",
                "        of which the one reducing inertia the most is greedily chosen.\n",
                "        Set to None to make the number of trials depend logarithmically\n",
                "        on the number of seeds (2+log(k)) which is the recommended setting.\n",
                "        Setting to 1 disables the greedy cluster selection and recovers the\n",
                "        vanilla k-means++ algorithm which was empirically shown to work less\n",
                "        well than its greedy variant.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    centers : ndarray of shape (n_clusters, n_features)\n",
                "        The initial centers for k-means.\n",
                "\n",
                "    indices : ndarray of shape (n_clusters,)\n",
                "        The index location of the chosen centers in the data array X. For a\n",
                "        given index and center, X[index] = center.\n",
                "\n",
                "    Notes\n",
                "    -----\n",
                "    Selects initial cluster centers for k-mean clustering in a smart way\n",
                "    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n",
                "    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n",
                "    on Discrete algorithms. 2007\n",
                "\n",
                "    Examples\n",
                "    --------\n",
                "\n",
                "    >>> from sklearn.cluster import kmeans_plusplus\n",
                "    >>> import numpy as np\n",
                "    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
                "    ...               [10, 2], [10, 4], [10, 0]])\n",
                "    >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n",
                "    >>> centers\n",
                "    array([[10,  2],\n",
                "           [ 1,  0]])\n",
                "    >>> indices\n",
                "    array([3, 2])\n",
                "    \"\"\"\n",
                "    # Check data\n",
                "    check_array(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32])\n",
                "    sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
                "\n",
                "    if X.shape[0] < n_clusters:\n",
                "        raise ValueError(\n",
                "            f\"n_samples={X.shape[0]} should be >= n_clusters={n_clusters}.\"\n",
                "        )\n",
                "\n",
                "    # Check parameters\n",
                "    if x_squared_norms is None:\n",
                "        x_squared_norms = row_norms(X, squared=True)\n",
                "    else:\n",
                "        x_squared_norms = check_array(x_squared_norms, dtype=X.dtype, ensure_2d=False)\n",
                "\n",
                "    if x_squared_norms.shape[0] != X.shape[0]:\n",
                "        raise ValueError(\n",
                "            f\"The length of x_squared_norms {x_squared_norms.shape[0]} should \"\n",
                "            f\"be equal to the length of n_samples {X.shape[0]}.\"\n",
                "        )\n",
                "\n",
                "    random_state = check_random_state(random_state)\n",
                "\n",
                "    # Call private k-means++\n",
                "    centers, indices = _kmeans_plusplus(\n",
                "        X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials\n",
                "    )\n",
                "\n",
                "    return centers, indices\n",
                "\n",
                "\n",
                "def _kmeans_plusplus(\n",
                "    X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials=None\n",
                "):\n",
                "    \"\"\"Computational component for initialization of n_clusters by\n",
                "    k-means++. Prior validation of data is assumed.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
                "        The data to pick seeds for.\n",
                "\n",
                "    n_clusters : int\n",
                "        The number of seeds to choose.\n",
                "\n",
                "    sample_weight : ndarray of shape (n_samples,)\n",
                "        The weights for each observation in `X`.\n",
                "\n",
                "    x_squared_norms : ndarray of shape (n_samples,)\n",
                "        Squared Euclidean norm of each data point.\n",
                "\n",
                "    random_state : RandomState instance\n",
                "        The generator used to initialize the centers.\n",
                "        See :term:`Glossary <random_state>`.\n",
                "\n",
                "    n_local_trials : int, default=None\n",
                "        The number of seeding trials for each center (except the first),\n",
                "        of which the one reducing inertia the most is greedily chosen.\n",
                "        Set to None to make the number of trials depend logarithmically\n",
                "        on the number of seeds (2+log(k)); this is the default.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    centers : ndarray of shape (n_clusters, n_features)\n",
                "        The initial centers for k-means.\n",
                "\n",
                "    indices : ndarray of shape (n_clusters,)\n",
                "        The index location of the chosen centers in the data array X. For a\n",
                "        given index and center, X[index] = center.\n",
                "    \"\"\"\n",
                "    n_samples, n_features = X.shape\n",
                "\n",
                "    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n",
                "\n",
                "    # Set the number of local seeding trials if none is given\n",
                "    if n_local_trials is None:\n",
                "        # This is what Arthur/Vassilvitskii tried, but did not report\n",
                "        # specific results for other than mentioning in the conclusion\n",
                "        # that it helped.\n",
                "        n_local_trials = 2 + int(np.log(n_clusters))\n",
                "\n",
                "    # Pick first center randomly and track index of point\n",
                "    center_id = random_state.choice(n_samples, p=sample_weight / sample_weight.sum())\n",
                "    indices = np.full(n_clusters, -1, dtype=int)\n",
                "    if sp.issparse(X):\n",
                "        centers[0] = X[[center_id]].toarray()\n",
                "    else:\n",
                "        centers[0] = X[center_id]\n",
                "    indices[0] = center_id\n",
                "\n",
                "    # Initialize list of closest distances and calculate current potential\n",
                "    closest_dist_sq = _euclidean_distances(\n",
                "        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True\n",
                "    )\n",
                "    current_pot = closest_dist_sq @ sample_weight\n",
                "\n",
                "    # Pick the remaining n_clusters-1 points\n",
                "    for c in range(1, n_clusters):\n",
                "        # Choose center candidates by sampling with probability proportional\n",
                "        # to the squared distance to the closest existing center\n",
                "        rand_vals = random_state.uniform(size=n_local_trials) * current_pot\n",
                "        candidate_ids = np.searchsorted(\n",
                "            stable_cumsum(sample_weight * closest_dist_sq), rand_vals\n",
                "        )\n",
                "        # XXX: numerical imprecision can result in a candidate_id out of range\n",
                "        np.clip(candidate_ids, None, closest_dist_sq.size - 1, out=candidate_ids)\n",
                "\n",
                "        # Compute distances to center candidates\n",
                "        distance_to_candidates = _euclidean_distances(\n",
                "            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True\n",
                "        )\n",
                "\n",
                "        # update closest distances squared and potential for each candidate\n",
                "        np.minimum(closest_dist_sq, distance_to_candidates, out=distance_to_candidates)\n",
                "        candidates_pot = distance_to_candidates @ sample_weight.reshape(-1, 1)\n",
                "\n",
                "        # Decide which candidate is the best\n",
                "        best_candidate = np.argmin(candidates_pot)\n",
                "        current_pot = candidates_pot[best_candidate]\n",
                "        closest_dist_sq = distance_to_candidates[best_candidate]\n",
                "        best_candidate = candidate_ids[best_candidate]\n",
                "\n",
                "        # Permanently add best center candidate found in local tries\n",
                "        if sp.issparse(X):\n",
                "            centers[c] = X[[best_candidate]].toarray()\n",
                "        else:\n",
                "            centers[c] = X[best_candidate]\n",
                "        indices[c] = best_candidate\n",
                "\n",
                "    return centers, indices\n",
                "\n",
                "\n",
                "###############################################################################\n",
                "# K-means batch estimation by EM (expectation maximization)\n",
                "\n",
                "\n",
                "def _tolerance(X, tol):\n",
                "    \"\"\"Return a tolerance which is dependent on the dataset.\"\"\"\n",
                "    if tol == 0:\n",
                "        return 0\n",
                "    if sp.issparse(X):\n",
                "        variances = mean_variance_axis(X, axis=0)[1]\n",
                "    else:\n",
                "        variances = np.var(X, axis=0)\n",
                "    return np.mean(variances) * tol\n",
                "\n",
                "\n",
                "@validate_params(\n",
                "    {\n",
                "        \"X\": [\"array-like\", \"sparse matrix\"],\n",
                "        \"sample_weight\": [\"array-like\", None],\n",
                "        \"return_n_iter\": [bool],\n",
                "    },\n",
                "    prefer_skip_nested_validation=False,\n",
                ")\n",
                "def k_means(\n",
                "    X,\n",
                "    n_clusters,\n",
                "    *,\n",
                "    sample_weight=None,\n",
                "    init=\"k-means++\",\n",
                "    n_init=\"auto\",\n",
                "    max_iter=300,\n",
                "    verbose=False,\n",
                "    tol=1e-4,\n",
                "    random_state=None,\n",
                "    copy_x=True,\n",
                "    algorithm=\"lloyd\",\n",
                "    return_n_iter=False,\n",
                "):\n",
                "    \"\"\"Perform K-means clustering algorithm.\n",
                "\n",
                "    Read more in the :ref:`User Guide <k_means>`.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "        The observations to cluster. It must be noted that the data\n",
                "        will be converted to C ordering, which will cause a memory copy\n",
                "        if the given data is not C-contiguous.\n",
                "\n",
                "    n_clusters : int\n",
                "        The number of clusters to form as well as the number of\n",
                "        centroids to generate.\n",
                "\n",
                "    sample_weight : array-like of shape (n_samples,), default=None\n",
                "        The weights for each observation in `X`. If `None`, all observations\n",
                "        are assigned equal weight. `sample_weight` is not used during\n",
                "        initialization if `init` is a callable or a user provided array.\n",
                "\n",
                "    init : {'k-means++', 'random'}, callable or array-like of shape \\\n",
                "            (n_clusters, n_features), default='k-means++'\n",
                "        Method for initialization:\n",
                "\n",
                "        - `'k-means++'` : selects initial cluster centers for k-mean\n",
                "          clustering in a smart way to speed up convergence. See section\n",
                "          Notes in k_init for more details.\n",
                "        - `'random'`: choose `n_clusters` observations (rows) at random from data\n",
                "          for the initial centroids.\n",
                "        - If an array is passed, it should be of shape `(n_clusters, n_features)`\n",
                "          and gives the initial centers.\n",
                "        - If a callable is passed, it should take arguments `X`, `n_clusters` and a\n",
                "          random state and return an initialization.\n",
                "\n",
                "    n_init : 'auto' or int, default=\"auto\"\n",
                "        Number of time the k-means algorithm will be run with different\n",
                "        centroid seeds. The final results will be the best output of\n",
                "        n_init consecutive runs in terms of inertia.\n",
                "\n",
                "        When `n_init='auto'`, the number of runs depends on the value of init:\n",
                "        10 if using `init='random'` or `init` is a callable;\n",
                "        1 if using `init='k-means++'` or `init` is an array-like.\n",
                "\n",
                "        .. versionadded:: 1.2\n",
                "           Added 'auto' option for `n_init`.\n",
                "\n",
                "        .. versionchanged:: 1.4\n",
                "           Default value for `n_init` changed to `'auto'`.\n",
                "\n",
                "    max_iter : int, default=300\n",
                "        Maximum number of iterations of the k-means algorithm to run.\n",
                "\n",
                "    verbose : bool, default=False\n",
                "        Verbosity mode.\n",
                "\n",
                "    tol : float, default=1e-4\n",
                "        Relative tolerance with regards to Frobenius norm of the difference\n",
                "        in the cluster centers of two consecutive iterations to declare\n",
                "        convergence.\n",
                "\n",
                "    random_state : int, RandomState instance or None, default=None\n",
                "        Determines random number generation for centroid initialization. Use\n",
                "        an int to make the randomness deterministic.\n",
                "        See :term:`Glossary <random_state>`.\n",
                "\n",
                "    copy_x : bool, default=True\n",
                "        When pre-computing distances it is more numerically accurate to center\n",
                "        the data first. If `copy_x` is True (default), then the original data is\n",
                "        not modified. If False, the original data is modified, and put back\n",
                "        before the function returns, but small numerical differences may be\n",
                "        introduced by subtracting and then adding the data mean. Note that if\n",
                "        the original data is not C-contiguous, a copy will be made even if\n",
                "        `copy_x` is False. If the original data is sparse, but not in CSR format,\n",
                "        a copy will be made even if `copy_x` is False.\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    algorithm : {\"lloyd\", \"elkan\", \"auto\", \"full\"}, default=\"lloyd\"\n"
                ],
                "after": [
                    "    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n"
                ],
                "parent_version_range": {
                    "start": 391,
                    "end": 392
                },
                "child_version_range": {
                    "start": 391,
                    "end": 392
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "k_means",
                        "signature": "def k_means(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    init=\"k-means++\",\n    n_init=\"auto\",\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    random_state=None,\n    copy_x=True,\n    algorithm=\"lloyd\",\n    return_n_iter=False,\n):",
                        "at_line": 301
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: sklearn/cluster/_kmeans.py\nCode:\n           def k_means(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    init=\"k-means++\",\n    n_init=\"auto\",\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    random_state=None,\n    copy_x=True,\n    algorithm=\"lloyd\",\n    return_n_iter=False,\n):\n               ...\n388 388            `copy_x` is False. If the original data is sparse, but not in CSR format,\n389 389            a copy will be made even if `copy_x` is False.\n390 390    \n391      -     algorithm : {\"lloyd\", \"elkan\", \"auto\", \"full\"}, default=\"lloyd\"\n    391  +     algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n392 392            K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n393 393            The `\"elkan\"` variation can be more efficient on some datasets with\n394 394            well-defined clusters, by using the triangle inequality. However it's\n         ...\n",
                "file_path": "sklearn/cluster/_kmeans.py",
                "identifiers_before": [
                    "algorithm",
                    "default"
                ],
                "identifiers_after": [
                    "algorithm",
                    "default"
                ],
                "prefix": [
                    "        `copy_x` is False. If the original data is sparse, but not in CSR format,\n",
                    "        a copy will be made even if `copy_x` is False.\n",
                    "\n"
                ],
                "suffix": [
                    "        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n",
                    "        The `\"elkan\"` variation can be more efficient on some datasets with\n",
                    "        well-defined clusters, by using the triangle inequality. However it's\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    2
                ]
            },
            [
                "        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n",
                "        The `\"elkan\"` variation can be more efficient on some datasets with\n",
                "        well-defined clusters, by using the triangle inequality. However it's\n",
                "        more memory intensive due to the allocation of an extra array of shape\n",
                "        `(n_samples, n_clusters)`.\n",
                "\n"
            ],
            {
                "type": "delete",
                "before": [
                    "        `\"auto\"` and `\"full\"` are deprecated and they will be removed in\n",
                    "        Scikit-Learn 1.3. They are both aliases for `\"lloyd\"`.\n",
                    "\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 398,
                    "end": 401
                },
                "child_version_range": {
                    "start": 398,
                    "end": 398
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "k_means",
                        "signature": "def k_means(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    init=\"k-means++\",\n    n_init=\"auto\",\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    random_state=None,\n    copy_x=True,\n    algorithm=\"lloyd\",\n    return_n_iter=False,\n):",
                        "at_line": 301
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: sklearn/cluster/_kmeans.py\nCode:\n           def k_means(\n    X,\n    n_clusters,\n    *,\n    sample_weight=None,\n    init=\"k-means++\",\n    n_init=\"auto\",\n    max_iter=300,\n    verbose=False,\n    tol=1e-4,\n    random_state=None,\n    copy_x=True,\n    algorithm=\"lloyd\",\n    return_n_iter=False,\n):\n               ...\n395 395            more memory intensive due to the allocation of an extra array of shape\n396 396            `(n_samples, n_clusters)`.\n397 397    \n398      -         `\"auto\"` and `\"full\"` are deprecated and they will be removed in\n399      -         Scikit-Learn 1.3. They are both aliases for `\"lloyd\"`.\n400      - \n401 398            .. versionchanged:: 0.18\n402 399                Added Elkan algorithm\n403 400    \n         ...\n",
                "file_path": "sklearn/cluster/_kmeans.py",
                "identifiers_before": [
                    "Learn",
                    "Scikit",
                    "They",
                    "aliases",
                    "are",
                    "be",
                    "both",
                    "deprecated",
                    "removed",
                    "they",
                    "will"
                ],
                "identifiers_after": [],
                "prefix": [
                    "        more memory intensive due to the allocation of an extra array of shape\n",
                    "        `(n_samples, n_clusters)`.\n",
                    "\n"
                ],
                "suffix": [
                    "        .. versionchanged:: 0.18\n",
                    "            Added Elkan algorithm\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    3
                ]
            },
            [
                "        .. versionchanged:: 0.18\n",
                "            Added Elkan algorithm\n",
                "\n",
                "        .. versionchanged:: 1.1\n",
                "            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n",
                "            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n",
                "\n",
                "    return_n_iter : bool, default=False\n",
                "        Whether or not to return the number of iterations.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    centroid : ndarray of shape (n_clusters, n_features)\n",
                "        Centroids found at the last iteration of k-means.\n",
                "\n",
                "    label : ndarray of shape (n_samples,)\n",
                "        The `label[i]` is the code or index of the centroid the\n",
                "        i'th observation is closest to.\n",
                "\n",
                "    inertia : float\n",
                "        The final value of the inertia criterion (sum of squared distances to\n",
                "        the closest centroid for all observations in the training set).\n",
                "\n",
                "    best_n_iter : int\n",
                "        Number of iterations corresponding to the best results.\n",
                "        Returned only if `return_n_iter` is set to True.\n",
                "\n",
                "    Examples\n",
                "    --------\n",
                "    >>> import numpy as np\n",
                "    >>> from sklearn.cluster import k_means\n",
                "    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
                "    ...               [10, 2], [10, 4], [10, 0]])\n",
                "    >>> centroid, label, inertia = k_means(\n",
                "    ...     X, n_clusters=2, n_init=\"auto\", random_state=0\n",
                "    ... )\n",
                "    >>> centroid\n",
                "    array([[10.,  2.],\n",
                "           [ 1.,  2.]])\n",
                "    >>> label\n",
                "    array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
                "    >>> inertia\n",
                "    16.0\n",
                "    \"\"\"\n",
                "    est = KMeans(\n",
                "        n_clusters=n_clusters,\n",
                "        init=init,\n",
                "        n_init=n_init,\n",
                "        max_iter=max_iter,\n",
                "        verbose=verbose,\n",
                "        tol=tol,\n",
                "        random_state=random_state,\n",
                "        copy_x=copy_x,\n",
                "        algorithm=algorithm,\n",
                "    ).fit(X, sample_weight=sample_weight)\n",
                "    if return_n_iter:\n",
                "        return est.cluster_centers_, est.labels_, est.inertia_, est.n_iter_\n",
                "    else:\n",
                "        return est.cluster_centers_, est.labels_, est.inertia_\n",
                "\n",
                "\n",
                "def _kmeans_single_elkan(\n",
                "    X,\n",
                "    sample_weight,\n",
                "    centers_init,\n",
                "    max_iter=300,\n",
                "    verbose=False,\n",
                "    tol=1e-4,\n",
                "    n_threads=1,\n",
                "):\n",
                "    \"\"\"A single run of k-means elkan, assumes preparation completed prior.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
                "        The observations to cluster. If sparse matrix, must be in CSR format.\n",
                "\n",
                "    sample_weight : array-like of shape (n_samples,)\n",
                "        The weights for each observation in X.\n",
                "\n",
                "    centers_init : ndarray of shape (n_clusters, n_features)\n",
                "        The initial centers.\n",
                "\n",
                "    max_iter : int, default=300\n",
                "        Maximum number of iterations of the k-means algorithm to run.\n",
                "\n",
                "    verbose : bool, default=False\n",
                "        Verbosity mode.\n",
                "\n",
                "    tol : float, default=1e-4\n",
                "        Relative tolerance with regards to Frobenius norm of the difference\n",
                "        in the cluster centers of two consecutive iterations to declare\n",
                "        convergence.\n",
                "        It's not advised to set `tol=0` since convergence might never be\n",
                "        declared due to rounding errors. Use a very small number instead.\n",
                "\n",
                "    n_threads : int, default=1\n",
                "        The number of OpenMP threads to use for the computation. Parallelism is\n",
                "        sample-wise on the main cython loop which assigns each sample to its\n",
                "        closest center.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    centroid : ndarray of shape (n_clusters, n_features)\n",
                "        Centroids found at the last iteration of k-means.\n",
                "\n",
                "    label : ndarray of shape (n_samples,)\n",
                "        label[i] is the code or index of the centroid the\n",
                "        i'th observation is closest to.\n",
                "\n",
                "    inertia : float\n",
                "        The final value of the inertia criterion (sum of squared distances to\n",
                "        the closest centroid for all observations in the training set).\n",
                "\n",
                "    n_iter : int\n",
                "        Number of iterations run.\n",
                "    \"\"\"\n",
                "    n_samples = X.shape[0]\n",
                "    n_clusters = centers_init.shape[0]\n",
                "\n",
                "    # Buffers to avoid new allocations at each iteration.\n",
                "    centers = centers_init\n",
                "    centers_new = np.zeros_like(centers)\n",
                "    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\n",
                "    labels = np.full(n_samples, -1, dtype=np.int32)\n",
                "    labels_old = labels.copy()\n",
                "    center_half_distances = euclidean_distances(centers) / 2\n",
                "    distance_next_center = np.partition(\n",
                "        np.asarray(center_half_distances), kth=1, axis=0\n",
                "    )[1]\n",
                "    upper_bounds = np.zeros(n_samples, dtype=X.dtype)\n",
                "    lower_bounds = np.zeros((n_samples, n_clusters), dtype=X.dtype)\n",
                "    center_shift = np.zeros(n_clusters, dtype=X.dtype)\n",
                "\n",
                "    if sp.issparse(X):\n",
                "        init_bounds = init_bounds_sparse\n",
                "        elkan_iter = elkan_iter_chunked_sparse\n",
                "        _inertia = _inertia_sparse\n",
                "    else:\n",
                "        init_bounds = init_bounds_dense\n",
                "        elkan_iter = elkan_iter_chunked_dense\n",
                "        _inertia = _inertia_dense\n",
                "\n",
                "    init_bounds(\n",
                "        X,\n",
                "        centers,\n",
                "        center_half_distances,\n",
                "        labels,\n",
                "        upper_bounds,\n",
                "        lower_bounds,\n",
                "        n_threads=n_threads,\n",
                "    )\n",
                "\n",
                "    strict_convergence = False\n",
                "\n",
                "    for i in range(max_iter):\n",
                "        elkan_iter(\n",
                "            X,\n",
                "            sample_weight,\n",
                "            centers,\n",
                "            centers_new,\n",
                "            weight_in_clusters,\n",
                "            center_half_distances,\n",
                "            distance_next_center,\n",
                "            upper_bounds,\n",
                "            lower_bounds,\n",
                "            labels,\n",
                "            center_shift,\n",
                "            n_threads,\n",
                "        )\n",
                "\n",
                "        # compute new pairwise distances between centers and closest other\n",
                "        # center of each center for next iterations\n",
                "        center_half_distances = euclidean_distances(centers_new) / 2\n",
                "        distance_next_center = np.partition(\n",
                "            np.asarray(center_half_distances), kth=1, axis=0\n",
                "        )[1]\n",
                "\n",
                "        if verbose:\n",
                "            inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n",
                "            print(f\"Iteration {i}, inertia {inertia}\")\n",
                "\n",
                "        centers, centers_new = centers_new, centers\n",
                "\n",
                "        if np.array_equal(labels, labels_old):\n",
                "            # First check the labels for strict convergence.\n",
                "            if verbose:\n",
                "                print(f\"Converged at iteration {i}: strict convergence.\")\n",
                "            strict_convergence = True\n",
                "            break\n",
                "        else:\n",
                "            # No strict convergence, check for tol based convergence.\n",
                "            center_shift_tot = (center_shift**2).sum()\n",
                "            if center_shift_tot <= tol:\n",
                "                if verbose:\n",
                "                    print(\n",
                "                        f\"Converged at iteration {i}: center shift \"\n",
                "                        f\"{center_shift_tot} within tolerance {tol}.\"\n",
                "                    )\n",
                "                break\n",
                "\n",
                "        labels_old[:] = labels\n",
                "\n",
                "    if not strict_convergence:\n",
                "        # rerun E-step so that predicted labels match cluster centers\n",
                "        elkan_iter(\n",
                "            X,\n",
                "            sample_weight,\n",
                "            centers,\n",
                "            centers,\n",
                "            weight_in_clusters,\n",
                "            center_half_distances,\n",
                "            distance_next_center,\n",
                "            upper_bounds,\n",
                "            lower_bounds,\n",
                "            labels,\n",
                "            center_shift,\n",
                "            n_threads,\n",
                "            update_centers=False,\n",
                "        )\n",
                "\n",
                "    inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n",
                "\n",
                "    return labels, inertia, centers, i + 1\n",
                "\n",
                "\n",
                "def _kmeans_single_lloyd(\n",
                "    X,\n",
                "    sample_weight,\n",
                "    centers_init,\n",
                "    max_iter=300,\n",
                "    verbose=False,\n",
                "    tol=1e-4,\n",
                "    n_threads=1,\n",
                "):\n",
                "    \"\"\"A single run of k-means lloyd, assumes preparation completed prior.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
                "        The observations to cluster. If sparse matrix, must be in CSR format.\n",
                "\n",
                "    sample_weight : ndarray of shape (n_samples,)\n",
                "        The weights for each observation in X.\n",
                "\n",
                "    centers_init : ndarray of shape (n_clusters, n_features)\n",
                "        The initial centers.\n",
                "\n",
                "    max_iter : int, default=300\n",
                "        Maximum number of iterations of the k-means algorithm to run.\n",
                "\n",
                "    verbose : bool, default=False\n",
                "        Verbosity mode\n",
                "\n",
                "    tol : float, default=1e-4\n",
                "        Relative tolerance with regards to Frobenius norm of the difference\n",
                "        in the cluster centers of two consecutive iterations to declare\n",
                "        convergence.\n",
                "        It's not advised to set `tol=0` since convergence might never be\n",
                "        declared due to rounding errors. Use a very small number instead.\n",
                "\n",
                "    n_threads : int, default=1\n",
                "        The number of OpenMP threads to use for the computation. Parallelism is\n",
                "        sample-wise on the main cython loop which assigns each sample to its\n",
                "        closest center.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    centroid : ndarray of shape (n_clusters, n_features)\n",
                "        Centroids found at the last iteration of k-means.\n",
                "\n",
                "    label : ndarray of shape (n_samples,)\n",
                "        label[i] is the code or index of the centroid the\n",
                "        i'th observation is closest to.\n",
                "\n",
                "    inertia : float\n",
                "        The final value of the inertia criterion (sum of squared distances to\n",
                "        the closest centroid for all observations in the training set).\n",
                "\n",
                "    n_iter : int\n",
                "        Number of iterations run.\n",
                "    \"\"\"\n",
                "    n_clusters = centers_init.shape[0]\n",
                "\n",
                "    # Buffers to avoid new allocations at each iteration.\n",
                "    centers = centers_init\n",
                "    centers_new = np.zeros_like(centers)\n",
                "    labels = np.full(X.shape[0], -1, dtype=np.int32)\n",
                "    labels_old = labels.copy()\n",
                "    weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)\n",
                "    center_shift = np.zeros(n_clusters, dtype=X.dtype)\n",
                "\n",
                "    if sp.issparse(X):\n",
                "        lloyd_iter = lloyd_iter_chunked_sparse\n",
                "        _inertia = _inertia_sparse\n",
                "    else:\n",
                "        lloyd_iter = lloyd_iter_chunked_dense\n",
                "        _inertia = _inertia_dense\n",
                "\n",
                "    strict_convergence = False\n",
                "\n",
                "    # Threadpoolctl context to limit the number of threads in second level of\n",
                "    # nested parallelism (i.e. BLAS) to avoid oversubscription.\n",
                "    with threadpool_limits(limits=1, user_api=\"blas\"):\n",
                "        for i in range(max_iter):\n",
                "            lloyd_iter(\n",
                "                X,\n",
                "                sample_weight,\n",
                "                centers,\n",
                "                centers_new,\n",
                "                weight_in_clusters,\n",
                "                labels,\n",
                "                center_shift,\n",
                "                n_threads,\n",
                "            )\n",
                "\n",
                "            if verbose:\n",
                "                inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n",
                "                print(f\"Iteration {i}, inertia {inertia}.\")\n",
                "\n",
                "            centers, centers_new = centers_new, centers\n",
                "\n",
                "            if np.array_equal(labels, labels_old):\n",
                "                # First check the labels for strict convergence.\n",
                "                if verbose:\n",
                "                    print(f\"Converged at iteration {i}: strict convergence.\")\n",
                "                strict_convergence = True\n",
                "                break\n",
                "            else:\n",
                "                # No strict convergence, check for tol based convergence.\n",
                "                center_shift_tot = (center_shift**2).sum()\n",
                "                if center_shift_tot <= tol:\n",
                "                    if verbose:\n",
                "                        print(\n",
                "                            f\"Converged at iteration {i}: center shift \"\n",
                "                            f\"{center_shift_tot} within tolerance {tol}.\"\n",
                "                        )\n",
                "                    break\n",
                "\n",
                "            labels_old[:] = labels\n",
                "\n",
                "        if not strict_convergence:\n",
                "            # rerun E-step so that predicted labels match cluster centers\n",
                "            lloyd_iter(\n",
                "                X,\n",
                "                sample_weight,\n",
                "                centers,\n",
                "                centers,\n",
                "                weight_in_clusters,\n",
                "                labels,\n",
                "                center_shift,\n",
                "                n_threads,\n",
                "                update_centers=False,\n",
                "            )\n",
                "\n",
                "    inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n",
                "\n",
                "    return labels, inertia, centers, i + 1\n",
                "\n",
                "\n",
                "def _labels_inertia(X, sample_weight, centers, n_threads=1, return_inertia=True):\n",
                "    \"\"\"E step of the K-means EM algorithm.\n",
                "\n",
                "    Compute the labels and the inertia of the given samples and centers.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
                "        The input samples to assign to the labels. If sparse matrix, must\n",
                "        be in CSR format.\n",
                "\n",
                "    sample_weight : ndarray of shape (n_samples,)\n",
                "        The weights for each observation in X.\n",
                "\n",
                "    x_squared_norms : ndarray of shape (n_samples,)\n",
                "        Precomputed squared euclidean norm of each data point, to speed up\n",
                "        computations.\n",
                "\n",
                "    centers : ndarray of shape (n_clusters, n_features)\n",
                "        The cluster centers.\n",
                "\n",
                "    n_threads : int, default=1\n",
                "        The number of OpenMP threads to use for the computation. Parallelism is\n",
                "        sample-wise on the main cython loop which assigns each sample to its\n",
                "        closest center.\n",
                "\n",
                "    return_inertia : bool, default=True\n",
                "        Whether to compute and return the inertia.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    labels : ndarray of shape (n_samples,)\n",
                "        The resulting assignment.\n",
                "\n",
                "    inertia : float\n",
                "        Sum of squared distances of samples to their closest cluster center.\n",
                "        Inertia is only returned if return_inertia is True.\n",
                "    \"\"\"\n",
                "    n_samples = X.shape[0]\n",
                "    n_clusters = centers.shape[0]\n",
                "\n",
                "    labels = np.full(n_samples, -1, dtype=np.int32)\n",
                "    center_shift = np.zeros(n_clusters, dtype=centers.dtype)\n",
                "\n",
                "    if sp.issparse(X):\n",
                "        _labels = lloyd_iter_chunked_sparse\n",
                "        _inertia = _inertia_sparse\n",
                "    else:\n",
                "        _labels = lloyd_iter_chunked_dense\n",
                "        _inertia = _inertia_dense\n",
                "\n",
                "    _labels(\n",
                "        X,\n",
                "        sample_weight,\n",
                "        centers,\n",
                "        centers_new=None,\n",
                "        weight_in_clusters=None,\n",
                "        labels=labels,\n",
                "        center_shift=center_shift,\n",
                "        n_threads=n_threads,\n",
                "        update_centers=False,\n",
                "    )\n",
                "\n",
                "    if return_inertia:\n",
                "        inertia = _inertia(X, sample_weight, centers, labels, n_threads)\n",
                "        return labels, inertia\n",
                "\n",
                "    return labels\n",
                "\n",
                "\n",
                "def _labels_inertia_threadpool_limit(\n",
                "    X, sample_weight, centers, n_threads=1, return_inertia=True\n",
                "):\n",
                "    \"\"\"Same as _labels_inertia but in a threadpool_limits context.\"\"\"\n",
                "    with threadpool_limits(limits=1, user_api=\"blas\"):\n",
                "        result = _labels_inertia(X, sample_weight, centers, n_threads, return_inertia)\n",
                "\n",
                "    return result\n",
                "\n",
                "\n",
                "class _BaseKMeans(\n",
                "    ClassNamePrefixFeaturesOutMixin, TransformerMixin, ClusterMixin, BaseEstimator, ABC\n",
                "):\n",
                "    \"\"\"Base class for KMeans and MiniBatchKMeans\"\"\"\n",
                "\n",
                "    _parameter_constraints: dict = {\n",
                "        \"n_clusters\": [Interval(Integral, 1, None, closed=\"left\")],\n",
                "        \"init\": [StrOptions({\"k-means++\", \"random\"}), callable, \"array-like\"],\n",
                "        \"n_init\": [\n",
                "            StrOptions({\"auto\"}),\n",
                "            Interval(Integral, 1, None, closed=\"left\"),\n",
                "        ],\n",
                "        \"max_iter\": [Interval(Integral, 1, None, closed=\"left\")],\n",
                "        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n",
                "        \"verbose\": [\"verbose\"],\n",
                "        \"random_state\": [\"random_state\"],\n",
                "    }\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        n_clusters,\n",
                "        *,\n",
                "        init,\n",
                "        n_init,\n",
                "        max_iter,\n",
                "        tol,\n",
                "        verbose,\n",
                "        random_state,\n",
                "    ):\n",
                "        self.n_clusters = n_clusters\n",
                "        self.init = init\n",
                "        self.max_iter = max_iter\n",
                "        self.tol = tol\n",
                "        self.n_init = n_init\n",
                "        self.verbose = verbose\n",
                "        self.random_state = random_state\n",
                "\n",
                "    def _check_params_vs_input(self, X, default_n_init=None):\n",
                "        # n_clusters\n",
                "        if X.shape[0] < self.n_clusters:\n",
                "            raise ValueError(\n",
                "                f\"n_samples={X.shape[0]} should be >= n_clusters={self.n_clusters}.\"\n",
                "            )\n",
                "\n",
                "        # tol\n",
                "        self._tol = _tolerance(X, self.tol)\n",
                "\n",
                "        # n-init\n",
                "        if self.n_init == \"auto\":\n",
                "            if isinstance(self.init, str) and self.init == \"k-means++\":\n",
                "                self._n_init = 1\n",
                "            elif isinstance(self.init, str) and self.init == \"random\":\n",
                "                self._n_init = default_n_init\n",
                "            elif callable(self.init):\n",
                "                self._n_init = default_n_init\n",
                "            else:  # array-like\n",
                "                self._n_init = 1\n",
                "        else:\n",
                "            self._n_init = self.n_init\n",
                "\n",
                "        if _is_arraylike_not_scalar(self.init) and self._n_init != 1:\n",
                "            warnings.warn(\n",
                "                (\n",
                "                    \"Explicit initial center position passed: performing only\"\n",
                "                    f\" one init in {self.__class__.__name__} instead of \"\n",
                "                    f\"n_init={self._n_init}.\"\n",
                "                ),\n",
                "                RuntimeWarning,\n",
                "                stacklevel=2,\n",
                "            )\n",
                "            self._n_init = 1\n",
                "\n",
                "    @abstractmethod\n",
                "    def _warn_mkl_vcomp(self, n_active_threads):\n",
                "        \"\"\"Issue an estimator specific warning when vcomp and mkl are both present\n",
                "\n",
                "        This method is called by `_check_mkl_vcomp`.\n",
                "        \"\"\"\n",
                "\n",
                "    def _check_mkl_vcomp(self, X, n_samples):\n",
                "        \"\"\"Check when vcomp and mkl are both present\"\"\"\n",
                "        # The BLAS call inside a prange in lloyd_iter_chunked_dense is known to\n",
                "        # cause a small memory leak when there are less chunks than the number\n",
                "        # of available threads. It only happens when the OpenMP library is\n",
                "        # vcomp (microsoft OpenMP) and the BLAS library is MKL. see #18653\n",
                "        if sp.issparse(X):\n",
                "            return\n",
                "\n",
                "        n_active_threads = int(np.ceil(n_samples / CHUNK_SIZE))\n",
                "        if n_active_threads < self._n_threads:\n",
                "            modules = threadpool_info()\n",
                "            has_vcomp = \"vcomp\" in [module[\"prefix\"] for module in modules]\n",
                "            has_mkl = (\"mkl\", \"intel\") in [\n",
                "                (module[\"internal_api\"], module.get(\"threading_layer\", None))\n",
                "                for module in modules\n",
                "            ]\n",
                "            if has_vcomp and has_mkl:\n",
                "                self._warn_mkl_vcomp(n_active_threads)\n",
                "\n",
                "    def _validate_center_shape(self, X, centers):\n",
                "        \"\"\"Check if centers is compatible with X and n_clusters.\"\"\"\n",
                "        if centers.shape[0] != self.n_clusters:\n",
                "            raise ValueError(\n",
                "                f\"The shape of the initial centers {centers.shape} does not \"\n",
                "                f\"match the number of clusters {self.n_clusters}.\"\n",
                "            )\n",
                "        if centers.shape[1] != X.shape[1]:\n",
                "            raise ValueError(\n",
                "                f\"The shape of the initial centers {centers.shape} does not \"\n",
                "                f\"match the number of features of the data {X.shape[1]}.\"\n",
                "            )\n",
                "\n",
                "    def _check_test_data(self, X):\n",
                "        X = self._validate_data(\n",
                "            X,\n",
                "            accept_sparse=\"csr\",\n",
                "            reset=False,\n",
                "            dtype=[np.float64, np.float32],\n",
                "            order=\"C\",\n",
                "            accept_large_sparse=False,\n",
                "        )\n",
                "        return X\n",
                "\n",
                "    def _init_centroids(\n",
                "        self,\n",
                "        X,\n",
                "        x_squared_norms,\n",
                "        init,\n",
                "        random_state,\n",
                "        sample_weight,\n",
                "        init_size=None,\n",
                "        n_centroids=None,\n",
                "    ):\n",
                "        \"\"\"Compute the initial centroids.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
                "            The input samples.\n",
                "\n",
                "        x_squared_norms : ndarray of shape (n_samples,)\n",
                "            Squared euclidean norm of each data point. Pass it if you have it\n",
                "            at hands already to avoid it being recomputed here.\n",
                "\n",
                "        init : {'k-means++', 'random'}, callable or ndarray of shape \\\n",
                "                (n_clusters, n_features)\n",
                "            Method for initialization.\n",
                "\n",
                "        random_state : RandomState instance\n",
                "            Determines random number generation for centroid initialization.\n",
                "            See :term:`Glossary <random_state>`.\n",
                "\n",
                "        sample_weight : ndarray of shape (n_samples,)\n",
                "            The weights for each observation in X. `sample_weight` is not used\n",
                "            during initialization if `init` is a callable or a user provided\n",
                "            array.\n",
                "\n",
                "        init_size : int, default=None\n",
                "            Number of samples to randomly sample for speeding up the\n",
                "            initialization (sometimes at the expense of accuracy).\n",
                "\n",
                "        n_centroids : int, default=None\n",
                "            Number of centroids to initialize.\n",
                "            If left to 'None' the number of centroids will be equal to\n",
                "            number of clusters to form (self.n_clusters).\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        centers : ndarray of shape (n_clusters, n_features)\n",
                "            Initial centroids of clusters.\n",
                "        \"\"\"\n",
                "        n_samples = X.shape[0]\n",
                "        n_clusters = self.n_clusters if n_centroids is None else n_centroids\n",
                "\n",
                "        if init_size is not None and init_size < n_samples:\n",
                "            init_indices = random_state.randint(0, n_samples, init_size)\n",
                "            X = X[init_indices]\n",
                "            x_squared_norms = x_squared_norms[init_indices]\n",
                "            n_samples = X.shape[0]\n",
                "            sample_weight = sample_weight[init_indices]\n",
                "\n",
                "        if isinstance(init, str) and init == \"k-means++\":\n",
                "            centers, _ = _kmeans_plusplus(\n",
                "                X,\n",
                "                n_clusters,\n",
                "                random_state=random_state,\n",
                "                x_squared_norms=x_squared_norms,\n",
                "                sample_weight=sample_weight,\n",
                "            )\n",
                "        elif isinstance(init, str) and init == \"random\":\n",
                "            seeds = random_state.choice(\n",
                "                n_samples,\n",
                "                size=n_clusters,\n",
                "                replace=False,\n",
                "                p=sample_weight / sample_weight.sum(),\n",
                "            )\n",
                "            centers = X[seeds]\n",
                "        elif _is_arraylike_not_scalar(self.init):\n",
                "            centers = init\n",
                "        elif callable(init):\n",
                "            centers = init(X, n_clusters, random_state=random_state)\n",
                "            centers = check_array(centers, dtype=X.dtype, copy=False, order=\"C\")\n",
                "            self._validate_center_shape(X, centers)\n",
                "\n",
                "        if sp.issparse(centers):\n",
                "            centers = centers.toarray()\n",
                "\n",
                "        return centers\n",
                "\n",
                "    def fit_predict(self, X, y=None, sample_weight=None):\n",
                "        \"\"\"Compute cluster centers and predict cluster index for each sample.\n",
                "\n",
                "        Convenience method; equivalent to calling fit(X) followed by\n",
                "        predict(X).\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "            New data to transform.\n",
                "\n",
                "        y : Ignored\n",
                "            Not used, present here for API consistency by convention.\n",
                "\n",
                "        sample_weight : array-like of shape (n_samples,), default=None\n",
                "            The weights for each observation in X. If None, all observations\n",
                "            are assigned equal weight.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        labels : ndarray of shape (n_samples,)\n",
                "            Index of the cluster each sample belongs to.\n",
                "        \"\"\"\n",
                "        return self.fit(X, sample_weight=sample_weight).labels_\n",
                "\n",
                "    def predict(self, X, sample_weight=\"deprecated\"):\n",
                "        \"\"\"Predict the closest cluster each sample in X belongs to.\n",
                "\n",
                "        In the vector quantization literature, `cluster_centers_` is called\n",
                "        the code book and each value returned by `predict` is the index of\n",
                "        the closest code in the code book.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "            New data to predict.\n",
                "\n",
                "        sample_weight : array-like of shape (n_samples,), default=None\n",
                "            The weights for each observation in X. If None, all observations\n",
                "            are assigned equal weight.\n",
                "\n",
                "            .. deprecated:: 1.3\n",
                "               The parameter `sample_weight` is deprecated in version 1.3\n",
                "               and will be removed in 1.5.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        labels : ndarray of shape (n_samples,)\n",
                "            Index of the cluster each sample belongs to.\n",
                "        \"\"\"\n",
                "        check_is_fitted(self)\n",
                "\n",
                "        X = self._check_test_data(X)\n",
                "        if not (isinstance(sample_weight, str) and sample_weight == \"deprecated\"):\n",
                "            warnings.warn(\n",
                "                (\n",
                "                    \"'sample_weight' was deprecated in version 1.3 and \"\n",
                "                    \"will be removed in 1.5.\"\n",
                "                ),\n",
                "                FutureWarning,\n",
                "            )\n",
                "            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
                "        else:\n",
                "            sample_weight = _check_sample_weight(None, X, dtype=X.dtype)\n",
                "\n",
                "        labels = _labels_inertia_threadpool_limit(\n",
                "            X,\n",
                "            sample_weight,\n",
                "            self.cluster_centers_,\n",
                "            n_threads=self._n_threads,\n",
                "            return_inertia=False,\n",
                "        )\n",
                "\n",
                "        return labels\n",
                "\n",
                "    def fit_transform(self, X, y=None, sample_weight=None):\n",
                "        \"\"\"Compute clustering and transform X to cluster-distance space.\n",
                "\n",
                "        Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "            New data to transform.\n",
                "\n",
                "        y : Ignored\n",
                "            Not used, present here for API consistency by convention.\n",
                "\n",
                "        sample_weight : array-like of shape (n_samples,), default=None\n",
                "            The weights for each observation in X. If None, all observations\n",
                "            are assigned equal weight.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        X_new : ndarray of shape (n_samples, n_clusters)\n",
                "            X transformed in the new space.\n",
                "        \"\"\"\n",
                "        return self.fit(X, sample_weight=sample_weight)._transform(X)\n",
                "\n",
                "    def transform(self, X):\n",
                "        \"\"\"Transform X to a cluster-distance space.\n",
                "\n",
                "        In the new space, each dimension is the distance to the cluster\n",
                "        centers. Note that even if X is sparse, the array returned by\n",
                "        `transform` will typically be dense.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "            New data to transform.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        X_new : ndarray of shape (n_samples, n_clusters)\n",
                "            X transformed in the new space.\n",
                "        \"\"\"\n",
                "        check_is_fitted(self)\n",
                "\n",
                "        X = self._check_test_data(X)\n",
                "        return self._transform(X)\n",
                "\n",
                "    def _transform(self, X):\n",
                "        \"\"\"Guts of transform method; no input validation.\"\"\"\n",
                "        return euclidean_distances(X, self.cluster_centers_)\n",
                "\n",
                "    def score(self, X, y=None, sample_weight=None):\n",
                "        \"\"\"Opposite of the value of X on the K-means objective.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "            New data.\n",
                "\n",
                "        y : Ignored\n",
                "            Not used, present here for API consistency by convention.\n",
                "\n",
                "        sample_weight : array-like of shape (n_samples,), default=None\n",
                "            The weights for each observation in X. If None, all observations\n",
                "            are assigned equal weight.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        score : float\n",
                "            Opposite of the value of X on the K-means objective.\n",
                "        \"\"\"\n",
                "        check_is_fitted(self)\n",
                "\n",
                "        X = self._check_test_data(X)\n",
                "        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
                "\n",
                "        _, scores = _labels_inertia_threadpool_limit(\n",
                "            X, sample_weight, self.cluster_centers_, self._n_threads\n",
                "        )\n",
                "        return -scores\n",
                "\n",
                "    def _more_tags(self):\n",
                "        return {\n",
                "            \"_xfail_checks\": {\n",
                "                \"check_sample_weights_invariance\": (\n",
                "                    \"zero sample_weight is not equivalent to removing samples\"\n",
                "                ),\n",
                "            },\n",
                "        }\n",
                "\n",
                "\n",
                "class KMeans(_BaseKMeans):\n",
                "    \"\"\"K-Means clustering.\n",
                "\n",
                "    Read more in the :ref:`User Guide <k_means>`.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "\n",
                "    n_clusters : int, default=8\n",
                "        The number of clusters to form as well as the number of\n",
                "        centroids to generate.\n",
                "\n",
                "        For an example of how to choose an optimal value for `n_clusters` refer to\n",
                "        :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n",
                "\n",
                "    init : {'k-means++', 'random'}, callable or array-like of shape \\\n",
                "            (n_clusters, n_features), default='k-means++'\n",
                "        Method for initialization:\n",
                "\n",
                "        * 'k-means++' : selects initial cluster centroids using sampling \\\n",
                "            based on an empirical probability distribution of the points' \\\n",
                "            contribution to the overall inertia. This technique speeds up \\\n",
                "            convergence. The algorithm implemented is \"greedy k-means++\". It \\\n",
                "            differs from the vanilla k-means++ by making several trials at \\\n",
                "            each sampling step and choosing the best centroid among them.\n",
                "\n",
                "        * 'random': choose `n_clusters` observations (rows) at random from \\\n",
                "        data for the initial centroids.\n",
                "\n",
                "        * If an array is passed, it should be of shape (n_clusters, n_features)\\\n",
                "        and gives the initial centers.\n",
                "\n",
                "        * If a callable is passed, it should take arguments X, n_clusters and a\\\n",
                "        random state and return an initialization.\n",
                "\n",
                "        For an example of how to use the different `init` strategy, see the example\n",
                "        entitled :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n",
                "\n",
                "    n_init : 'auto' or int, default='auto'\n",
                "        Number of times the k-means algorithm is run with different centroid\n",
                "        seeds. The final results is the best output of `n_init` consecutive runs\n",
                "        in terms of inertia. Several runs are recommended for sparse\n",
                "        high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n",
                "\n",
                "        When `n_init='auto'`, the number of runs depends on the value of init:\n",
                "        10 if using `init='random'` or `init` is a callable;\n",
                "        1 if using `init='k-means++'` or `init` is an array-like.\n",
                "\n",
                "        .. versionadded:: 1.2\n",
                "           Added 'auto' option for `n_init`.\n",
                "\n",
                "        .. versionchanged:: 1.4\n",
                "           Default value for `n_init` changed to `'auto'`.\n",
                "\n",
                "    max_iter : int, default=300\n",
                "        Maximum number of iterations of the k-means algorithm for a\n",
                "        single run.\n",
                "\n",
                "    tol : float, default=1e-4\n",
                "        Relative tolerance with regards to Frobenius norm of the difference\n",
                "        in the cluster centers of two consecutive iterations to declare\n",
                "        convergence.\n",
                "\n",
                "    verbose : int, default=0\n",
                "        Verbosity mode.\n",
                "\n",
                "    random_state : int, RandomState instance or None, default=None\n",
                "        Determines random number generation for centroid initialization. Use\n",
                "        an int to make the randomness deterministic.\n",
                "        See :term:`Glossary <random_state>`.\n",
                "\n",
                "    copy_x : bool, default=True\n",
                "        When pre-computing distances it is more numerically accurate to center\n",
                "        the data first. If copy_x is True (default), then the original data is\n",
                "        not modified. If False, the original data is modified, and put back\n",
                "        before the function returns, but small numerical differences may be\n",
                "        introduced by subtracting and then adding the data mean. Note that if\n",
                "        the original data is not C-contiguous, a copy will be made even if\n",
                "        copy_x is False. If the original data is sparse, but not in CSR format,\n",
                "        a copy will be made even if copy_x is False.\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    algorithm : {\"lloyd\", \"elkan\", \"auto\", \"full\"}, default=\"lloyd\"\n"
                ],
                "after": [
                    "    algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n"
                ],
                "parent_version_range": {
                    "start": 1296,
                    "end": 1297
                },
                "child_version_range": {
                    "start": 1293,
                    "end": 1294
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "KMeans",
                        "signature": "class KMeans(_BaseKMeans):",
                        "at_line": 1215
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: sklearn/cluster/_kmeans.py\nCode:\n             class KMeans(_BaseKMeans):\n                 ...\n1293 1290            copy_x is False. If the original data is sparse, but not in CSR format,\n1294 1291            a copy will be made even if copy_x is False.\n1295 1292    \n1296       -     algorithm : {\"lloyd\", \"elkan\", \"auto\", \"full\"}, default=\"lloyd\"\n     1293  +     algorithm : {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n1297 1294            K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n1298 1295            The `\"elkan\"` variation can be more efficient on some datasets with\n1299 1296            well-defined clusters, by using the triangle inequality. However it's\n           ...\n",
                "file_path": "sklearn/cluster/_kmeans.py",
                "identifiers_before": [
                    "algorithm",
                    "default"
                ],
                "identifiers_after": [
                    "algorithm",
                    "default"
                ],
                "prefix": [
                    "        copy_x is False. If the original data is sparse, but not in CSR format,\n",
                    "        a copy will be made even if copy_x is False.\n",
                    "\n"
                ],
                "suffix": [
                    "        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n",
                    "        The `\"elkan\"` variation can be more efficient on some datasets with\n",
                    "        well-defined clusters, by using the triangle inequality. However it's\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    0
                ]
            },
            [
                "        K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n",
                "        The `\"elkan\"` variation can be more efficient on some datasets with\n",
                "        well-defined clusters, by using the triangle inequality. However it's\n",
                "        more memory intensive due to the allocation of an extra array of shape\n",
                "        `(n_samples, n_clusters)`.\n",
                "\n"
            ],
            {
                "type": "delete",
                "before": [
                    "        `\"auto\"` and `\"full\"` are deprecated and they will be removed in\n",
                    "        Scikit-Learn 1.3. They are both aliases for `\"lloyd\"`.\n",
                    "\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 1303,
                    "end": 1306
                },
                "child_version_range": {
                    "start": 1300,
                    "end": 1300
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "KMeans",
                        "signature": "class KMeans(_BaseKMeans):",
                        "at_line": 1215
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: sklearn/cluster/_kmeans.py\nCode:\n             class KMeans(_BaseKMeans):\n                 ...\n1300 1297            more memory intensive due to the allocation of an extra array of shape\n1301 1298            `(n_samples, n_clusters)`.\n1302 1299    \n1303       -         `\"auto\"` and `\"full\"` are deprecated and they will be removed in\n1304       -         Scikit-Learn 1.3. They are both aliases for `\"lloyd\"`.\n1305       - \n1306 1300            .. versionchanged:: 0.18\n1307 1301                Added Elkan algorithm\n1308 1302    \n           ...\n",
                "file_path": "sklearn/cluster/_kmeans.py",
                "identifiers_before": [
                    "Learn",
                    "Scikit",
                    "They",
                    "aliases",
                    "are",
                    "be",
                    "both",
                    "deprecated",
                    "removed",
                    "they",
                    "will"
                ],
                "identifiers_after": [],
                "prefix": [
                    "        more memory intensive due to the allocation of an extra array of shape\n",
                    "        `(n_samples, n_clusters)`.\n",
                    "\n"
                ],
                "suffix": [
                    "        .. versionchanged:: 0.18\n",
                    "            Added Elkan algorithm\n",
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    1
                ]
            },
            [
                "        .. versionchanged:: 0.18\n",
                "            Added Elkan algorithm\n",
                "\n",
                "        .. versionchanged:: 1.1\n",
                "            Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n",
                "            Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n",
                "\n",
                "    Attributes\n",
                "    ----------\n",
                "    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
                "        Coordinates of cluster centers. If the algorithm stops before fully\n",
                "        converging (see ``tol`` and ``max_iter``), these will not be\n",
                "        consistent with ``labels_``.\n",
                "\n",
                "    labels_ : ndarray of shape (n_samples,)\n",
                "        Labels of each point\n",
                "\n",
                "    inertia_ : float\n",
                "        Sum of squared distances of samples to their closest cluster center,\n",
                "        weighted by the sample weights if provided.\n",
                "\n",
                "    n_iter_ : int\n",
                "        Number of iterations run.\n",
                "\n",
                "    n_features_in_ : int\n",
                "        Number of features seen during :term:`fit`.\n",
                "\n",
                "        .. versionadded:: 0.24\n",
                "\n",
                "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
                "        Names of features seen during :term:`fit`. Defined only when `X`\n",
                "        has feature names that are all strings.\n",
                "\n",
                "        .. versionadded:: 1.0\n",
                "\n",
                "    See Also\n",
                "    --------\n",
                "    MiniBatchKMeans : Alternative online implementation that does incremental\n",
                "        updates of the centers positions using mini-batches.\n",
                "        For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
                "        probably much faster than the default batch implementation.\n",
                "\n",
                "    Notes\n",
                "    -----\n",
                "    The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n",
                "\n",
                "    The average complexity is given by O(k n T), where n is the number of\n",
                "    samples and T is the number of iteration.\n",
                "\n",
                "    The worst case complexity is given by O(n^(k+2/p)) with\n",
                "    n = n_samples, p = n_features.\n",
                "    Refer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\n",
                "    SoCG2006.<10.1145/1137856.1137880>` for more details.\n",
                "\n",
                "    In practice, the k-means algorithm is very fast (one of the fastest\n",
                "    clustering algorithms available), but it falls in local minima. That's why\n",
                "    it can be useful to restart it several times.\n",
                "\n",
                "    If the algorithm stops before fully converging (because of ``tol`` or\n",
                "    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n",
                "    i.e. the ``cluster_centers_`` will not be the means of the points in each\n",
                "    cluster. Also, the estimator will reassign ``labels_`` after the last\n",
                "    iteration to make ``labels_`` consistent with ``predict`` on the training\n",
                "    set.\n",
                "\n",
                "    Examples\n",
                "    --------\n",
                "\n",
                "    >>> from sklearn.cluster import KMeans\n",
                "    >>> import numpy as np\n",
                "    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
                "    ...               [10, 2], [10, 4], [10, 0]])\n",
                "    >>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
                "    >>> kmeans.labels_\n",
                "    array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
                "    >>> kmeans.predict([[0, 0], [12, 3]])\n",
                "    array([1, 0], dtype=int32)\n",
                "    >>> kmeans.cluster_centers_\n",
                "    array([[10.,  2.],\n",
                "           [ 1.,  2.]])\n",
                "\n",
                "    For a more detailed example of K-Means using the iris dataset see\n",
                "    :ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`.\n",
                "\n",
                "    For examples of common problems with K-Means and how to address them see\n",
                "    :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`.\n",
                "\n",
                "    For an example of how to use K-Means to perform color quantization see\n",
                "    :ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.\n",
                "\n",
                "    For a demonstration of how K-Means can be used to cluster text documents see\n",
                "    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n",
                "\n",
                "    For a comparison between K-Means and MiniBatchKMeans refer to example\n",
                "    :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`.\n",
                "    \"\"\"\n",
                "\n",
                "    _parameter_constraints: dict = {\n",
                "        **_BaseKMeans._parameter_constraints,\n",
                "        \"copy_x\": [\"boolean\"],\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        \"algorithm\": [\n",
                    "            StrOptions({\"lloyd\", \"elkan\", \"auto\", \"full\"}, deprecated={\"auto\", \"full\"})\n",
                    "        ],\n"
                ],
                "after": [
                    "        \"algorithm\": [StrOptions({\"lloyd\", \"elkan\"})],\n"
                ],
                "parent_version_range": {
                    "start": 1406,
                    "end": 1409
                },
                "child_version_range": {
                    "start": 1400,
                    "end": 1401
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "KMeans",
                        "signature": "class KMeans(_BaseKMeans):",
                        "at_line": 1215
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: sklearn/cluster/_kmeans.py\nCode:\n             class KMeans(_BaseKMeans):\n                 ...\n1403 1397        _parameter_constraints: dict = {\n1404 1398            **_BaseKMeans._parameter_constraints,\n1405 1399            \"copy_x\": [\"boolean\"],\n1406       -         \"algorithm\": [\n1407       -             StrOptions({\"lloyd\", \"elkan\", \"auto\", \"full\"}, deprecated={\"auto\", \"full\"})\n1408       -         ],\n     1400  +         \"algorithm\": [StrOptions({\"lloyd\", \"elkan\"})],\n1409 1401        }\n1410 1402    \n1411 1403        def __init__(\n           ...\n",
                "file_path": "sklearn/cluster/_kmeans.py",
                "identifiers_before": [
                    "StrOptions",
                    "deprecated"
                ],
                "identifiers_after": [
                    "StrOptions"
                ],
                "prefix": [
                    "    _parameter_constraints: dict = {\n",
                    "        **_BaseKMeans._parameter_constraints,\n",
                    "        \"copy_x\": [\"boolean\"],\n"
                ],
                "suffix": [
                    "    }\n",
                    "\n",
                    "    def __init__(\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "    }\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        n_clusters=8,\n",
                "        *,\n",
                "        init=\"k-means++\",\n",
                "        n_init=\"auto\",\n",
                "        max_iter=300,\n",
                "        tol=1e-4,\n",
                "        verbose=0,\n",
                "        random_state=None,\n",
                "        copy_x=True,\n",
                "        algorithm=\"lloyd\",\n",
                "    ):\n",
                "        super().__init__(\n",
                "            n_clusters=n_clusters,\n",
                "            init=init,\n",
                "            n_init=n_init,\n",
                "            max_iter=max_iter,\n",
                "            tol=tol,\n",
                "            verbose=verbose,\n",
                "            random_state=random_state,\n",
                "        )\n",
                "\n",
                "        self.copy_x = copy_x\n",
                "        self.algorithm = algorithm\n",
                "\n",
                "    def _check_params_vs_input(self, X):\n",
                "        super()._check_params_vs_input(X, default_n_init=10)\n",
                "\n",
                "        self._algorithm = self.algorithm\n"
            ],
            {
                "type": "delete",
                "before": [
                    "        if self._algorithm in (\"auto\", \"full\"):\n",
                    "            warnings.warn(\n",
                    "                (\n",
                    "                    f\"algorithm='{self._algorithm}' is deprecated, it will be \"\n",
                    "                    \"removed in 1.3. Using 'lloyd' instead.\"\n",
                    "                ),\n",
                    "                FutureWarning,\n",
                    "            )\n",
                    "            self._algorithm = \"lloyd\"\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 1441,
                    "end": 1450
                },
                "child_version_range": {
                    "start": 1433,
                    "end": 1433
                },
                "control_flow": [
                    {
                        "type": "if_statement",
                        "statement": "if self._algorithm in (\"auto\", \"full\"):",
                        "start_line": 1441,
                        "end_line": 1449
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "KMeans",
                        "signature": "class KMeans(_BaseKMeans):",
                        "at_line": 1215
                    },
                    {
                        "type": "function",
                        "name": "_check_params_vs_input",
                        "signature": "def _check_params_vs_input(self, X):",
                        "at_line": 1437
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: sklearn/cluster/_kmeans.py\nCode:\n             class KMeans(_BaseKMeans):\n                 ...\n                 def _check_params_vs_input(self, X):\n                     ...\n1438 1430            super()._check_params_vs_input(X, default_n_init=10)\n1439 1431    \n1440 1432            self._algorithm = self.algorithm\n1441       -         if self._algorithm in (\"auto\", \"full\"):\n1442       -             warnings.warn(\n1443       -                 (\n1444       -                     f\"algorithm='{self._algorithm}' is deprecated, it will be \"\n1445       -                     \"removed in 1.3. Using 'lloyd' instead.\"\n1446       -                 ),\n1447       -                 FutureWarning,\n1448       -             )\n1449       -             self._algorithm = \"lloyd\"\n1450 1433            if self._algorithm == \"elkan\" and self.n_clusters == 1:\n1451 1434                warnings.warn(\n1452 1435                    (\n           ...\n",
                "file_path": "sklearn/cluster/_kmeans.py",
                "identifiers_before": [
                    "FutureWarning",
                    "_algorithm",
                    "self",
                    "warn",
                    "warnings"
                ],
                "identifiers_after": [],
                "prefix": [
                    "        super()._check_params_vs_input(X, default_n_init=10)\n",
                    "\n",
                    "        self._algorithm = self.algorithm\n"
                ],
                "suffix": [
                    "        if self._algorithm == \"elkan\" and self.n_clusters == 1:\n",
                    "            warnings.warn(\n",
                    "                (\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        if self._algorithm == \"elkan\" and self.n_clusters == 1:\n",
                "            warnings.warn(\n",
                "                (\n",
                "                    \"algorithm='elkan' doesn't make sense for a single \"\n",
                "                    \"cluster. Using 'lloyd' instead.\"\n",
                "                ),\n",
                "                RuntimeWarning,\n",
                "            )\n",
                "            self._algorithm = \"lloyd\"\n",
                "\n",
                "    def _warn_mkl_vcomp(self, n_active_threads):\n",
                "        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n",
                "        warnings.warn(\n",
                "            \"KMeans is known to have a memory leak on Windows \"\n",
                "            \"with MKL, when there are less chunks than available \"\n",
                "            \"threads. You can avoid it by setting the environment\"\n",
                "            f\" variable OMP_NUM_THREADS={n_active_threads}.\"\n",
                "        )\n",
                "\n",
                "    @_fit_context(prefer_skip_nested_validation=True)\n",
                "    def fit(self, X, y=None, sample_weight=None):\n",
                "        \"\"\"Compute k-means clustering.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "            Training instances to cluster. It must be noted that the data\n",
                "            will be converted to C ordering, which will cause a memory\n",
                "            copy if the given data is not C-contiguous.\n",
                "            If a sparse matrix is passed, a copy will be made if it's not in\n",
                "            CSR format.\n",
                "\n",
                "        y : Ignored\n",
                "            Not used, present here for API consistency by convention.\n",
                "\n",
                "        sample_weight : array-like of shape (n_samples,), default=None\n",
                "            The weights for each observation in X. If None, all observations\n",
                "            are assigned equal weight. `sample_weight` is not used during\n",
                "            initialization if `init` is a callable or a user provided array.\n",
                "\n",
                "            .. versionadded:: 0.20\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        self : object\n",
                "            Fitted estimator.\n",
                "        \"\"\"\n",
                "        X = self._validate_data(\n",
                "            X,\n",
                "            accept_sparse=\"csr\",\n",
                "            dtype=[np.float64, np.float32],\n",
                "            order=\"C\",\n",
                "            copy=self.copy_x,\n",
                "            accept_large_sparse=False,\n",
                "        )\n",
                "\n",
                "        self._check_params_vs_input(X)\n",
                "\n",
                "        random_state = check_random_state(self.random_state)\n",
                "        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
                "        self._n_threads = _openmp_effective_n_threads()\n",
                "\n",
                "        # Validate init array\n",
                "        init = self.init\n",
                "        init_is_array_like = _is_arraylike_not_scalar(init)\n",
                "        if init_is_array_like:\n",
                "            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n",
                "            self._validate_center_shape(X, init)\n",
                "\n",
                "        # subtract of mean of x for more accurate distance computations\n",
                "        if not sp.issparse(X):\n",
                "            X_mean = X.mean(axis=0)\n",
                "            # The copy was already done above\n",
                "            X -= X_mean\n",
                "\n",
                "            if init_is_array_like:\n",
                "                init -= X_mean\n",
                "\n",
                "        # precompute squared norms of data points\n",
                "        x_squared_norms = row_norms(X, squared=True)\n",
                "\n",
                "        if self._algorithm == \"elkan\":\n",
                "            kmeans_single = _kmeans_single_elkan\n",
                "        else:\n",
                "            kmeans_single = _kmeans_single_lloyd\n",
                "            self._check_mkl_vcomp(X, X.shape[0])\n",
                "\n",
                "        best_inertia, best_labels = None, None\n",
                "\n",
                "        for i in range(self._n_init):\n",
                "            # Initialize centers\n",
                "            centers_init = self._init_centroids(\n",
                "                X,\n",
                "                x_squared_norms=x_squared_norms,\n",
                "                init=init,\n",
                "                random_state=random_state,\n",
                "                sample_weight=sample_weight,\n",
                "            )\n",
                "            if self.verbose:\n",
                "                print(\"Initialization complete\")\n",
                "\n",
                "            # run a k-means once\n",
                "            labels, inertia, centers, n_iter_ = kmeans_single(\n",
                "                X,\n",
                "                sample_weight,\n",
                "                centers_init,\n",
                "                max_iter=self.max_iter,\n",
                "                verbose=self.verbose,\n",
                "                tol=self._tol,\n",
                "                n_threads=self._n_threads,\n",
                "            )\n",
                "\n",
                "            # determine if these results are the best so far\n",
                "            # we chose a new run if it has a better inertia and the clustering is\n",
                "            # different from the best so far (it's possible that the inertia is\n",
                "            # slightly better even if the clustering is the same with potentially\n",
                "            # permuted labels, due to rounding errors)\n",
                "            if best_inertia is None or (\n",
                "                inertia < best_inertia\n",
                "                and not _is_same_clustering(labels, best_labels, self.n_clusters)\n",
                "            ):\n",
                "                best_labels = labels\n",
                "                best_centers = centers\n",
                "                best_inertia = inertia\n",
                "                best_n_iter = n_iter_\n",
                "\n",
                "        if not sp.issparse(X):\n",
                "            if not self.copy_x:\n",
                "                X += X_mean\n",
                "            best_centers += X_mean\n",
                "\n",
                "        distinct_clusters = len(set(best_labels))\n",
                "        if distinct_clusters < self.n_clusters:\n",
                "            warnings.warn(\n",
                "                \"Number of distinct clusters ({}) found smaller than \"\n",
                "                \"n_clusters ({}). Possibly due to duplicate points \"\n",
                "                \"in X.\".format(distinct_clusters, self.n_clusters),\n",
                "                ConvergenceWarning,\n",
                "                stacklevel=2,\n",
                "            )\n",
                "\n",
                "        self.cluster_centers_ = best_centers\n",
                "        self._n_features_out = self.cluster_centers_.shape[0]\n",
                "        self.labels_ = best_labels\n",
                "        self.inertia_ = best_inertia\n",
                "        self.n_iter_ = best_n_iter\n",
                "        return self\n",
                "\n",
                "\n",
                "def _mini_batch_step(\n",
                "    X,\n",
                "    sample_weight,\n",
                "    centers,\n",
                "    centers_new,\n",
                "    weight_sums,\n",
                "    random_state,\n",
                "    random_reassign=False,\n",
                "    reassignment_ratio=0.01,\n",
                "    verbose=False,\n",
                "    n_threads=1,\n",
                "):\n",
                "    \"\"\"Incremental update of the centers for the Minibatch K-Means algorithm.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "\n",
                "    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
                "        The original data array. If sparse, must be in CSR format.\n",
                "\n",
                "    x_squared_norms : ndarray of shape (n_samples,)\n",
                "        Squared euclidean norm of each data point.\n",
                "\n",
                "    sample_weight : ndarray of shape (n_samples,)\n",
                "        The weights for each observation in `X`.\n",
                "\n",
                "    centers : ndarray of shape (n_clusters, n_features)\n",
                "        The cluster centers before the current iteration\n",
                "\n",
                "    centers_new : ndarray of shape (n_clusters, n_features)\n",
                "        The cluster centers after the current iteration. Modified in-place.\n",
                "\n",
                "    weight_sums : ndarray of shape (n_clusters,)\n",
                "        The vector in which we keep track of the numbers of points in a\n",
                "        cluster. This array is modified in place.\n",
                "\n",
                "    random_state : RandomState instance\n",
                "        Determines random number generation for low count centers reassignment.\n",
                "        See :term:`Glossary <random_state>`.\n",
                "\n",
                "    random_reassign : boolean, default=False\n",
                "        If True, centers with very low counts are randomly reassigned\n",
                "        to observations.\n",
                "\n",
                "    reassignment_ratio : float, default=0.01\n",
                "        Control the fraction of the maximum number of counts for a\n",
                "        center to be reassigned. A higher value means that low count\n",
                "        centers are more likely to be reassigned, which means that the\n",
                "        model will take longer to converge, but should converge in a\n",
                "        better clustering.\n",
                "\n",
                "    verbose : bool, default=False\n",
                "        Controls the verbosity.\n",
                "\n",
                "    n_threads : int, default=1\n",
                "        The number of OpenMP threads to use for the computation.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    inertia : float\n",
                "        Sum of squared distances of samples to their closest cluster center.\n",
                "        The inertia is computed after finding the labels and before updating\n",
                "        the centers.\n",
                "    \"\"\"\n",
                "    # Perform label assignment to nearest centers\n",
                "    # For better efficiency, it's better to run _mini_batch_step in a\n",
                "    # threadpool_limit context than using _labels_inertia_threadpool_limit here\n",
                "    labels, inertia = _labels_inertia(X, sample_weight, centers, n_threads=n_threads)\n",
                "\n",
                "    # Update centers according to the labels\n",
                "    if sp.issparse(X):\n",
                "        _minibatch_update_sparse(\n",
                "            X, sample_weight, centers, centers_new, weight_sums, labels, n_threads\n",
                "        )\n",
                "    else:\n",
                "        _minibatch_update_dense(\n",
                "            X,\n",
                "            sample_weight,\n",
                "            centers,\n",
                "            centers_new,\n",
                "            weight_sums,\n",
                "            labels,\n",
                "            n_threads,\n",
                "        )\n",
                "\n",
                "    # Reassign clusters that have very low weight\n",
                "    if random_reassign and reassignment_ratio > 0:\n",
                "        to_reassign = weight_sums < reassignment_ratio * weight_sums.max()\n",
                "\n",
                "        # pick at most .5 * batch_size samples as new centers\n",
                "        if to_reassign.sum() > 0.5 * X.shape[0]:\n",
                "            indices_dont_reassign = np.argsort(weight_sums)[int(0.5 * X.shape[0]) :]\n",
                "            to_reassign[indices_dont_reassign] = False\n",
                "        n_reassigns = to_reassign.sum()\n",
                "\n",
                "        if n_reassigns:\n",
                "            # Pick new clusters amongst observations with uniform probability\n",
                "            new_centers = random_state.choice(\n",
                "                X.shape[0], replace=False, size=n_reassigns\n",
                "            )\n",
                "            if verbose:\n",
                "                print(f\"[MiniBatchKMeans] Reassigning {n_reassigns} cluster centers.\")\n",
                "\n",
                "            if sp.issparse(X):\n",
                "                assign_rows_csr(\n",
                "                    X,\n",
                "                    new_centers.astype(np.intp, copy=False),\n",
                "                    np.where(to_reassign)[0].astype(np.intp, copy=False),\n",
                "                    centers_new,\n",
                "                )\n",
                "            else:\n",
                "                centers_new[to_reassign] = X[new_centers]\n",
                "\n",
                "        # reset counts of reassigned centers, but don't reset them too small\n",
                "        # to avoid instant reassignment. This is a pretty dirty hack as it\n",
                "        # also modifies the learning rates.\n",
                "        weight_sums[to_reassign] = np.min(weight_sums[~to_reassign])\n",
                "\n",
                "    return inertia\n",
                "\n",
                "\n",
                "class MiniBatchKMeans(_BaseKMeans):\n",
                "    \"\"\"\n",
                "    Mini-Batch K-Means clustering.\n",
                "\n",
                "    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "\n",
                "    n_clusters : int, default=8\n",
                "        The number of clusters to form as well as the number of\n",
                "        centroids to generate.\n",
                "\n",
                "    init : {'k-means++', 'random'}, callable or array-like of shape \\\n",
                "            (n_clusters, n_features), default='k-means++'\n",
                "        Method for initialization:\n",
                "\n",
                "        'k-means++' : selects initial cluster centroids using sampling based on\n",
                "        an empirical probability distribution of the points' contribution to the\n",
                "        overall inertia. This technique speeds up convergence. The algorithm\n",
                "        implemented is \"greedy k-means++\". It differs from the vanilla k-means++\n",
                "        by making several trials at each sampling step and choosing the best centroid\n",
                "        among them.\n",
                "\n",
                "        'random': choose `n_clusters` observations (rows) at random from data\n",
                "        for the initial centroids.\n",
                "\n",
                "        If an array is passed, it should be of shape (n_clusters, n_features)\n",
                "        and gives the initial centers.\n",
                "\n",
                "        If a callable is passed, it should take arguments X, n_clusters and a\n",
                "        random state and return an initialization.\n",
                "\n",
                "    max_iter : int, default=100\n",
                "        Maximum number of iterations over the complete dataset before\n",
                "        stopping independently of any early stopping criterion heuristics.\n",
                "\n",
                "    batch_size : int, default=1024\n",
                "        Size of the mini batches.\n",
                "        For faster computations, you can set the ``batch_size`` greater than\n",
                "        256 * number of cores to enable parallelism on all cores.\n",
                "\n",
                "        .. versionchanged:: 1.0\n",
                "           `batch_size` default changed from 100 to 1024.\n",
                "\n",
                "    verbose : int, default=0\n",
                "        Verbosity mode.\n",
                "\n",
                "    compute_labels : bool, default=True\n",
                "        Compute label assignment and inertia for the complete dataset\n",
                "        once the minibatch optimization has converged in fit.\n",
                "\n",
                "    random_state : int, RandomState instance or None, default=None\n",
                "        Determines random number generation for centroid initialization and\n",
                "        random reassignment. Use an int to make the randomness deterministic.\n",
                "        See :term:`Glossary <random_state>`.\n",
                "\n",
                "    tol : float, default=0.0\n",
                "        Control early stopping based on the relative center changes as\n",
                "        measured by a smoothed, variance-normalized of the mean center\n",
                "        squared position changes. This early stopping heuristics is\n",
                "        closer to the one used for the batch variant of the algorithms\n",
                "        but induces a slight computational and memory overhead over the\n",
                "        inertia heuristic.\n",
                "\n",
                "        To disable convergence detection based on normalized center\n",
                "        change, set tol to 0.0 (default).\n",
                "\n",
                "    max_no_improvement : int, default=10\n",
                "        Control early stopping based on the consecutive number of mini\n",
                "        batches that does not yield an improvement on the smoothed inertia.\n",
                "\n",
                "        To disable convergence detection based on inertia, set\n",
                "        max_no_improvement to None.\n",
                "\n",
                "    init_size : int, default=None\n",
                "        Number of samples to randomly sample for speeding up the\n",
                "        initialization (sometimes at the expense of accuracy): the\n",
                "        only algorithm is initialized by running a batch KMeans on a\n",
                "        random subset of the data. This needs to be larger than n_clusters.\n",
                "\n",
                "        If `None`, the heuristic is `init_size = 3 * batch_size` if\n",
                "        `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.\n",
                "\n",
                "    n_init : 'auto' or int, default=\"auto\"\n",
                "        Number of random initializations that are tried.\n",
                "        In contrast to KMeans, the algorithm is only run once, using the best of\n",
                "        the `n_init` initializations as measured by inertia. Several runs are\n",
                "        recommended for sparse high-dimensional problems (see\n",
                "        :ref:`kmeans_sparse_high_dim`).\n",
                "\n",
                "        When `n_init='auto'`, the number of runs depends on the value of init:\n",
                "        3 if using `init='random'` or `init` is a callable;\n",
                "        1 if using `init='k-means++'` or `init` is an array-like.\n",
                "\n",
                "        .. versionadded:: 1.2\n",
                "           Added 'auto' option for `n_init`.\n",
                "\n",
                "        .. versionchanged:: 1.4\n",
                "           Default value for `n_init` changed to `'auto'` in version.\n",
                "\n",
                "    reassignment_ratio : float, default=0.01\n",
                "        Control the fraction of the maximum number of counts for a center to\n",
                "        be reassigned. A higher value means that low count centers are more\n",
                "        easily reassigned, which means that the model will take longer to\n",
                "        converge, but should converge in a better clustering. However, too high\n",
                "        a value may cause convergence issues, especially with a small batch\n",
                "        size.\n",
                "\n",
                "    Attributes\n",
                "    ----------\n",
                "\n",
                "    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
                "        Coordinates of cluster centers.\n",
                "\n",
                "    labels_ : ndarray of shape (n_samples,)\n",
                "        Labels of each point (if compute_labels is set to True).\n",
                "\n",
                "    inertia_ : float\n",
                "        The value of the inertia criterion associated with the chosen\n",
                "        partition if compute_labels is set to True. If compute_labels is set to\n",
                "        False, it's an approximation of the inertia based on an exponentially\n",
                "        weighted average of the batch inertiae.\n",
                "        The inertia is defined as the sum of square distances of samples to\n",
                "        their cluster center, weighted by the sample weights if provided.\n",
                "\n",
                "    n_iter_ : int\n",
                "        Number of iterations over the full dataset.\n",
                "\n",
                "    n_steps_ : int\n",
                "        Number of minibatches processed.\n",
                "\n",
                "        .. versionadded:: 1.0\n",
                "\n",
                "    n_features_in_ : int\n",
                "        Number of features seen during :term:`fit`.\n",
                "\n",
                "        .. versionadded:: 0.24\n",
                "\n",
                "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
                "        Names of features seen during :term:`fit`. Defined only when `X`\n",
                "        has feature names that are all strings.\n",
                "\n",
                "        .. versionadded:: 1.0\n",
                "\n",
                "    See Also\n",
                "    --------\n",
                "    KMeans : The classic implementation of the clustering method based on the\n",
                "        Lloyd's algorithm. It consumes the whole set of input data at each\n",
                "        iteration.\n",
                "\n",
                "    Notes\n",
                "    -----\n",
                "    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n",
                "\n",
                "    When there are too few points in the dataset, some centers may be\n",
                "    duplicated, which means that a proper clustering in terms of the number\n",
                "    of requesting clusters and the number of returned clusters will not\n",
                "    always match. One solution is to set `reassignment_ratio=0`, which\n",
                "    prevents reassignments of clusters that are too small.\n",
                "\n",
                "    Examples\n",
                "    --------\n",
                "    >>> from sklearn.cluster import MiniBatchKMeans\n",
                "    >>> import numpy as np\n",
                "    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
                "    ...               [4, 2], [4, 0], [4, 4],\n",
                "    ...               [4, 5], [0, 1], [2, 2],\n",
                "    ...               [3, 2], [5, 5], [1, -1]])\n",
                "    >>> # manually fit on batches\n",
                "    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
                "    ...                          random_state=0,\n",
                "    ...                          batch_size=6,\n",
                "    ...                          n_init=\"auto\")\n",
                "    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n",
                "    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n",
                "    >>> kmeans.cluster_centers_\n",
                "    array([[3.375, 3.  ],\n",
                "           [0.75 , 0.5 ]])\n",
                "    >>> kmeans.predict([[0, 0], [4, 4]])\n",
                "    array([1, 0], dtype=int32)\n",
                "    >>> # fit on the whole data\n",
                "    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
                "    ...                          random_state=0,\n",
                "    ...                          batch_size=6,\n",
                "    ...                          max_iter=10,\n",
                "    ...                          n_init=\"auto\").fit(X)\n",
                "    >>> kmeans.cluster_centers_\n",
                "    array([[3.55102041, 2.48979592],\n",
                "           [1.06896552, 1.        ]])\n",
                "    >>> kmeans.predict([[0, 0], [4, 4]])\n",
                "    array([1, 0], dtype=int32)\n",
                "    \"\"\"\n",
                "\n",
                "    _parameter_constraints: dict = {\n",
                "        **_BaseKMeans._parameter_constraints,\n",
                "        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n",
                "        \"compute_labels\": [\"boolean\"],\n",
                "        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\n",
                "        \"init_size\": [Interval(Integral, 1, None, closed=\"left\"), None],\n",
                "        \"reassignment_ratio\": [Interval(Real, 0, None, closed=\"left\")],\n",
                "    }\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        n_clusters=8,\n",
                "        *,\n",
                "        init=\"k-means++\",\n",
                "        max_iter=100,\n",
                "        batch_size=1024,\n",
                "        verbose=0,\n",
                "        compute_labels=True,\n",
                "        random_state=None,\n",
                "        tol=0.0,\n",
                "        max_no_improvement=10,\n",
                "        init_size=None,\n",
                "        n_init=\"auto\",\n",
                "        reassignment_ratio=0.01,\n",
                "    ):\n",
                "        super().__init__(\n",
                "            n_clusters=n_clusters,\n",
                "            init=init,\n",
                "            max_iter=max_iter,\n",
                "            verbose=verbose,\n",
                "            random_state=random_state,\n",
                "            tol=tol,\n",
                "            n_init=n_init,\n",
                "        )\n",
                "\n",
                "        self.max_no_improvement = max_no_improvement\n",
                "        self.batch_size = batch_size\n",
                "        self.compute_labels = compute_labels\n",
                "        self.init_size = init_size\n",
                "        self.reassignment_ratio = reassignment_ratio\n",
                "\n",
                "    def _check_params_vs_input(self, X):\n",
                "        super()._check_params_vs_input(X, default_n_init=3)\n",
                "\n",
                "        self._batch_size = min(self.batch_size, X.shape[0])\n",
                "\n",
                "        # init_size\n",
                "        self._init_size = self.init_size\n",
                "        if self._init_size is None:\n",
                "            self._init_size = 3 * self._batch_size\n",
                "            if self._init_size < self.n_clusters:\n",
                "                self._init_size = 3 * self.n_clusters\n",
                "        elif self._init_size < self.n_clusters:\n",
                "            warnings.warn(\n",
                "                (\n",
                "                    f\"init_size={self._init_size} should be larger than \"\n",
                "                    f\"n_clusters={self.n_clusters}. Setting it to \"\n",
                "                    \"min(3*n_clusters, n_samples)\"\n",
                "                ),\n",
                "                RuntimeWarning,\n",
                "                stacklevel=2,\n",
                "            )\n",
                "            self._init_size = 3 * self.n_clusters\n",
                "        self._init_size = min(self._init_size, X.shape[0])\n",
                "\n",
                "        # reassignment_ratio\n",
                "        if self.reassignment_ratio < 0:\n",
                "            raise ValueError(\n",
                "                \"reassignment_ratio should be >= 0, got \"\n",
                "                f\"{self.reassignment_ratio} instead.\"\n",
                "            )\n",
                "\n",
                "    def _warn_mkl_vcomp(self, n_active_threads):\n",
                "        \"\"\"Warn when vcomp and mkl are both present\"\"\"\n",
                "        warnings.warn(\n",
                "            \"MiniBatchKMeans is known to have a memory leak on \"\n",
                "            \"Windows with MKL, when there are less chunks than \"\n",
                "            \"available threads. You can prevent it by setting \"\n",
                "            f\"batch_size >= {self._n_threads * CHUNK_SIZE} or by \"\n",
                "            \"setting the environment variable \"\n",
                "            f\"OMP_NUM_THREADS={n_active_threads}\"\n",
                "        )\n",
                "\n",
                "    def _mini_batch_convergence(\n",
                "        self, step, n_steps, n_samples, centers_squared_diff, batch_inertia\n",
                "    ):\n",
                "        \"\"\"Helper function to encapsulate the early stopping logic\"\"\"\n",
                "        # Normalize inertia to be able to compare values when\n",
                "        # batch_size changes\n",
                "        batch_inertia /= self._batch_size\n",
                "\n",
                "        # count steps starting from 1 for user friendly verbose mode.\n",
                "        step = step + 1\n",
                "\n",
                "        # Ignore first iteration because it's inertia from initialization.\n",
                "        if step == 1:\n",
                "            if self.verbose:\n",
                "                print(\n",
                "                    f\"Minibatch step {step}/{n_steps}: mean batch \"\n",
                "                    f\"inertia: {batch_inertia}\"\n",
                "                )\n",
                "            return False\n",
                "\n",
                "        # Compute an Exponentially Weighted Average of the inertia to\n",
                "        # monitor the convergence while discarding minibatch-local stochastic\n",
                "        # variability: https://en.wikipedia.org/wiki/Moving_average\n",
                "        if self._ewa_inertia is None:\n",
                "            self._ewa_inertia = batch_inertia\n",
                "        else:\n",
                "            alpha = self._batch_size * 2.0 / (n_samples + 1)\n",
                "            alpha = min(alpha, 1)\n",
                "            self._ewa_inertia = self._ewa_inertia * (1 - alpha) + batch_inertia * alpha\n",
                "\n",
                "        # Log progress to be able to monitor convergence\n",
                "        if self.verbose:\n",
                "            print(\n",
                "                f\"Minibatch step {step}/{n_steps}: mean batch inertia: \"\n",
                "                f\"{batch_inertia}, ewa inertia: {self._ewa_inertia}\"\n",
                "            )\n",
                "\n",
                "        # Early stopping based on absolute tolerance on squared change of\n",
                "        # centers position\n",
                "        if self._tol > 0.0 and centers_squared_diff <= self._tol:\n",
                "            if self.verbose:\n",
                "                print(f\"Converged (small centers change) at step {step}/{n_steps}\")\n",
                "            return True\n",
                "\n",
                "        # Early stopping heuristic due to lack of improvement on smoothed\n",
                "        # inertia\n",
                "        if self._ewa_inertia_min is None or self._ewa_inertia < self._ewa_inertia_min:\n",
                "            self._no_improvement = 0\n",
                "            self._ewa_inertia_min = self._ewa_inertia\n",
                "        else:\n",
                "            self._no_improvement += 1\n",
                "\n",
                "        if (\n",
                "            self.max_no_improvement is not None\n",
                "            and self._no_improvement >= self.max_no_improvement\n",
                "        ):\n",
                "            if self.verbose:\n",
                "                print(\n",
                "                    \"Converged (lack of improvement in inertia) at step \"\n",
                "                    f\"{step}/{n_steps}\"\n",
                "                )\n",
                "            return True\n",
                "\n",
                "        return False\n",
                "\n",
                "    def _random_reassign(self):\n",
                "        \"\"\"Check if a random reassignment needs to be done.\n",
                "\n",
                "        Do random reassignments each time 10 * n_clusters samples have been\n",
                "        processed.\n",
                "\n",
                "        If there are empty clusters we always want to reassign.\n",
                "        \"\"\"\n",
                "        self._n_since_last_reassign += self._batch_size\n",
                "        if (self._counts == 0).any() or self._n_since_last_reassign >= (\n",
                "            10 * self.n_clusters\n",
                "        ):\n",
                "            self._n_since_last_reassign = 0\n",
                "            return True\n",
                "        return False\n",
                "\n",
                "    @_fit_context(prefer_skip_nested_validation=True)\n",
                "    def fit(self, X, y=None, sample_weight=None):\n",
                "        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "            Training instances to cluster. It must be noted that the data\n",
                "            will be converted to C ordering, which will cause a memory copy\n",
                "            if the given data is not C-contiguous.\n",
                "            If a sparse matrix is passed, a copy will be made if it's not in\n",
                "            CSR format.\n",
                "\n",
                "        y : Ignored\n",
                "            Not used, present here for API consistency by convention.\n",
                "\n",
                "        sample_weight : array-like of shape (n_samples,), default=None\n",
                "            The weights for each observation in X. If None, all observations\n",
                "            are assigned equal weight. `sample_weight` is not used during\n",
                "            initialization if `init` is a callable or a user provided array.\n",
                "\n",
                "            .. versionadded:: 0.20\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        self : object\n",
                "            Fitted estimator.\n",
                "        \"\"\"\n",
                "        X = self._validate_data(\n",
                "            X,\n",
                "            accept_sparse=\"csr\",\n",
                "            dtype=[np.float64, np.float32],\n",
                "            order=\"C\",\n",
                "            accept_large_sparse=False,\n",
                "        )\n",
                "\n",
                "        self._check_params_vs_input(X)\n",
                "        random_state = check_random_state(self.random_state)\n",
                "        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
                "        self._n_threads = _openmp_effective_n_threads()\n",
                "        n_samples, n_features = X.shape\n",
                "\n",
                "        # Validate init array\n",
                "        init = self.init\n",
                "        if _is_arraylike_not_scalar(init):\n",
                "            init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n",
                "            self._validate_center_shape(X, init)\n",
                "\n",
                "        self._check_mkl_vcomp(X, self._batch_size)\n",
                "\n",
                "        # precompute squared norms of data points\n",
                "        x_squared_norms = row_norms(X, squared=True)\n",
                "\n",
                "        # Validation set for the init\n",
                "        validation_indices = random_state.randint(0, n_samples, self._init_size)\n",
                "        X_valid = X[validation_indices]\n",
                "        sample_weight_valid = sample_weight[validation_indices]\n",
                "\n",
                "        # perform several inits with random subsets\n",
                "        best_inertia = None\n",
                "        for init_idx in range(self._n_init):\n",
                "            if self.verbose:\n",
                "                print(f\"Init {init_idx + 1}/{self._n_init} with method {init}\")\n",
                "\n",
                "            # Initialize the centers using only a fraction of the data as we\n",
                "            # expect n_samples to be very large when using MiniBatchKMeans.\n",
                "            cluster_centers = self._init_centroids(\n",
                "                X,\n",
                "                x_squared_norms=x_squared_norms,\n",
                "                init=init,\n",
                "                random_state=random_state,\n",
                "                init_size=self._init_size,\n",
                "                sample_weight=sample_weight,\n",
                "            )\n",
                "\n",
                "            # Compute inertia on a validation set.\n",
                "            _, inertia = _labels_inertia_threadpool_limit(\n",
                "                X_valid,\n",
                "                sample_weight_valid,\n",
                "                cluster_centers,\n",
                "                n_threads=self._n_threads,\n",
                "            )\n",
                "\n",
                "            if self.verbose:\n",
                "                print(f\"Inertia for init {init_idx + 1}/{self._n_init}: {inertia}\")\n",
                "            if best_inertia is None or inertia < best_inertia:\n",
                "                init_centers = cluster_centers\n",
                "                best_inertia = inertia\n",
                "\n",
                "        centers = init_centers\n",
                "        centers_new = np.empty_like(centers)\n",
                "\n",
                "        # Initialize counts\n",
                "        self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n",
                "\n",
                "        # Attributes to monitor the convergence\n",
                "        self._ewa_inertia = None\n",
                "        self._ewa_inertia_min = None\n",
                "        self._no_improvement = 0\n",
                "\n",
                "        # Initialize number of samples seen since last reassignment\n",
                "        self._n_since_last_reassign = 0\n",
                "\n",
                "        n_steps = (self.max_iter * n_samples) // self._batch_size\n",
                "\n",
                "        with threadpool_limits(limits=1, user_api=\"blas\"):\n",
                "            # Perform the iterative optimization until convergence\n",
                "            for i in range(n_steps):\n",
                "                # Sample a minibatch from the full dataset\n",
                "                minibatch_indices = random_state.randint(0, n_samples, self._batch_size)\n",
                "\n",
                "                # Perform the actual update step on the minibatch data\n",
                "                batch_inertia = _mini_batch_step(\n",
                "                    X=X[minibatch_indices],\n",
                "                    sample_weight=sample_weight[minibatch_indices],\n",
                "                    centers=centers,\n",
                "                    centers_new=centers_new,\n",
                "                    weight_sums=self._counts,\n",
                "                    random_state=random_state,\n",
                "                    random_reassign=self._random_reassign(),\n",
                "                    reassignment_ratio=self.reassignment_ratio,\n",
                "                    verbose=self.verbose,\n",
                "                    n_threads=self._n_threads,\n",
                "                )\n",
                "\n",
                "                if self._tol > 0.0:\n",
                "                    centers_squared_diff = np.sum((centers_new - centers) ** 2)\n",
                "                else:\n",
                "                    centers_squared_diff = 0\n",
                "\n",
                "                centers, centers_new = centers_new, centers\n",
                "\n",
                "                # Monitor convergence and do early stopping if necessary\n",
                "                if self._mini_batch_convergence(\n",
                "                    i, n_steps, n_samples, centers_squared_diff, batch_inertia\n",
                "                ):\n",
                "                    break\n",
                "\n",
                "        self.cluster_centers_ = centers\n",
                "        self._n_features_out = self.cluster_centers_.shape[0]\n",
                "\n",
                "        self.n_steps_ = i + 1\n",
                "        self.n_iter_ = int(np.ceil(((i + 1) * self._batch_size) / n_samples))\n",
                "\n",
                "        if self.compute_labels:\n",
                "            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n",
                "                X,\n",
                "                sample_weight,\n",
                "                self.cluster_centers_,\n",
                "                n_threads=self._n_threads,\n",
                "            )\n",
                "        else:\n",
                "            self.inertia_ = self._ewa_inertia * n_samples\n",
                "\n",
                "        return self\n",
                "\n",
                "    @_fit_context(prefer_skip_nested_validation=True)\n",
                "    def partial_fit(self, X, y=None, sample_weight=None):\n",
                "        \"\"\"Update k means estimate on a single mini-batch X.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                "            Training instances to cluster. It must be noted that the data\n",
                "            will be converted to C ordering, which will cause a memory copy\n",
                "            if the given data is not C-contiguous.\n",
                "            If a sparse matrix is passed, a copy will be made if it's not in\n",
                "            CSR format.\n",
                "\n",
                "        y : Ignored\n",
                "            Not used, present here for API consistency by convention.\n",
                "\n",
                "        sample_weight : array-like of shape (n_samples,), default=None\n",
                "            The weights for each observation in X. If None, all observations\n",
                "            are assigned equal weight. `sample_weight` is not used during\n",
                "            initialization if `init` is a callable or a user provided array.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        self : object\n",
                "            Return updated estimator.\n",
                "        \"\"\"\n",
                "        has_centers = hasattr(self, \"cluster_centers_\")\n",
                "\n",
                "        X = self._validate_data(\n",
                "            X,\n",
                "            accept_sparse=\"csr\",\n",
                "            dtype=[np.float64, np.float32],\n",
                "            order=\"C\",\n",
                "            accept_large_sparse=False,\n",
                "            reset=not has_centers,\n",
                "        )\n",
                "\n",
                "        self._random_state = getattr(\n",
                "            self, \"_random_state\", check_random_state(self.random_state)\n",
                "        )\n",
                "        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
                "        self.n_steps_ = getattr(self, \"n_steps_\", 0)\n",
                "\n",
                "        # precompute squared norms of data points\n",
                "        x_squared_norms = row_norms(X, squared=True)\n",
                "\n",
                "        if not has_centers:\n",
                "            # this instance has not been fitted yet (fit or partial_fit)\n",
                "            self._check_params_vs_input(X)\n",
                "            self._n_threads = _openmp_effective_n_threads()\n",
                "\n",
                "            # Validate init array\n",
                "            init = self.init\n",
                "            if _is_arraylike_not_scalar(init):\n",
                "                init = check_array(init, dtype=X.dtype, copy=True, order=\"C\")\n",
                "                self._validate_center_shape(X, init)\n",
                "\n",
                "            self._check_mkl_vcomp(X, X.shape[0])\n",
                "\n",
                "            # initialize the cluster centers\n",
                "            self.cluster_centers_ = self._init_centroids(\n",
                "                X,\n",
                "                x_squared_norms=x_squared_norms,\n",
                "                init=init,\n",
                "                random_state=self._random_state,\n",
                "                init_size=self._init_size,\n",
                "                sample_weight=sample_weight,\n",
                "            )\n",
                "\n",
                "            # Initialize counts\n",
                "            self._counts = np.zeros(self.n_clusters, dtype=X.dtype)\n",
                "\n",
                "            # Initialize number of samples seen since last reassignment\n",
                "            self._n_since_last_reassign = 0\n",
                "\n",
                "        with threadpool_limits(limits=1, user_api=\"blas\"):\n",
                "            _mini_batch_step(\n",
                "                X,\n",
                "                sample_weight=sample_weight,\n",
                "                centers=self.cluster_centers_,\n",
                "                centers_new=self.cluster_centers_,\n",
                "                weight_sums=self._counts,\n",
                "                random_state=self._random_state,\n",
                "                random_reassign=self._random_reassign(),\n",
                "                reassignment_ratio=self.reassignment_ratio,\n",
                "                verbose=self.verbose,\n",
                "                n_threads=self._n_threads,\n",
                "            )\n",
                "\n",
                "        if self.compute_labels:\n",
                "            self.labels_, self.inertia_ = _labels_inertia_threadpool_limit(\n",
                "                X,\n",
                "                sample_weight,\n",
                "                self.cluster_centers_,\n",
                "                n_threads=self._n_threads,\n",
                "            )\n",
                "\n",
                "        self.n_steps_ += 1\n",
                "        self._n_features_out = self.cluster_centers_.shape[0]\n",
                "\n",
                "        return self"
            ]
        ],
        "sklearn/cluster/tests/test_k_means.py": [
            [
                "\"\"\"Testing for K-means\"\"\"\n",
                "import re\n",
                "import sys\n",
                "from io import StringIO\n",
                "\n",
                "import numpy as np\n",
                "import pytest\n",
                "from scipy import sparse as sp\n",
                "\n",
                "from sklearn.base import clone\n",
                "from sklearn.cluster import KMeans, MiniBatchKMeans, k_means, kmeans_plusplus\n",
                "from sklearn.cluster._k_means_common import (\n",
                "    _euclidean_dense_dense_wrapper,\n",
                "    _euclidean_sparse_dense_wrapper,\n",
                "    _inertia_dense,\n",
                "    _inertia_sparse,\n",
                "    _is_same_clustering,\n",
                "    _relocate_empty_clusters_dense,\n",
                "    _relocate_empty_clusters_sparse,\n",
                ")\n",
                "from sklearn.cluster._kmeans import _labels_inertia, _mini_batch_step\n",
                "from sklearn.datasets import make_blobs\n",
                "from sklearn.exceptions import ConvergenceWarning\n",
                "from sklearn.metrics import pairwise_distances, pairwise_distances_argmin\n",
                "from sklearn.metrics.cluster import v_measure_score\n",
                "from sklearn.metrics.pairwise import euclidean_distances\n",
                "from sklearn.utils._testing import (\n",
                "    assert_allclose,\n",
                "    assert_array_equal,\n",
                "    create_memmap_backed_data,\n",
                ")\n",
                "from sklearn.utils.extmath import row_norms\n",
                "from sklearn.utils.fixes import CSR_CONTAINERS, threadpool_limits\n",
                "\n",
                "# non centered, sparse centers to check the\n",
                "centers = np.array(\n",
                "    [\n",
                "        [0.0, 5.0, 0.0, 0.0, 0.0],\n",
                "        [1.0, 1.0, 4.0, 0.0, 0.0],\n",
                "        [1.0, 0.0, 0.0, 5.0, 1.0],\n",
                "    ]\n",
                ")\n",
                "n_samples = 100\n",
                "n_clusters, n_features = centers.shape\n",
                "X, true_labels = make_blobs(\n",
                "    n_samples=n_samples, centers=centers, cluster_std=1.0, random_state=42\n",
                ")\n",
                "X_as_any_csr = [container(X) for container in CSR_CONTAINERS]\n",
                "data_containers = [np.array] + CSR_CONTAINERS\n",
                "data_containers_ids = (\n",
                "    [\"dense\", \"sparse_matrix\", \"sparse_array\"]\n",
                "    if len(X_as_any_csr) == 2\n",
                "    else [\"dense\", \"sparse_matrix\"]\n",
                ")\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"array_constr\", data_containers, ids=data_containers_ids)\n",
                "@pytest.mark.parametrize(\"algo\", [\"lloyd\", \"elkan\"])\n",
                "@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\n",
                "def test_kmeans_results(array_constr, algo, dtype):\n",
                "    # Checks that KMeans works as intended on toy dataset by comparing with\n",
                "    # expected results computed by hand.\n",
                "    X = array_constr([[0, 0], [0.5, 0], [0.5, 1], [1, 1]], dtype=dtype)\n",
                "    sample_weight = [3, 1, 1, 3]\n",
                "    init_centers = np.array([[0, 0], [1, 1]], dtype=dtype)\n",
                "\n",
                "    expected_labels = [0, 0, 1, 1]\n",
                "    expected_inertia = 0.375\n",
                "    expected_centers = np.array([[0.125, 0], [0.875, 1]], dtype=dtype)\n",
                "    expected_n_iter = 2\n",
                "\n",
                "    kmeans = KMeans(n_clusters=2, n_init=1, init=init_centers, algorithm=algo)\n",
                "    kmeans.fit(X, sample_weight=sample_weight)\n",
                "\n",
                "    assert_array_equal(kmeans.labels_, expected_labels)\n",
                "    assert_allclose(kmeans.inertia_, expected_inertia)\n",
                "    assert_allclose(kmeans.cluster_centers_, expected_centers)\n",
                "    assert kmeans.n_iter_ == expected_n_iter\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"array_constr\", data_containers, ids=data_containers_ids)\n",
                "@pytest.mark.parametrize(\"algo\", [\"lloyd\", \"elkan\"])\n",
                "def test_kmeans_relocated_clusters(array_constr, algo):\n",
                "    # check that empty clusters are relocated as expected\n",
                "    X = array_constr([[0, 0], [0.5, 0], [0.5, 1], [1, 1]])\n",
                "\n",
                "    # second center too far from others points will be empty at first iter\n",
                "    init_centers = np.array([[0.5, 0.5], [3, 3]])\n",
                "\n",
                "    kmeans = KMeans(n_clusters=2, n_init=1, init=init_centers, algorithm=algo)\n",
                "    kmeans.fit(X)\n",
                "\n",
                "    expected_n_iter = 3\n",
                "    expected_inertia = 0.25\n",
                "    assert_allclose(kmeans.inertia_, expected_inertia)\n",
                "    assert kmeans.n_iter_ == expected_n_iter\n",
                "\n",
                "    # There are two acceptable ways of relocating clusters in this example, the output\n",
                "    # depends on how the argpartition strategy breaks ties. We accept both outputs.\n",
                "    try:\n",
                "        expected_labels = [0, 0, 1, 1]\n",
                "        expected_centers = [[0.25, 0], [0.75, 1]]\n",
                "        assert_array_equal(kmeans.labels_, expected_labels)\n",
                "        assert_allclose(kmeans.cluster_centers_, expected_centers)\n",
                "    except AssertionError:\n",
                "        expected_labels = [1, 1, 0, 0]\n",
                "        expected_centers = [[0.75, 1.0], [0.25, 0.0]]\n",
                "        assert_array_equal(kmeans.labels_, expected_labels)\n",
                "        assert_allclose(kmeans.cluster_centers_, expected_centers)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"array_constr\", data_containers, ids=data_containers_ids)\n",
                "def test_relocate_empty_clusters(array_constr):\n",
                "    # test for the _relocate_empty_clusters_(dense/sparse) helpers\n",
                "\n",
                "    # Synthetic dataset with 3 obvious clusters of different sizes\n",
                "    X = np.array([-10.0, -9.5, -9, -8.5, -8, -1, 1, 9, 9.5, 10]).reshape(-1, 1)\n",
                "    X = array_constr(X)\n",
                "    sample_weight = np.ones(10)\n",
                "\n",
                "    # centers all initialized to the first point of X\n",
                "    centers_old = np.array([-10.0, -10, -10]).reshape(-1, 1)\n",
                "\n",
                "    # With this initialization, all points will be assigned to the first center\n",
                "    # At this point a center in centers_new is the weighted sum of the points\n",
                "    # it contains if it's not empty, otherwise it is the same as before.\n",
                "    centers_new = np.array([-16.5, -10, -10]).reshape(-1, 1)\n",
                "    weight_in_clusters = np.array([10.0, 0, 0])\n",
                "    labels = np.zeros(10, dtype=np.int32)\n",
                "\n",
                "    if array_constr is np.array:\n",
                "        _relocate_empty_clusters_dense(\n",
                "            X, sample_weight, centers_old, centers_new, weight_in_clusters, labels\n",
                "        )\n",
                "    else:\n",
                "        _relocate_empty_clusters_sparse(\n",
                "            X.data,\n",
                "            X.indices,\n",
                "            X.indptr,\n",
                "            sample_weight,\n",
                "            centers_old,\n",
                "            centers_new,\n",
                "            weight_in_clusters,\n",
                "            labels,\n",
                "        )\n",
                "\n",
                "    # The relocation scheme will take the 2 points farthest from the center and\n",
                "    # assign them to the 2 empty clusters, i.e. points at 10 and at 9.9. The\n",
                "    # first center will be updated to contain the other 8 points.\n",
                "    assert_array_equal(weight_in_clusters, [8, 1, 1])\n",
                "    assert_allclose(centers_new, [[-36], [10], [9.5]])\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"distribution\", [\"normal\", \"blobs\"])\n",
                "@pytest.mark.parametrize(\"array_constr\", data_containers, ids=data_containers_ids)\n",
                "@pytest.mark.parametrize(\"tol\", [1e-2, 1e-8, 1e-100, 0])\n",
                "def test_kmeans_elkan_results(distribution, array_constr, tol, global_random_seed):\n",
                "    # Check that results are identical between lloyd and elkan algorithms\n",
                "    rnd = np.random.RandomState(global_random_seed)\n",
                "    if distribution == \"normal\":\n",
                "        X = rnd.normal(size=(5000, 10))\n",
                "    else:\n",
                "        X, _ = make_blobs(random_state=rnd)\n",
                "    X[X < 0] = 0\n",
                "    X = array_constr(X)\n",
                "\n",
                "    km_lloyd = KMeans(n_clusters=5, random_state=global_random_seed, n_init=1, tol=tol)\n",
                "    km_elkan = KMeans(\n",
                "        algorithm=\"elkan\",\n",
                "        n_clusters=5,\n",
                "        random_state=global_random_seed,\n",
                "        n_init=1,\n",
                "        tol=tol,\n",
                "    )\n",
                "\n",
                "    km_lloyd.fit(X)\n",
                "    km_elkan.fit(X)\n",
                "    assert_allclose(km_elkan.cluster_centers_, km_lloyd.cluster_centers_)\n",
                "    assert_array_equal(km_elkan.labels_, km_lloyd.labels_)\n",
                "    assert km_elkan.n_iter_ == km_lloyd.n_iter_\n",
                "    assert km_elkan.inertia_ == pytest.approx(km_lloyd.inertia_, rel=1e-6)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"algorithm\", [\"lloyd\", \"elkan\"])\n",
                "def test_kmeans_convergence(algorithm, global_random_seed):\n",
                "    # Check that KMeans stops when convergence is reached when tol=0. (#16075)\n",
                "    rnd = np.random.RandomState(global_random_seed)\n",
                "    X = rnd.normal(size=(5000, 10))\n",
                "    max_iter = 300\n",
                "\n",
                "    km = KMeans(\n",
                "        algorithm=algorithm,\n",
                "        n_clusters=5,\n",
                "        random_state=global_random_seed,\n",
                "        n_init=1,\n",
                "        tol=0,\n",
                "        max_iter=max_iter,\n",
                "    ).fit(X)\n",
                "\n",
                "    assert km.n_iter_ < max_iter\n",
                "\n",
                "\n"
            ],
            {
                "type": "delete",
                "before": [
                    "@pytest.mark.parametrize(\"algorithm\", [\"auto\", \"full\"])\n",
                    "def test_algorithm_auto_full_deprecation_warning(algorithm):\n",
                    "    X = np.random.rand(100, 2)\n",
                    "    kmeans = KMeans(algorithm=algorithm)\n",
                    "    with pytest.warns(\n",
                    "        FutureWarning,\n",
                    "        match=(\n",
                    "            f\"algorithm='{algorithm}' is deprecated, it will \"\n",
                    "            \"be removed in 1.3. Using 'lloyd' instead.\"\n",
                    "        ),\n",
                    "    ):\n",
                    "        kmeans.fit(X)\n",
                    "        assert kmeans._algorithm == \"lloyd\"\n",
                    "\n",
                    "\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 202,
                    "end": 217
                },
                "child_version_range": {
                    "start": 202,
                    "end": 202
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "call",
                        "name": "pytest.mark.parametrize",
                        "signature": "pytest.mark.parametrize(\"algorithm\", [\"auto\", \"full\"])",
                        "at_line": 202,
                        "argument": "\"algorithm\""
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: sklearn/cluster/tests/test_k_means.py\nCode:\n199 199        assert km.n_iter_ < max_iter\n200 200    \n201 201    \n202      - @pytest.mark.parametrize(\"algorithm\", [\"auto\", \"full\"])\n203      - def test_algorithm_auto_full_deprecation_warning(algorithm):\n204      -     X = np.random.rand(100, 2)\n205      -     kmeans = KMeans(algorithm=algorithm)\n206      -     with pytest.warns(\n207      -         FutureWarning,\n208      -         match=(\n209      -             f\"algorithm='{algorithm}' is deprecated, it will \"\n210      -             \"be removed in 1.3. Using 'lloyd' instead.\"\n211      -         ),\n212      -     ):\n213      -         kmeans.fit(X)\n214      -         assert kmeans._algorithm == \"lloyd\"\n215      - \n216      - \n217 202    @pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n218 203    def test_predict_sample_weight_deprecation_warning(Estimator):\n219 204        X = np.random.rand(100, 2)\n         ...\n",
                "file_path": "sklearn/cluster/tests/test_k_means.py",
                "identifiers_before": [
                    "FutureWarning",
                    "KMeans",
                    "X",
                    "_algorithm",
                    "algorithm",
                    "fit",
                    "kmeans",
                    "mark",
                    "match",
                    "np",
                    "parametrize",
                    "pytest",
                    "rand",
                    "random",
                    "test_algorithm_auto_full_deprecation_warning",
                    "warns"
                ],
                "identifiers_after": [],
                "prefix": [
                    "    assert km.n_iter_ < max_iter\n",
                    "\n",
                    "\n"
                ],
                "suffix": [
                    "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                    "def test_predict_sample_weight_deprecation_warning(Estimator):\n",
                    "    X = np.random.rand(100, 2)\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_predict_sample_weight_deprecation_warning(Estimator):\n",
                "    X = np.random.rand(100, 2)\n",
                "    sample_weight = np.random.uniform(size=100)\n",
                "    kmeans = Estimator()\n",
                "    kmeans.fit(X, sample_weight=sample_weight)\n",
                "    warn_msg = (\n",
                "        \"'sample_weight' was deprecated in version 1.3 and will be removed in 1.5.\"\n",
                "    )\n",
                "    with pytest.warns(FutureWarning, match=warn_msg):\n",
                "        kmeans.predict(X, sample_weight=sample_weight)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"X_csr\", X_as_any_csr)\n",
                "def test_minibatch_update_consistency(X_csr, global_random_seed):\n",
                "    # Check that dense and sparse minibatch update give the same results\n",
                "    rng = np.random.RandomState(global_random_seed)\n",
                "\n",
                "    centers_old = centers + rng.normal(size=centers.shape)\n",
                "    centers_old_csr = centers_old.copy()\n",
                "\n",
                "    centers_new = np.zeros_like(centers_old)\n",
                "    centers_new_csr = np.zeros_like(centers_old_csr)\n",
                "\n",
                "    weight_sums = np.zeros(centers_old.shape[0], dtype=X.dtype)\n",
                "    weight_sums_csr = np.zeros(centers_old.shape[0], dtype=X.dtype)\n",
                "\n",
                "    sample_weight = np.ones(X.shape[0], dtype=X.dtype)\n",
                "\n",
                "    # extract a small minibatch\n",
                "    X_mb = X[:10]\n",
                "    X_mb_csr = X_csr[:10]\n",
                "    sample_weight_mb = sample_weight[:10]\n",
                "\n",
                "    # step 1: compute the dense minibatch update\n",
                "    old_inertia = _mini_batch_step(\n",
                "        X_mb,\n",
                "        sample_weight_mb,\n",
                "        centers_old,\n",
                "        centers_new,\n",
                "        weight_sums,\n",
                "        np.random.RandomState(global_random_seed),\n",
                "        random_reassign=False,\n",
                "    )\n",
                "    assert old_inertia > 0.0\n",
                "\n",
                "    # compute the new inertia on the same batch to check that it decreased\n",
                "    labels, new_inertia = _labels_inertia(X_mb, sample_weight_mb, centers_new)\n",
                "    assert new_inertia > 0.0\n",
                "    assert new_inertia < old_inertia\n",
                "\n",
                "    # step 2: compute the sparse minibatch update\n",
                "    old_inertia_csr = _mini_batch_step(\n",
                "        X_mb_csr,\n",
                "        sample_weight_mb,\n",
                "        centers_old_csr,\n",
                "        centers_new_csr,\n",
                "        weight_sums_csr,\n",
                "        np.random.RandomState(global_random_seed),\n",
                "        random_reassign=False,\n",
                "    )\n",
                "    assert old_inertia_csr > 0.0\n",
                "\n",
                "    # compute the new inertia on the same batch to check that it decreased\n",
                "    labels_csr, new_inertia_csr = _labels_inertia(\n",
                "        X_mb_csr, sample_weight_mb, centers_new_csr\n",
                "    )\n",
                "    assert new_inertia_csr > 0.0\n",
                "    assert new_inertia_csr < old_inertia_csr\n",
                "\n",
                "    # step 3: check that sparse and dense updates lead to the same results\n",
                "    assert_array_equal(labels, labels_csr)\n",
                "    assert_allclose(centers_new, centers_new_csr)\n",
                "    assert_allclose(old_inertia, old_inertia_csr)\n",
                "    assert_allclose(new_inertia, new_inertia_csr)\n",
                "\n",
                "\n",
                "def _check_fitted_model(km):\n",
                "    # check that the number of clusters centers and distinct labels match\n",
                "    # the expectation\n",
                "    centers = km.cluster_centers_\n",
                "    assert centers.shape == (n_clusters, n_features)\n",
                "\n",
                "    labels = km.labels_\n",
                "    assert np.unique(labels).shape[0] == n_clusters\n",
                "\n",
                "    # check that the labels assignment are perfect (up to a permutation)\n",
                "    assert_allclose(v_measure_score(true_labels, labels), 1.0)\n",
                "    assert km.inertia_ > 0.0\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"input_data\",\n",
                "    [X] + X_as_any_csr,\n",
                "    ids=data_containers_ids,\n",
                ")\n",
                "@pytest.mark.parametrize(\n",
                "    \"init\",\n",
                "    [\"random\", \"k-means++\", centers, lambda X, k, random_state: centers],\n",
                "    ids=[\"random\", \"k-means++\", \"ndarray\", \"callable\"],\n",
                ")\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_all_init(Estimator, input_data, init):\n",
                "    # Check KMeans and MiniBatchKMeans with all possible init.\n",
                "    n_init = 10 if isinstance(init, str) else 1\n",
                "    km = Estimator(\n",
                "        init=init, n_clusters=n_clusters, random_state=42, n_init=n_init\n",
                "    ).fit(input_data)\n",
                "    _check_fitted_model(km)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"init\",\n",
                "    [\"random\", \"k-means++\", centers, lambda X, k, random_state: centers],\n",
                "    ids=[\"random\", \"k-means++\", \"ndarray\", \"callable\"],\n",
                ")\n",
                "def test_minibatch_kmeans_partial_fit_init(init):\n",
                "    # Check MiniBatchKMeans init with partial_fit\n",
                "    n_init = 10 if isinstance(init, str) else 1\n",
                "    km = MiniBatchKMeans(\n",
                "        init=init, n_clusters=n_clusters, random_state=0, n_init=n_init\n",
                "    )\n",
                "    for i in range(100):\n",
                "        # \"random\" init requires many batches to recover the true labels.\n",
                "        km.partial_fit(X)\n",
                "    _check_fitted_model(km)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"init, expected_n_init\",\n",
                "    [\n",
                "        (\"k-means++\", 1),\n",
                "        (\"random\", \"default\"),\n",
                "        (\n",
                "            lambda X, n_clusters, random_state: random_state.uniform(\n",
                "                size=(n_clusters, X.shape[1])\n",
                "            ),\n",
                "            \"default\",\n",
                "        ),\n",
                "        (\"array-like\", 1),\n",
                "    ],\n",
                ")\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_kmeans_init_auto_with_initial_centroids(Estimator, init, expected_n_init):\n",
                "    \"\"\"Check that `n_init=\"auto\"` chooses the right number of initializations.\n",
                "    Non-regression test for #26657:\n",
                "    https://github.com/scikit-learn/scikit-learn/pull/26657\n",
                "    \"\"\"\n",
                "    n_sample, n_features, n_clusters = 100, 10, 5\n",
                "    X = np.random.randn(n_sample, n_features)\n",
                "    if init == \"array-like\":\n",
                "        init = np.random.randn(n_clusters, n_features)\n",
                "    if expected_n_init == \"default\":\n",
                "        expected_n_init = 3 if Estimator is MiniBatchKMeans else 10\n",
                "\n",
                "    kmeans = Estimator(n_clusters=n_clusters, init=init, n_init=\"auto\").fit(X)\n",
                "    assert kmeans._n_init == expected_n_init\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_fortran_aligned_data(Estimator, global_random_seed):\n",
                "    # Check that KMeans works with fortran-aligned data.\n",
                "    X_fortran = np.asfortranarray(X)\n",
                "    centers_fortran = np.asfortranarray(centers)\n",
                "\n",
                "    km_c = Estimator(\n",
                "        n_clusters=n_clusters, init=centers, n_init=1, random_state=global_random_seed\n",
                "    ).fit(X)\n",
                "    km_f = Estimator(\n",
                "        n_clusters=n_clusters,\n",
                "        init=centers_fortran,\n",
                "        n_init=1,\n",
                "        random_state=global_random_seed,\n",
                "    ).fit(X_fortran)\n",
                "    assert_allclose(km_c.cluster_centers_, km_f.cluster_centers_)\n",
                "    assert_array_equal(km_c.labels_, km_f.labels_)\n",
                "\n",
                "\n",
                "def test_minibatch_kmeans_verbose():\n",
                "    # Check verbose mode of MiniBatchKMeans for better coverage.\n",
                "    km = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, verbose=1)\n",
                "    old_stdout = sys.stdout\n",
                "    sys.stdout = StringIO()\n",
                "    try:\n",
                "        km.fit(X)\n",
                "    finally:\n",
                "        sys.stdout = old_stdout\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"algorithm\", [\"lloyd\", \"elkan\"])\n",
                "@pytest.mark.parametrize(\"tol\", [1e-2, 0])\n",
                "def test_kmeans_verbose(algorithm, tol, capsys):\n",
                "    # Check verbose mode of KMeans for better coverage.\n",
                "    X = np.random.RandomState(0).normal(size=(5000, 10))\n",
                "\n",
                "    KMeans(\n",
                "        algorithm=algorithm,\n",
                "        n_clusters=n_clusters,\n",
                "        random_state=42,\n",
                "        init=\"random\",\n",
                "        n_init=1,\n",
                "        tol=tol,\n",
                "        verbose=1,\n",
                "    ).fit(X)\n",
                "\n",
                "    captured = capsys.readouterr()\n",
                "\n",
                "    assert re.search(r\"Initialization complete\", captured.out)\n",
                "    assert re.search(r\"Iteration [0-9]+, inertia\", captured.out)\n",
                "\n",
                "    if tol == 0:\n",
                "        assert re.search(r\"strict convergence\", captured.out)\n",
                "    else:\n",
                "        assert re.search(r\"center shift .* within tolerance\", captured.out)\n",
                "\n",
                "\n",
                "def test_minibatch_kmeans_warning_init_size():\n",
                "    # Check that a warning is raised when init_size is smaller than n_clusters\n",
                "    with pytest.warns(\n",
                "        RuntimeWarning, match=r\"init_size.* should be larger than n_clusters\"\n",
                "    ):\n",
                "        MiniBatchKMeans(init_size=10, n_clusters=20).fit(X)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_warning_n_init_precomputed_centers(Estimator):\n",
                "    # Check that a warning is raised when n_init > 1 and an array is passed for\n",
                "    # the init parameter.\n",
                "    with pytest.warns(\n",
                "        RuntimeWarning,\n",
                "        match=\"Explicit initial center position passed: performing only one init\",\n",
                "    ):\n",
                "        Estimator(init=centers, n_clusters=n_clusters, n_init=10).fit(X)\n",
                "\n",
                "\n",
                "def test_minibatch_sensible_reassign(global_random_seed):\n",
                "    # check that identical initial clusters are reassigned\n",
                "    # also a regression test for when there are more desired reassignments than\n",
                "    # samples.\n",
                "    zeroed_X, true_labels = make_blobs(\n",
                "        n_samples=100, centers=5, random_state=global_random_seed\n",
                "    )\n",
                "    zeroed_X[::2, :] = 0\n",
                "\n",
                "    km = MiniBatchKMeans(\n",
                "        n_clusters=20, batch_size=10, random_state=global_random_seed, init=\"random\"\n",
                "    ).fit(zeroed_X)\n",
                "    # there should not be too many exact zero cluster centers\n",
                "    assert km.cluster_centers_.any(axis=1).sum() > 10\n",
                "\n",
                "    # do the same with batch-size > X.shape[0] (regression test)\n",
                "    km = MiniBatchKMeans(\n",
                "        n_clusters=20, batch_size=200, random_state=global_random_seed, init=\"random\"\n",
                "    ).fit(zeroed_X)\n",
                "    # there should not be too many exact zero cluster centers\n",
                "    assert km.cluster_centers_.any(axis=1).sum() > 10\n",
                "\n",
                "    # do the same with partial_fit API\n",
                "    km = MiniBatchKMeans(n_clusters=20, random_state=global_random_seed, init=\"random\")\n",
                "    for i in range(100):\n",
                "        km.partial_fit(zeroed_X)\n",
                "    # there should not be too many exact zero cluster centers\n",
                "    assert km.cluster_centers_.any(axis=1).sum() > 10\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"input_data\",\n",
                "    [X] + X_as_any_csr,\n",
                "    ids=data_containers_ids,\n",
                ")\n",
                "def test_minibatch_reassign(input_data, global_random_seed):\n",
                "    # Check the reassignment part of the minibatch step with very high or very\n",
                "    # low reassignment ratio.\n",
                "    perfect_centers = np.empty((n_clusters, n_features))\n",
                "    for i in range(n_clusters):\n",
                "        perfect_centers[i] = X[true_labels == i].mean(axis=0)\n",
                "\n",
                "    sample_weight = np.ones(n_samples)\n",
                "    centers_new = np.empty_like(perfect_centers)\n",
                "\n",
                "    # Give a perfect initialization, but a large reassignment_ratio, as a\n",
                "    # result many centers should be reassigned and the model should no longer\n",
                "    # be good\n",
                "    score_before = -_labels_inertia(input_data, sample_weight, perfect_centers, 1)[1]\n",
                "\n",
                "    _mini_batch_step(\n",
                "        input_data,\n",
                "        sample_weight,\n",
                "        perfect_centers,\n",
                "        centers_new,\n",
                "        np.zeros(n_clusters),\n",
                "        np.random.RandomState(global_random_seed),\n",
                "        random_reassign=True,\n",
                "        reassignment_ratio=1,\n",
                "    )\n",
                "\n",
                "    score_after = -_labels_inertia(input_data, sample_weight, centers_new, 1)[1]\n",
                "\n",
                "    assert score_before > score_after\n",
                "\n",
                "    # Give a perfect initialization, with a small reassignment_ratio,\n",
                "    # no center should be reassigned.\n",
                "    _mini_batch_step(\n",
                "        input_data,\n",
                "        sample_weight,\n",
                "        perfect_centers,\n",
                "        centers_new,\n",
                "        np.zeros(n_clusters),\n",
                "        np.random.RandomState(global_random_seed),\n",
                "        random_reassign=True,\n",
                "        reassignment_ratio=1e-15,\n",
                "    )\n",
                "\n",
                "    assert_allclose(centers_new, perfect_centers)\n",
                "\n",
                "\n",
                "def test_minibatch_with_many_reassignments():\n",
                "    # Test for the case that the number of clusters to reassign is bigger\n",
                "    # than the batch_size. Run the test with 100 clusters and a batch_size of\n",
                "    # 10 because it turned out that these values ensure that the number of\n",
                "    # clusters to reassign is always bigger than the batch_size.\n",
                "    MiniBatchKMeans(\n",
                "        n_clusters=100,\n",
                "        batch_size=10,\n",
                "        init_size=n_samples,\n",
                "        random_state=42,\n",
                "        verbose=True,\n",
                "    ).fit(X)\n",
                "\n",
                "\n",
                "def test_minibatch_kmeans_init_size():\n",
                "    # Check the internal _init_size attribute of MiniBatchKMeans\n",
                "\n",
                "    # default init size should be 3 * batch_size\n",
                "    km = MiniBatchKMeans(n_clusters=10, batch_size=5, n_init=1).fit(X)\n",
                "    assert km._init_size == 15\n",
                "\n",
                "    # if 3 * batch size < n_clusters, it should then be 3 * n_clusters\n",
                "    km = MiniBatchKMeans(n_clusters=10, batch_size=1, n_init=1).fit(X)\n",
                "    assert km._init_size == 30\n",
                "\n",
                "    # it should not be larger than n_samples\n",
                "    km = MiniBatchKMeans(\n",
                "        n_clusters=10, batch_size=5, n_init=1, init_size=n_samples + 1\n",
                "    ).fit(X)\n",
                "    assert km._init_size == n_samples\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"tol, max_no_improvement\", [(1e-4, None), (0, 10)])\n",
                "def test_minibatch_declared_convergence(capsys, tol, max_no_improvement):\n",
                "    # Check convergence detection based on ewa batch inertia or on\n",
                "    # small center change.\n",
                "    X, _, centers = make_blobs(centers=3, random_state=0, return_centers=True)\n",
                "\n",
                "    km = MiniBatchKMeans(\n",
                "        n_clusters=3,\n",
                "        init=centers,\n",
                "        batch_size=20,\n",
                "        tol=tol,\n",
                "        random_state=0,\n",
                "        max_iter=10,\n",
                "        n_init=1,\n",
                "        verbose=1,\n",
                "        max_no_improvement=max_no_improvement,\n",
                "    )\n",
                "\n",
                "    km.fit(X)\n",
                "    assert 1 < km.n_iter_ < 10\n",
                "\n",
                "    captured = capsys.readouterr()\n",
                "    if max_no_improvement is None:\n",
                "        assert \"Converged (small centers change)\" in captured.out\n",
                "    if tol == 0:\n",
                "        assert \"Converged (lack of improvement in inertia)\" in captured.out\n",
                "\n",
                "\n",
                "def test_minibatch_iter_steps():\n",
                "    # Check consistency of n_iter_ and n_steps_ attributes.\n",
                "    batch_size = 30\n",
                "    n_samples = X.shape[0]\n",
                "    km = MiniBatchKMeans(n_clusters=3, batch_size=batch_size, random_state=0).fit(X)\n",
                "\n",
                "    # n_iter_ is the number of started epochs\n",
                "    assert km.n_iter_ == np.ceil((km.n_steps_ * batch_size) / n_samples)\n",
                "    assert isinstance(km.n_iter_, int)\n",
                "\n",
                "    # without stopping condition, max_iter should be reached\n",
                "    km = MiniBatchKMeans(\n",
                "        n_clusters=3,\n",
                "        batch_size=batch_size,\n",
                "        random_state=0,\n",
                "        tol=0,\n",
                "        max_no_improvement=None,\n",
                "        max_iter=10,\n",
                "    ).fit(X)\n",
                "\n",
                "    assert km.n_iter_ == 10\n",
                "    assert km.n_steps_ == (10 * n_samples) // batch_size\n",
                "    assert isinstance(km.n_steps_, int)\n",
                "\n",
                "\n",
                "def test_kmeans_copyx():\n",
                "    # Check that copy_x=False returns nearly equal X after de-centering.\n",
                "    my_X = X.copy()\n",
                "    km = KMeans(copy_x=False, n_clusters=n_clusters, random_state=42)\n",
                "    km.fit(my_X)\n",
                "    _check_fitted_model(km)\n",
                "\n",
                "    # check that my_X is de-centered\n",
                "    assert_allclose(my_X, X)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_score_max_iter(Estimator, global_random_seed):\n",
                "    # Check that fitting KMeans or MiniBatchKMeans with more iterations gives\n",
                "    # better score\n",
                "    X = np.random.RandomState(global_random_seed).randn(100, 10)\n",
                "\n",
                "    km1 = Estimator(n_init=1, random_state=global_random_seed, max_iter=1)\n",
                "    s1 = km1.fit(X).score(X)\n",
                "    km2 = Estimator(n_init=1, random_state=global_random_seed, max_iter=10)\n",
                "    s2 = km2.fit(X).score(X)\n",
                "    assert s2 > s1\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"array_constr\", data_containers, ids=data_containers_ids)\n",
                "@pytest.mark.parametrize(\n",
                "    \"Estimator, algorithm\",\n",
                "    [(KMeans, \"lloyd\"), (KMeans, \"elkan\"), (MiniBatchKMeans, None)],\n",
                ")\n",
                "@pytest.mark.parametrize(\"max_iter\", [2, 100])\n",
                "def test_kmeans_predict(\n",
                "    Estimator, algorithm, array_constr, max_iter, global_dtype, global_random_seed\n",
                "):\n",
                "    # Check the predict method and the equivalence between fit.predict and\n",
                "    # fit_predict.\n",
                "    X, _ = make_blobs(\n",
                "        n_samples=200, n_features=10, centers=10, random_state=global_random_seed\n",
                "    )\n",
                "    X = array_constr(X, dtype=global_dtype)\n",
                "\n",
                "    km = Estimator(\n",
                "        n_clusters=10,\n",
                "        init=\"random\",\n",
                "        n_init=10,\n",
                "        max_iter=max_iter,\n",
                "        random_state=global_random_seed,\n",
                "    )\n",
                "    if algorithm is not None:\n",
                "        km.set_params(algorithm=algorithm)\n",
                "    km.fit(X)\n",
                "    labels = km.labels_\n",
                "\n",
                "    # re-predict labels for training set using predict\n",
                "    pred = km.predict(X)\n",
                "    assert_array_equal(pred, labels)\n",
                "\n",
                "    # re-predict labels for training set using fit_predict\n",
                "    pred = km.fit_predict(X)\n",
                "    assert_array_equal(pred, labels)\n",
                "\n",
                "    # predict centroid labels\n",
                "    pred = km.predict(km.cluster_centers_)\n",
                "    assert_array_equal(pred, np.arange(10))\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"X_csr\", X_as_any_csr)\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_dense_sparse(Estimator, X_csr, global_random_seed):\n",
                "    # Check that the results are the same for dense and sparse input.\n",
                "    sample_weight = np.random.RandomState(global_random_seed).random_sample(\n",
                "        (n_samples,)\n",
                "    )\n",
                "    km_dense = Estimator(\n",
                "        n_clusters=n_clusters, random_state=global_random_seed, n_init=1\n",
                "    )\n",
                "    km_dense.fit(X, sample_weight=sample_weight)\n",
                "    km_sparse = Estimator(\n",
                "        n_clusters=n_clusters, random_state=global_random_seed, n_init=1\n",
                "    )\n",
                "    km_sparse.fit(X_csr, sample_weight=sample_weight)\n",
                "\n",
                "    assert_array_equal(km_dense.labels_, km_sparse.labels_)\n",
                "    assert_allclose(km_dense.cluster_centers_, km_sparse.cluster_centers_)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"X_csr\", X_as_any_csr)\n",
                "@pytest.mark.parametrize(\n",
                "    \"init\", [\"random\", \"k-means++\", centers], ids=[\"random\", \"k-means++\", \"ndarray\"]\n",
                ")\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_predict_dense_sparse(Estimator, init, X_csr):\n",
                "    # check that models trained on sparse input also works for dense input at\n",
                "    # predict time and vice versa.\n",
                "    n_init = 10 if isinstance(init, str) else 1\n",
                "    km = Estimator(n_clusters=n_clusters, init=init, n_init=n_init, random_state=0)\n",
                "\n",
                "    km.fit(X_csr)\n",
                "    assert_array_equal(km.predict(X), km.labels_)\n",
                "\n",
                "    km.fit(X)\n",
                "    assert_array_equal(km.predict(X_csr), km.labels_)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"array_constr\", data_containers, ids=data_containers_ids)\n",
                "@pytest.mark.parametrize(\"dtype\", [np.int32, np.int64])\n",
                "@pytest.mark.parametrize(\"init\", [\"k-means++\", \"ndarray\"])\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_integer_input(Estimator, array_constr, dtype, init, global_random_seed):\n",
                "    # Check that KMeans and MiniBatchKMeans work with integer input.\n",
                "    X_dense = np.array([[0, 0], [10, 10], [12, 9], [-1, 1], [2, 0], [8, 10]])\n",
                "    X = array_constr(X_dense, dtype=dtype)\n",
                "\n",
                "    n_init = 1 if init == \"ndarray\" else 10\n",
                "    init = X_dense[:2] if init == \"ndarray\" else init\n",
                "\n",
                "    km = Estimator(\n",
                "        n_clusters=2, init=init, n_init=n_init, random_state=global_random_seed\n",
                "    )\n",
                "    if Estimator is MiniBatchKMeans:\n",
                "        km.set_params(batch_size=2)\n",
                "\n",
                "    km.fit(X)\n",
                "\n",
                "    # Internally integer input should be converted to float64\n",
                "    assert km.cluster_centers_.dtype == np.float64\n",
                "\n",
                "    expected_labels = [0, 1, 1, 0, 0, 1]\n",
                "    assert_allclose(v_measure_score(km.labels_, expected_labels), 1.0)\n",
                "\n",
                "    # Same with partial_fit (#14314)\n",
                "    if Estimator is MiniBatchKMeans:\n",
                "        km = clone(km).partial_fit(X)\n",
                "        assert km.cluster_centers_.dtype == np.float64\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_transform(Estimator, global_random_seed):\n",
                "    # Check the transform method\n",
                "    km = Estimator(n_clusters=n_clusters, random_state=global_random_seed).fit(X)\n",
                "\n",
                "    # Transorfming cluster_centers_ should return the pairwise distances\n",
                "    # between centers\n",
                "    Xt = km.transform(km.cluster_centers_)\n",
                "    assert_allclose(Xt, pairwise_distances(km.cluster_centers_))\n",
                "    # In particular, diagonal must be 0\n",
                "    assert_array_equal(Xt.diagonal(), np.zeros(n_clusters))\n",
                "\n",
                "    # Transorfming X should return the pairwise distances between X and the\n",
                "    # centers\n",
                "    Xt = km.transform(X)\n",
                "    assert_allclose(Xt, pairwise_distances(X, km.cluster_centers_))\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_fit_transform(Estimator, global_random_seed):\n",
                "    # Check equivalence between fit.transform and fit_transform\n",
                "    X1 = Estimator(random_state=global_random_seed, n_init=1).fit(X).transform(X)\n",
                "    X2 = Estimator(random_state=global_random_seed, n_init=1).fit_transform(X)\n",
                "    assert_allclose(X1, X2)\n",
                "\n",
                "\n",
                "def test_n_init(global_random_seed):\n",
                "    # Check that increasing the number of init increases the quality\n",
                "    previous_inertia = np.inf\n",
                "    for n_init in [1, 5, 10]:\n",
                "        # set max_iter=1 to avoid finding the global minimum and get the same\n",
                "        # inertia each time\n",
                "        km = KMeans(\n",
                "            n_clusters=n_clusters,\n",
                "            init=\"random\",\n",
                "            n_init=n_init,\n",
                "            random_state=global_random_seed,\n",
                "            max_iter=1,\n",
                "        ).fit(X)\n",
                "        assert km.inertia_ <= previous_inertia\n",
                "\n",
                "\n",
                "def test_k_means_function(global_random_seed):\n",
                "    # test calling the k_means function directly\n",
                "    cluster_centers, labels, inertia = k_means(\n",
                "        X, n_clusters=n_clusters, sample_weight=None, random_state=global_random_seed\n",
                "    )\n",
                "\n",
                "    assert cluster_centers.shape == (n_clusters, n_features)\n",
                "    assert np.unique(labels).shape[0] == n_clusters\n",
                "\n",
                "    # check that the labels assignment are perfect (up to a permutation)\n",
                "    assert_allclose(v_measure_score(true_labels, labels), 1.0)\n",
                "    assert inertia > 0.0\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"input_data\",\n",
                "    [X] + X_as_any_csr,\n",
                "    ids=data_containers_ids,\n",
                ")\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_float_precision(Estimator, input_data, global_random_seed):\n",
                "    # Check that the results are the same for single and double precision.\n",
                "    km = Estimator(n_init=1, random_state=global_random_seed)\n",
                "\n",
                "    inertia = {}\n",
                "    Xt = {}\n",
                "    centers = {}\n",
                "    labels = {}\n",
                "\n",
                "    for dtype in [np.float64, np.float32]:\n",
                "        X = input_data.astype(dtype, copy=False)\n",
                "        km.fit(X)\n",
                "\n",
                "        inertia[dtype] = km.inertia_\n",
                "        Xt[dtype] = km.transform(X)\n",
                "        centers[dtype] = km.cluster_centers_\n",
                "        labels[dtype] = km.labels_\n",
                "\n",
                "        # dtype of cluster centers has to be the dtype of the input data\n",
                "        assert km.cluster_centers_.dtype == dtype\n",
                "\n",
                "        # same with partial_fit\n",
                "        if Estimator is MiniBatchKMeans:\n",
                "            km.partial_fit(X[0:3])\n",
                "            assert km.cluster_centers_.dtype == dtype\n",
                "\n",
                "    # compare arrays with low precision since the difference between 32 and\n",
                "    # 64 bit comes from an accumulation of rounding errors.\n",
                "    assert_allclose(inertia[np.float32], inertia[np.float64], rtol=1e-4)\n",
                "    assert_allclose(Xt[np.float32], Xt[np.float64], atol=Xt[np.float64].max() * 1e-4)\n",
                "    assert_allclose(\n",
                "        centers[np.float32], centers[np.float64], atol=centers[np.float64].max() * 1e-4\n",
                "    )\n",
                "    assert_array_equal(labels[np.float32], labels[np.float64])\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"dtype\", [np.int32, np.int64, np.float32, np.float64])\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_centers_not_mutated(Estimator, dtype):\n",
                "    # Check that KMeans and MiniBatchKMeans won't mutate the user provided\n",
                "    # init centers silently even if input data and init centers have the same\n",
                "    # type.\n",
                "    X_new_type = X.astype(dtype, copy=False)\n",
                "    centers_new_type = centers.astype(dtype, copy=False)\n",
                "\n",
                "    km = Estimator(init=centers_new_type, n_clusters=n_clusters, n_init=1)\n",
                "    km.fit(X_new_type)\n",
                "\n",
                "    assert not np.may_share_memory(km.cluster_centers_, centers_new_type)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"input_data\",\n",
                "    [X] + X_as_any_csr,\n",
                "    ids=data_containers_ids,\n",
                ")\n",
                "def test_kmeans_init_fitted_centers(input_data):\n",
                "    # Check that starting fitting from a local optimum shouldn't change the\n",
                "    # solution\n",
                "    km1 = KMeans(n_clusters=n_clusters).fit(input_data)\n",
                "    km2 = KMeans(n_clusters=n_clusters, init=km1.cluster_centers_, n_init=1).fit(\n",
                "        input_data\n",
                "    )\n",
                "\n",
                "    assert_allclose(km1.cluster_centers_, km2.cluster_centers_)\n",
                "\n",
                "\n",
                "def test_kmeans_warns_less_centers_than_unique_points(global_random_seed):\n",
                "    # Check KMeans when the number of found clusters is smaller than expected\n",
                "    X = np.asarray([[0, 0], [0, 1], [1, 0], [1, 0]])  # last point is duplicated\n",
                "    km = KMeans(n_clusters=4, random_state=global_random_seed)\n",
                "\n",
                "    # KMeans should warn that fewer labels than cluster centers have been used\n",
                "    msg = (\n",
                "        r\"Number of distinct clusters \\(3\\) found smaller than \"\n",
                "        r\"n_clusters \\(4\\). Possibly due to duplicate points in X.\"\n",
                "    )\n",
                "    with pytest.warns(ConvergenceWarning, match=msg):\n",
                "        km.fit(X)\n",
                "        # only three distinct points, so only three clusters\n",
                "        # can have points assigned to them\n",
                "        assert set(km.labels_) == set(range(3))\n",
                "\n",
                "\n",
                "def _sort_centers(centers):\n",
                "    return np.sort(centers, axis=0)\n",
                "\n",
                "\n",
                "def test_weighted_vs_repeated(global_random_seed):\n",
                "    # Check that a sample weight of N should yield the same result as an N-fold\n",
                "    # repetition of the sample. Valid only if init is precomputed, otherwise\n",
                "    # rng produces different results. Not valid for MinibatchKMeans due to rng\n",
                "    # to extract minibatches.\n",
                "    sample_weight = np.random.RandomState(global_random_seed).randint(\n",
                "        1, 5, size=n_samples\n",
                "    )\n",
                "    X_repeat = np.repeat(X, sample_weight, axis=0)\n",
                "\n",
                "    km = KMeans(\n",
                "        init=centers, n_init=1, n_clusters=n_clusters, random_state=global_random_seed\n",
                "    )\n",
                "\n",
                "    km_weighted = clone(km).fit(X, sample_weight=sample_weight)\n",
                "    repeated_labels = np.repeat(km_weighted.labels_, sample_weight)\n",
                "    km_repeated = clone(km).fit(X_repeat)\n",
                "\n",
                "    assert_array_equal(km_repeated.labels_, repeated_labels)\n",
                "    assert_allclose(km_weighted.inertia_, km_repeated.inertia_)\n",
                "    assert_allclose(\n",
                "        _sort_centers(km_weighted.cluster_centers_),\n",
                "        _sort_centers(km_repeated.cluster_centers_),\n",
                "    )\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"input_data\",\n",
                "    [X] + X_as_any_csr,\n",
                "    ids=data_containers_ids,\n",
                ")\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_unit_weights_vs_no_weights(Estimator, input_data, global_random_seed):\n",
                "    # Check that not passing sample weights should be equivalent to passing\n",
                "    # sample weights all equal to one.\n",
                "    sample_weight = np.ones(n_samples)\n",
                "\n",
                "    km = Estimator(n_clusters=n_clusters, random_state=global_random_seed, n_init=1)\n",
                "    km_none = clone(km).fit(input_data, sample_weight=None)\n",
                "    km_ones = clone(km).fit(input_data, sample_weight=sample_weight)\n",
                "\n",
                "    assert_array_equal(km_none.labels_, km_ones.labels_)\n",
                "    assert_allclose(km_none.cluster_centers_, km_ones.cluster_centers_)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"input_data\",\n",
                "    [X] + X_as_any_csr,\n",
                "    ids=data_containers_ids,\n",
                ")\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_scaled_weights(Estimator, input_data, global_random_seed):\n",
                "    # Check that scaling all sample weights by a common factor\n",
                "    # shouldn't change the result\n",
                "    sample_weight = np.random.RandomState(global_random_seed).uniform(size=n_samples)\n",
                "\n",
                "    km = Estimator(n_clusters=n_clusters, random_state=global_random_seed, n_init=1)\n",
                "    km_orig = clone(km).fit(input_data, sample_weight=sample_weight)\n",
                "    km_scaled = clone(km).fit(input_data, sample_weight=0.5 * sample_weight)\n",
                "\n",
                "    assert_array_equal(km_orig.labels_, km_scaled.labels_)\n",
                "    assert_allclose(km_orig.cluster_centers_, km_scaled.cluster_centers_)\n",
                "\n",
                "\n",
                "def test_kmeans_elkan_iter_attribute():\n",
                "    # Regression test on bad n_iter_ value. Previous bug n_iter_ was one off\n",
                "    # it's right value (#11340).\n",
                "    km = KMeans(algorithm=\"elkan\", max_iter=1).fit(X)\n",
                "    assert km.n_iter_ == 1\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"array_constr\", data_containers, ids=data_containers_ids)\n",
                "def test_kmeans_empty_cluster_relocated(array_constr):\n",
                "    # check that empty clusters are correctly relocated when using sample\n",
                "    # weights (#13486)\n",
                "    X = array_constr([[-1], [1]])\n",
                "    sample_weight = [1.9, 0.1]\n",
                "    init = np.array([[-1], [10]])\n",
                "\n",
                "    km = KMeans(n_clusters=2, init=init, n_init=1)\n",
                "    km.fit(X, sample_weight=sample_weight)\n",
                "\n",
                "    assert len(set(km.labels_)) == 2\n",
                "    assert_allclose(km.cluster_centers_, [[-1], [1]])\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_result_equal_in_diff_n_threads(Estimator, global_random_seed):\n",
                "    # Check that KMeans/MiniBatchKMeans give the same results in parallel mode\n",
                "    # than in sequential mode.\n",
                "    rnd = np.random.RandomState(global_random_seed)\n",
                "    X = rnd.normal(size=(50, 10))\n",
                "\n",
                "    with threadpool_limits(limits=1, user_api=\"openmp\"):\n",
                "        result_1 = (\n",
                "            Estimator(n_clusters=n_clusters, random_state=global_random_seed)\n",
                "            .fit(X)\n",
                "            .labels_\n",
                "        )\n",
                "    with threadpool_limits(limits=2, user_api=\"openmp\"):\n",
                "        result_2 = (\n",
                "            Estimator(n_clusters=n_clusters, random_state=global_random_seed)\n",
                "            .fit(X)\n",
                "            .labels_\n",
                "        )\n",
                "    assert_array_equal(result_1, result_2)\n",
                "\n",
                "\n",
                "def test_warning_elkan_1_cluster():\n",
                "    # Check warning messages specific to KMeans\n",
                "    with pytest.warns(\n",
                "        RuntimeWarning,\n",
                "        match=\"algorithm='elkan' doesn't make sense for a single cluster\",\n",
                "    ):\n",
                "        KMeans(n_clusters=1, algorithm=\"elkan\").fit(X)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"array_constr\", data_containers, ids=data_containers_ids)\n",
                "@pytest.mark.parametrize(\"algo\", [\"lloyd\", \"elkan\"])\n",
                "def test_k_means_1_iteration(array_constr, algo, global_random_seed):\n",
                "    # check the results after a single iteration (E-step M-step E-step) by\n",
                "    # comparing against a pure python implementation.\n",
                "    X = np.random.RandomState(global_random_seed).uniform(size=(100, 5))\n",
                "    init_centers = X[:5]\n",
                "    X = array_constr(X)\n",
                "\n",
                "    def py_kmeans(X, init):\n",
                "        new_centers = init.copy()\n",
                "        labels = pairwise_distances_argmin(X, init)\n",
                "        for label in range(init.shape[0]):\n",
                "            new_centers[label] = X[labels == label].mean(axis=0)\n",
                "        labels = pairwise_distances_argmin(X, new_centers)\n",
                "        return labels, new_centers\n",
                "\n",
                "    py_labels, py_centers = py_kmeans(X, init_centers)\n",
                "\n",
                "    cy_kmeans = KMeans(\n",
                "        n_clusters=5, n_init=1, init=init_centers, algorithm=algo, max_iter=1\n",
                "    ).fit(X)\n",
                "    cy_labels = cy_kmeans.labels_\n",
                "    cy_centers = cy_kmeans.cluster_centers_\n",
                "\n",
                "    assert_array_equal(py_labels, cy_labels)\n",
                "    assert_allclose(py_centers, cy_centers)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\n",
                "@pytest.mark.parametrize(\"squared\", [True, False])\n",
                "def test_euclidean_distance(dtype, squared, global_random_seed):\n",
                "    # Check that the _euclidean_(dense/sparse)_dense helpers produce correct\n",
                "    # results\n",
                "    rng = np.random.RandomState(global_random_seed)\n",
                "    a_sparse = sp.random(\n",
                "        1, 100, density=0.5, format=\"csr\", random_state=rng, dtype=dtype\n",
                "    )\n",
                "    a_dense = a_sparse.toarray().reshape(-1)\n",
                "    b = rng.randn(100).astype(dtype, copy=False)\n",
                "    b_squared_norm = (b**2).sum()\n",
                "\n",
                "    expected = ((a_dense - b) ** 2).sum()\n",
                "    expected = expected if squared else np.sqrt(expected)\n",
                "\n",
                "    distance_dense_dense = _euclidean_dense_dense_wrapper(a_dense, b, squared)\n",
                "    distance_sparse_dense = _euclidean_sparse_dense_wrapper(\n",
                "        a_sparse.data, a_sparse.indices, b, b_squared_norm, squared\n",
                "    )\n",
                "\n",
                "    rtol = 1e-4 if dtype == np.float32 else 1e-7\n",
                "    assert_allclose(distance_dense_dense, distance_sparse_dense, rtol=rtol)\n",
                "    assert_allclose(distance_dense_dense, expected, rtol=rtol)\n",
                "    assert_allclose(distance_sparse_dense, expected, rtol=rtol)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\n",
                "def test_inertia(dtype, global_random_seed):\n",
                "    # Check that the _inertia_(dense/sparse) helpers produce correct results.\n",
                "    rng = np.random.RandomState(global_random_seed)\n",
                "    X_sparse = sp.random(\n",
                "        100, 10, density=0.5, format=\"csr\", random_state=rng, dtype=dtype\n",
                "    )\n",
                "    X_dense = X_sparse.toarray()\n",
                "    sample_weight = rng.randn(100).astype(dtype, copy=False)\n",
                "    centers = rng.randn(5, 10).astype(dtype, copy=False)\n",
                "    labels = rng.randint(5, size=100, dtype=np.int32)\n",
                "\n",
                "    distances = ((X_dense - centers[labels]) ** 2).sum(axis=1)\n",
                "    expected = np.sum(distances * sample_weight)\n",
                "\n",
                "    inertia_dense = _inertia_dense(X_dense, sample_weight, centers, labels, n_threads=1)\n",
                "    inertia_sparse = _inertia_sparse(\n",
                "        X_sparse, sample_weight, centers, labels, n_threads=1\n",
                "    )\n",
                "\n",
                "    rtol = 1e-4 if dtype == np.float32 else 1e-6\n",
                "    assert_allclose(inertia_dense, inertia_sparse, rtol=rtol)\n",
                "    assert_allclose(inertia_dense, expected, rtol=rtol)\n",
                "    assert_allclose(inertia_sparse, expected, rtol=rtol)\n",
                "\n",
                "    # Check the single_label parameter.\n",
                "    label = 1\n",
                "    mask = labels == label\n",
                "    distances = ((X_dense[mask] - centers[label]) ** 2).sum(axis=1)\n",
                "    expected = np.sum(distances * sample_weight[mask])\n",
                "\n",
                "    inertia_dense = _inertia_dense(\n",
                "        X_dense, sample_weight, centers, labels, n_threads=1, single_label=label\n",
                "    )\n",
                "    inertia_sparse = _inertia_sparse(\n",
                "        X_sparse, sample_weight, centers, labels, n_threads=1, single_label=label\n",
                "    )\n",
                "\n",
                "    assert_allclose(inertia_dense, inertia_sparse, rtol=rtol)\n",
                "    assert_allclose(inertia_dense, expected, rtol=rtol)\n",
                "    assert_allclose(inertia_sparse, expected, rtol=rtol)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"Klass, default_n_init\", [(KMeans, 10), (MiniBatchKMeans, 3)])\n",
                "def test_n_init_auto(Klass, default_n_init):\n",
                "    est = Klass(n_init=\"auto\", init=\"k-means++\")\n",
                "    est.fit(X)\n",
                "    assert est._n_init == 1\n",
                "\n",
                "    est = Klass(n_init=\"auto\", init=\"random\")\n",
                "    est.fit(X)\n",
                "    assert est._n_init == 10 if Klass.__name__ == \"KMeans\" else 3\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "def test_sample_weight_unchanged(Estimator):\n",
                "    # Check that sample_weight is not modified in place by KMeans (#17204)\n",
                "    X = np.array([[1], [2], [4]])\n",
                "    sample_weight = np.array([0.5, 0.2, 0.3])\n",
                "    Estimator(n_clusters=2, random_state=0).fit(X, sample_weight=sample_weight)\n",
                "\n",
                "    assert_array_equal(sample_weight, np.array([0.5, 0.2, 0.3]))\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])\n",
                "@pytest.mark.parametrize(\n",
                "    \"param, match\",\n",
                "    [\n",
                "        ({\"n_clusters\": n_samples + 1}, r\"n_samples.* should be >= n_clusters\"),\n",
                "        (\n",
                "            {\"init\": X[:2]},\n",
                "            r\"The shape of the initial centers .* does not match \"\n",
                "            r\"the number of clusters\",\n",
                "        ),\n",
                "        (\n",
                "            {\"init\": lambda X_, k, random_state: X_[:2]},\n",
                "            r\"The shape of the initial centers .* does not match \"\n",
                "            r\"the number of clusters\",\n",
                "        ),\n",
                "        (\n",
                "            {\"init\": X[:8, :2]},\n",
                "            r\"The shape of the initial centers .* does not match \"\n",
                "            r\"the number of features of the data\",\n",
                "        ),\n",
                "        (\n",
                "            {\"init\": lambda X_, k, random_state: X_[:8, :2]},\n",
                "            r\"The shape of the initial centers .* does not match \"\n",
                "            r\"the number of features of the data\",\n",
                "        ),\n",
                "    ],\n",
                ")\n",
                "def test_wrong_params(Estimator, param, match):\n",
                "    # Check that error are raised with clear error message when wrong values\n",
                "    # are passed for the parameters\n",
                "    # Set n_init=1 by default to avoid warning with precomputed init\n",
                "    km = Estimator(n_init=1)\n",
                "    with pytest.raises(ValueError, match=match):\n",
                "        km.set_params(**param).fit(X)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"param, match\",\n",
                "    [\n",
                "        (\n",
                "            {\"x_squared_norms\": X[:2]},\n",
                "            r\"The length of x_squared_norms .* should \"\n",
                "            r\"be equal to the length of n_samples\",\n",
                "        ),\n",
                "    ],\n",
                ")\n",
                "def test_kmeans_plusplus_wrong_params(param, match):\n",
                "    with pytest.raises(ValueError, match=match):\n",
                "        kmeans_plusplus(X, n_clusters, **param)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"input_data\",\n",
                "    [X] + X_as_any_csr,\n",
                ")\n",
                "@pytest.mark.parametrize(\"dtype\", [np.float64, np.float32])\n",
                "def test_kmeans_plusplus_output(input_data, dtype, global_random_seed):\n",
                "    # Check for the correct number of seeds and all positive values\n",
                "    data = input_data.astype(dtype)\n",
                "    centers, indices = kmeans_plusplus(\n",
                "        data, n_clusters, random_state=global_random_seed\n",
                "    )\n",
                "\n",
                "    # Check there are the correct number of indices and that all indices are\n",
                "    # positive and within the number of samples\n",
                "    assert indices.shape[0] == n_clusters\n",
                "    assert (indices >= 0).all()\n",
                "    assert (indices <= data.shape[0]).all()\n",
                "\n",
                "    # Check for the correct number of seeds and that they are bound by the data\n",
                "    assert centers.shape[0] == n_clusters\n",
                "    assert (centers.max(axis=0) <= data.max(axis=0)).all()\n",
                "    assert (centers.min(axis=0) >= data.min(axis=0)).all()\n",
                "\n",
                "    # Check that indices correspond to reported centers\n",
                "    # Use X for comparison rather than data, test still works against centers\n",
                "    # calculated with sparse data.\n",
                "    assert_allclose(X[indices].astype(dtype), centers)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"x_squared_norms\", [row_norms(X, squared=True), None])\n",
                "def test_kmeans_plusplus_norms(x_squared_norms):\n",
                "    # Check that defining x_squared_norms returns the same as default=None.\n",
                "    centers, indices = kmeans_plusplus(X, n_clusters, x_squared_norms=x_squared_norms)\n",
                "\n",
                "    assert_allclose(X[indices], centers)\n",
                "\n",
                "\n",
                "def test_kmeans_plusplus_dataorder(global_random_seed):\n",
                "    # Check that memory layout does not effect result\n",
                "    centers_c, _ = kmeans_plusplus(X, n_clusters, random_state=global_random_seed)\n",
                "\n",
                "    X_fortran = np.asfortranarray(X)\n",
                "\n",
                "    centers_fortran, _ = kmeans_plusplus(\n",
                "        X_fortran, n_clusters, random_state=global_random_seed\n",
                "    )\n",
                "\n",
                "    assert_allclose(centers_c, centers_fortran)\n",
                "\n",
                "\n",
                "def test_is_same_clustering():\n",
                "    # Sanity check for the _is_same_clustering utility function\n",
                "    labels1 = np.array([1, 0, 0, 1, 2, 0, 2, 1], dtype=np.int32)\n",
                "    assert _is_same_clustering(labels1, labels1, 3)\n",
                "\n",
                "    # these other labels represent the same clustering since we can retrieve the first\n",
                "    # labels by simply renaming the labels: 0 -> 1, 1 -> 2, 2 -> 0.\n",
                "    labels2 = np.array([0, 2, 2, 0, 1, 2, 1, 0], dtype=np.int32)\n",
                "    assert _is_same_clustering(labels1, labels2, 3)\n",
                "\n",
                "    # these other labels do not represent the same clustering since not all ones are\n",
                "    # mapped to a same value\n",
                "    labels3 = np.array([1, 0, 0, 2, 2, 0, 2, 1], dtype=np.int32)\n",
                "    assert not _is_same_clustering(labels1, labels3, 3)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"kwargs\", ({\"init\": np.str_(\"k-means++\")}, {\"init\": [[0, 0], [1, 1]], \"n_init\": 1})\n",
                ")\n",
                "def test_kmeans_with_array_like_or_np_scalar_init(kwargs):\n",
                "    \"\"\"Check that init works with numpy scalar strings.\n",
                "\n",
                "    Non-regression test for #21964.\n",
                "    \"\"\"\n",
                "    X = np.asarray([[0, 0], [0.5, 0], [0.5, 1], [1, 1]], dtype=np.float64)\n",
                "\n",
                "    clustering = KMeans(n_clusters=2, **kwargs)\n",
                "    # Does not raise\n",
                "    clustering.fit(X)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\n",
                "    \"Klass, method\",\n",
                "    [(KMeans, \"fit\"), (MiniBatchKMeans, \"fit\"), (MiniBatchKMeans, \"partial_fit\")],\n",
                ")\n",
                "def test_feature_names_out(Klass, method):\n",
                "    \"\"\"Check `feature_names_out` for `KMeans` and `MiniBatchKMeans`.\"\"\"\n",
                "    class_name = Klass.__name__.lower()\n",
                "    kmeans = Klass()\n",
                "    getattr(kmeans, method)(X)\n",
                "    n_clusters = kmeans.cluster_centers_.shape[0]\n",
                "\n",
                "    names_out = kmeans.get_feature_names_out()\n",
                "    assert_array_equal([f\"{class_name}{i}\" for i in range(n_clusters)], names_out)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"csr_container\", CSR_CONTAINERS + [None])\n",
                "def test_predict_does_not_change_cluster_centers(csr_container):\n",
                "    \"\"\"Check that predict does not change cluster centers.\n",
                "\n",
                "    Non-regression test for gh-24253.\n",
                "    \"\"\"\n",
                "    X, _ = make_blobs(n_samples=200, n_features=10, centers=10, random_state=0)\n",
                "    if csr_container is not None:\n",
                "        X = csr_container(X)\n",
                "\n",
                "    kmeans = KMeans()\n",
                "    y_pred1 = kmeans.fit_predict(X)\n",
                "    # Make cluster_centers readonly\n",
                "    kmeans.cluster_centers_ = create_memmap_backed_data(kmeans.cluster_centers_)\n",
                "    kmeans.labels_ = create_memmap_backed_data(kmeans.labels_)\n",
                "\n",
                "    y_pred2 = kmeans.predict(X)\n",
                "    assert_array_equal(y_pred1, y_pred2)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"init\", [\"k-means++\", \"random\"])\n",
                "def test_sample_weight_init(init, global_random_seed):\n",
                "    \"\"\"Check that sample weight is used during init.\n",
                "\n",
                "    `_init_centroids` is shared across all classes inheriting from _BaseKMeans so\n",
                "    it's enough to check for KMeans.\n",
                "    \"\"\"\n",
                "    rng = np.random.RandomState(global_random_seed)\n",
                "    X, _ = make_blobs(\n",
                "        n_samples=200, n_features=10, centers=10, random_state=global_random_seed\n",
                "    )\n",
                "    x_squared_norms = row_norms(X, squared=True)\n",
                "\n",
                "    kmeans = KMeans()\n",
                "    clusters_weighted = kmeans._init_centroids(\n",
                "        X=X,\n",
                "        x_squared_norms=x_squared_norms,\n",
                "        init=init,\n",
                "        sample_weight=rng.uniform(size=X.shape[0]),\n",
                "        n_centroids=5,\n",
                "        random_state=np.random.RandomState(global_random_seed),\n",
                "    )\n",
                "    clusters = kmeans._init_centroids(\n",
                "        X=X,\n",
                "        x_squared_norms=x_squared_norms,\n",
                "        init=init,\n",
                "        sample_weight=np.ones(X.shape[0]),\n",
                "        n_centroids=5,\n",
                "        random_state=np.random.RandomState(global_random_seed),\n",
                "    )\n",
                "    with pytest.raises(AssertionError):\n",
                "        assert_allclose(clusters_weighted, clusters)\n",
                "\n",
                "\n",
                "@pytest.mark.parametrize(\"init\", [\"k-means++\", \"random\"])\n",
                "def test_sample_weight_zero(init, global_random_seed):\n",
                "    \"\"\"Check that if sample weight is 0, this sample won't be chosen.\n",
                "\n",
                "    `_init_centroids` is shared across all classes inheriting from _BaseKMeans so\n",
                "    it's enough to check for KMeans.\n",
                "    \"\"\"\n",
                "    rng = np.random.RandomState(global_random_seed)\n",
                "    X, _ = make_blobs(\n",
                "        n_samples=100, n_features=5, centers=5, random_state=global_random_seed\n",
                "    )\n",
                "    sample_weight = rng.uniform(size=X.shape[0])\n",
                "    sample_weight[::2] = 0\n",
                "    x_squared_norms = row_norms(X, squared=True)\n",
                "\n",
                "    kmeans = KMeans()\n",
                "    clusters_weighted = kmeans._init_centroids(\n",
                "        X=X,\n",
                "        x_squared_norms=x_squared_norms,\n",
                "        init=init,\n",
                "        sample_weight=sample_weight,\n",
                "        n_centroids=10,\n",
                "        random_state=np.random.RandomState(global_random_seed),\n",
                "    )\n",
                "    # No center should be one of the 0 sample weight point\n",
                "    # (i.e. be at a distance=0 from it)\n",
                "    d = euclidean_distances(X[::2], clusters_weighted)\n",
                "    assert not np.any(np.isclose(d, 0))"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "bi-directional",
            "reason": "sync doc"
        },
        {
            "edit_hunk_pair": [
                0,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        },
        {
            "edit_hunk_pair": [
                1,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        },
        {
            "edit_hunk_pair": [
                2,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "sync doc"
        },
        {
            "edit_hunk_pair": [
                2,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "sync doc and implement"
        },
        {
            "edit_hunk_pair": [
                2,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "sync doc and implement"
        },
        {
            "edit_hunk_pair": [
                3,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "sync doc and implement"
        },
        {
            "edit_hunk_pair": [
                4,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "ok to be bi-directional or no relation"
        },
        {
            "edit_hunk_pair": [
                4,
                6
            ],
            "edit_order": "bi-directional",
            "reason": "implement and test"
        }
    ]
}