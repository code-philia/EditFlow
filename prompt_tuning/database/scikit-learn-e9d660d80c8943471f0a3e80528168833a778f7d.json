{
    "language": "python",
    "commit_url": "https://github.com/scikit-learn/scikit-learn/commit/e9d660d80c8943471f0a3e80528168833a778f7d",
    "commit_message": "add sample_weight to base score and weight_boosting staged_score",
    "commit_snapshots": {
        "sklearn/base.py": [
            [
                "\"\"\"Base classes for all estimators.\"\"\"\n",
                "# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n",
                "# License: BSD 3 clause\n",
                "\n",
                "import copy\n",
                "import inspect\n",
                "import warnings\n",
                "\n",
                "import numpy as np\n",
                "from scipy import sparse\n",
                "from .externals import six\n",
                "\n",
                "\n",
                "###############################################################################\n",
                "def clone(estimator, safe=True):\n",
                "    \"\"\"Constructs a new estimator with the same parameters.\n",
                "\n",
                "    Clone does a deep copy of the model in an estimator\n",
                "    without actually copying attached data. It yields a new estimator\n",
                "    with the same parameters that has not been fit on any data.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    estimator: estimator object, or list, tuple or set of objects\n",
                "        The estimator or group of estimators to be cloned\n",
                "\n",
                "    safe: boolean, optional\n",
                "        If safe is false, clone will fall back to a deepcopy on objects\n",
                "        that are not estimators.\n",
                "\n",
                "    \"\"\"\n",
                "    estimator_type = type(estimator)\n",
                "    # XXX: not handling dictionaries\n",
                "    if estimator_type in (list, tuple, set, frozenset):\n",
                "        return estimator_type([clone(e, safe=safe) for e in estimator])\n",
                "    elif not hasattr(estimator, 'get_params'):\n",
                "        if not safe:\n",
                "            return copy.deepcopy(estimator)\n",
                "        else:\n",
                "            raise TypeError(\"Cannot clone object '%s' (type %s): \"\n",
                "                            \"it does not seem to be a scikit-learn estimator \"\n",
                "                            \"it does not implement a 'get_params' methods.\"\n",
                "                            % (repr(estimator), type(estimator)))\n",
                "    klass = estimator.__class__\n",
                "    new_object_params = estimator.get_params(deep=False)\n",
                "    for name, param in six.iteritems(new_object_params):\n",
                "        new_object_params[name] = clone(param, safe=False)\n",
                "    new_object = klass(**new_object_params)\n",
                "    params_set = new_object.get_params(deep=False)\n",
                "\n",
                "    # quick sanity check of the parameters of the clone\n",
                "    for name in new_object_params:\n",
                "        param1 = new_object_params[name]\n",
                "        param2 = params_set[name]\n",
                "        if isinstance(param1, np.ndarray):\n",
                "            # For most ndarrays, we do not test for complete equality\n",
                "            if not isinstance(param2, type(param1)):\n",
                "                equality_test = False\n",
                "            elif (param1.ndim > 0\n",
                "                    and param1.shape[0] > 0\n",
                "                    and isinstance(param2, np.ndarray)\n",
                "                    and param2.ndim > 0\n",
                "                    and param2.shape[0] > 0):\n",
                "                equality_test = (\n",
                "                    param1.shape == param2.shape\n",
                "                    and param1.dtype == param2.dtype\n",
                "                    # We have to use '.flat' for 2D arrays\n",
                "                    and param1.flat[0] == param2.flat[0]\n",
                "                    and param1.flat[-1] == param2.flat[-1]\n",
                "                )\n",
                "            else:\n",
                "                equality_test = np.all(param1 == param2)\n",
                "        elif sparse.issparse(param1):\n",
                "            # For sparse matrices equality doesn't work\n",
                "            if not sparse.issparse(param2):\n",
                "                equality_test = False\n",
                "            elif param1.size == 0 or param2.size == 0:\n",
                "                equality_test = (\n",
                "                    param1.__class__ == param2.__class__\n",
                "                    and param1.size == 0\n",
                "                    and param2.size == 0\n",
                "                )\n",
                "            else:\n",
                "                equality_test = (\n",
                "                    param1.__class__ == param2.__class__\n",
                "                    and param1.data[0] == param2.data[0]\n",
                "                    and param1.data[-1] == param2.data[-1]\n",
                "                    and param1.nnz == param2.nnz\n",
                "                    and param1.shape == param2.shape\n",
                "                )\n",
                "        else:\n",
                "            equality_test = new_object_params[name] == params_set[name]\n",
                "        if not equality_test:\n",
                "            raise RuntimeError('Cannot clone object %s, as the constructor '\n",
                "                               'does not seem to set parameter %s' %\n",
                "                               (estimator, name))\n",
                "\n",
                "    return new_object\n",
                "\n",
                "\n",
                "###############################################################################\n",
                "def _pprint(params, offset=0, printer=repr):\n",
                "    \"\"\"Pretty print the dictionary 'params'\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    params: dict\n",
                "        The dictionary to pretty print\n",
                "\n",
                "    offset: int\n",
                "        The offset in characters to add at the begin of each line.\n",
                "\n",
                "    printer:\n",
                "        The function to convert entries to strings, typically\n",
                "        the builtin str or repr\n",
                "\n",
                "    \"\"\"\n",
                "    # Do a multi-line justified repr:\n",
                "    options = np.get_printoptions()\n",
                "    np.set_printoptions(precision=5, threshold=64, edgeitems=2)\n",
                "    params_list = list()\n",
                "    this_line_length = offset\n",
                "    line_sep = ',\\n' + (1 + offset // 2) * ' '\n",
                "    for i, (k, v) in enumerate(sorted(six.iteritems(params))):\n",
                "        if type(v) is float:\n",
                "            # use str for representing floating point numbers\n",
                "            # this way we get consistent representation across\n",
                "            # architectures and versions.\n",
                "            this_repr = '%s=%s' % (k, str(v))\n",
                "        else:\n",
                "            # use repr of the rest\n",
                "            this_repr = '%s=%s' % (k, printer(v))\n",
                "        if len(this_repr) > 500:\n",
                "            this_repr = this_repr[:300] + '...' + this_repr[-100:]\n",
                "        if i > 0:\n",
                "            if (this_line_length + len(this_repr) >= 75 or '\\n' in this_repr):\n",
                "                params_list.append(line_sep)\n",
                "                this_line_length = len(line_sep)\n",
                "            else:\n",
                "                params_list.append(', ')\n",
                "                this_line_length += 2\n",
                "        params_list.append(this_repr)\n",
                "        this_line_length += len(this_repr)\n",
                "\n",
                "    np.set_printoptions(**options)\n",
                "    lines = ''.join(params_list)\n",
                "    # Strip trailing space to avoid nightmare in doctests\n",
                "    lines = '\\n'.join(l.rstrip(' ') for l in lines.split('\\n'))\n",
                "    return lines\n",
                "\n",
                "\n",
                "###############################################################################\n",
                "class BaseEstimator(object):\n",
                "    \"\"\"Base class for all estimators in scikit-learn\n",
                "\n",
                "    Notes\n",
                "    -----\n",
                "    All estimators should specify all the parameters that can be set\n",
                "    at the class level in their __init__ as explicit keyword\n",
                "    arguments (no *args, **kwargs).\n",
                "    \"\"\"\n",
                "\n",
                "    @classmethod\n",
                "    def _get_param_names(cls):\n",
                "        \"\"\"Get parameter names for the estimator\"\"\"\n",
                "        # fetch the constructor or the original constructor before\n",
                "        # deprecation wrapping if any\n",
                "        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n",
                "        if init is object.__init__:\n",
                "            # No explicit constructor to introspect\n",
                "            return []\n",
                "\n",
                "        # introspect the constructor arguments to find the model parameters\n",
                "        # to represent\n",
                "        args, varargs, kw, default = inspect.getargspec(init)\n",
                "        if varargs is not None:\n",
                "            raise RuntimeError(\"scikit-learn estimators should always \"\n",
                "                               \"specify their parameters in the signature\"\n",
                "                               \" of their __init__ (no varargs).\"\n",
                "                               \" %s doesn't follow this convention.\"\n",
                "                               % (cls, ))\n",
                "        # Remove 'self'\n",
                "        # XXX: This is going to fail if the init is a staticmethod, but\n",
                "        # who would do this?\n",
                "        args.pop(0)\n",
                "        args.sort()\n",
                "        return args\n",
                "\n",
                "    def get_params(self, deep=True):\n",
                "        \"\"\"Get parameters for this estimator.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        deep: boolean, optional\n",
                "            If True, will return the parameters for this estimator and\n",
                "            contained subobjects that are estimators.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        params : mapping of string to any\n",
                "            Parameter names mapped to their values.\n",
                "        \"\"\"\n",
                "        out = dict()\n",
                "        for key in self._get_param_names():\n",
                "            # We need deprecation warnings to always be on in order to\n",
                "            # catch deprecated param values.\n",
                "            # This is set in utils/__init__.py but it gets overwritten\n",
                "            # when running under python3 somehow.\n",
                "            warnings.simplefilter(\"always\", DeprecationWarning)\n",
                "            try:\n",
                "                with warnings.catch_warnings(record=True) as w:\n",
                "                    value = getattr(self, key, None)\n",
                "                if len(w) and w[0].category == DeprecationWarning:\n",
                "                # if the parameter is deprecated, don't show it\n",
                "                    continue\n",
                "            finally:\n",
                "                warnings.filters.pop(0)\n",
                "\n",
                "            # XXX: should we rather test if instance of estimator?\n",
                "            if deep and hasattr(value, 'get_params'):\n",
                "                deep_items = value.get_params().items()\n",
                "                out.update((key + '__' + k, val) for k, val in deep_items)\n",
                "            out[key] = value\n",
                "        return out\n",
                "\n",
                "    def set_params(self, **params):\n",
                "        \"\"\"Set the parameters of this estimator.\n",
                "\n",
                "        The method works on simple estimators as well as on nested objects\n",
                "        (such as pipelines). The former have parameters of the form\n",
                "        ``<component>__<parameter>`` so that it's possible to update each\n",
                "        component of a nested object.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        self\n",
                "        \"\"\"\n",
                "        if not params:\n",
                "            # Simple optimisation to gain speed (inspect is slow)\n",
                "            return self\n",
                "        valid_params = self.get_params(deep=True)\n",
                "        for key, value in six.iteritems(params):\n",
                "            split = key.split('__', 1)\n",
                "            if len(split) > 1:\n",
                "                # nested objects case\n",
                "                name, sub_name = split\n",
                "                if not name in valid_params:\n",
                "                    raise ValueError('Invalid parameter %s for estimator %s' %\n",
                "                                     (name, self))\n",
                "                sub_object = valid_params[name]\n",
                "                sub_object.set_params(**{sub_name: value})\n",
                "            else:\n",
                "                # simple objects case\n",
                "                if not key in valid_params:\n",
                "                    raise ValueError('Invalid parameter %s ' 'for estimator %s'\n",
                "                                     % (key, self.__class__.__name__))\n",
                "                setattr(self, key, value)\n",
                "        return self\n",
                "\n",
                "    def __repr__(self):\n",
                "        class_name = self.__class__.__name__\n",
                "        return '%s(%s)' % (class_name, _pprint(self.get_params(deep=False),\n",
                "                                               offset=len(class_name),),)\n",
                "\n",
                "\n",
                "###############################################################################\n",
                "class ClassifierMixin(object):\n",
                "    \"\"\"Mixin class for all classifiers in scikit-learn.\"\"\"\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    def score(self, X, y):\n"
                ],
                "after": [
                    "    def score(self, X, y, sample_weight=None):\n"
                ],
                "parent_version_range": {
                    "start": 269,
                    "end": 270
                },
                "child_version_range": {
                    "start": 269,
                    "end": 270
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "ClassifierMixin",
                        "signature": "class ClassifierMixin(object):",
                        "at_line": 266
                    },
                    {
                        "type": "function",
                        "name": "score",
                        "signature": "def score(self, X, y):",
                        "at_line": 269
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: sklearn/base.py\nCode:\n266 266    class ClassifierMixin(object):\n267 267        \"\"\"Mixin class for all classifiers in scikit-learn.\"\"\"\n268 268    \n269      -     def score(self, X, y):\n    269  +     def score(self, X, y, sample_weight=None):\n270 270            \"\"\"Returns the mean accuracy on the given test data and labels.\n271 271    \n272 272            Parameters\n         ...\n",
                "file_path": "sklearn/base.py",
                "identifiers_before": [
                    "X",
                    "score",
                    "self",
                    "y"
                ],
                "identifiers_after": [
                    "X",
                    "sample_weight",
                    "score",
                    "self",
                    "y"
                ],
                "prefix": [
                    "class ClassifierMixin(object):\n",
                    "    \"\"\"Mixin class for all classifiers in scikit-learn.\"\"\"\n",
                    "\n"
                ],
                "suffix": [
                    "        \"\"\"Returns the mean accuracy on the given test data and labels.\n",
                    "\n",
                    "        Parameters\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "self",
                            "position": {
                                "start": {
                                    "line": 269,
                                    "column": 14
                                },
                                "end": {
                                    "line": 269,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "X",
                            "position": {
                                "start": {
                                    "line": 269,
                                    "column": 20
                                },
                                "end": {
                                    "line": 269,
                                    "column": 21
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 269,
                                    "column": 23
                                },
                                "end": {
                                    "line": 269,
                                    "column": 24
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "self",
                            "position": {
                                "start": {
                                    "line": 269,
                                    "column": 14
                                },
                                "end": {
                                    "line": 269,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "X",
                            "position": {
                                "start": {
                                    "line": 269,
                                    "column": 20
                                },
                                "end": {
                                    "line": 269,
                                    "column": 21
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 269,
                                    "column": 23
                                },
                                "end": {
                                    "line": 269,
                                    "column": 24
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 2,
                        "detail": {
                            "identifier": "sample_weight",
                            "position": {
                                "start": {
                                    "line": 269,
                                    "column": 26
                                },
                                "end": {
                                    "line": 269,
                                    "column": 39
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 0,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    3,
                    6
                ]
            },
            [
                "        \"\"\"Returns the mean accuracy on the given test data and labels.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like, shape = (n_samples, n_features)\n",
                "            Test samples.\n",
                "\n",
                "        y : array-like, shape = (n_samples,)\n",
                "            True labels for X.\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        sample_weight : array-like, shape = [n_samples], optional\n",
                    "            Sample weights.\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 280,
                    "end": 280
                },
                "child_version_range": {
                    "start": 280,
                    "end": 283
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "ClassifierMixin",
                        "signature": "class ClassifierMixin(object):",
                        "at_line": 266
                    },
                    {
                        "type": "function",
                        "name": "score",
                        "signature": "def score(self, X, y):",
                        "at_line": 269
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: sklearn/base.py\nCode:\n           class ClassifierMixin(object):\n               ...\n               def score(self, X, y):\n                   ...\n277 277            y : array-like, shape = (n_samples,)\n278 278                True labels for X.\n279 279    \n    280  +         sample_weight : array-like, shape = [n_samples], optional\n    281  +             Sample weights.\n    282  + \n280 283            Returns\n281 284            -------\n282 285            score : float\n         ...\n",
                "file_path": "sklearn/base.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "Sample",
                    "array",
                    "like",
                    "n_samples",
                    "optional",
                    "sample_weight",
                    "shape",
                    "weights"
                ],
                "prefix": [
                    "        y : array-like, shape = (n_samples,)\n",
                    "            True labels for X.\n",
                    "\n"
                ],
                "suffix": [
                    "        Returns\n",
                    "        -------\n",
                    "        score : float\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    4,
                    7
                ]
            },
            [
                "        Returns\n",
                "        -------\n",
                "        score : float\n",
                "            Mean accuracy of self.predict(X) wrt. y.\n",
                "\n",
                "        \"\"\"\n",
                "        from .metrics import accuracy_score\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        return accuracy_score(y, self.predict(X))\n"
                ],
                "after": [
                    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n"
                ],
                "parent_version_range": {
                    "start": 287,
                    "end": 288
                },
                "child_version_range": {
                    "start": 290,
                    "end": 291
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "ClassifierMixin",
                        "signature": "class ClassifierMixin(object):",
                        "at_line": 266
                    },
                    {
                        "type": "function",
                        "name": "score",
                        "signature": "def score(self, X, y):",
                        "at_line": 269
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: sklearn/base.py\nCode:\n           class ClassifierMixin(object):\n               ...\n               def score(self, X, y):\n                   ...\n284 287    \n285 288            \"\"\"\n286 289            from .metrics import accuracy_score\n287      -         return accuracy_score(y, self.predict(X))\n    290  +         return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n288 291    \n289 292    \n290 293    ###############################################################################\n         ...\n",
                "file_path": "sklearn/base.py",
                "identifiers_before": [
                    "X",
                    "accuracy_score",
                    "predict",
                    "self",
                    "y"
                ],
                "identifiers_after": [
                    "X",
                    "accuracy_score",
                    "predict",
                    "sample_weight",
                    "self",
                    "y"
                ],
                "prefix": [
                    "\n",
                    "        \"\"\"\n",
                    "        from .metrics import accuracy_score\n"
                ],
                "suffix": [
                    "\n",
                    "\n",
                    "###############################################################################\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "self",
                            "position": {
                                "start": {
                                    "line": 287,
                                    "column": 33
                                },
                                "end": {
                                    "line": 287,
                                    "column": 37
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "X",
                            "position": {
                                "start": {
                                    "line": 287,
                                    "column": 46
                                },
                                "end": {
                                    "line": 287,
                                    "column": 47
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 287,
                                    "column": 30
                                },
                                "end": {
                                    "line": 287,
                                    "column": 31
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "self",
                            "position": {
                                "start": {
                                    "line": 290,
                                    "column": 33
                                },
                                "end": {
                                    "line": 290,
                                    "column": 37
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "X",
                            "position": {
                                "start": {
                                    "line": 290,
                                    "column": 46
                                },
                                "end": {
                                    "line": 290,
                                    "column": 47
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 290,
                                    "column": 30
                                },
                                "end": {
                                    "line": 290,
                                    "column": 31
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 0,
                        "detail": {
                            "identifier": "sample_weight",
                            "position": {
                                "start": {
                                    "line": 290,
                                    "column": 64
                                },
                                "end": {
                                    "line": 290,
                                    "column": 77
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 2,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    5,
                    8,
                    9
                ]
            },
            [
                "\n",
                "\n",
                "###############################################################################\n",
                "class RegressorMixin(object):\n",
                "    \"\"\"Mixin class for all regression estimators in scikit-learn.\"\"\"\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    def score(self, X, y):\n"
                ],
                "after": [
                    "    def score(self, X, y, sample_weight=None):\n"
                ],
                "parent_version_range": {
                    "start": 294,
                    "end": 295
                },
                "child_version_range": {
                    "start": 297,
                    "end": 298
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RegressorMixin",
                        "signature": "class RegressorMixin(object):",
                        "at_line": 291
                    },
                    {
                        "type": "function",
                        "name": "score",
                        "signature": "def score(self, X, y):",
                        "at_line": 294
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: sklearn/base.py\nCode:\n291 294    class RegressorMixin(object):\n292 295        \"\"\"Mixin class for all regression estimators in scikit-learn.\"\"\"\n293 296    \n294      -     def score(self, X, y):\n    297  +     def score(self, X, y, sample_weight=None):\n295 298            \"\"\"Returns the coefficient of determination R^2 of the prediction.\n296 299    \n297 300            The coefficient R^2 is defined as (1 - u/v), where u is the regression\n         ...\n",
                "file_path": "sklearn/base.py",
                "identifiers_before": [
                    "X",
                    "score",
                    "self",
                    "y"
                ],
                "identifiers_after": [
                    "X",
                    "sample_weight",
                    "score",
                    "self",
                    "y"
                ],
                "prefix": [
                    "class RegressorMixin(object):\n",
                    "    \"\"\"Mixin class for all regression estimators in scikit-learn.\"\"\"\n",
                    "\n"
                ],
                "suffix": [
                    "        \"\"\"Returns the coefficient of determination R^2 of the prediction.\n",
                    "\n",
                    "        The coefficient R^2 is defined as (1 - u/v), where u is the regression\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "self",
                            "position": {
                                "start": {
                                    "line": 294,
                                    "column": 14
                                },
                                "end": {
                                    "line": 294,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "X",
                            "position": {
                                "start": {
                                    "line": 294,
                                    "column": 20
                                },
                                "end": {
                                    "line": 294,
                                    "column": 21
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 294,
                                    "column": 23
                                },
                                "end": {
                                    "line": 294,
                                    "column": 24
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "self",
                            "position": {
                                "start": {
                                    "line": 297,
                                    "column": 14
                                },
                                "end": {
                                    "line": 297,
                                    "column": 18
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "X",
                            "position": {
                                "start": {
                                    "line": 297,
                                    "column": 20
                                },
                                "end": {
                                    "line": 297,
                                    "column": 21
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 297,
                                    "column": 23
                                },
                                "end": {
                                    "line": 297,
                                    "column": 24
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 5,
                        "detail": {
                            "identifier": "sample_weight",
                            "position": {
                                "start": {
                                    "line": 297,
                                    "column": 26
                                },
                                "end": {
                                    "line": 297,
                                    "column": 39
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 3,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    0,
                    6
                ]
            },
            [
                "        \"\"\"Returns the coefficient of determination R^2 of the prediction.\n",
                "\n",
                "        The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
                "        sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
                "        sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
                "        Best possible score is 1.0, lower values are worse.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like, shape = (n_samples, n_features)\n",
                "            Test samples.\n",
                "\n",
                "        y : array-like, shape = (n_samples,)\n",
                "            True values for X.\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        sample_weight : array-like, shape = [n_samples], optional\n",
                    "            Sample weights.\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 310,
                    "end": 310
                },
                "child_version_range": {
                    "start": 313,
                    "end": 316
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RegressorMixin",
                        "signature": "class RegressorMixin(object):",
                        "at_line": 291
                    },
                    {
                        "type": "function",
                        "name": "score",
                        "signature": "def score(self, X, y):",
                        "at_line": 294
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: sklearn/base.py\nCode:\n           class RegressorMixin(object):\n               ...\n               def score(self, X, y):\n                   ...\n307 310            y : array-like, shape = (n_samples,)\n308 311                True values for X.\n309 312    \n    313  +         sample_weight : array-like, shape = [n_samples], optional\n    314  +             Sample weights.\n    315  + \n310 316            Returns\n311 317            -------\n312 318            score : float\n         ...\n",
                "file_path": "sklearn/base.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "Sample",
                    "array",
                    "like",
                    "n_samples",
                    "optional",
                    "sample_weight",
                    "shape",
                    "weights"
                ],
                "prefix": [
                    "        y : array-like, shape = (n_samples,)\n",
                    "            True values for X.\n",
                    "\n"
                ],
                "suffix": [
                    "        Returns\n",
                    "        -------\n",
                    "        score : float\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    1,
                    7
                ]
            },
            [
                "        Returns\n",
                "        -------\n",
                "        score : float\n",
                "            R^2 of self.predict(X) wrt. y.\n",
                "        \"\"\"\n",
                "\n",
                "        from .metrics import r2_score\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        return r2_score(y, self.predict(X))\n"
                ],
                "after": [
                    "        return r2_score(y, self.predict(X), sample_weight=sample_weight)\n"
                ],
                "parent_version_range": {
                    "start": 317,
                    "end": 318
                },
                "child_version_range": {
                    "start": 323,
                    "end": 324
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "RegressorMixin",
                        "signature": "class RegressorMixin(object):",
                        "at_line": 291
                    },
                    {
                        "type": "function",
                        "name": "score",
                        "signature": "def score(self, X, y):",
                        "at_line": 294
                    }
                ],
                "idx": 5,
                "hunk_diff": "File: sklearn/base.py\nCode:\n           class RegressorMixin(object):\n               ...\n               def score(self, X, y):\n                   ...\n314 320            \"\"\"\n315 321    \n316 322            from .metrics import r2_score\n317      -         return r2_score(y, self.predict(X))\n    323  +         return r2_score(y, self.predict(X), sample_weight=sample_weight)\n318 324    \n319 325    \n320 326    ###############################################################################\n         ...\n",
                "file_path": "sklearn/base.py",
                "identifiers_before": [
                    "X",
                    "predict",
                    "r2_score",
                    "self",
                    "y"
                ],
                "identifiers_after": [
                    "X",
                    "predict",
                    "r2_score",
                    "sample_weight",
                    "self",
                    "y"
                ],
                "prefix": [
                    "        \"\"\"\n",
                    "\n",
                    "        from .metrics import r2_score\n"
                ],
                "suffix": [
                    "\n",
                    "\n",
                    "###############################################################################\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "self",
                            "position": {
                                "start": {
                                    "line": 317,
                                    "column": 27
                                },
                                "end": {
                                    "line": 317,
                                    "column": 31
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "X",
                            "position": {
                                "start": {
                                    "line": 317,
                                    "column": 40
                                },
                                "end": {
                                    "line": 317,
                                    "column": 41
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 317,
                                    "column": 24
                                },
                                "end": {
                                    "line": 317,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "self",
                            "position": {
                                "start": {
                                    "line": 323,
                                    "column": 27
                                },
                                "end": {
                                    "line": 323,
                                    "column": 31
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "X",
                            "position": {
                                "start": {
                                    "line": 323,
                                    "column": 40
                                },
                                "end": {
                                    "line": 323,
                                    "column": 41
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 323,
                                    "column": 24
                                },
                                "end": {
                                    "line": 323,
                                    "column": 25
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 3,
                        "detail": {
                            "identifier": "sample_weight",
                            "position": {
                                "start": {
                                    "line": 323,
                                    "column": 58
                                },
                                "end": {
                                    "line": 323,
                                    "column": 71
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/base.py",
                            "hunk_idx": 5,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    2,
                    8,
                    9
                ]
            },
            [
                "\n",
                "\n",
                "###############################################################################\n",
                "class ClusterMixin(object):\n",
                "    \"\"\"Mixin class for all cluster estimators in scikit-learn.\"\"\"\n",
                "    def fit_predict(self, X, y=None):\n",
                "        \"\"\"Performs clustering on X and returns cluster labels.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : ndarray, shape (n_samples, n_features)\n",
                "            Input data.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        y : ndarray, shape (n_samples,)\n",
                "            cluster labels\n",
                "        \"\"\"\n",
                "        # non-optimized default implementation; override when a better\n",
                "        # method is possible for a given clustering algorithm\n",
                "        self.fit(X)\n",
                "        return self.labels_\n",
                "\n",
                "\n",
                "class BiclusterMixin(object):\n",
                "    \"\"\"Mixin class for all bicluster estimators in scikit-learn\"\"\"\n",
                "\n",
                "    @property\n",
                "    def biclusters_(self):\n",
                "        \"\"\"Convenient way to get row and column indicators together.\n",
                "\n",
                "        Returns the ``rows_`` and ``columns_`` members.\n",
                "        \"\"\"\n",
                "        return self.rows_, self.columns_\n",
                "\n",
                "    def get_indices(self, i):\n",
                "        \"\"\"Row and column indices of the i'th bicluster.\n",
                "\n",
                "        Only works if ``rows_`` and ``columns_`` attributes exist.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        row_ind : np.array, dtype=np.intp\n",
                "            Indices of rows in the dataset that belong to the bicluster.\n",
                "        col_ind : np.array, dtype=np.intp\n",
                "            Indices of columns in the dataset that belong to the bicluster.\n",
                "\n",
                "        \"\"\"\n",
                "        from .cluster.bicluster.utils import get_indices\n",
                "        return get_indices(self.rows_[i], self.columns_[i])\n",
                "\n",
                "    def get_shape(self, i):\n",
                "        \"\"\"Shape of the i'th bicluster.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        shape : (int, int)\n",
                "            Number of rows and columns (resp.) in the bicluster.\n",
                "        \"\"\"\n",
                "        from .cluster.bicluster.utils import get_shape\n",
                "        return get_shape(self.rows_[i], self.columns_[i])\n",
                "\n",
                "    def get_submatrix(self, i, data):\n",
                "        \"\"\"Returns the submatrix corresponding to bicluster `i`.\n",
                "\n",
                "        Works with sparse matrices. Only works if ``rows_`` and\n",
                "        ``columns_`` attributes exist.\n",
                "\n",
                "        \"\"\"\n",
                "        from .cluster.bicluster.utils import get_submatrix\n",
                "        return get_submatrix(self.rows_[i], self.columns_[i], data)\n",
                "\n",
                "\n",
                "###############################################################################\n",
                "class TransformerMixin(object):\n",
                "    \"\"\"Mixin class for all transformers in scikit-learn.\"\"\"\n",
                "\n",
                "    def fit_transform(self, X, y=None, **fit_params):\n",
                "        \"\"\"Fit to data, then transform it.\n",
                "\n",
                "        Fits transformer to X and y with optional parameters fit_params\n",
                "        and returns a transformed version of X.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : numpy array of shape [n_samples, n_features]\n",
                "            Training set.\n",
                "\n",
                "        y : numpy array of shape [n_samples]\n",
                "            Target values.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        X_new : numpy array of shape [n_samples, n_features_new]\n",
                "            Transformed array.\n",
                "\n",
                "        \"\"\"\n",
                "        # non-optimized default implementation; override when a better\n",
                "        # method is possible for a given clustering algorithm\n",
                "        if y is None:\n",
                "            # fit method of arity 1 (unsupervised transformation)\n",
                "            return self.fit(X, **fit_params).transform(X)\n",
                "        else:\n",
                "            # fit method of arity 2 (supervised transformation)\n",
                "            return self.fit(X, y, **fit_params).transform(X)\n",
                "\n",
                "\n",
                "###############################################################################\n",
                "class MetaEstimatorMixin(object):\n",
                "    \"\"\"Mixin class for all meta estimators in scikit-learn.\"\"\"\n",
                "    # this is just a tag for the moment\n",
                "\n",
                "\n",
                "###############################################################################\n",
                "# XXX: Temporary solution to figure out if an estimator is a classifier\n",
                "\n",
                "def _get_sub_estimator(estimator):\n",
                "    \"\"\"Returns the final estimator if there is any.\"\"\"\n",
                "    if hasattr(estimator, 'estimator'):\n",
                "        # GridSearchCV and other CV-tuned estimators\n",
                "        return _get_sub_estimator(estimator.estimator)\n",
                "    if hasattr(estimator, 'steps'):\n",
                "        # Pipeline\n",
                "        return _get_sub_estimator(estimator.steps[-1][1])\n",
                "    return estimator\n",
                "\n",
                "\n",
                "def is_classifier(estimator):\n",
                "    \"\"\"Returns True if the given estimator is (probably) a classifier.\"\"\"\n",
                "    estimator = _get_sub_estimator(estimator)\n",
                "    return isinstance(estimator, ClassifierMixin)"
            ]
        ],
        "sklearn/ensemble/weight_boosting.py": [
            [
                "\"\"\"Weight Boosting\n",
                "\n",
                "This module contains weight boosting estimators for both classification and\n",
                "regression.\n",
                "\n",
                "The module structure is the following:\n",
                "\n",
                "- The ``BaseAdaBoost`` base class implements a common ``fit`` method\n",
                "  for all the estimators in the module. Regression and classification\n",
                "  only differ from each other in the loss function that is optimized.\n",
                "\n",
                "- ``AdaBoostClassifier`` implements adaptive boosting (AdaBoost-SAMME) for\n",
                "  classification problems.\n",
                "\n",
                "- ``AdaBoostRegressor`` implements adaptive boosting (AdaBoost.R2) for\n",
                "  regression problems.\n",
                "\"\"\"\n",
                "\n",
                "# Authors: Noel Dawe <noel@dawe.me>\n",
                "#          Gilles Louppe <g.louppe@gmail.com>\n",
                "# Licence: BSD 3 clause\n",
                "\n",
                "from abc import ABCMeta, abstractmethod\n",
                "\n",
                "import numpy as np\n",
                "from numpy.core.umath_tests import inner1d\n",
                "\n",
                "from .base import BaseEnsemble\n",
                "from ..base import ClassifierMixin, RegressorMixin\n",
                "from ..externals import six\n",
                "from ..externals.six.moves import xrange, zip\n",
                "from ..tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
                "from ..tree._tree import DTYPE\n",
                "from ..utils import array2d, check_arrays, check_random_state, column_or_1d\n",
                "from ..metrics import accuracy_score, r2_score\n",
                "\n",
                "\n",
                "__all__ = [\n",
                "    'AdaBoostClassifier',\n",
                "    'AdaBoostRegressor',\n",
                "]\n",
                "\n",
                "\n",
                "class BaseWeightBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):\n",
                "    \"\"\"Base class for AdaBoost estimators.\n",
                "\n",
                "    Warning: This class should not be used directly. Use derived classes\n",
                "    instead.\n",
                "    \"\"\"\n",
                "\n",
                "    @abstractmethod\n",
                "    def __init__(self,\n",
                "                 base_estimator=None,\n",
                "                 n_estimators=50,\n",
                "                 estimator_params=tuple(),\n",
                "                 learning_rate=1.,\n",
                "                 random_state=None):\n",
                "\n",
                "        super(BaseWeightBoosting, self).__init__(\n",
                "            base_estimator=base_estimator,\n",
                "            n_estimators=n_estimators,\n",
                "            estimator_params=estimator_params)\n",
                "\n",
                "        self.learning_rate = learning_rate\n",
                "        self.random_state = random_state\n",
                "\n",
                "    def fit(self, X, y, sample_weight=None):\n",
                "        \"\"\"Build a boosted classifier/regressor from the training set (X, y).\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The training input samples.\n",
                "\n",
                "        y : array-like of shape = [n_samples]\n",
                "            The target values (class labels in classification, real numbers in\n",
                "            regression).\n",
                "\n",
                "        sample_weight : array-like of shape = [n_samples], optional\n",
                "            Sample weights. If None, the sample weights are initialized to\n",
                "            1 / n_samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        self : object\n",
                "            Returns self.\n",
                "        \"\"\"\n",
                "        # Check parameters\n",
                "        if self.learning_rate <= 0:\n",
                "            raise ValueError(\"learning_rate must be greater than zero\")\n",
                "\n",
                "        # Check data\n",
                "        X, y = check_arrays(X, y, sparse_format=\"dense\")\n",
                "\n",
                "        y = column_or_1d(y, warn=True)\n",
                "\n",
                "        if ((getattr(X, \"dtype\", None) != DTYPE) or\n",
                "                (X.ndim != 2) or (not X.flags.contiguous)):\n",
                "            X = np.ascontiguousarray(array2d(X), dtype=DTYPE)\n",
                "\n",
                "        if sample_weight is None:\n",
                "            # Initialize weights to 1 / n_samples\n",
                "            sample_weight = np.empty(X.shape[0], dtype=np.float)\n",
                "            sample_weight[:] = 1. / X.shape[0]\n",
                "        else:\n",
                "            # Normalize existing weights\n",
                "            sample_weight = np.copy(sample_weight) / sample_weight.sum()\n",
                "\n",
                "            # Check that the sample weights sum is positive\n",
                "            if sample_weight.sum() <= 0:\n",
                "                raise ValueError(\n",
                "                    \"Attempting to fit with a non-positive \"\n",
                "                    \"weighted number of samples.\")\n",
                "\n",
                "        # Check parameters\n",
                "        self._validate_estimator()\n",
                "\n",
                "        # Clear any previous fit results\n",
                "        self.estimators_ = []\n",
                "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float)\n",
                "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float)\n",
                "\n",
                "        for iboost in xrange(self.n_estimators):\n",
                "            # Boosting step\n",
                "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
                "                iboost,\n",
                "                X, y,\n",
                "                sample_weight)\n",
                "\n",
                "            # Early termination\n",
                "            if sample_weight is None:\n",
                "                break\n",
                "\n",
                "            self.estimator_weights_[iboost] = estimator_weight\n",
                "            self.estimator_errors_[iboost] = estimator_error\n",
                "\n",
                "            # Stop if error is zero\n",
                "            if estimator_error == 0:\n",
                "                break\n",
                "\n",
                "            sample_weight_sum = np.sum(sample_weight)\n",
                "\n",
                "            # Stop if the sum of sample weights has become non-positive\n",
                "            if sample_weight_sum <= 0:\n",
                "                break\n",
                "\n",
                "            if iboost < self.n_estimators - 1:\n",
                "                # Normalize\n",
                "                sample_weight /= sample_weight_sum\n",
                "\n",
                "        return self\n",
                "\n",
                "    def _check_fitted(self):\n",
                "        if not hasattr(self, \"estimators_\"):\n",
                "            raise ValueError(\"call fit first\")\n",
                "\n",
                "    @abstractmethod\n",
                "    def _boost(self, iboost, X, y, sample_weight):\n",
                "        \"\"\"Implement a single boost.\n",
                "\n",
                "        Warning: This method needs to be overriden by subclasses.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        iboost : int\n",
                "            The index of the current boost iteration.\n",
                "\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The training input samples.\n",
                "\n",
                "        y : array-like of shape = [n_samples]\n",
                "            The target values (class labels).\n",
                "\n",
                "        sample_weight : array-like of shape = [n_samples]\n",
                "            The current sample weights.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        sample_weight : array-like of shape = [n_samples] or None\n",
                "            The reweighted sample weights.\n",
                "            If None then boosting has terminated early.\n",
                "\n",
                "        estimator_weight : float\n",
                "            The weight for the current boost.\n",
                "            If None then boosting has terminated early.\n",
                "\n",
                "        error : float\n",
                "            The classification error for the current boost.\n",
                "            If None then boosting has terminated early.\n",
                "        \"\"\"\n",
                "        pass\n",
                "\n"
            ],
            {
                "type": "replace",
                "before": [
                    "    def staged_score(self, X, y):\n"
                ],
                "after": [
                    "    def staged_score(self, X, y, sample_weight=None):\n"
                ],
                "parent_version_range": {
                    "start": 192,
                    "end": 193
                },
                "child_version_range": {
                    "start": 192,
                    "end": 193
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "BaseWeightBoosting",
                        "signature": "class BaseWeightBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):",
                        "at_line": 43
                    },
                    {
                        "type": "function",
                        "name": "staged_score",
                        "signature": "def staged_score(self, X, y):",
                        "at_line": 192
                    }
                ],
                "idx": 6,
                "hunk_diff": "File: sklearn/ensemble/weight_boosting.py\nCode:\n           class BaseWeightBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):\n               ...\n189 189            \"\"\"\n190 190            pass\n191 191    \n192      -     def staged_score(self, X, y):\n    192  +     def staged_score(self, X, y, sample_weight=None):\n193 193            \"\"\"Return staged scores for X, y.\n194 194    \n195 195            This generator method yields the ensemble score after each iteration of\n         ...\n",
                "file_path": "sklearn/ensemble/weight_boosting.py",
                "identifiers_before": [
                    "X",
                    "self",
                    "staged_score",
                    "y"
                ],
                "identifiers_after": [
                    "X",
                    "sample_weight",
                    "self",
                    "staged_score",
                    "y"
                ],
                "prefix": [
                    "        \"\"\"\n",
                    "        pass\n",
                    "\n"
                ],
                "suffix": [
                    "        \"\"\"Return staged scores for X, y.\n",
                    "\n",
                    "        This generator method yields the ensemble score after each iteration of\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [
                    {
                        "to_hunk_idx": 8,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 192,
                                    "column": 30
                                },
                                "end": {
                                    "line": 192,
                                    "column": 31
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 9,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 192,
                                    "column": 30
                                },
                                "end": {
                                    "line": 192,
                                    "column": 31
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_callee": [],
                "head_dependency_caller": [
                    {
                        "to_hunk_idx": 8,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 192,
                                    "column": 30
                                },
                                "end": {
                                    "line": 192,
                                    "column": 31
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 9,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 192,
                                    "column": 30
                                },
                                "end": {
                                    "line": 192,
                                    "column": 31
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 8,
                        "detail": {
                            "identifier": "sample_weight",
                            "position": {
                                "start": {
                                    "line": 192,
                                    "column": 33
                                },
                                "end": {
                                    "line": 192,
                                    "column": 46
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 9,
                        "detail": {
                            "identifier": "sample_weight",
                            "position": {
                                "start": {
                                    "line": 192,
                                    "column": 33
                                },
                                "end": {
                                    "line": 192,
                                    "column": 46
                                }
                            },
                            "type": "identifier",
                            "kind": "parameter",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 6,
                            "dependency_checked": true
                        }
                    }
                ],
                "other_clones": [
                    0,
                    3
                ]
            },
            [
                "        \"\"\"Return staged scores for X, y.\n",
                "\n",
                "        This generator method yields the ensemble score after each iteration of\n",
                "        boosting and therefore allows monitoring, such as to determine the\n",
                "        score on a test set after each boost.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like, shape = [n_samples, n_features]\n",
                "            Training set.\n",
                "\n",
                "        y : array-like, shape = [n_samples]\n",
                "            Labels for X.\n",
                "\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        sample_weight : array-like, shape = [n_samples], optional\n",
                    "            Sample weights.\n",
                    "\n"
                ],
                "parent_version_range": {
                    "start": 207,
                    "end": 207
                },
                "child_version_range": {
                    "start": 207,
                    "end": 210
                },
                "control_flow": [],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "BaseWeightBoosting",
                        "signature": "class BaseWeightBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):",
                        "at_line": 43
                    },
                    {
                        "type": "function",
                        "name": "staged_score",
                        "signature": "def staged_score(self, X, y):",
                        "at_line": 192
                    }
                ],
                "idx": 7,
                "hunk_diff": "File: sklearn/ensemble/weight_boosting.py\nCode:\n           class BaseWeightBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):\n               ...\n               def staged_score(self, X, y):\n                   ...\n204 204            y : array-like, shape = [n_samples]\n205 205                Labels for X.\n206 206    \n    207  +         sample_weight : array-like, shape = [n_samples], optional\n    208  +             Sample weights.\n    209  + \n207 210            Returns\n208 211            -------\n209 212            z : float\n         ...\n",
                "file_path": "sklearn/ensemble/weight_boosting.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "Sample",
                    "array",
                    "like",
                    "n_samples",
                    "optional",
                    "sample_weight",
                    "shape",
                    "weights"
                ],
                "prefix": [
                    "        y : array-like, shape = [n_samples]\n",
                    "            Labels for X.\n",
                    "\n"
                ],
                "suffix": [
                    "        Returns\n",
                    "        -------\n",
                    "        z : float\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    1,
                    4
                ]
            },
            [
                "        Returns\n",
                "        -------\n",
                "        z : float\n",
                "        \"\"\"\n",
                "        for y_pred in self.staged_predict(X):\n",
                "            if isinstance(self, ClassifierMixin):\n"
            ],
            {
                "type": "replace",
                "before": [
                    "                yield accuracy_score(y, y_pred)\n"
                ],
                "after": [
                    "                yield accuracy_score(y, y_pred, sample_weight=sample_weight)\n"
                ],
                "parent_version_range": {
                    "start": 213,
                    "end": 214
                },
                "child_version_range": {
                    "start": 216,
                    "end": 217
                },
                "control_flow": [
                    {
                        "type": "for_statement",
                        "statement": "for y_pred in self.staged_predict(X):",
                        "start_line": 211,
                        "end_line": 215
                    },
                    {
                        "type": "if_statement",
                        "statement": "if isinstance(self, ClassifierMixin):",
                        "start_line": 212,
                        "end_line": 215
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "BaseWeightBoosting",
                        "signature": "class BaseWeightBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):",
                        "at_line": 43
                    },
                    {
                        "type": "function",
                        "name": "staged_score",
                        "signature": "def staged_score(self, X, y):",
                        "at_line": 192
                    }
                ],
                "idx": 8,
                "hunk_diff": "File: sklearn/ensemble/weight_boosting.py\nCode:\n           class BaseWeightBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):\n               ...\n               def staged_score(self, X, y):\n                   ...\n210 213            \"\"\"\n211 214            for y_pred in self.staged_predict(X):\n212 215                if isinstance(self, ClassifierMixin):\n213      -                 yield accuracy_score(y, y_pred)\n    216  +                 yield accuracy_score(y, y_pred, sample_weight=sample_weight)\n214 217                else:\n         ...\n",
                "file_path": "sklearn/ensemble/weight_boosting.py",
                "identifiers_before": [
                    "accuracy_score",
                    "y",
                    "y_pred"
                ],
                "identifiers_after": [
                    "accuracy_score",
                    "sample_weight",
                    "y",
                    "y_pred"
                ],
                "prefix": [
                    "        \"\"\"\n",
                    "        for y_pred in self.staged_predict(X):\n",
                    "            if isinstance(self, ClassifierMixin):\n"
                ],
                "suffix": [
                    "            else:\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 213,
                                    "column": 37
                                },
                                "end": {
                                    "line": 213,
                                    "column": 38
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 8,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 216,
                                    "column": 37
                                },
                                "end": {
                                    "line": 216,
                                    "column": 38
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 8,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "sample_weight",
                            "position": {
                                "start": {
                                    "line": 216,
                                    "column": 62
                                },
                                "end": {
                                    "line": 216,
                                    "column": 75
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 8,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    2,
                    5,
                    9
                ]
            },
            [
                "            else:\n"
            ],
            {
                "type": "replace",
                "before": [
                    "                yield r2_score(y, y_pred)\n"
                ],
                "after": [
                    "                yield r2_score(y, y_pred, sample_weight=sample_weight)\n"
                ],
                "parent_version_range": {
                    "start": 215,
                    "end": 216
                },
                "child_version_range": {
                    "start": 218,
                    "end": 219
                },
                "control_flow": [
                    {
                        "type": "for_statement",
                        "statement": "for y_pred in self.staged_predict(X):",
                        "start_line": 211,
                        "end_line": 215
                    },
                    {
                        "type": "if_statement",
                        "statement": "if isinstance(self, ClassifierMixin):",
                        "start_line": 212,
                        "end_line": 215
                    },
                    {
                        "type": "else_clause",
                        "statement": "else:",
                        "start_line": 214,
                        "end_line": 215
                    }
                ],
                "structural_path": [
                    {
                        "type": "class",
                        "name": "BaseWeightBoosting",
                        "signature": "class BaseWeightBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):",
                        "at_line": 43
                    },
                    {
                        "type": "function",
                        "name": "staged_score",
                        "signature": "def staged_score(self, X, y):",
                        "at_line": 192
                    }
                ],
                "idx": 9,
                "hunk_diff": "File: sklearn/ensemble/weight_boosting.py\nCode:\n           class BaseWeightBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):\n               ...\n               def staged_score(self, X, y):\n                   ...\n214 217                else:\n215      -                 yield r2_score(y, y_pred)\n    218  +                 yield r2_score(y, y_pred, sample_weight=sample_weight)\n216 219    \n217 220        @property\n218 221        def feature_importances_(self):\n         ...\n",
                "file_path": "sklearn/ensemble/weight_boosting.py",
                "identifiers_before": [
                    "r2_score",
                    "y",
                    "y_pred"
                ],
                "identifiers_after": [
                    "r2_score",
                    "sample_weight",
                    "y",
                    "y_pred"
                ],
                "prefix": [
                    "            else:\n"
                ],
                "suffix": [
                    "\n",
                    "    @property\n",
                    "    def feature_importances_(self):\n"
                ],
                "base_dependency_callee": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 215,
                                    "column": 31
                                },
                                "end": {
                                    "line": 215,
                                    "column": 32
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 9,
                            "dependency_checked": true
                        }
                    }
                ],
                "base_dependency_caller": [],
                "head_dependency_callee": [
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "y",
                            "position": {
                                "start": {
                                    "line": 218,
                                    "column": 31
                                },
                                "end": {
                                    "line": 218,
                                    "column": 32
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 9,
                            "dependency_checked": true
                        }
                    },
                    {
                        "to_hunk_idx": 6,
                        "detail": {
                            "identifier": "sample_weight",
                            "position": {
                                "start": {
                                    "line": 218,
                                    "column": 56
                                },
                                "end": {
                                    "line": 218,
                                    "column": 69
                                }
                            },
                            "type": "identifier",
                            "kind": "unknown",
                            "abs_file_path": "/data2/chenyan/repos/scikit-learn/sklearn/ensemble/weight_boosting.py",
                            "hunk_idx": 9,
                            "dependency_checked": true
                        }
                    }
                ],
                "head_dependency_caller": [],
                "other_clones": [
                    2,
                    5,
                    8
                ]
            },
            [
                "\n",
                "    @property\n",
                "    def feature_importances_(self):\n",
                "        \"\"\"Return the feature importances (the higher, the more important the\n",
                "           feature).\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        feature_importances_ : array, shape = [n_features]\n",
                "        \"\"\"\n",
                "        if self.estimators_ is None or len(self.estimators_) == 0:\n",
                "            raise ValueError(\"Estimator not fitted, \"\n",
                "                             \"call `fit` before `feature_importances_`.\")\n",
                "\n",
                "        try:\n",
                "            norm = self.estimator_weights_.sum()\n",
                "            return (sum(weight * clf.feature_importances_ for weight, clf\n",
                "                    in zip(self.estimator_weights_, self.estimators_))\n",
                "                    / norm)\n",
                "\n",
                "        except AttributeError:\n",
                "            raise AttributeError(\n",
                "                \"Unable to compute feature importances \"\n",
                "                \"since base_estimator does not have a \"\n",
                "                \"feature_importances_ attribute\")\n",
                "\n",
                "\n",
                "def _samme_proba(estimator, n_classes, X):\n",
                "    \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
                "\n",
                "    References\n",
                "    ----------\n",
                "    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
                "\n",
                "    \"\"\"\n",
                "    proba = estimator.predict_proba(X)\n",
                "\n",
                "    # Displace zero probabilities so the log is defined.\n",
                "    # Also fix negative elements which may occur with\n",
                "    # negative sample weights.\n",
                "    proba[proba <= 0] = 1e-5\n",
                "    log_proba = np.log(proba)\n",
                "\n",
                "    return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
                "                           * log_proba.sum(axis=1)[:, np.newaxis])\n",
                "\n",
                "\n",
                "class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):\n",
                "    \"\"\"An AdaBoost classifier.\n",
                "\n",
                "    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
                "    classifier on the original dataset and then fits additional copies of the\n",
                "    classifier on the same dataset but where the weights of incorrectly\n",
                "    classified instances are adjusted such that subsequent classifiers focus\n",
                "    more on difficult cases.\n",
                "\n",
                "    This class implements the algorithm known as AdaBoost-SAMME [2].\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
                "        The base estimator from which the boosted ensemble is built.\n",
                "        Support for sample weighting is required, as well as proper `classes_`\n",
                "        and `n_classes_` attributes.\n",
                "\n",
                "    n_estimators : integer, optional (default=50)\n",
                "        The maximum number of estimators at which boosting is terminated.\n",
                "        In case of perfect fit, the learning procedure is stopped early.\n",
                "\n",
                "    learning_rate : float, optional (default=1.)\n",
                "        Learning rate shrinks the contribution of each classifier by\n",
                "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
                "        ``n_estimators``.\n",
                "\n",
                "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
                "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
                "        ``base_estimator`` must support calculation of class probabilities.\n",
                "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
                "        The SAMME.R algorithm typically converges faster than SAMME,\n",
                "        achieving a lower test error with fewer boosting iterations.\n",
                "\n",
                "    random_state : int, RandomState instance or None, optional (default=None)\n",
                "        If int, random_state is the seed used by the random number generator;\n",
                "        If RandomState instance, random_state is the random number generator;\n",
                "        If None, the random number generator is the RandomState instance used\n",
                "        by `np.random`.\n",
                "\n",
                "    Attributes\n",
                "    ----------\n",
                "    `estimators_` : list of classifiers\n",
                "        The collection of fitted sub-estimators.\n",
                "\n",
                "    `classes_` : array of shape = [n_classes]\n",
                "        The classes labels.\n",
                "\n",
                "    `n_classes_` : int\n",
                "        The number of classes.\n",
                "\n",
                "    `estimator_weights_` : array of floats\n",
                "        Weights for each estimator in the boosted ensemble.\n",
                "\n",
                "    `estimator_errors_` : array of floats\n",
                "        Classification error for each estimator in the boosted\n",
                "        ensemble.\n",
                "\n",
                "    `feature_importances_` : array of shape = [n_features]\n",
                "        The feature importances if supported by the ``base_estimator``.\n",
                "\n",
                "    See also\n",
                "    --------\n",
                "    AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier\n",
                "\n",
                "    References\n",
                "    ----------\n",
                "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
                "           on-Line Learning and an Application to Boosting\", 1995.\n",
                "\n",
                "    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
                "\n",
                "    \"\"\"\n",
                "    def __init__(self,\n",
                "                 base_estimator=None,\n",
                "                 n_estimators=50,\n",
                "                 learning_rate=1.,\n",
                "                 algorithm='SAMME.R',\n",
                "                 random_state=None):\n",
                "\n",
                "        super(AdaBoostClassifier, self).__init__(\n",
                "            base_estimator=base_estimator,\n",
                "            n_estimators=n_estimators,\n",
                "            learning_rate=learning_rate,\n",
                "            random_state=random_state)\n",
                "\n",
                "        self.algorithm = algorithm\n",
                "\n",
                "    def fit(self, X, y, sample_weight=None):\n",
                "        \"\"\"Build a boosted classifier from the training set (X, y).\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The training input samples.\n",
                "\n",
                "        y : array-like of shape = [n_samples]\n",
                "            The target values (class labels).\n",
                "\n",
                "        sample_weight : array-like of shape = [n_samples], optional\n",
                "            Sample weights. If None, the sample weights are initialized to\n",
                "            ``1 / n_samples``.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        self : object\n",
                "            Returns self.\n",
                "        \"\"\"\n",
                "        # Check that algorithm is supported\n",
                "        if self.algorithm not in ('SAMME', 'SAMME.R'):\n",
                "            raise ValueError(\"algorithm %s is not supported\"\n",
                "                             % self.algorithm)\n",
                "\n",
                "        # Fit\n",
                "        return super(AdaBoostClassifier, self).fit(X, y, sample_weight)\n",
                "\n",
                "    def _validate_estimator(self):\n",
                "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
                "        super(AdaBoostClassifier, self)._validate_estimator(\n",
                "            default=DecisionTreeClassifier(max_depth=1))\n",
                "\n",
                "        #  SAMME-R requires predict_proba-enabled base estimators\n",
                "        if self.algorithm == 'SAMME.R':\n",
                "            if not hasattr(self.base_estimator_, 'predict_proba'):\n",
                "                raise TypeError(\n",
                "                    \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n",
                "                    \"that the weak learner supports the calculation of class \"\n",
                "                    \"probabilities with a predict_proba method.\\n\"\n",
                "                    \"Please change the base estimator or set \"\n",
                "                    \"algorithm='SAMME' instead.\")\n",
                "\n",
                "    def _boost(self, iboost, X, y, sample_weight):\n",
                "        \"\"\"Implement a single boost.\n",
                "\n",
                "        Perform a single boost according to the real multi-class SAMME.R\n",
                "        algorithm or to the discrete SAMME algorithm and return the updated\n",
                "        sample weights.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        iboost : int\n",
                "            The index of the current boost iteration.\n",
                "\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The training input samples.\n",
                "\n",
                "        y : array-like of shape = [n_samples]\n",
                "            The target values (class labels).\n",
                "\n",
                "        sample_weight : array-like of shape = [n_samples]\n",
                "            The current sample weights.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        sample_weight : array-like of shape = [n_samples] or None\n",
                "            The reweighted sample weights.\n",
                "            If None then boosting has terminated early.\n",
                "\n",
                "        estimator_weight : float\n",
                "            The weight for the current boost.\n",
                "            If None then boosting has terminated early.\n",
                "\n",
                "        estimator_error : float\n",
                "            The classification error for the current boost.\n",
                "            If None then boosting has terminated early.\n",
                "        \"\"\"\n",
                "        if self.algorithm == 'SAMME.R':\n",
                "            return self._boost_real(iboost, X, y, sample_weight)\n",
                "\n",
                "        else:  # elif self.algorithm == \"SAMME\":\n",
                "            return self._boost_discrete(iboost, X, y, sample_weight)\n",
                "\n",
                "    def _boost_real(self, iboost, X, y, sample_weight):\n",
                "        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n",
                "        estimator = self._make_estimator()\n",
                "\n",
                "        try:\n",
                "            estimator.set_params(random_state=self.random_state)\n",
                "        except ValueError:\n",
                "            pass\n",
                "\n",
                "        estimator.fit(X, y, sample_weight=sample_weight)\n",
                "\n",
                "        y_predict_proba = estimator.predict_proba(X)\n",
                "\n",
                "        if iboost == 0:\n",
                "            self.classes_ = getattr(estimator, 'classes_', None)\n",
                "            self.n_classes_ = len(self.classes_)\n",
                "\n",
                "        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),\n",
                "                                       axis=0)\n",
                "\n",
                "        # Instances incorrectly classified\n",
                "        incorrect = y_predict != y\n",
                "\n",
                "        # Error fraction\n",
                "        estimator_error = np.mean(\n",
                "            np.average(incorrect, weights=sample_weight, axis=0))\n",
                "\n",
                "        # Stop if classification is perfect\n",
                "        if estimator_error <= 0:\n",
                "            return sample_weight, 1., 0.\n",
                "\n",
                "        # Construct y coding as described in Zhu et al [2]:\n",
                "        #\n",
                "        #    y_k = 1 if c == k else -1 / (K - 1)\n",
                "        #\n",
                "        # where K == n_classes_ and c, k in [0, K) are indices along the second\n",
                "        # axis of the y coding with c being the index corresponding to the true\n",
                "        # class label.\n",
                "        n_classes = self.n_classes_\n",
                "        classes = self.classes_\n",
                "        y_codes = np.array([-1. / (n_classes - 1), 1.])\n",
                "        y_coding = y_codes.take(classes == y[:, np.newaxis])\n",
                "\n",
                "        # Displace zero probabilities so the log is defined.\n",
                "        # Also fix negative elements which may occur with\n",
                "        # negative sample weights.\n",
                "        y_predict_proba[y_predict_proba <= 0] = 1e-5\n",
                "\n",
                "        # Boost weight using multi-class AdaBoost SAMME.R alg\n",
                "        estimator_weight = (-1. * self.learning_rate\n",
                "                                * (((n_classes - 1.) / n_classes) *\n",
                "                                   inner1d(y_coding, np.log(y_predict_proba))))\n",
                "\n",
                "        # Only boost the weights if it will fit again\n",
                "        if not iboost == self.n_estimators - 1:\n",
                "            # Only boost positive weights\n",
                "            sample_weight *= np.exp(estimator_weight *\n",
                "                                    ((sample_weight > 0) |\n",
                "                                     (estimator_weight < 0)))\n",
                "\n",
                "        return sample_weight, 1., estimator_error\n",
                "\n",
                "    def _boost_discrete(self, iboost, X, y, sample_weight):\n",
                "        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n",
                "        estimator = self._make_estimator()\n",
                "\n",
                "        try:\n",
                "            estimator.set_params(random_state=self.random_state)\n",
                "        except ValueError:\n",
                "            pass\n",
                "\n",
                "        estimator.fit(X, y, sample_weight=sample_weight)\n",
                "\n",
                "        y_predict = estimator.predict(X)\n",
                "\n",
                "        if iboost == 0:\n",
                "            self.classes_ = getattr(estimator, 'classes_', None)\n",
                "            self.n_classes_ = len(self.classes_)\n",
                "\n",
                "        # Instances incorrectly classified\n",
                "        incorrect = y_predict != y\n",
                "\n",
                "        # Error fraction\n",
                "        estimator_error = np.mean(\n",
                "            np.average(incorrect, weights=sample_weight, axis=0))\n",
                "\n",
                "        # Stop if classification is perfect\n",
                "        if estimator_error <= 0:\n",
                "            return sample_weight, 1., 0.\n",
                "\n",
                "        n_classes = self.n_classes_\n",
                "\n",
                "        # Stop if the error is at least as bad as random guessing\n",
                "        if estimator_error >= 1. - (1. / n_classes):\n",
                "            self.estimators_.pop(-1)\n",
                "            return None, None, None\n",
                "\n",
                "        # Boost weight using multi-class AdaBoost SAMME alg\n",
                "        estimator_weight = self.learning_rate * (\n",
                "            np.log((1. - estimator_error) / estimator_error) +\n",
                "            np.log(n_classes - 1.))\n",
                "\n",
                "        # Only boost the weights if I will fit again\n",
                "        if not iboost == self.n_estimators - 1:\n",
                "            # Only boost positive weights\n",
                "            sample_weight *= np.exp(estimator_weight * incorrect *\n",
                "                                    ((sample_weight > 0) |\n",
                "                                     (estimator_weight < 0)))\n",
                "\n",
                "        return sample_weight, estimator_weight, estimator_error\n",
                "\n",
                "    def predict(self, X):\n",
                "        \"\"\"Predict classes for X.\n",
                "\n",
                "        The predicted class of an input sample is computed as the weighted mean\n",
                "        prediction of the classifiers in the ensemble.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The input samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        y : array of shape = [n_samples]\n",
                "            The predicted classes.\n",
                "        \"\"\"\n",
                "        pred = self.decision_function(X)\n",
                "\n",
                "        if self.n_classes_ == 2:\n",
                "            return self.classes_.take(pred > 0, axis=0)\n",
                "\n",
                "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
                "\n",
                "    def staged_predict(self, X):\n",
                "        \"\"\"Return staged predictions for X.\n",
                "\n",
                "        The predicted class of an input sample is computed as the weighted mean\n",
                "        prediction of the classifiers in the ensemble.\n",
                "\n",
                "        This generator method yields the ensemble prediction after each\n",
                "        iteration of boosting and therefore allows monitoring, such as to\n",
                "        determine the prediction on a test set after each boost.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The input samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        y : generator of array, shape = [n_samples]\n",
                "            The predicted classes.\n",
                "        \"\"\"\n",
                "        n_classes = self.n_classes_\n",
                "        classes = self.classes_\n",
                "\n",
                "        if n_classes == 2:\n",
                "            for pred in self.staged_decision_function(X):\n",
                "                yield np.array(classes.take(pred > 0, axis=0))\n",
                "\n",
                "        else:\n",
                "            for pred in self.staged_decision_function(X):\n",
                "                yield np.array(classes.take(\n",
                "                    np.argmax(pred, axis=1), axis=0))\n",
                "\n",
                "    def decision_function(self, X):\n",
                "        \"\"\"Compute the decision function of ``X``.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The input samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        score : array, shape = [n_samples, k]\n",
                "            The decision function of the input samples. The order of\n",
                "            outputs is the same of that of the `classes_` attribute.\n",
                "            Binary classification is a special cases with ``k == 1``,\n",
                "            otherwise ``k==n_classes``. For binary classification,\n",
                "            values closer to -1 or 1 mean more like the first or second\n",
                "            class in ``classes_``, respectively.\n",
                "        \"\"\"\n",
                "        self._check_fitted()\n",
                "        X = np.asarray(X)\n",
                "\n",
                "        n_classes = self.n_classes_\n",
                "        classes = self.classes_[:, np.newaxis]\n",
                "        pred = None\n",
                "\n",
                "        if self.algorithm == 'SAMME.R':\n",
                "            # The weights are all 1. for SAMME.R\n",
                "            pred = sum(_samme_proba(estimator, n_classes, X)\n",
                "                       for estimator in self.estimators_)\n",
                "        else:   # self.algorithm == \"SAMME\"\n",
                "            pred = sum((estimator.predict(X) == classes).T * w\n",
                "                       for estimator, w in zip(self.estimators_,\n",
                "                                               self.estimator_weights_))\n",
                "\n",
                "        pred /= self.estimator_weights_.sum()\n",
                "        if n_classes == 2:\n",
                "            pred[:, 0] *= -1\n",
                "            return pred.sum(axis=1)\n",
                "        return pred\n",
                "\n",
                "    def staged_decision_function(self, X):\n",
                "        \"\"\"Compute decision function of ``X`` for each boosting iteration.\n",
                "\n",
                "        This method allows monitoring (i.e. determine error on testing set)\n",
                "        after each boosting iteration.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The input samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        score : generator of array, shape = [n_samples, k]\n",
                "            The decision function of the input samples. The order of\n",
                "            outputs is the same of that of the `classes_` attribute.\n",
                "            Binary classification is a special cases with ``k == 1``,\n",
                "            otherwise ``k==n_classes``. For binary classification,\n",
                "            values closer to -1 or 1 mean more like the first or second\n",
                "            class in ``classes_``, respectively.\n",
                "        \"\"\"\n",
                "        self._check_fitted()\n",
                "        X = np.asarray(X)\n",
                "\n",
                "        n_classes = self.n_classes_\n",
                "        classes = self.classes_[:, np.newaxis]\n",
                "        pred = None\n",
                "        norm = 0.\n",
                "\n",
                "        for weight, estimator in zip(self.estimator_weights_,\n",
                "                                     self.estimators_):\n",
                "            norm += weight\n",
                "\n",
                "            if self.algorithm == 'SAMME.R':\n",
                "                # The weights are all 1. for SAMME.R\n",
                "                current_pred = _samme_proba(estimator, n_classes, X)\n",
                "            else:  # elif self.algorithm == \"SAMME\":\n",
                "                current_pred = estimator.predict(X)\n",
                "                current_pred = (current_pred == classes).T * weight\n",
                "\n",
                "            if pred is None:\n",
                "                pred = current_pred\n",
                "            else:\n",
                "                pred += current_pred\n",
                "\n",
                "            if n_classes == 2:\n",
                "                tmp_pred = np.copy(pred)\n",
                "                tmp_pred[:, 0] *= -1\n",
                "                yield (tmp_pred / norm).sum(axis=1)\n",
                "            else:\n",
                "                yield pred / norm\n",
                "\n",
                "    def predict_proba(self, X):\n",
                "        \"\"\"Predict class probabilities for X.\n",
                "\n",
                "        The predicted class probabilities of an input sample is computed as\n",
                "        the weighted mean predicted class probabilities of the classifiers\n",
                "        in the ensemble.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The input samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        p : array of shape = [n_samples]\n",
                "            The class probabilities of the input samples. The order of\n",
                "            outputs is the same of that of the `classes_` attribute.\n",
                "        \"\"\"\n",
                "        X = np.asarray(X)\n",
                "        n_classes = self.n_classes_\n",
                "\n",
                "        if self.algorithm == 'SAMME.R':\n",
                "            # The weights are all 1. for SAMME.R\n",
                "            proba = sum(_samme_proba(estimator, n_classes, X)\n",
                "                        for estimator in self.estimators_)\n",
                "        else:   # self.algorithm == \"SAMME\"\n",
                "            proba = sum(estimator.predict_proba(X) * w\n",
                "                        for estimator, w in zip(self.estimators_,\n",
                "                                                self.estimator_weights_))\n",
                "\n",
                "        proba /= self.estimator_weights_.sum()\n",
                "        proba = np.exp((1. / (n_classes - 1)) * proba)\n",
                "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
                "        normalizer[normalizer == 0.0] = 1.0\n",
                "        proba /= normalizer\n",
                "\n",
                "        return proba\n",
                "\n",
                "    def staged_predict_proba(self, X):\n",
                "        \"\"\"Predict class probabilities for X.\n",
                "\n",
                "        The predicted class probabilities of an input sample is computed as\n",
                "        the weighted mean predicted class probabilities of the classifiers\n",
                "        in the ensemble.\n",
                "\n",
                "        This generator method yields the ensemble predicted class probabilities\n",
                "        after each iteration of boosting and therefore allows monitoring, such\n",
                "        as to determine the predicted class probabilities on a test set after\n",
                "        each boost.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The input samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        p : generator of array, shape = [n_samples]\n",
                "            The class probabilities of the input samples. The order of\n",
                "            outputs is the same of that of the `classes_` attribute.\n",
                "        \"\"\"\n",
                "        n_classes = self.n_classes_\n",
                "        proba = None\n",
                "        norm = 0.\n",
                "\n",
                "        for weight, estimator in zip(self.estimator_weights_,\n",
                "                                     self.estimators_):\n",
                "            norm += weight\n",
                "\n",
                "            if self.algorithm == 'SAMME.R':\n",
                "                # The weights are all 1. for SAMME.R\n",
                "                current_proba = _samme_proba(estimator, n_classes, X)\n",
                "            else:  # elif self.algorithm == \"SAMME\":\n",
                "                current_proba = estimator.predict_proba(X) * weight\n",
                "\n",
                "            if proba is None:\n",
                "                proba = current_proba\n",
                "            else:\n",
                "                proba += current_proba\n",
                "\n",
                "            real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))\n",
                "            normalizer = real_proba.sum(axis=1)[:, np.newaxis]\n",
                "            normalizer[normalizer == 0.0] = 1.0\n",
                "            real_proba /= normalizer\n",
                "\n",
                "            yield real_proba\n",
                "\n",
                "    def predict_log_proba(self, X):\n",
                "        \"\"\"Predict class log-probabilities for X.\n",
                "\n",
                "        The predicted class log-probabilities of an input sample is computed as\n",
                "        the weighted mean predicted class log-probabilities of the classifiers\n",
                "        in the ensemble.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The input samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        p : array of shape = [n_samples]\n",
                "            The class probabilities of the input samples. The order of\n",
                "            outputs is the same of that of the `classes_` attribute.\n",
                "        \"\"\"\n",
                "        return np.log(self.predict_proba(X))\n",
                "\n",
                "\n",
                "class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):\n",
                "    \"\"\"An AdaBoost regressor.\n",
                "\n",
                "    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
                "    regressor on the original dataset and then fits additional copies of the\n",
                "    regressor on the same dataset but where the weights of instances are\n",
                "    adjusted according to the error of the current prediction. As such,\n",
                "    subsequent regressors focus more on difficult cases.\n",
                "\n",
                "    This class implements the algorithm known as AdaBoost.R2 [2].\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    base_estimator : object, optional (default=DecisionTreeRegressor)\n",
                "        The base estimator from which the boosted ensemble is built.\n",
                "        Support for sample weighting is required.\n",
                "\n",
                "    n_estimators : integer, optional (default=50)\n",
                "        The maximum number of estimators at which boosting is terminated.\n",
                "        In case of perfect fit, the learning procedure is stopped early.\n",
                "\n",
                "    learning_rate : float, optional (default=1.)\n",
                "        Learning rate shrinks the contribution of each regressor by\n",
                "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
                "        ``n_estimators``.\n",
                "\n",
                "    loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n",
                "        The loss function to use when updating the weights after each\n",
                "        boosting iteration.\n",
                "\n",
                "    random_state : int, RandomState instance or None, optional (default=None)\n",
                "        If int, random_state is the seed used by the random number generator;\n",
                "        If RandomState instance, random_state is the random number generator;\n",
                "        If None, the random number generator is the RandomState instance used\n",
                "        by `np.random`.\n",
                "\n",
                "    Attributes\n",
                "    ----------\n",
                "    `estimators_` : list of classifiers\n",
                "        The collection of fitted sub-estimators.\n",
                "\n",
                "    `estimator_weights_` : array of floats\n",
                "        Weights for each estimator in the boosted ensemble.\n",
                "\n",
                "    `estimator_errors_` : array of floats\n",
                "        Regression error for each estimator in the boosted ensemble.\n",
                "\n",
                "    `feature_importances_` : array of shape = [n_features]\n",
                "        The feature importances if supported by the ``base_estimator``.\n",
                "\n",
                "    See also\n",
                "    --------\n",
                "    AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor\n",
                "\n",
                "    References\n",
                "    ----------\n",
                "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
                "           on-Line Learning and an Application to Boosting\", 1995.\n",
                "\n",
                "    .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
                "\n",
                "    \"\"\"\n",
                "    def __init__(self,\n",
                "                 base_estimator=None,\n",
                "                 n_estimators=50,\n",
                "                 learning_rate=1.,\n",
                "                 loss='linear',\n",
                "                 random_state=None):\n",
                "\n",
                "        super(AdaBoostRegressor, self).__init__(\n",
                "            base_estimator=base_estimator,\n",
                "            n_estimators=n_estimators,\n",
                "            learning_rate=learning_rate,\n",
                "            random_state=random_state)\n",
                "\n",
                "        self.loss = loss\n",
                "        self.random_state = random_state\n",
                "\n",
                "    def fit(self, X, y, sample_weight=None):\n",
                "        \"\"\"Build a boosted regressor from the training set (X, y).\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The training input samples.\n",
                "\n",
                "        y : array-like of shape = [n_samples]\n",
                "            The target values (real numbers).\n",
                "\n",
                "        sample_weight : array-like of shape = [n_samples], optional\n",
                "            Sample weights. If None, the sample weights are initialized to\n",
                "            1 / n_samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        self : object\n",
                "            Returns self.\n",
                "        \"\"\"\n",
                "        # Check loss\n",
                "        if self.loss not in ('linear', 'square', 'exponential'):\n",
                "            raise ValueError(\n",
                "                \"loss must be 'linear', 'square', or 'exponential'\")\n",
                "\n",
                "        # Fit\n",
                "        return super(AdaBoostRegressor, self).fit(X, y, sample_weight)\n",
                "\n",
                "    def _validate_estimator(self):\n",
                "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
                "        super(AdaBoostRegressor, self)._validate_estimator(\n",
                "            default=DecisionTreeRegressor(max_depth=3))\n",
                "\n",
                "    def _boost(self, iboost, X, y, sample_weight):\n",
                "        \"\"\"Implement a single boost for regression\n",
                "\n",
                "        Perform a single boost according to the AdaBoost.R2 algorithm and\n",
                "        return the updated sample weights.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        iboost : int\n",
                "            The index of the current boost iteration.\n",
                "\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The training input samples.\n",
                "\n",
                "        y : array-like of shape = [n_samples]\n",
                "            The target values (class labels in classification, real numbers in\n",
                "            regression).\n",
                "\n",
                "        sample_weight : array-like of shape = [n_samples]\n",
                "            The current sample weights.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        sample_weight : array-like of shape = [n_samples] or None\n",
                "            The reweighted sample weights.\n",
                "            If None then boosting has terminated early.\n",
                "\n",
                "        estimator_weight : float\n",
                "            The weight for the current boost.\n",
                "            If None then boosting has terminated early.\n",
                "\n",
                "        estimator_error : float\n",
                "            The regression error for the current boost.\n",
                "            If None then boosting has terminated early.\n",
                "        \"\"\"\n",
                "        estimator = self._make_estimator()\n",
                "\n",
                "        try:\n",
                "            estimator.set_params(random_state=self.random_state)\n",
                "        except ValueError:\n",
                "            pass\n",
                "\n",
                "        generator = check_random_state(self.random_state)\n",
                "\n",
                "        # Weighted sampling of the training set with replacement\n",
                "        # For NumPy >= 1.7.0 use np.random.choice\n",
                "        cdf = sample_weight.cumsum()\n",
                "        cdf /= cdf[-1]\n",
                "        uniform_samples = generator.random_sample(X.shape[0])\n",
                "        bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')\n",
                "        # searchsorted returns a scalar\n",
                "        bootstrap_idx = np.array(bootstrap_idx, copy=False)\n",
                "\n",
                "        # Fit on the bootstrapped sample and obtain a prediction\n",
                "        # for all samples in the training set\n",
                "        estimator.fit(X[bootstrap_idx], y[bootstrap_idx])\n",
                "        y_predict = estimator.predict(X)\n",
                "\n",
                "        error_vect = np.abs(y_predict - y)\n",
                "        error_max = error_vect.max()\n",
                "\n",
                "        if error_max != 0.:\n",
                "            error_vect /= error_max\n",
                "\n",
                "        if self.loss == 'square':\n",
                "            error_vect **= 2\n",
                "        elif self.loss == 'exponential':\n",
                "            error_vect = 1. - np.exp(- error_vect)\n",
                "\n",
                "        # Calculate the average loss\n",
                "        estimator_error = (sample_weight * error_vect).sum()\n",
                "\n",
                "        if estimator_error <= 0:\n",
                "            # Stop if fit is perfect\n",
                "            return sample_weight, 1., 0.\n",
                "\n",
                "        elif estimator_error >= 0.5:\n",
                "            # Discard current estimator only if it isn't the only one\n",
                "            if len(self.estimators_) > 1:\n",
                "                self.estimators_.pop(-1)\n",
                "            return None, None, None\n",
                "\n",
                "        beta = estimator_error / (1. - estimator_error)\n",
                "\n",
                "        # Boost weight using AdaBoost.R2 alg\n",
                "        estimator_weight = self.learning_rate * np.log(1. / beta)\n",
                "\n",
                "        if not iboost == self.n_estimators - 1:\n",
                "            sample_weight *= np.power(\n",
                "                beta,\n",
                "                (1. - error_vect) * self.learning_rate)\n",
                "\n",
                "        return sample_weight, estimator_weight, estimator_error\n",
                "\n",
                "    def _get_median_predict(self, X, limit):\n",
                "        # Evaluate predictions of all estimators\n",
                "        predictions = np.array([\n",
                "            est.predict(X) for est in self.estimators_[:limit]]).T\n",
                "\n",
                "        # Sort the predictions\n",
                "        sorted_idx = np.argsort(predictions, axis=1)\n",
                "\n",
                "        # Find index of median prediction for each sample\n",
                "        weight_cdf = self.estimator_weights_[sorted_idx].cumsum(axis=1)\n",
                "        median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]\n",
                "        median_idx = median_or_above.argmax(axis=1)\n",
                "        median_estimators = sorted_idx[np.arange(len(X)), median_idx]\n",
                "\n",
                "        # Return median predictions\n",
                "        return predictions[np.arange(len(X)), median_estimators]\n",
                "\n",
                "    def predict(self, X):\n",
                "        \"\"\"Predict regression value for X.\n",
                "\n",
                "        The predicted regression value of an input sample is computed\n",
                "        as the weighted median prediction of the classifiers in the ensemble.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The input samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        y : array of shape = [n_samples]\n",
                "            The predicted regression values.\n",
                "        \"\"\"\n",
                "        self._check_fitted()\n",
                "        X = np.asarray(X)\n",
                "        return self._get_median_predict(X, len(self.estimators_))\n",
                "\n",
                "    def staged_predict(self, X):\n",
                "        \"\"\"Return staged predictions for X.\n",
                "\n",
                "        The predicted regression value of an input sample is computed\n",
                "        as the weighted median prediction of the classifiers in the ensemble.\n",
                "\n",
                "        This generator method yields the ensemble prediction after each\n",
                "        iteration of boosting and therefore allows monitoring, such as to\n",
                "        determine the prediction on a test set after each boost.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        X : array-like of shape = [n_samples, n_features]\n",
                "            The input samples.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        y : generator of array, shape = [n_samples]\n",
                "            The predicted regression values.\n",
                "        \"\"\"\n",
                "        self._check_fitted()\n",
                "        X = np.asarray(X)\n",
                "        for i, _ in enumerate(self.estimators_, 1):\n",
                "            yield self._get_median_predict(X, limit=i)"
            ]
        ]
    },
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "bi-directional",
            "reason": "def and comment"
        },
        {
            "edit_hunk_pair": [
                0,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "def and return"
        },
        {
            "edit_hunk_pair": [
                0,
                3
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        },
        {
            "edit_hunk_pair": [
                0,
                6
            ],
            "edit_order": "no relation",
            "reason": "different func name to be considered as clone"
        },
        {
            "edit_hunk_pair": [
                1,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "doc and implement"
        },
        {
            "edit_hunk_pair": [
                1,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        },
        {
            "edit_hunk_pair": [
                1,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        },
        {
            "edit_hunk_pair": [
                3,
                4
            ],
            "edit_order": "bi-directional",
            "reason": "def and comment"
        },
        {
            "edit_hunk_pair": [
                3,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "def and return"
        },
        {
            "edit_hunk_pair": [
                4,
                5
            ],
            "edit_order": "bi-directional",
            "reason": "doc and implement"
        },
        {
            "edit_hunk_pair": [
                4,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "clone"
        },
        {
            "edit_hunk_pair": [
                6,
                7
            ],
            "edit_order": "bi-directional",
            "reason": "def and comment"
        },
        {
            "edit_hunk_pair": [
                6,
                8
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                6,
                9
            ],
            "edit_order": "bi-directional",
            "reason": "def and use"
        },
        {
            "edit_hunk_pair": [
                7,
                8
            ],
            "edit_order": "bi-directional",
            "reason": "doc and implement"
        },
        {
            "edit_hunk_pair": [
                7,
                9
            ],
            "edit_order": "bi-directional",
            "reason": "doc and implement"
        }
    ]
}