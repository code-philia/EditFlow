{
    "language": "python",
    "commit_url": "https://github.com/PostHog/posthog/commit/d009c8a5b3e1310947711f543ebd2ee63984a1bc",
    "commit_message": "fix(batch-exports): Use unique per batch stage table name (#24346)",
    "commit_snapshots": {
        "posthog/temporal/batch_exports/bigquery_batch_export.py": [
            [
                "import asyncio\n",
                "import collections.abc\n",
                "import contextlib\n",
                "import dataclasses\n",
                "import datetime as dt\n",
                "import json\n",
                "\n",
                "import pyarrow as pa\n",
                "from django.conf import settings\n",
                "from google.cloud import bigquery\n",
                "from google.oauth2 import service_account\n",
                "from temporalio import activity, workflow\n",
                "from temporalio.common import RetryPolicy\n",
                "\n",
                "from posthog.batch_exports.models import BatchExportRun\n",
                "from posthog.batch_exports.service import (\n",
                "    BatchExportField,\n",
                "    BatchExportModel,\n",
                "    BatchExportSchema,\n",
                "    BigQueryBatchExportInputs,\n",
                ")\n",
                "from posthog.temporal.batch_exports.base import PostHogWorkflow\n",
                "from posthog.temporal.batch_exports.batch_exports import (\n",
                "    FinishBatchExportRunInputs,\n",
                "    RecordsCompleted,\n",
                "    StartBatchExportRunInputs,\n",
                "    default_fields,\n",
                "    execute_batch_export_insert_activity,\n",
                "    get_data_interval,\n",
                "    iter_model_records,\n",
                "    start_batch_export_run,\n",
                ")\n",
                "from posthog.temporal.batch_exports.metrics import (\n",
                "    get_bytes_exported_metric,\n",
                "    get_rows_exported_metric,\n",
                ")\n",
                "from posthog.temporal.batch_exports.temporary_file import (\n",
                "    BatchExportWriter,\n",
                "    FlushCallable,\n",
                "    JSONLBatchExportWriter,\n",
                "    ParquetBatchExportWriter,\n",
                ")\n",
                "from posthog.temporal.batch_exports.utils import (\n",
                "    JsonType,\n",
                "    apeek_first_and_rewind,\n",
                "    cast_record_batch_json_columns,\n",
                "    set_status_to_running_task,\n",
                ")\n",
                "from posthog.temporal.common.clickhouse import get_client\n",
                "from posthog.temporal.common.heartbeat import Heartbeater\n",
                "from posthog.temporal.common.logger import bind_temporal_worker_logger\n",
                "from posthog.temporal.common.utils import (\n",
                "    BatchExportHeartbeatDetails,\n",
                "    should_resume_from_activity_heartbeat,\n",
                ")\n",
                "\n",
                "\n",
                "def get_bigquery_fields_from_record_schema(\n",
                "    record_schema: pa.Schema, known_json_columns: list[str]\n",
                ") -> list[bigquery.SchemaField]:\n",
                "    \"\"\"Generate a list of supported BigQuery fields from PyArrow schema.\n",
                "\n",
                "    This function is used to map custom schemas to BigQuery-supported types. Some loss\n",
                "    of precision is expected.\n",
                "\n",
                "    Arguments:\n",
                "        record_schema: The schema of a PyArrow RecordBatch from which we'll attempt to\n",
                "            derive BigQuery-supported types.\n",
                "        known_json_columns: If a string type field is a known JSON column then use JSON\n",
                "            as its BigQuery type.\n",
                "    \"\"\"\n",
                "    bq_schema: list[bigquery.SchemaField] = []\n",
                "\n",
                "    for name in record_schema.names:\n",
                "        pa_field = record_schema.field(name)\n",
                "\n",
                "        if pa.types.is_string(pa_field.type) or isinstance(pa_field.type, JsonType):\n",
                "            if pa_field.name in known_json_columns:\n",
                "                bq_type = \"JSON\"\n",
                "            else:\n",
                "                bq_type = \"STRING\"\n",
                "\n",
                "        elif pa.types.is_binary(pa_field.type):\n",
                "            bq_type = \"BYTES\"\n",
                "\n",
                "        elif pa.types.is_signed_integer(pa_field.type) or pa.types.is_unsigned_integer(pa_field.type):\n",
                "            # The latter comparison is hoping we don't overflow, but BigQuery doesn't have an uint64 type.\n",
                "            bq_type = \"INT64\"\n",
                "\n",
                "        elif pa.types.is_floating(pa_field.type):\n",
                "            bq_type = \"FLOAT64\"\n",
                "\n",
                "        elif pa.types.is_boolean(pa_field.type):\n",
                "            bq_type = \"BOOL\"\n",
                "\n",
                "        elif pa.types.is_timestamp(pa_field.type):\n",
                "            bq_type = \"TIMESTAMP\"\n",
                "\n",
                "        else:\n",
                "            raise TypeError(f\"Unsupported type: {pa_field.type}\")\n",
                "\n",
                "        bq_schema.append(bigquery.SchemaField(name, bq_type))\n",
                "\n",
                "    return bq_schema\n",
                "\n",
                "\n",
                "@dataclasses.dataclass\n",
                "class BigQueryHeartbeatDetails(BatchExportHeartbeatDetails):\n",
                "    \"\"\"The BigQuery batch export details included in every heartbeat.\"\"\"\n",
                "\n",
                "    pass\n",
                "\n",
                "\n",
                "@dataclasses.dataclass\n",
                "class BigQueryInsertInputs:\n",
                "    \"\"\"Inputs for BigQuery.\"\"\"\n",
                "\n",
                "    team_id: int\n",
                "    project_id: str\n",
                "    dataset_id: str\n",
                "    table_id: str\n",
                "    private_key: str\n",
                "    private_key_id: str\n",
                "    token_uri: str\n",
                "    client_email: str\n",
                "    data_interval_start: str\n",
                "    data_interval_end: str\n",
                "    exclude_events: list[str] | None = None\n",
                "    include_events: list[str] | None = None\n",
                "    use_json_type: bool = False\n",
                "    run_id: str | None = None\n",
                "    is_backfill: bool = False\n",
                "    batch_export_model: BatchExportModel | None = None\n",
                "    # TODO: Remove after updating existing batch exports\n",
                "    batch_export_schema: BatchExportSchema | None = None\n",
                "\n",
                "\n",
                "class BigQueryClient(bigquery.Client):\n",
                "    async def acreate_table(\n",
                "        self,\n",
                "        project_id: str,\n",
                "        dataset_id: str,\n",
                "        table_id: str,\n",
                "        table_schema: list[bigquery.SchemaField],\n",
                "        exists_ok: bool = True,\n",
                "    ) -> bigquery.Table:\n",
                "        \"\"\"Create a table in BigQuery.\"\"\"\n",
                "        fully_qualified_name = f\"{project_id}.{dataset_id}.{table_id}\"\n",
                "        table = bigquery.Table(fully_qualified_name, schema=table_schema)\n",
                "\n",
                "        if \"timestamp\" in [field.name for field in table_schema]:\n",
                "            # TODO: Maybe choosing which column to use as parititoning should be a configuration parameter.\n",
                "            # 'timestamp' is used for backwards compatibility.\n",
                "            table.time_partitioning = bigquery.TimePartitioning(\n",
                "                type_=bigquery.TimePartitioningType.DAY, field=\"timestamp\"\n",
                "            )\n",
                "\n",
                "        table = await asyncio.to_thread(self.create_table, table, exists_ok=exists_ok)\n",
                "\n",
                "        return table\n",
                "\n",
                "    async def adelete_table(\n",
                "        self,\n",
                "        project_id: str,\n",
                "        dataset_id: str,\n",
                "        table_id: str,\n",
                "        table_schema: list[bigquery.SchemaField],\n",
                "        not_found_ok: bool = True,\n",
                "    ) -> None:\n",
                "        \"\"\"Delete a table in BigQuery.\"\"\"\n",
                "        fully_qualified_name = f\"{project_id}.{dataset_id}.{table_id}\"\n",
                "        table = bigquery.Table(fully_qualified_name, schema=table_schema)\n",
                "\n",
                "        await asyncio.to_thread(self.delete_table, table, not_found_ok=not_found_ok)\n",
                "\n",
                "        return None\n",
                "\n",
                "    @contextlib.asynccontextmanager\n",
                "    async def managed_table(\n",
                "        self,\n",
                "        project_id: str,\n",
                "        dataset_id: str,\n",
                "        table_id: str,\n",
                "        table_schema: list[bigquery.SchemaField],\n",
                "        exists_ok: bool = True,\n",
                "        not_found_ok: bool = True,\n",
                "        delete: bool = True,\n",
                "        create: bool = True,\n",
                "    ) -> collections.abc.AsyncGenerator[bigquery.Table, None]:\n",
                "        \"\"\"Manage a table in BigQuery by ensure it exists while in context.\"\"\"\n",
                "        if create is True:\n",
                "            table = await self.acreate_table(project_id, dataset_id, table_id, table_schema, exists_ok)\n",
                "        else:\n",
                "            fully_qualified_name = f\"{project_id}.{dataset_id}.{table_id}\"\n",
                "            table = bigquery.Table(fully_qualified_name, schema=table_schema)\n",
                "\n",
                "        try:\n",
                "            yield table\n",
                "        finally:\n",
                "            if delete is True:\n",
                "                await self.adelete_table(project_id, dataset_id, table_id, table_schema, not_found_ok)\n",
                "\n",
                "    async def amerge_person_tables(\n",
                "        self,\n",
                "        final_table: bigquery.Table,\n",
                "        stage_table: bigquery.Table,\n",
                "        merge_key: collections.abc.Iterable[bigquery.SchemaField],\n",
                "        person_version_key: str = \"person_version\",\n",
                "        person_distinct_id_version_key: str = \"person_distinct_id_version\",\n",
                "    ):\n",
                "        \"\"\"Merge two identical person model tables in BigQuery.\"\"\"\n",
                "        job_config = bigquery.QueryJobConfig()\n",
                "\n",
                "        merge_condition = \"ON \"\n",
                "\n",
                "        for n, field in enumerate(merge_key):\n",
                "            if n > 0:\n",
                "                merge_condition += \" AND \"\n",
                "            merge_condition += f\"final.`{field.name}` = stage.`{field.name}`\"\n",
                "\n",
                "        update_clause = \"\"\n",
                "        values = \"\"\n",
                "        field_names = \"\"\n",
                "        for n, field in enumerate(final_table.schema):\n",
                "            if n > 0:\n",
                "                update_clause += \", \"\n",
                "                values += \", \"\n",
                "                field_names += \", \"\n",
                "\n",
                "            update_clause += f\"final.`{field.name}` = stage.`{field.name}`\"\n",
                "            field_names += f\"`{field.name}`\"\n",
                "            values += f\"stage.`{field.name}`\"\n",
                "\n",
                "        merge_query = f\"\"\"\n",
                "        MERGE `{final_table.full_table_id.replace(\":\", \".\", 1)}` final\n",
                "        USING `{stage_table.full_table_id.replace(\":\", \".\", 1)}` stage\n",
                "        {merge_condition}\n",
                "\n",
                "        WHEN MATCHED AND (stage.`{person_version_key}` > final.`{person_version_key}` OR stage.`{person_distinct_id_version_key}` > final.`{person_distinct_id_version_key}`) THEN\n",
                "            UPDATE SET\n",
                "                {update_clause}\n",
                "        WHEN NOT MATCHED BY TARGET THEN\n",
                "            INSERT ({field_names})\n",
                "            VALUES ({values});\n",
                "        \"\"\"\n",
                "\n",
                "        query_job = self.query(merge_query, job_config=job_config)\n",
                "        return await asyncio.to_thread(query_job.result)\n",
                "\n",
                "    async def load_parquet_file(self, parquet_file, table, table_schema):\n",
                "        \"\"\"Execute a COPY FROM query with given connection to copy contents of parquet_file.\"\"\"\n",
                "        job_config = bigquery.LoadJobConfig(\n",
                "            source_format=\"PARQUET\",\n",
                "            schema=table_schema,\n",
                "        )\n",
                "\n",
                "        load_job = self.load_table_from_file(parquet_file, table, job_config=job_config, rewind=True)\n",
                "        return await asyncio.to_thread(load_job.result)\n",
                "\n",
                "    async def load_jsonl_file(self, jsonl_file, table, table_schema):\n",
                "        \"\"\"Execute a COPY FROM query with given connection to copy contents of jsonl_file.\"\"\"\n",
                "        job_config = bigquery.LoadJobConfig(\n",
                "            source_format=\"NEWLINE_DELIMITED_JSON\",\n",
                "            schema=table_schema,\n",
                "        )\n",
                "\n",
                "        load_job = self.load_table_from_file(jsonl_file, table, job_config=job_config, rewind=True)\n",
                "        return await asyncio.to_thread(load_job.result)\n",
                "\n",
                "\n",
                "@contextlib.contextmanager\n",
                "def bigquery_client(inputs: BigQueryInsertInputs):\n",
                "    \"\"\"Manage a BigQuery client.\"\"\"\n",
                "    credentials = service_account.Credentials.from_service_account_info(\n",
                "        {\n",
                "            \"private_key\": inputs.private_key,\n",
                "            \"private_key_id\": inputs.private_key_id,\n",
                "            \"token_uri\": inputs.token_uri,\n",
                "            \"client_email\": inputs.client_email,\n",
                "            \"project_id\": inputs.project_id,\n",
                "        },\n",
                "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
                "    )\n",
                "    client = BigQueryClient(\n",
                "        project=inputs.project_id,\n",
                "        credentials=credentials,\n",
                "    )\n",
                "\n",
                "    try:\n",
                "        yield client\n",
                "    finally:\n",
                "        client.close()\n",
                "\n",
                "\n",
                "def bigquery_default_fields() -> list[BatchExportField]:\n",
                "    \"\"\"Default fields for a BigQuery batch export.\n",
                "\n",
                "    Starting from the common default fields, we add and tweak some fields for\n",
                "    backwards compatibility.\n",
                "    \"\"\"\n",
                "    batch_export_fields = default_fields()\n",
                "    batch_export_fields.append(\n",
                "        {\n",
                "            \"expression\": \"nullIf(JSONExtractString(properties, '$ip'), '')\",\n",
                "            \"alias\": \"ip\",\n",
                "        }\n",
                "    )\n",
                "    # Fields kept or removed for backwards compatibility with legacy apps schema.\n",
                "    batch_export_fields.append({\"expression\": \"toJSONString(elements_chain)\", \"alias\": \"elements\"})\n",
                "    batch_export_fields.append({\"expression\": \"''\", \"alias\": \"site_url\"})\n",
                "    batch_export_fields.append({\"expression\": \"NOW64()\", \"alias\": \"bq_ingested_timestamp\"})\n",
                "    batch_export_fields.pop(batch_export_fields.index({\"expression\": \"created_at\", \"alias\": \"created_at\"}))\n",
                "\n",
                "    return batch_export_fields\n",
                "\n",
                "\n",
                "@activity.defn\n",
                "async def insert_into_bigquery_activity(inputs: BigQueryInsertInputs) -> RecordsCompleted:\n",
                "    \"\"\"Activity streams data from ClickHouse to BigQuery.\"\"\"\n",
                "    logger = await bind_temporal_worker_logger(team_id=inputs.team_id, destination=\"BigQuery\")\n",
                "    logger.info(\n",
                "        \"Batch exporting range %s - %s to BigQuery: %s.%s.%s\",\n",
                "        inputs.data_interval_start,\n",
                "        inputs.data_interval_end,\n",
                "        inputs.project_id,\n",
                "        inputs.dataset_id,\n",
                "        inputs.table_id,\n",
                "    )\n",
                "\n",
                "    async with (\n",
                "        Heartbeater() as heartbeater,\n",
                "        set_status_to_running_task(run_id=inputs.run_id, logger=logger),\n",
                "        get_client(team_id=inputs.team_id) as client,\n",
                "    ):\n",
                "        if not await client.is_alive():\n",
                "            raise ConnectionError(\"Cannot establish connection to ClickHouse\")\n",
                "\n",
                "        should_resume, details = await should_resume_from_activity_heartbeat(activity, BigQueryHeartbeatDetails, logger)\n",
                "\n",
                "        if should_resume is True and details is not None:\n",
                "            data_interval_start = details.last_inserted_at.isoformat()\n",
                "        else:\n",
                "            data_interval_start = inputs.data_interval_start\n",
                "\n",
                "        model: BatchExportModel | BatchExportSchema | None = None\n",
                "        if inputs.batch_export_schema is None and \"batch_export_model\" in {\n",
                "            field.name for field in dataclasses.fields(inputs)\n",
                "        }:\n",
                "            model = inputs.batch_export_model\n",
                "        else:\n",
                "            model = inputs.batch_export_schema\n",
                "\n",
                "        records_iterator = iter_model_records(\n",
                "            client=client,\n",
                "            model=model,\n",
                "            team_id=inputs.team_id,\n",
                "            interval_start=data_interval_start,\n",
                "            interval_end=inputs.data_interval_end,\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            destination_default_fields=bigquery_default_fields(),\n",
                "            is_backfill=inputs.is_backfill,\n",
                "        )\n",
                "\n",
                "        first_record_batch, records_iterator = await apeek_first_and_rewind(records_iterator)\n",
                "        if first_record_batch is None:\n",
                "            return 0\n",
                "\n",
                "        if inputs.use_json_type is True:\n",
                "            json_type = \"JSON\"\n",
                "            json_columns = [\"properties\", \"set\", \"set_once\", \"person_properties\"]\n",
                "        else:\n",
                "            json_type = \"STRING\"\n",
                "            json_columns = []\n",
                "\n",
                "        first_record_batch = cast_record_batch_json_columns(first_record_batch, json_columns=json_columns)\n",
                "\n",
                "        if model is None or (isinstance(model, BatchExportModel) and model.name == \"events\"):\n",
                "            schema = [\n",
                "                bigquery.SchemaField(\"uuid\", \"STRING\"),\n",
                "                bigquery.SchemaField(\"event\", \"STRING\"),\n",
                "                bigquery.SchemaField(\"properties\", json_type),\n",
                "                bigquery.SchemaField(\"elements\", \"STRING\"),\n",
                "                bigquery.SchemaField(\"set\", json_type),\n",
                "                bigquery.SchemaField(\"set_once\", json_type),\n",
                "                bigquery.SchemaField(\"distinct_id\", \"STRING\"),\n",
                "                bigquery.SchemaField(\"team_id\", \"INT64\"),\n",
                "                bigquery.SchemaField(\"ip\", \"STRING\"),\n",
                "                bigquery.SchemaField(\"site_url\", \"STRING\"),\n",
                "                bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
                "                bigquery.SchemaField(\"bq_ingested_timestamp\", \"TIMESTAMP\"),\n",
                "            ]\n",
                "        else:\n",
                "            column_names = [column for column in first_record_batch.schema.names if column != \"_inserted_at\"]\n",
                "            record_schema = first_record_batch.select(column_names).schema\n",
                "            schema = get_bigquery_fields_from_record_schema(record_schema, known_json_columns=json_columns)\n",
                "\n",
                "        rows_exported = get_rows_exported_metric()\n",
                "        bytes_exported = get_bytes_exported_metric()\n",
                "\n",
                "        # TODO: Expose this as a configuration parameter\n",
                "        # Currently, only allow merging persons model, as it's required.\n",
                "        # Although all exports could potentially benefit from merging, merging can have an impact on cost,\n",
                "        # so users should decide whether to opt-in or not.\n",
                "        requires_merge = (\n",
                "            isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n",
                "        )\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        stage_table_name = f\"stage_{inputs.table_id}\" if requires_merge else inputs.table_id\n"
                ],
                "after": [
                    "        data_interval_end_str = dt.datetime.fromisoformat(inputs.data_interval_end).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                    "        stage_table_name = f\"stage_{inputs.table_id}_{data_interval_end_str}\" if requires_merge else inputs.table_id\n"
                ],
                "parent_version_range": {
                    "start": 407,
                    "end": 408
                },
                "child_version_range": {
                    "start": 407,
                    "end": 409
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "async with (\n        Heartbeater() as heartbeater,\n        set_status_to_running_task(run_id=inputs.run_id, logger=logger),\n        get_client(team_id=inputs.team_id) as client,\n    ):",
                        "start_line": 329,
                        "end_line": 484
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "insert_into_bigquery_activity",
                        "signature": "def insert_into_bigquery_activity(inputs: BigQueryInsertInputs)->RecordsCompleted:",
                        "at_line": 317
                    }
                ],
                "idx": 0,
                "hunk_diff": "File: posthog/temporal/batch_exports/bigquery_batch_export.py\nCode:\n           def insert_into_bigquery_activity(inputs: BigQueryInsertInputs)->RecordsCompleted:\n               ...\n404 404            requires_merge = (\n405 405                isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n406 406            )\n407      -         stage_table_name = f\"stage_{inputs.table_id}\" if requires_merge else inputs.table_id\n    407  +         data_interval_end_str = dt.datetime.fromisoformat(inputs.data_interval_end).strftime(\"%Y-%m-%d_%H-%M-%S\")\n    408  +         stage_table_name = f\"stage_{inputs.table_id}_{data_interval_end_str}\" if requires_merge else inputs.table_id\n408 409    \n409 410            with bigquery_client(inputs) as bq_client:\n410 411                async with (\n         ...\n",
                "file_path": "posthog/temporal/batch_exports/bigquery_batch_export.py",
                "identifiers_before": [
                    "inputs",
                    "requires_merge",
                    "stage_table_name",
                    "table_id"
                ],
                "identifiers_after": [
                    "data_interval_end",
                    "data_interval_end_str",
                    "datetime",
                    "dt",
                    "fromisoformat",
                    "inputs",
                    "requires_merge",
                    "stage_table_name",
                    "strftime",
                    "table_id"
                ],
                "prefix": [
                    "        requires_merge = (\n",
                    "            isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n",
                    "        )\n"
                ],
                "suffix": [
                    "\n",
                    "        with bigquery_client(inputs) as bq_client:\n",
                    "            async with (\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    1,
                    2
                ]
            },
            [
                "\n",
                "        with bigquery_client(inputs) as bq_client:\n",
                "            async with (\n",
                "                bq_client.managed_table(\n",
                "                    inputs.project_id,\n",
                "                    inputs.dataset_id,\n",
                "                    inputs.table_id,\n",
                "                    schema,\n",
                "                    delete=False,\n",
                "                ) as bigquery_table,\n",
                "                bq_client.managed_table(\n",
                "                    inputs.project_id,\n",
                "                    inputs.dataset_id,\n",
                "                    stage_table_name,\n",
                "                    schema,\n",
                "                    create=requires_merge,\n",
                "                    delete=requires_merge,\n",
                "                ) as bigquery_stage_table,\n",
                "            ):\n",
                "\n",
                "                async def flush_to_bigquery(\n",
                "                    local_results_file,\n",
                "                    records_since_last_flush: int,\n",
                "                    bytes_since_last_flush: int,\n",
                "                    flush_counter: int,\n",
                "                    last_inserted_at,\n",
                "                    last: bool,\n",
                "                    error: Exception | None,\n",
                "                ):\n",
                "                    logger.debug(\n",
                "                        \"Loading %s records of size %s bytes\",\n",
                "                        records_since_last_flush,\n",
                "                        bytes_since_last_flush,\n",
                "                    )\n",
                "                    table = bigquery_stage_table if requires_merge else bigquery_table\n",
                "\n",
                "                    await bq_client.load_jsonl_file(local_results_file, table, schema)\n",
                "\n",
                "                    rows_exported.add(records_since_last_flush)\n",
                "                    bytes_exported.add(bytes_since_last_flush)\n",
                "\n",
                "                    heartbeater.details = (str(last_inserted_at),)\n",
                "\n",
                "                record_schema = pa.schema(\n",
                "                    # NOTE: For some reason, some batches set non-nullable fields as non-nullable, whereas other\n",
                "                    # record batches have them as nullable.\n",
                "                    # Until we figure it out, we set all fields to nullable. There are some fields we know\n",
                "                    # are not nullable, but I'm opting for the more flexible option until we out why schemas differ\n",
                "                    # between batches.\n",
                "                    [\n",
                "                        field.with_nullable(True)\n",
                "                        for field in first_record_batch.select([field.name for field in schema]).schema\n",
                "                    ]\n",
                "                )\n",
                "                writer = JSONLBatchExportWriter(\n",
                "                    max_bytes=settings.BATCH_EXPORT_BIGQUERY_UPLOAD_CHUNK_SIZE_BYTES,\n",
                "                    flush_callable=flush_to_bigquery,\n",
                "                )\n",
                "\n",
                "                async with writer.open_temporary_file():\n",
                "                    async for record_batch in records_iterator:\n",
                "                        record_batch = cast_record_batch_json_columns(record_batch, json_columns=json_columns)\n",
                "\n",
                "                        await writer.write_record_batch(record_batch)\n",
                "\n",
                "                if requires_merge:\n",
                "                    merge_key = (\n",
                "                        bigquery.SchemaField(\"team_id\", \"INT64\"),\n",
                "                        bigquery.SchemaField(\"distinct_id\", \"STRING\"),\n",
                "                    )\n",
                "                    await bq_client.amerge_person_tables(\n",
                "                        final_table=bigquery_table,\n",
                "                        stage_table=bigquery_stage_table,\n",
                "                        merge_key=merge_key,\n",
                "                    )\n",
                "\n",
                "                return writer.records_total\n",
                "\n",
                "\n",
                "def get_batch_export_writer(\n",
                "    inputs: BigQueryInsertInputs, flush_callable: FlushCallable, max_bytes: int, schema: pa.Schema | None = None\n",
                ") -> BatchExportWriter:\n",
                "    \"\"\"Return the `BatchExportWriter` corresponding to the inputs for this BigQuery batch export.\"\"\"\n",
                "    writer: BatchExportWriter\n",
                "\n",
                "    if inputs.use_json_type is False:\n",
                "        # JSON field is not supported with Parquet\n",
                "        writer = ParquetBatchExportWriter(\n",
                "            max_bytes=max_bytes,\n",
                "            flush_callable=flush_callable,\n",
                "            schema=schema,\n",
                "        )\n",
                "    else:\n",
                "        writer = JSONLBatchExportWriter(\n",
                "            max_bytes=settings.BATCH_EXPORT_BIGQUERY_UPLOAD_CHUNK_SIZE_BYTES,\n",
                "            flush_callable=flush_callable,\n",
                "        )\n",
                "\n",
                "    return writer\n",
                "\n",
                "\n",
                "@workflow.defn(name=\"bigquery-export\", failure_exception_types=[workflow.NondeterminismError])\n",
                "class BigQueryBatchExportWorkflow(PostHogWorkflow):\n",
                "    \"\"\"A Temporal Workflow to export ClickHouse data into BigQuery.\n",
                "\n",
                "    This Workflow is intended to be executed both manually and by a Temporal\n",
                "    Schedule. When ran by a schedule, `data_interval_end` should be set to\n",
                "    `None` so that we will fetch the end of the interval from the Temporal\n",
                "    search attribute `TemporalScheduledStartTime`.\n",
                "    \"\"\"\n",
                "\n",
                "    @staticmethod\n",
                "    def parse_inputs(inputs: list[str]) -> BigQueryBatchExportInputs:\n",
                "        \"\"\"Parse inputs from the management command CLI.\"\"\"\n",
                "        loaded = json.loads(inputs[0])\n",
                "        return BigQueryBatchExportInputs(**loaded)\n",
                "\n",
                "    @workflow.run\n",
                "    async def run(self, inputs: BigQueryBatchExportInputs):\n",
                "        \"\"\"Workflow implementation to export data to BigQuery.\"\"\"\n",
                "        data_interval_start, data_interval_end = get_data_interval(inputs.interval, inputs.data_interval_end)\n",
                "\n",
                "        start_batch_export_run_inputs = StartBatchExportRunInputs(\n",
                "            team_id=inputs.team_id,\n",
                "            batch_export_id=inputs.batch_export_id,\n",
                "            data_interval_start=data_interval_start.isoformat(),\n",
                "            data_interval_end=data_interval_end.isoformat(),\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            is_backfill=inputs.is_backfill,\n",
                "        )\n",
                "        run_id = await workflow.execute_activity(\n",
                "            start_batch_export_run,\n",
                "            start_batch_export_run_inputs,\n",
                "            start_to_close_timeout=dt.timedelta(minutes=5),\n",
                "            retry_policy=RetryPolicy(\n",
                "                initial_interval=dt.timedelta(seconds=10),\n",
                "                maximum_interval=dt.timedelta(seconds=60),\n",
                "                maximum_attempts=0,\n",
                "                non_retryable_error_types=[\"NotNullViolation\", \"IntegrityError\"],\n",
                "            ),\n",
                "        )\n",
                "\n",
                "        finish_inputs = FinishBatchExportRunInputs(\n",
                "            id=run_id,\n",
                "            batch_export_id=inputs.batch_export_id,\n",
                "            status=BatchExportRun.Status.COMPLETED,\n",
                "            team_id=inputs.team_id,\n",
                "        )\n",
                "\n",
                "        insert_inputs = BigQueryInsertInputs(\n",
                "            team_id=inputs.team_id,\n",
                "            table_id=inputs.table_id,\n",
                "            dataset_id=inputs.dataset_id,\n",
                "            project_id=inputs.project_id,\n",
                "            private_key=inputs.private_key,\n",
                "            private_key_id=inputs.private_key_id,\n",
                "            token_uri=inputs.token_uri,\n",
                "            client_email=inputs.client_email,\n",
                "            data_interval_start=data_interval_start.isoformat(),\n",
                "            data_interval_end=data_interval_end.isoformat(),\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            use_json_type=inputs.use_json_type,\n",
                "            run_id=run_id,\n",
                "            is_backfill=inputs.is_backfill,\n",
                "            batch_export_model=inputs.batch_export_model,\n",
                "            # TODO: Remove after updating existing batch exports.\n",
                "            batch_export_schema=inputs.batch_export_schema,\n",
                "        )\n",
                "\n",
                "        await execute_batch_export_insert_activity(\n",
                "            insert_into_bigquery_activity,\n",
                "            insert_inputs,\n",
                "            interval=inputs.interval,\n",
                "            non_retryable_error_types=[\n",
                "                # Raised on missing permissions.\n",
                "                \"Forbidden\",\n",
                "                # Invalid token.\n",
                "                \"RefreshError\",\n",
                "                # Usually means the dataset or project doesn't exist.\n",
                "                \"NotFound\",\n",
                "                # Raised when something about dataset is wrong (not alphanumeric, too long, etc).\n",
                "                \"BadRequest\",\n",
                "                # Raised when table_id isn't valid. Sadly, `ValueError` is rather generic, but we\n",
                "                # don't anticipate a `ValueError` thrown from our own export code.\n",
                "                \"ValueError\",\n",
                "            ],\n",
                "            finish_inputs=finish_inputs,\n",
                "        )"
            ]
        ],
        "posthog/temporal/batch_exports/postgres_batch_export.py": [
            [
                "import collections.abc\n",
                "import contextlib\n",
                "import csv\n",
                "import dataclasses\n",
                "import datetime as dt\n",
                "import json\n",
                "import typing\n",
                "\n",
                "import psycopg\n",
                "import pyarrow as pa\n",
                "from django.conf import settings\n",
                "from psycopg import sql\n",
                "from temporalio import activity, workflow\n",
                "from temporalio.common import RetryPolicy\n",
                "\n",
                "from posthog.batch_exports.models import BatchExportRun\n",
                "from posthog.batch_exports.service import (\n",
                "    BatchExportField,\n",
                "    BatchExportModel,\n",
                "    BatchExportSchema,\n",
                "    PostgresBatchExportInputs,\n",
                ")\n",
                "from posthog.temporal.batch_exports.base import PostHogWorkflow\n",
                "from posthog.temporal.batch_exports.batch_exports import (\n",
                "    FinishBatchExportRunInputs,\n",
                "    RecordsCompleted,\n",
                "    StartBatchExportRunInputs,\n",
                "    default_fields,\n",
                "    execute_batch_export_insert_activity,\n",
                "    get_data_interval,\n",
                "    iter_model_records,\n",
                "    start_batch_export_run,\n",
                ")\n",
                "from posthog.temporal.batch_exports.metrics import (\n",
                "    get_bytes_exported_metric,\n",
                "    get_rows_exported_metric,\n",
                ")\n",
                "from posthog.temporal.batch_exports.temporary_file import CSVBatchExportWriter\n",
                "from posthog.temporal.batch_exports.utils import (\n",
                "    JsonType,\n",
                "    apeek_first_and_rewind,\n",
                "    cast_record_batch_json_columns,\n",
                "    set_status_to_running_task,\n",
                ")\n",
                "from posthog.temporal.common.clickhouse import get_client\n",
                "from posthog.temporal.common.heartbeat import Heartbeater\n",
                "from posthog.temporal.common.logger import bind_temporal_worker_logger\n",
                "\n",
                "PostgreSQLField = tuple[str, typing.LiteralString]\n",
                "Fields = collections.abc.Iterable[PostgreSQLField]\n",
                "\n",
                "\n",
                "class PostgreSQLConnectionError(Exception):\n",
                "    pass\n",
                "\n",
                "\n",
                "@dataclasses.dataclass\n",
                "class PostgresInsertInputs:\n",
                "    \"\"\"Inputs for Postgres insert activity.\"\"\"\n",
                "\n",
                "    team_id: int\n",
                "    user: str\n",
                "    password: str\n",
                "    host: str\n",
                "    database: str\n",
                "    table_name: str\n",
                "    data_interval_start: str\n",
                "    data_interval_end: str\n",
                "    has_self_signed_cert: bool = False\n",
                "    schema: str = \"public\"\n",
                "    port: int = 5432\n",
                "    exclude_events: list[str] | None = None\n",
                "    include_events: list[str] | None = None\n",
                "    run_id: str | None = None\n",
                "    is_backfill: bool = False\n",
                "    batch_export_model: BatchExportModel | None = None\n",
                "    batch_export_schema: BatchExportSchema | None = None\n",
                "\n",
                "\n",
                "class PostgreSQLClient:\n",
                "    \"\"\"PostgreSQL connection client used in batch exports.\"\"\"\n",
                "\n",
                "    def __init__(self, user: str, password: str, host: str, port: int, database: str, has_self_signed_cert: bool):\n",
                "        self.user = user\n",
                "        self.password = password\n",
                "        self.database = database\n",
                "        self.host = host\n",
                "        self.port = port\n",
                "        self.has_self_signed_cert = has_self_signed_cert\n",
                "\n",
                "        self._connection: None | psycopg.AsyncConnection = None\n",
                "\n",
                "    @classmethod\n",
                "    def from_inputs(cls, inputs: PostgresInsertInputs) -> typing.Self:\n",
                "        \"\"\"Initialize `PostgreSQLClient` from `PostgresInsertInputs`.\"\"\"\n",
                "        return cls(\n",
                "            user=inputs.user,\n",
                "            password=inputs.password,\n",
                "            database=inputs.database,\n",
                "            host=inputs.host,\n",
                "            port=inputs.port,\n",
                "            has_self_signed_cert=inputs.has_self_signed_cert,\n",
                "        )\n",
                "\n",
                "    @property\n",
                "    def connection(self) -> psycopg.AsyncConnection:\n",
                "        \"\"\"Raise if a `psycopg.AsyncConnection` hasn't been established, else return it.\"\"\"\n",
                "        if self._connection is None:\n",
                "            raise PostgreSQLConnectionError(\"Not connected, open a connection by calling connect\")\n",
                "        return self._connection\n",
                "\n",
                "    @contextlib.asynccontextmanager\n",
                "    async def connect(self) -> typing.AsyncIterator[typing.Self]:\n",
                "        \"\"\"Manage a PostgreSQL connection.\n",
                "\n",
                "        By using a context manager Pyscopg will take care of closing the connection.\n",
                "        \"\"\"\n",
                "        kwargs: dict[str, typing.Any] = {}\n",
                "        if self.has_self_signed_cert:\n",
                "            # Disable certificate verification for self-signed certificates.\n",
                "            kwargs[\"sslrootcert\"] = None\n",
                "\n",
                "        connection = await psycopg.AsyncConnection.connect(\n",
                "            user=self.user,\n",
                "            password=self.password,\n",
                "            dbname=self.database,\n",
                "            host=self.host,\n",
                "            port=self.port,\n",
                "            sslmode=\"prefer\" if settings.TEST else \"require\",\n",
                "            **kwargs,\n",
                "        )\n",
                "        async with connection as connection:\n",
                "            self._connection = connection\n",
                "            yield self\n",
                "\n",
                "    async def acreate_table(\n",
                "        self,\n",
                "        schema: str | None,\n",
                "        table_name: str,\n",
                "        fields: Fields,\n",
                "        exists_ok: bool = True,\n",
                "        primary_key: Fields | None = None,\n",
                "    ) -> None:\n",
                "        \"\"\"Create a table in PostgreSQL.\n",
                "\n",
                "        Args:\n",
                "            schema: Name of the schema where the table is to be created.\n",
                "            table_name: Name of the table to create.\n",
                "            fields: An iterable of PostgreSQL fields for the table.\n",
                "            exists_ok: Whether to ignore if the table already exists.\n",
                "            primary_key: Optionally set a primary key on these fields, needed for merges.\n",
                "        \"\"\"\n",
                "        if schema:\n",
                "            table_identifier = sql.Identifier(schema, table_name)\n",
                "        else:\n",
                "            table_identifier = sql.Identifier(table_name)\n",
                "\n",
                "        if exists_ok is True:\n",
                "            base_query = \"CREATE TABLE IF NOT EXISTS {table} ({fields}{pkey})\"\n",
                "        else:\n",
                "            base_query = \"CREATE TABLE {table} ({fields}{pkey})\"\n",
                "\n",
                "        if primary_key is not None:\n",
                "            primary_key_clause = sql.SQL(\", PRIMARY KEY ({fields})\").format(\n",
                "                fields=sql.SQL(\",\").join(sql.Identifier(field[0]) for field in primary_key)\n",
                "            )\n",
                "\n",
                "        async with self.connection.transaction():\n",
                "            async with self.connection.cursor() as cursor:\n",
                "                await cursor.execute(\"SET TRANSACTION READ WRITE\")\n",
                "\n",
                "                await cursor.execute(\n",
                "                    sql.SQL(base_query).format(\n",
                "                        pkey=primary_key_clause if primary_key else sql.SQL(\"\"),\n",
                "                        table=table_identifier,\n",
                "                        fields=sql.SQL(\",\").join(\n",
                "                            sql.SQL(\"{field} {type}\").format(\n",
                "                                field=sql.Identifier(field),\n",
                "                                type=sql.SQL(field_type),\n",
                "                            )\n",
                "                            for field, field_type in fields\n",
                "                        ),\n",
                "                    )\n",
                "                )\n",
                "\n",
                "    async def adelete_table(self, schema: str | None, table_name: str, not_found_ok: bool = True) -> None:\n",
                "        \"\"\"Delete a table in PostgreSQL.\n",
                "\n",
                "        Args:\n",
                "            schema: Name of the schema where the table to delete is located.\n",
                "            table_name: Name of the table to delete.\n",
                "            not_found_ok: Whether to ignore if the table doesn't exist.\n",
                "        \"\"\"\n",
                "        if schema:\n",
                "            table_identifier = sql.Identifier(schema, table_name)\n",
                "        else:\n",
                "            table_identifier = sql.Identifier(table_name)\n",
                "\n",
                "        if not_found_ok is True:\n",
                "            base_query = \"DROP TABLE IF EXISTS {table}\"\n",
                "        else:\n",
                "            base_query = \"DROP TABLE {table}\"\n",
                "\n",
                "        async with self.connection.transaction():\n",
                "            async with self.connection.cursor() as cursor:\n",
                "                await cursor.execute(\"SET TRANSACTION READ WRITE\")\n",
                "\n",
                "                await cursor.execute(sql.SQL(base_query).format(table=table_identifier))\n",
                "\n",
                "    @contextlib.asynccontextmanager\n",
                "    async def managed_table(\n",
                "        self,\n",
                "        schema: str,\n",
                "        table_name: str,\n",
                "        fields: Fields,\n",
                "        primary_key: Fields | None = None,\n",
                "        exists_ok: bool = True,\n",
                "        not_found_ok: bool = True,\n",
                "        delete: bool = True,\n",
                "        create: bool = True,\n",
                "    ) -> collections.abc.AsyncGenerator[str, None]:\n",
                "        \"\"\"Manage a table in PostgreSQL by ensure it exists while in context.\n",
                "\n",
                "        Managing a table implies two operations: creation of a table, which happens upon entering the\n",
                "        context manager, and deletion of the table, which happens upon exiting.\n",
                "\n",
                "        Args:\n",
                "            schema: Schema where the managed table is.\n",
                "            table_name: A name for the managed table.\n",
                "            fields: An iterable of PostgreSQL fields for the table when it has to be created.\n",
                "            primary_key: Optionally set a primary key on these fields on creation.\n",
                "            exists_ok: Whether to ignore if the table already exists on creation.\n",
                "            not_found_ok: Whether to ignore if the table doesn't exist.\n",
                "            delete: If `False`, do not delete the table on exiting context manager.\n",
                "            create: If `False`, do not attempt to create the table.\n",
                "        \"\"\"\n",
                "        if create is True:\n",
                "            await self.acreate_table(schema, table_name, fields, exists_ok, primary_key=primary_key)\n",
                "\n",
                "        try:\n",
                "            yield table_name\n",
                "        finally:\n",
                "            if delete is True:\n",
                "                await self.adelete_table(schema, table_name, not_found_ok)\n",
                "\n",
                "    async def amerge_person_tables(\n",
                "        self,\n",
                "        final_table_name: str,\n",
                "        stage_table_name: str,\n",
                "        schema: str,\n",
                "        merge_key: Fields,\n",
                "        update_when_matched: Fields,\n",
                "        person_version_key: str = \"person_version\",\n",
                "        person_distinct_id_version_key: str = \"person_distinct_id_version\",\n",
                "    ) -> None:\n",
                "        \"\"\"Merge two identical person model tables in PostgreSQL.\n",
                "\n",
                "        Merging utilizes PostgreSQL's `INSERT INTO ... ON CONFLICT` statement. PostgreSQL version\n",
                "        15 and later supports a `MERGE` command, but to ensure support for older versions of PostgreSQL\n",
                "        we do not use it. There are differences in the way concurrency is managed in `MERGE` but those\n",
                "        are less relevant concerns for us than compatibility.\n",
                "        \"\"\"\n",
                "        if schema:\n",
                "            final_table_identifier = sql.Identifier(schema, final_table_name)\n",
                "            stage_table_identifier = sql.Identifier(schema, stage_table_name)\n",
                "\n",
                "        else:\n",
                "            final_table_identifier = sql.Identifier(final_table_name)\n",
                "            stage_table_identifier = sql.Identifier(stage_table_name)\n",
                "\n",
                "        and_separator = sql.SQL(\"AND\")\n",
                "        merge_condition = and_separator.join(\n",
                "            sql.SQL(\"{final_field} = {stage_field}\").format(\n",
                "                final_field=sql.Identifier(\"final\", field[0]),\n",
                "                stage_field=sql.Identifier(schema, stage_table_name, field[0]),\n",
                "            )\n",
                "            for field in merge_key\n",
                "        )\n",
                "\n",
                "        comma = sql.SQL(\",\")\n",
                "        update_clause = comma.join(\n",
                "            sql.SQL(\"{final_field} = EXCLUDED.{stage_field}\").format(\n",
                "                final_field=sql.Identifier(field[0]),\n",
                "                stage_field=sql.Identifier(field[0]),\n",
                "            )\n",
                "            for field in update_when_matched\n",
                "        )\n",
                "        field_names = comma.join(sql.Identifier(field[0]) for field in update_when_matched)\n",
                "        conflict_fields = comma.join(sql.Identifier(field[0]) for field in merge_key)\n",
                "\n",
                "        merge_query = sql.SQL(\n",
                "            \"\"\"\\\n",
                "        INSERT INTO {final_table} AS final ({field_names})\n",
                "        SELECT {field_names} FROM {stage_table}\n",
                "        ON CONFLICT ({conflict_fields}) DO UPDATE SET\n",
                "            {update_clause}\n",
                "        WHERE (EXCLUDED.{person_version_key} > final.{person_version_key} OR EXCLUDED.{person_distinct_id_version_key} > final.{person_distinct_id_version_key})\n",
                "        \"\"\"\n",
                "        ).format(\n",
                "            final_table=final_table_identifier,\n",
                "            conflict_fields=conflict_fields,\n",
                "            stage_table=stage_table_identifier,\n",
                "            merge_condition=merge_condition,\n",
                "            person_version_key=sql.Identifier(person_version_key),\n",
                "            person_distinct_id_version_key=sql.Identifier(person_distinct_id_version_key),\n",
                "            update_clause=update_clause,\n",
                "            field_names=field_names,\n",
                "        )\n",
                "\n",
                "        async with self.connection.transaction():\n",
                "            async with self.connection.cursor() as cursor:\n",
                "                if schema:\n",
                "                    await cursor.execute(sql.SQL(\"SET search_path TO {schema}\").format(schema=sql.Identifier(schema)))\n",
                "                await cursor.execute(\"SET TRANSACTION READ WRITE\")\n",
                "\n",
                "                await cursor.execute(merge_query)\n",
                "\n",
                "    async def copy_tsv_to_postgres(\n",
                "        self,\n",
                "        tsv_file,\n",
                "        schema: str,\n",
                "        table_name: str,\n",
                "        schema_columns: list[str],\n",
                "    ) -> None:\n",
                "        \"\"\"Execute a COPY FROM query with given connection to copy contents of tsv_file.\n",
                "\n",
                "        Arguments:\n",
                "            tsv_file: A file-like object to interpret as TSV to copy its contents.\n",
                "            schema: The schema where the table we are COPYing into exists.\n",
                "            table_name: The name of the table we are COPYing into.\n",
                "            schema_columns: The column names of the table we are COPYing into.\n",
                "        \"\"\"\n",
                "        tsv_file.seek(0)\n",
                "\n",
                "        async with self.connection.transaction():\n",
                "            async with self.connection.cursor() as cursor:\n",
                "                if schema:\n",
                "                    await cursor.execute(sql.SQL(\"SET search_path TO {schema}\").format(schema=sql.Identifier(schema)))\n",
                "\n",
                "                await cursor.execute(\"SET TRANSACTION READ WRITE\")\n",
                "\n",
                "                async with cursor.copy(\n",
                "                    # TODO: Switch to binary encoding as CSV has a million edge cases.\n",
                "                    sql.SQL(\"COPY {table_name} ({fields}) FROM STDIN WITH (FORMAT CSV, DELIMITER '\\t')\").format(\n",
                "                        table_name=sql.Identifier(table_name),\n",
                "                        fields=sql.SQL(\",\").join(sql.Identifier(column) for column in schema_columns),\n",
                "                    )\n",
                "                ) as copy:\n",
                "                    while data := tsv_file.read():\n",
                "                        await copy.write(data)\n",
                "\n",
                "\n",
                "def postgres_default_fields() -> list[BatchExportField]:\n",
                "    batch_export_fields = default_fields()\n",
                "    batch_export_fields.append(\n",
                "        {\n",
                "            \"expression\": \"nullIf(JSONExtractString(properties, '$ip'), '')\",\n",
                "            \"alias\": \"ip\",\n",
                "        }\n",
                "    )\n",
                "    # Fields kept or removed for backwards compatibility with legacy apps schema.\n",
                "    batch_export_fields.append({\"expression\": \"toJSONString(toJSONString(elements_chain))\", \"alias\": \"elements\"})\n",
                "    batch_export_fields.append({\"expression\": \"Null::Nullable(String)\", \"alias\": \"site_url\"})\n",
                "    batch_export_fields.pop(batch_export_fields.index({\"expression\": \"created_at\", \"alias\": \"created_at\"}))\n",
                "    # Team ID is (for historical reasons) an INTEGER (4 bytes) in PostgreSQL, but in ClickHouse is stored as Int64.\n",
                "    # We can't encode it as an Int64, as this includes 4 extra bytes, and PostgreSQL will reject the data with a\n",
                "    # 'incorrect binary data format' error on the column, so we cast it to Int32.\n",
                "    team_id_field = batch_export_fields.pop(\n",
                "        batch_export_fields.index(BatchExportField(expression=\"team_id\", alias=\"team_id\"))\n",
                "    )\n",
                "    team_id_field[\"expression\"] = \"toInt32(team_id)\"\n",
                "    batch_export_fields.append(team_id_field)\n",
                "    return batch_export_fields\n",
                "\n",
                "\n",
                "def get_postgres_fields_from_record_schema(\n",
                "    record_schema: pa.Schema, known_json_columns: list[str]\n",
                ") -> list[PostgreSQLField]:\n",
                "    \"\"\"Generate a list of supported PostgreSQL fields from PyArrow schema.\n",
                "\n",
                "    This function is used to map custom schemas to PostgreSQL-supported types. Some loss of precision is\n",
                "    expected.\n",
                "    \"\"\"\n",
                "    pg_schema: list[PostgreSQLField] = []\n",
                "\n",
                "    for name in record_schema.names:\n",
                "        pa_field = record_schema.field(name)\n",
                "\n",
                "        if pa.types.is_string(pa_field.type) or isinstance(pa_field.type, JsonType):\n",
                "            if pa_field.name in known_json_columns:\n",
                "                pg_type = \"JSONB\"\n",
                "            else:\n",
                "                pg_type = \"TEXT\"\n",
                "\n",
                "        elif pa.types.is_signed_integer(pa_field.type) or pa.types.is_unsigned_integer(pa_field.type):\n",
                "            if pa.types.is_uint64(pa_field.type) or pa.types.is_int64(pa_field.type):\n",
                "                pg_type = \"BIGINT\"\n",
                "            else:\n",
                "                pg_type = \"INTEGER\"\n",
                "\n",
                "        elif pa.types.is_floating(pa_field.type):\n",
                "            if pa.types.is_float64(pa_field.type):\n",
                "                pg_type = \"DOUBLE PRECISION\"\n",
                "            else:\n",
                "                pg_type = \"REAL\"\n",
                "\n",
                "        elif pa.types.is_boolean(pa_field.type):\n",
                "            pg_type = \"BOOLEAN\"\n",
                "\n",
                "        elif pa.types.is_timestamp(pa_field.type):\n",
                "            if pa_field.type.tz is not None:\n",
                "                pg_type = \"TIMESTAMPTZ\"\n",
                "            else:\n",
                "                pg_type = \"TIMESTAMP\"\n",
                "\n",
                "        else:\n",
                "            raise TypeError(f\"Unsupported type: {pa_field.type}\")\n",
                "\n",
                "        pg_schema.append((name, pg_type))\n",
                "\n",
                "    return pg_schema\n",
                "\n",
                "\n",
                "@activity.defn\n",
                "async def insert_into_postgres_activity(inputs: PostgresInsertInputs) -> RecordsCompleted:\n",
                "    \"\"\"Activity streams data from ClickHouse to Postgres.\"\"\"\n",
                "    logger = await bind_temporal_worker_logger(team_id=inputs.team_id, destination=\"PostgreSQL\")\n",
                "    logger.info(\n",
                "        \"Batch exporting range %s - %s to PostgreSQL: %s.%s.%s\",\n",
                "        inputs.data_interval_start,\n",
                "        inputs.data_interval_end,\n",
                "        inputs.database,\n",
                "        inputs.schema,\n",
                "        inputs.table_name,\n",
                "    )\n",
                "\n",
                "    async with (\n",
                "        Heartbeater(),\n",
                "        set_status_to_running_task(run_id=inputs.run_id, logger=logger),\n",
                "        get_client(team_id=inputs.team_id) as client,\n",
                "    ):\n",
                "        if not await client.is_alive():\n",
                "            raise ConnectionError(\"Cannot establish connection to ClickHouse\")\n",
                "\n",
                "        model: BatchExportModel | BatchExportSchema | None = None\n",
                "        if inputs.batch_export_schema is None and \"batch_export_model\" in {\n",
                "            field.name for field in dataclasses.fields(inputs)\n",
                "        }:\n",
                "            model = inputs.batch_export_model\n",
                "        else:\n",
                "            model = inputs.batch_export_schema\n",
                "\n",
                "        record_batch_iterator = iter_model_records(\n",
                "            client=client,\n",
                "            model=model,\n",
                "            team_id=inputs.team_id,\n",
                "            interval_start=inputs.data_interval_start,\n",
                "            interval_end=inputs.data_interval_end,\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            destination_default_fields=postgres_default_fields(),\n",
                "            is_backfill=inputs.is_backfill,\n",
                "        )\n",
                "        first_record_batch, record_batch_iterator = await apeek_first_and_rewind(record_batch_iterator)\n",
                "        if first_record_batch is None:\n",
                "            return 0\n",
                "\n",
                "        if model is None or (isinstance(model, BatchExportModel) and model.name == \"events\"):\n",
                "            table_fields: Fields = [\n",
                "                (\"uuid\", \"VARCHAR(200)\"),\n",
                "                (\"event\", \"VARCHAR(200)\"),\n",
                "                (\"properties\", \"JSONB\"),\n",
                "                (\"elements\", \"JSONB\"),\n",
                "                (\"set\", \"JSONB\"),\n",
                "                (\"set_once\", \"JSONB\"),\n",
                "                (\"distinct_id\", \"VARCHAR(200)\"),\n",
                "                (\"team_id\", \"INTEGER\"),\n",
                "                (\"ip\", \"VARCHAR(200)\"),\n",
                "                (\"site_url\", \"VARCHAR(200)\"),\n",
                "                (\"timestamp\", \"TIMESTAMP WITH TIME ZONE\"),\n",
                "            ]\n",
                "\n",
                "        else:\n",
                "            column_names = [column for column in first_record_batch.schema.names if column != \"_inserted_at\"]\n",
                "            record_schema = first_record_batch.select(column_names).schema\n",
                "            table_fields = get_postgres_fields_from_record_schema(\n",
                "                record_schema, known_json_columns=[\"properties\", \"set\", \"set_once\", \"person_properties\"]\n",
                "            )\n",
                "\n",
                "        schema_columns = [field[0] for field in table_fields]\n",
                "\n",
                "        rows_exported = get_rows_exported_metric()\n",
                "        bytes_exported = get_bytes_exported_metric()\n",
                "\n",
                "        requires_merge = (\n",
                "            isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n",
                "        )\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        stagle_table_name = f\"stage_{inputs.table_name}\" if requires_merge else inputs.table_name\n"
                ],
                "after": [
                    "        data_interval_end_str = dt.datetime.fromisoformat(inputs.data_interval_end).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                    "        stagle_table_name = (\n",
                    "            f\"stage_{inputs.table_name}_{data_interval_end_str}\" if requires_merge else inputs.table_name\n",
                    "        )\n"
                ],
                "parent_version_range": {
                    "start": 497,
                    "end": 498
                },
                "child_version_range": {
                    "start": 497,
                    "end": 501
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "async with (\n        Heartbeater(),\n        set_status_to_running_task(run_id=inputs.run_id, logger=logger),\n        get_client(team_id=inputs.team_id) as client,\n    ):",
                        "start_line": 436,
                        "end_line": 572
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "insert_into_postgres_activity",
                        "signature": "def insert_into_postgres_activity(inputs: PostgresInsertInputs)->RecordsCompleted:",
                        "at_line": 424
                    }
                ],
                "idx": 1,
                "hunk_diff": "File: posthog/temporal/batch_exports/postgres_batch_export.py\nCode:\n           def insert_into_postgres_activity(inputs: PostgresInsertInputs)->RecordsCompleted:\n               ...\n494 494            requires_merge = (\n495 495                isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n496 496            )\n497      -         stagle_table_name = f\"stage_{inputs.table_name}\" if requires_merge else inputs.table_name\n    497  +         data_interval_end_str = dt.datetime.fromisoformat(inputs.data_interval_end).strftime(\"%Y-%m-%d_%H-%M-%S\")\n    498  +         stagle_table_name = (\n    499  +             f\"stage_{inputs.table_name}_{data_interval_end_str}\" if requires_merge else inputs.table_name\n    500  +         )\n498 501    \n499 502            if requires_merge:\n500 503                primary_key: Fields | None = ((\"team_id\", \"INTEGER\"), (\"distinct_id\", \"VARCHAR(200)\"))\n         ...\n",
                "file_path": "posthog/temporal/batch_exports/postgres_batch_export.py",
                "identifiers_before": [
                    "inputs",
                    "requires_merge",
                    "stagle_table_name",
                    "table_name"
                ],
                "identifiers_after": [
                    "data_interval_end",
                    "data_interval_end_str",
                    "datetime",
                    "dt",
                    "fromisoformat",
                    "inputs",
                    "requires_merge",
                    "stagle_table_name",
                    "strftime",
                    "table_name"
                ],
                "prefix": [
                    "        requires_merge = (\n",
                    "            isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n",
                    "        )\n"
                ],
                "suffix": [
                    "\n",
                    "        if requires_merge:\n",
                    "            primary_key: Fields | None = ((\"team_id\", \"INTEGER\"), (\"distinct_id\", \"VARCHAR(200)\"))\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    0,
                    2
                ]
            },
            [
                "\n",
                "        if requires_merge:\n",
                "            primary_key: Fields | None = ((\"team_id\", \"INTEGER\"), (\"distinct_id\", \"VARCHAR(200)\"))\n",
                "        else:\n",
                "            primary_key = None\n",
                "\n",
                "        async with PostgreSQLClient.from_inputs(inputs).connect() as pg_client:\n",
                "            async with (\n",
                "                pg_client.managed_table(\n",
                "                    inputs.schema, inputs.table_name, table_fields, delete=False, primary_key=primary_key\n",
                "                ) as pg_table,\n",
                "                pg_client.managed_table(\n",
                "                    inputs.schema,\n",
                "                    stagle_table_name,\n",
                "                    table_fields,\n",
                "                    create=requires_merge,\n",
                "                    delete=requires_merge,\n",
                "                    primary_key=primary_key,\n",
                "                ) as pg_stage_table,\n",
                "            ):\n",
                "\n",
                "                async def flush_to_postgres(\n",
                "                    local_results_file,\n",
                "                    records_since_last_flush,\n",
                "                    bytes_since_last_flush,\n",
                "                    flush_counter: int,\n",
                "                    last_inserted_at,\n",
                "                    last: bool,\n",
                "                    error: Exception | None,\n",
                "                ):\n",
                "                    logger.debug(\n",
                "                        \"Copying %s records of size %s bytes\",\n",
                "                        records_since_last_flush,\n",
                "                        bytes_since_last_flush,\n",
                "                    )\n",
                "\n",
                "                    table = pg_stage_table if requires_merge else pg_table\n",
                "                    await pg_client.copy_tsv_to_postgres(\n",
                "                        local_results_file,\n",
                "                        inputs.schema,\n",
                "                        table,\n",
                "                        schema_columns,\n",
                "                    )\n",
                "                    rows_exported.add(records_since_last_flush)\n",
                "                    bytes_exported.add(bytes_since_last_flush)\n",
                "\n",
                "                writer = CSVBatchExportWriter(\n",
                "                    max_bytes=settings.BATCH_EXPORT_POSTGRES_UPLOAD_CHUNK_SIZE_BYTES,\n",
                "                    flush_callable=flush_to_postgres,\n",
                "                    field_names=schema_columns,\n",
                "                    delimiter=\"\\t\",\n",
                "                    quoting=csv.QUOTE_MINIMAL,\n",
                "                    escape_char=None,\n",
                "                )\n",
                "\n",
                "                async with writer.open_temporary_file():\n",
                "                    async for record_batch in record_batch_iterator:\n",
                "                        record_batch = cast_record_batch_json_columns(record_batch, json_columns=())\n",
                "\n",
                "                        await writer.write_record_batch(record_batch)\n",
                "\n",
                "                if requires_merge:\n",
                "                    merge_key: Fields = (\n",
                "                        (\"team_id\", \"INT\"),\n",
                "                        (\"distinct_id\", \"TEXT\"),\n",
                "                    )\n",
                "                    await pg_client.amerge_person_tables(\n",
                "                        final_table_name=pg_table,\n",
                "                        stage_table_name=pg_stage_table,\n",
                "                        schema=inputs.schema,\n",
                "                        update_when_matched=table_fields,\n",
                "                        merge_key=merge_key,\n",
                "                    )\n",
                "\n",
                "                return writer.records_total\n",
                "\n",
                "\n",
                "@workflow.defn(name=\"postgres-export\", failure_exception_types=[workflow.NondeterminismError])\n",
                "class PostgresBatchExportWorkflow(PostHogWorkflow):\n",
                "    \"\"\"A Temporal Workflow to export ClickHouse data into Postgres.\n",
                "\n",
                "    This Workflow is intended to be executed both manually and by a Temporal\n",
                "    Schedule. When ran by a schedule, `data_interval_end` should be set to\n",
                "    `None` so that we will fetch the end of the interval from the Temporal\n",
                "    search attribute `TemporalScheduledStartTime`.\n",
                "    \"\"\"\n",
                "\n",
                "    @staticmethod\n",
                "    def parse_inputs(inputs: list[str]) -> PostgresBatchExportInputs:\n",
                "        \"\"\"Parse inputs from the management command CLI.\"\"\"\n",
                "        loaded = json.loads(inputs[0])\n",
                "        return PostgresBatchExportInputs(**loaded)\n",
                "\n",
                "    @workflow.run\n",
                "    async def run(self, inputs: PostgresBatchExportInputs):\n",
                "        \"\"\"Workflow implementation to export data to Postgres.\"\"\"\n",
                "        data_interval_start, data_interval_end = get_data_interval(inputs.interval, inputs.data_interval_end)\n",
                "\n",
                "        start_batch_export_run_inputs = StartBatchExportRunInputs(\n",
                "            team_id=inputs.team_id,\n",
                "            batch_export_id=inputs.batch_export_id,\n",
                "            data_interval_start=data_interval_start.isoformat(),\n",
                "            data_interval_end=data_interval_end.isoformat(),\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            is_backfill=inputs.is_backfill,\n",
                "        )\n",
                "        run_id = await workflow.execute_activity(\n",
                "            start_batch_export_run,\n",
                "            start_batch_export_run_inputs,\n",
                "            start_to_close_timeout=dt.timedelta(minutes=5),\n",
                "            retry_policy=RetryPolicy(\n",
                "                initial_interval=dt.timedelta(seconds=10),\n",
                "                maximum_interval=dt.timedelta(seconds=60),\n",
                "                maximum_attempts=0,\n",
                "                non_retryable_error_types=[\"NotNullViolation\", \"IntegrityError\"],\n",
                "            ),\n",
                "        )\n",
                "\n",
                "        finish_inputs = FinishBatchExportRunInputs(\n",
                "            id=run_id,\n",
                "            batch_export_id=inputs.batch_export_id,\n",
                "            status=BatchExportRun.Status.COMPLETED,\n",
                "            team_id=inputs.team_id,\n",
                "        )\n",
                "\n",
                "        insert_inputs = PostgresInsertInputs(\n",
                "            team_id=inputs.team_id,\n",
                "            user=inputs.user,\n",
                "            password=inputs.password,\n",
                "            host=inputs.host,\n",
                "            port=inputs.port,\n",
                "            database=inputs.database,\n",
                "            schema=inputs.schema,\n",
                "            table_name=inputs.table_name,\n",
                "            has_self_signed_cert=inputs.has_self_signed_cert,\n",
                "            data_interval_start=data_interval_start.isoformat(),\n",
                "            data_interval_end=data_interval_end.isoformat(),\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            run_id=run_id,\n",
                "            batch_export_model=inputs.batch_export_model,\n",
                "            batch_export_schema=inputs.batch_export_schema,\n",
                "        )\n",
                "\n",
                "        await execute_batch_export_insert_activity(\n",
                "            insert_into_postgres_activity,\n",
                "            insert_inputs,\n",
                "            interval=inputs.interval,\n",
                "            non_retryable_error_types=[\n",
                "                # Raised on errors that are related to database operation.\n",
                "                # For example: unexpected disconnect, database or other object not found.\n",
                "                \"OperationalError\",\n",
                "                # The schema name provided is invalid (usually because it doesn't exist).\n",
                "                \"InvalidSchemaName\",\n",
                "                # Missing permissions to, e.g., insert into table.\n",
                "                \"InsufficientPrivilege\",\n",
                "                # Issue with exported data compared to schema, retrying won't help.\n",
                "                \"NotNullViolation\",\n",
                "            ],\n",
                "            finish_inputs=finish_inputs,\n",
                "        )"
            ]
        ],
        "posthog/temporal/batch_exports/redshift_batch_export.py": [
            [
                "import collections.abc\n",
                "import contextlib\n",
                "import dataclasses\n",
                "import datetime as dt\n",
                "import json\n",
                "import typing\n",
                "\n",
                "import psycopg\n",
                "import pyarrow as pa\n",
                "from psycopg import sql\n",
                "from temporalio import activity, workflow\n",
                "from temporalio.common import RetryPolicy\n",
                "\n",
                "from posthog.batch_exports.models import BatchExportRun\n",
                "from posthog.batch_exports.service import (\n",
                "    BatchExportField,\n",
                "    BatchExportModel,\n",
                "    BatchExportSchema,\n",
                "    RedshiftBatchExportInputs,\n",
                ")\n",
                "from posthog.temporal.batch_exports.base import PostHogWorkflow\n",
                "from posthog.temporal.batch_exports.batch_exports import (\n",
                "    FinishBatchExportRunInputs,\n",
                "    RecordsCompleted,\n",
                "    StartBatchExportRunInputs,\n",
                "    default_fields,\n",
                "    execute_batch_export_insert_activity,\n",
                "    get_data_interval,\n",
                "    iter_model_records,\n",
                "    start_batch_export_run,\n",
                ")\n",
                "from posthog.temporal.batch_exports.metrics import get_rows_exported_metric\n",
                "from posthog.temporal.batch_exports.postgres_batch_export import (\n",
                "    Fields,\n",
                "    PostgresInsertInputs,\n",
                "    PostgreSQLClient,\n",
                "    PostgreSQLField,\n",
                ")\n",
                "from posthog.temporal.batch_exports.utils import JsonType, apeek_first_and_rewind, set_status_to_running_task\n",
                "from posthog.temporal.common.clickhouse import get_client\n",
                "from posthog.temporal.common.heartbeat import Heartbeater\n",
                "from posthog.temporal.common.logger import bind_temporal_worker_logger\n",
                "\n",
                "\n",
                "def remove_escaped_whitespace_recursive(value):\n",
                "    \"\"\"Remove all escaped whitespace characters from given value.\n",
                "\n",
                "    PostgreSQL supports constant escaped strings by appending an E' to each string that\n",
                "    contains whitespace in them (amongst other characters). See:\n",
                "    https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-STRINGS-ESCAPE\n",
                "\n",
                "    However, Redshift does not support this syntax. So, to avoid any escaping by\n",
                "    underlying PostgreSQL library, we remove the whitespace ourselves as defined in the\n",
                "    translation table WHITESPACE_TRANSLATE.\n",
                "\n",
                "    This function is recursive just to be extremely careful and catch any whitespace that\n",
                "    may be sneaked in a dictionary key or sequence.\n",
                "    \"\"\"\n",
                "    match value:\n",
                "        case str(s):\n",
                "            return \" \".join(s.replace(\"\\b\", \" \").split())\n",
                "\n",
                "        case bytes(b):\n",
                "            return remove_escaped_whitespace_recursive(b.decode(\"utf-8\"))\n",
                "\n",
                "        case [*sequence]:\n",
                "            # mypy could be bugged as it's raising a Statement unreachable error.\n",
                "            # But we are definitely reaching this statement in tests; hence the ignore comment.\n",
                "            # Maybe: https://github.com/python/mypy/issues/16272.\n",
                "            return type(value)(remove_escaped_whitespace_recursive(sequence_value) for sequence_value in sequence)  # type: ignore\n",
                "\n",
                "        case set(elements):\n",
                "            return {remove_escaped_whitespace_recursive(element) for element in elements}\n",
                "\n",
                "        case {**mapping}:\n",
                "            return {k: remove_escaped_whitespace_recursive(v) for k, v in mapping.items()}\n",
                "\n",
                "        case value:\n",
                "            return value\n",
                "\n",
                "\n",
                "class RedshiftClient(PostgreSQLClient):\n",
                "    @contextlib.asynccontextmanager\n",
                "    async def connect(self) -> typing.AsyncIterator[typing.Self]:\n",
                "        \"\"\"Manage a Redshift connection.\n",
                "\n",
                "        This just yields a Postgres connection but we adjust a couple of things required for\n",
                "        psycopg to work with Redshift:\n",
                "        1. Set UNICODE encoding to utf-8 as Redshift reports back UNICODE.\n",
                "        2. Set prepare_threshold to None on the connection as psycopg attempts to run DEALLOCATE ALL otherwise\n",
                "            which is not supported on Redshift.\n",
                "        \"\"\"\n",
                "        psycopg._encodings._py_codecs[\"UNICODE\"] = \"utf-8\"\n",
                "        psycopg._encodings.py_codecs.update((k.encode(), v) for k, v in psycopg._encodings._py_codecs.items())\n",
                "\n",
                "        async with super().connect():\n",
                "            self.connection.prepare_threshold = None\n",
                "            yield self\n",
                "\n",
                "    @contextlib.asynccontextmanager\n",
                "    async def async_client_cursor(self) -> typing.AsyncIterator[psycopg.AsyncClientCursor]:\n",
                "        \"\"\"Yield a AsyncClientCursor from a psycopg.AsyncConnection.\n",
                "\n",
                "        Keeps track of the current cursor_factory to set it after we are done.\n",
                "        \"\"\"\n",
                "        current_factory = self.connection.cursor_factory\n",
                "        self.connection.cursor_factory = psycopg.AsyncClientCursor\n",
                "\n",
                "        try:\n",
                "            async with self.connection.cursor() as cursor:\n",
                "                # Not a fan of typing.cast, but we know this is an psycopg.AsyncClientCursor\n",
                "                # as we have just set cursor_factory.\n",
                "                cursor = typing.cast(psycopg.AsyncClientCursor, cursor)\n",
                "                yield cursor\n",
                "        finally:\n",
                "            self.connection.cursor_factory = current_factory\n",
                "\n",
                "    async def amerge_identical_tables(\n",
                "        self,\n",
                "        final_table_name: str,\n",
                "        stage_table_name: str,\n",
                "        schema: str,\n",
                "        merge_key: Fields,\n",
                "        person_version_key: str = \"person_version\",\n",
                "        person_distinct_id_version_key: str = \"person_distinct_id_version\",\n",
                "    ) -> None:\n",
                "        \"\"\"Merge two identical tables in PostgreSQL.\"\"\"\n",
                "        if schema:\n",
                "            final_table_identifier = sql.Identifier(schema, final_table_name)\n",
                "            stage_table_identifier = sql.Identifier(schema, stage_table_name)\n",
                "\n",
                "        else:\n",
                "            final_table_identifier = sql.Identifier(final_table_name)\n",
                "            stage_table_identifier = sql.Identifier(stage_table_name)\n",
                "\n",
                "        and_separator = sql.SQL(\"AND\")\n",
                "        merge_condition = and_separator.join(\n",
                "            sql.SQL(\"{final_field} = {stage_field}\").format(\n",
                "                final_field=sql.Identifier(schema, final_table_name, field[0]),\n",
                "                stage_field=sql.Identifier(\"stage\", field[0]),\n",
                "            )\n",
                "            for field in merge_key\n",
                "        )\n",
                "\n",
                "        delete_condition = and_separator.join(\n",
                "            sql.SQL(\"{final_field} = {stage_field}\").format(\n",
                "                final_field=sql.Identifier(\"final\", field[0]),\n",
                "                stage_field=sql.Identifier(schema, stage_table_name, field[0]),\n",
                "            )\n",
                "            for field in merge_key\n",
                "        )\n",
                "\n",
                "        delete_query = sql.SQL(\n",
                "            \"\"\"\\\n",
                "        DELETE FROM {stage_table}\n",
                "        USING {final_table} AS final\n",
                "        WHERE {merge_condition}\n",
                "        AND {stage_table}.{stage_person_version_key} < final.{final_person_version_key}\n",
                "        AND {stage_table}.{stage_person_distinct_id_version_key} < final.{final_person_distinct_id_version_key};\n",
                "        \"\"\"\n",
                "        ).format(\n",
                "            final_table=final_table_identifier,\n",
                "            stage_table=stage_table_identifier,\n",
                "            merge_condition=delete_condition,\n",
                "            stage_person_version_key=sql.Identifier(person_version_key),\n",
                "            final_person_version_key=sql.Identifier(person_version_key),\n",
                "            stage_person_distinct_id_version_key=sql.Identifier(person_distinct_id_version_key),\n",
                "            final_person_distinct_id_version_key=sql.Identifier(person_distinct_id_version_key),\n",
                "        )\n",
                "\n",
                "        merge_query = sql.SQL(\n",
                "            \"\"\"\\\n",
                "        MERGE INTO {final_table}\n",
                "        USING {stage_table} AS stage\n",
                "        ON {merge_condition}\n",
                "        REMOVE DUPLICATES\n",
                "        \"\"\"\n",
                "        ).format(\n",
                "            final_table=final_table_identifier,\n",
                "            stage_table=stage_table_identifier,\n",
                "            merge_condition=merge_condition,\n",
                "        )\n",
                "\n",
                "        async with self.connection.transaction():\n",
                "            async with self.connection.cursor() as cursor:\n",
                "                await cursor.execute(delete_query)\n",
                "                await cursor.execute(merge_query)\n",
                "\n",
                "\n",
                "def redshift_default_fields() -> list[BatchExportField]:\n",
                "    batch_export_fields = default_fields()\n",
                "    batch_export_fields.append(\n",
                "        {\n",
                "            \"expression\": \"nullIf(JSONExtractString(properties, '$ip'), '')\",\n",
                "            \"alias\": \"ip\",\n",
                "        }\n",
                "    )\n",
                "    # Fields kept or removed for backwards compatibility with legacy apps schema.\n",
                "    batch_export_fields.append({\"expression\": \"''\", \"alias\": \"elements\"})\n",
                "    batch_export_fields.append({\"expression\": \"''\", \"alias\": \"site_url\"})\n",
                "    batch_export_fields.pop(batch_export_fields.index({\"expression\": \"created_at\", \"alias\": \"created_at\"}))\n",
                "    # Team ID is (for historical reasons) an INTEGER (4 bytes) in PostgreSQL, but in ClickHouse is stored as Int64.\n",
                "    # We can't encode it as an Int64, as this includes 4 extra bytes, and PostgreSQL will reject the data with a\n",
                "    # 'incorrect binary data format' error on the column, so we cast it to Int32.\n",
                "    team_id_field = batch_export_fields.pop(\n",
                "        batch_export_fields.index(BatchExportField(expression=\"team_id\", alias=\"team_id\"))\n",
                "    )\n",
                "    team_id_field[\"expression\"] = \"toInt32(team_id)\"\n",
                "    batch_export_fields.append(team_id_field)\n",
                "    return batch_export_fields\n",
                "\n",
                "\n",
                "def get_redshift_fields_from_record_schema(\n",
                "    record_schema: pa.Schema, known_super_columns: list[str], use_super: bool = False\n",
                ") -> Fields:\n",
                "    \"\"\"Generate a list of supported Redshift fields from PyArrow schema.\n",
                "\n",
                "    This function is used to map custom schemas to Redshift-supported types. Some loss of precision is\n",
                "    expected.\n",
                "    \"\"\"\n",
                "    pg_schema: list[PostgreSQLField] = []\n",
                "\n",
                "    for name in record_schema.names:\n",
                "        pa_field = record_schema.field(name)\n",
                "\n",
                "        if pa.types.is_string(pa_field.type) or isinstance(pa_field.type, JsonType):\n",
                "            if pa_field.name in known_super_columns and use_super is True:\n",
                "                pg_type = \"SUPER\"\n",
                "            else:\n",
                "                # Redshift treats `TEXT` as `VARCHAR(256)`, not as unlimited length like PostgreSQL.\n",
                "                # So, instead of `TEXT` we use the largest possible `VARCHAR`.\n",
                "                # See: https://docs.aws.amazon.com/redshift/latest/dg/r_Character_types.html\n",
                "                pg_type = \"VARCHAR(65535)\"\n",
                "\n",
                "        elif pa.types.is_signed_integer(pa_field.type) or pa.types.is_unsigned_integer(pa_field.type):\n",
                "            if pa.types.is_uint64(pa_field.type) or pa.types.is_int64(pa_field.type):\n",
                "                pg_type = \"BIGINT\"\n",
                "            else:\n",
                "                pg_type = \"INTEGER\"\n",
                "\n",
                "        elif pa.types.is_floating(pa_field.type):\n",
                "            if pa.types.is_float64(pa_field.type):\n",
                "                pg_type = \"DOUBLE PRECISION\"\n",
                "            else:\n",
                "                pg_type = \"REAL\"\n",
                "\n",
                "        elif pa.types.is_boolean(pa_field.type):\n",
                "            pg_type = \"BOOLEAN\"\n",
                "\n",
                "        elif pa.types.is_timestamp(pa_field.type):\n",
                "            if pa_field.type.tz is not None:\n",
                "                pg_type = \"TIMESTAMPTZ\"\n",
                "            else:\n",
                "                pg_type = \"TIMESTAMP\"\n",
                "\n",
                "        else:\n",
                "            raise TypeError(f\"Unsupported type: {pa_field.type}\")\n",
                "\n",
                "        pg_schema.append((name, pg_type))\n",
                "\n",
                "    return pg_schema\n",
                "\n",
                "\n",
                "async def insert_records_to_redshift(\n",
                "    records: collections.abc.AsyncGenerator[dict[str, typing.Any], None],\n",
                "    redshift_client: RedshiftClient,\n",
                "    schema: str | None,\n",
                "    table: str,\n",
                "    batch_size: int = 100,\n",
                ") -> int:\n",
                "    \"\"\"Execute an INSERT query with given Redshift connection.\n",
                "\n",
                "    The recommended way to insert multiple values into Redshift is using a COPY statement (see:\n",
                "    https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html). However, Redshift cannot COPY from local\n",
                "    files like Postgres, but only from files in S3 or executing commands in SSH hosts. Setting that up would\n",
                "    add complexity and require more configuration from the user compared to the old Redshift export plugin.\n",
                "    For this reasons, we are going with basic INSERT statements for now, and we can migrate to COPY from S3\n",
                "    later if the need arises.\n",
                "\n",
                "    Arguments:\n",
                "        record: A dictionary representing the record to insert. Each key should correspond to a column\n",
                "            in the destination table.\n",
                "        redshift_connection: A connection to Redshift setup by psycopg2.\n",
                "        schema: The schema that contains the table where to insert the record.\n",
                "        table: The name of the table where to insert the record.\n",
                "        batch_size: Number of records to insert in batch. Setting this too high could\n",
                "            make us go OOM or exceed Redshift's SQL statement size limit (16MB). Setting this too low\n",
                "            can significantly affect performance due to Redshift's poor handling of INSERTs.\n",
                "    \"\"\"\n",
                "    first_record_batch, records_iterator = await apeek_first_and_rewind(records)\n",
                "    if first_record_batch is None:\n",
                "        return 0\n",
                "\n",
                "    columns = first_record_batch.keys()\n",
                "\n",
                "    if schema:\n",
                "        table_identifier = sql.Identifier(schema, table)\n",
                "    else:\n",
                "        table_identifier = sql.Identifier(table)\n",
                "\n",
                "    pre_query = sql.SQL(\"INSERT INTO {table} ({fields}) VALUES\").format(\n",
                "        table=table_identifier,\n",
                "        fields=sql.SQL(\", \").join(map(sql.Identifier, columns)),\n",
                "    )\n",
                "    template = sql.SQL(\"({})\").format(sql.SQL(\", \").join(map(sql.Placeholder, columns)))\n",
                "    rows_exported = get_rows_exported_metric()\n",
                "\n",
                "    total_rows_exported = 0\n",
                "\n",
                "    async with redshift_client.connection.transaction():\n",
                "        async with redshift_client.async_client_cursor() as cursor:\n",
                "            batch = []\n",
                "            pre_query_str = pre_query.as_string(cursor).encode(\"utf-8\")\n",
                "\n",
                "            async def flush_to_redshift(batch):\n",
                "                nonlocal total_rows_exported\n",
                "\n",
                "                values = b\",\".join(batch).replace(b\" E'\", b\" '\")\n",
                "                await cursor.execute(pre_query_str + values)\n",
                "                rows_exported.add(len(batch))\n",
                "                total_rows_exported += len(batch)\n",
                "                # It would be nice to record BYTES_EXPORTED for Redshift, but it's not worth estimating\n",
                "                # the byte size of each batch the way things are currently written. We can revisit this\n",
                "                # in the future if we decide it's useful enough.\n",
                "\n",
                "            async for record in records_iterator:\n",
                "                batch.append(cursor.mogrify(template, record).encode(\"utf-8\"))\n",
                "                if len(batch) < batch_size:\n",
                "                    continue\n",
                "\n",
                "                await flush_to_redshift(batch)\n",
                "                batch = []\n",
                "\n",
                "            if len(batch) > 0:\n",
                "                await flush_to_redshift(batch)\n",
                "\n",
                "    return total_rows_exported\n",
                "\n",
                "\n",
                "@dataclasses.dataclass\n",
                "class RedshiftInsertInputs(PostgresInsertInputs):\n",
                "    \"\"\"Inputs for Redshift insert activity.\n",
                "\n",
                "    Inherit from PostgresInsertInputs as they are the same, but allow\n",
                "    for setting property_data_type which is unique to Redshift.\n",
                "    \"\"\"\n",
                "\n",
                "    properties_data_type: str = \"varchar\"\n",
                "\n",
                "\n",
                "@activity.defn\n",
                "async def insert_into_redshift_activity(inputs: RedshiftInsertInputs) -> RecordsCompleted:\n",
                "    \"\"\"Activity to insert data from ClickHouse to Redshift.\n",
                "\n",
                "    This activity executes the following steps:\n",
                "    1. Check if anything is to be exported.\n",
                "    2. Create destination table if not present.\n",
                "    3. Query rows to export.\n",
                "    4. Insert rows into Redshift.\n",
                "\n",
                "    Args:\n",
                "        inputs: The dataclass holding inputs for this activity. The inputs\n",
                "            include: connection configuration (e.g. host, user, port), batch export\n",
                "            query parameters (e.g. team_id, data_interval_start, include_events), and\n",
                "            the Redshift-specific properties_data_type to indicate the type of JSON-like\n",
                "            fields.\n",
                "    \"\"\"\n",
                "    logger = await bind_temporal_worker_logger(team_id=inputs.team_id, destination=\"Redshift\")\n",
                "    logger.info(\n",
                "        \"Batch exporting range %s - %s to Redshift: %s.%s.%s\",\n",
                "        inputs.data_interval_start,\n",
                "        inputs.data_interval_end,\n",
                "        inputs.database,\n",
                "        inputs.schema,\n",
                "        inputs.table_name,\n",
                "    )\n",
                "\n",
                "    async with (\n",
                "        Heartbeater(),\n",
                "        set_status_to_running_task(run_id=inputs.run_id, logger=logger),\n",
                "        get_client(team_id=inputs.team_id) as client,\n",
                "    ):\n",
                "        if not await client.is_alive():\n",
                "            raise ConnectionError(\"Cannot establish connection to ClickHouse\")\n",
                "\n",
                "        model: BatchExportModel | BatchExportSchema | None = None\n",
                "        if inputs.batch_export_schema is None and \"batch_export_model\" in {\n",
                "            field.name for field in dataclasses.fields(inputs)\n",
                "        }:\n",
                "            model = inputs.batch_export_model\n",
                "\n",
                "        else:\n",
                "            model = inputs.batch_export_schema\n",
                "\n",
                "        record_iterator = iter_model_records(\n",
                "            client=client,\n",
                "            model=model,\n",
                "            team_id=inputs.team_id,\n",
                "            interval_start=inputs.data_interval_start,\n",
                "            interval_end=inputs.data_interval_end,\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            destination_default_fields=redshift_default_fields(),\n",
                "            is_backfill=inputs.is_backfill,\n",
                "        )\n",
                "        first_record_batch, record_iterator = await apeek_first_and_rewind(record_iterator)\n",
                "        if first_record_batch is None:\n",
                "            return 0\n",
                "\n",
                "        known_super_columns = [\"properties\", \"set\", \"set_once\", \"person_properties\"]\n",
                "        if inputs.properties_data_type != \"varchar\":\n",
                "            properties_type = \"SUPER\"\n",
                "\n",
                "        else:\n",
                "            properties_type = \"VARCHAR(65535)\"\n",
                "\n",
                "        if model is None or (isinstance(model, BatchExportModel) and model.name == \"events\"):\n",
                "            table_fields: Fields = [\n",
                "                (\"uuid\", \"VARCHAR(200)\"),\n",
                "                (\"event\", \"VARCHAR(200)\"),\n",
                "                (\"properties\", properties_type),\n",
                "                (\"elements\", \"VARCHAR(65535)\"),\n",
                "                (\"set\", properties_type),\n",
                "                (\"set_once\", properties_type),\n",
                "                (\"distinct_id\", \"VARCHAR(200)\"),\n",
                "                (\"team_id\", \"INTEGER\"),\n",
                "                (\"ip\", \"VARCHAR(200)\"),\n",
                "                (\"site_url\", \"VARCHAR(200)\"),\n",
                "                (\"timestamp\", \"TIMESTAMP WITH TIME ZONE\"),\n",
                "            ]\n",
                "        else:\n",
                "            column_names = [column for column in first_record_batch.schema.names if column != \"_inserted_at\"]\n",
                "            record_schema = first_record_batch.select(column_names).schema\n",
                "            table_fields = get_redshift_fields_from_record_schema(\n",
                "                record_schema, known_super_columns=known_super_columns, use_super=properties_type == \"SUPER\"\n",
                "            )\n",
                "\n",
                "        requires_merge = (\n",
                "            isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n",
                "        )\n"
            ],
            {
                "type": "replace",
                "before": [
                    "        stagle_table_name = f\"stage_{inputs.table_name}\" if requires_merge else inputs.table_name\n"
                ],
                "after": [
                    "        data_interval_end_str = dt.datetime.fromisoformat(inputs.data_interval_end).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                    "        stagle_table_name = (\n",
                    "            f\"stage_{inputs.table_name}_{data_interval_end_str}\" if requires_merge else inputs.table_name\n",
                    "        )\n"
                ],
                "parent_version_range": {
                    "start": 440,
                    "end": 441
                },
                "child_version_range": {
                    "start": 440,
                    "end": 444
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "async with (\n        Heartbeater(),\n        set_status_to_running_task(run_id=inputs.run_id, logger=logger),\n        get_client(team_id=inputs.team_id) as client,\n    ):",
                        "start_line": 377,
                        "end_line": 500
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "insert_into_redshift_activity",
                        "signature": "def insert_into_redshift_activity(inputs: RedshiftInsertInputs)->RecordsCompleted:",
                        "at_line": 351
                    }
                ],
                "idx": 2,
                "hunk_diff": "File: posthog/temporal/batch_exports/redshift_batch_export.py\nCode:\n           def insert_into_redshift_activity(inputs: RedshiftInsertInputs)->RecordsCompleted:\n               ...\n437 437            requires_merge = (\n438 438                isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n439 439            )\n440      -         stagle_table_name = f\"stage_{inputs.table_name}\" if requires_merge else inputs.table_name\n    440  +         data_interval_end_str = dt.datetime.fromisoformat(inputs.data_interval_end).strftime(\"%Y-%m-%d_%H-%M-%S\")\n    441  +         stagle_table_name = (\n    442  +             f\"stage_{inputs.table_name}_{data_interval_end_str}\" if requires_merge else inputs.table_name\n    443  +         )\n441 444    \n442 445            if requires_merge:\n443 446                primary_key: Fields | None = ((\"team_id\", \"INTEGER\"), (\"distinct_id\", \"VARCHAR(200)\"))\n         ...\n",
                "file_path": "posthog/temporal/batch_exports/redshift_batch_export.py",
                "identifiers_before": [
                    "inputs",
                    "requires_merge",
                    "stagle_table_name",
                    "table_name"
                ],
                "identifiers_after": [
                    "data_interval_end",
                    "data_interval_end_str",
                    "datetime",
                    "dt",
                    "fromisoformat",
                    "inputs",
                    "requires_merge",
                    "stagle_table_name",
                    "strftime",
                    "table_name"
                ],
                "prefix": [
                    "        requires_merge = (\n",
                    "            isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n",
                    "        )\n"
                ],
                "suffix": [
                    "\n",
                    "        if requires_merge:\n",
                    "            primary_key: Fields | None = ((\"team_id\", \"INTEGER\"), (\"distinct_id\", \"VARCHAR(200)\"))\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": [
                    0,
                    1
                ]
            },
            [
                "\n",
                "        if requires_merge:\n",
                "            primary_key: Fields | None = ((\"team_id\", \"INTEGER\"), (\"distinct_id\", \"VARCHAR(200)\"))\n",
                "        else:\n",
                "            primary_key = None\n",
                "\n",
                "        async with RedshiftClient.from_inputs(inputs).connect() as redshift_client:\n",
                "            async with (\n",
                "                redshift_client.managed_table(\n",
                "                    inputs.schema, inputs.table_name, table_fields, delete=False, primary_key=primary_key\n",
                "                ) as redshift_table,\n",
                "                redshift_client.managed_table(\n",
                "                    inputs.schema,\n",
                "                    stagle_table_name,\n",
                "                    table_fields,\n",
                "                    create=requires_merge,\n",
                "                    delete=requires_merge,\n",
                "                    primary_key=primary_key,\n",
                "                ) as redshift_stage_table,\n",
                "            ):\n",
                "                schema_columns = {field[0] for field in table_fields}\n",
                "\n",
                "                def map_to_record(row: dict) -> dict:\n",
                "                    \"\"\"Map row to a record to insert to Redshift.\"\"\"\n",
                "                    record = {k: v for k, v in row.items() if k in schema_columns}\n",
                "\n",
                "                    for column in known_super_columns:\n",
                "                        if record.get(column, None) is not None:\n",
                "                            # TODO: We should be able to save a json.loads here.\n",
                "                            record[column] = json.dumps(\n",
                "                                remove_escaped_whitespace_recursive(json.loads(record[column])), ensure_ascii=False\n",
                "                            )\n",
                "\n",
                "                    return record\n",
                "\n",
                "                async def record_generator() -> collections.abc.AsyncGenerator[dict[str, typing.Any], None]:\n",
                "                    async for record_batch in record_iterator:\n",
                "                        for record in record_batch.to_pylist():\n",
                "                            yield map_to_record(record)\n",
                "\n",
                "                records_completed = await insert_records_to_redshift(\n",
                "                    record_generator(),\n",
                "                    redshift_client,\n",
                "                    inputs.schema,\n",
                "                    redshift_stage_table if requires_merge else redshift_table,\n",
                "                )\n",
                "\n",
                "                if requires_merge:\n",
                "                    merge_key: Fields = (\n",
                "                        (\"team_id\", \"INT\"),\n",
                "                        (\"distinct_id\", \"TEXT\"),\n",
                "                    )\n",
                "                    await redshift_client.amerge_identical_tables(\n",
                "                        final_table_name=redshift_table,\n",
                "                        stage_table_name=redshift_stage_table,\n",
                "                        schema=inputs.schema,\n",
                "                        merge_key=merge_key,\n",
                "                    )\n",
                "\n",
                "                return records_completed\n",
                "\n",
                "\n",
                "@workflow.defn(name=\"redshift-export\", failure_exception_types=[workflow.NondeterminismError])\n",
                "class RedshiftBatchExportWorkflow(PostHogWorkflow):\n",
                "    \"\"\"A Temporal Workflow to export ClickHouse data into Postgres.\n",
                "\n",
                "    This Workflow is intended to be executed both manually and by a Temporal\n",
                "    Schedule. When ran by a schedule, `data_interval_end` should be set to\n",
                "    `None` so that we will fetch the end of the interval from the Temporal\n",
                "    search attribute `TemporalScheduledStartTime`.\n",
                "    \"\"\"\n",
                "\n",
                "    @staticmethod\n",
                "    def parse_inputs(inputs: list[str]) -> RedshiftBatchExportInputs:\n",
                "        \"\"\"Parse inputs from the management command CLI.\"\"\"\n",
                "        loaded = json.loads(inputs[0])\n",
                "        return RedshiftBatchExportInputs(**loaded)\n",
                "\n",
                "    @workflow.run\n",
                "    async def run(self, inputs: RedshiftBatchExportInputs):\n",
                "        \"\"\"Workflow implementation to export data to Redshift.\"\"\"\n",
                "        data_interval_start, data_interval_end = get_data_interval(inputs.interval, inputs.data_interval_end)\n",
                "\n",
                "        start_batch_export_run_inputs = StartBatchExportRunInputs(\n",
                "            team_id=inputs.team_id,\n",
                "            batch_export_id=inputs.batch_export_id,\n",
                "            data_interval_start=data_interval_start.isoformat(),\n",
                "            data_interval_end=data_interval_end.isoformat(),\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            is_backfill=inputs.is_backfill,\n",
                "        )\n",
                "        run_id = await workflow.execute_activity(\n",
                "            start_batch_export_run,\n",
                "            start_batch_export_run_inputs,\n",
                "            start_to_close_timeout=dt.timedelta(minutes=5),\n",
                "            retry_policy=RetryPolicy(\n",
                "                initial_interval=dt.timedelta(seconds=10),\n",
                "                maximum_interval=dt.timedelta(seconds=60),\n",
                "                maximum_attempts=0,\n",
                "                non_retryable_error_types=[\"NotNullViolation\", \"IntegrityError\"],\n",
                "            ),\n",
                "        )\n",
                "\n",
                "        finish_inputs = FinishBatchExportRunInputs(\n",
                "            id=run_id,\n",
                "            batch_export_id=inputs.batch_export_id,\n",
                "            status=BatchExportRun.Status.COMPLETED,\n",
                "            team_id=inputs.team_id,\n",
                "        )\n",
                "\n",
                "        insert_inputs = RedshiftInsertInputs(\n",
                "            team_id=inputs.team_id,\n",
                "            user=inputs.user,\n",
                "            password=inputs.password,\n",
                "            host=inputs.host,\n",
                "            port=inputs.port,\n",
                "            database=inputs.database,\n",
                "            schema=inputs.schema,\n",
                "            table_name=inputs.table_name,\n",
                "            has_self_signed_cert=inputs.has_self_signed_cert,\n",
                "            data_interval_start=data_interval_start.isoformat(),\n",
                "            data_interval_end=data_interval_end.isoformat(),\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            properties_data_type=inputs.properties_data_type,\n",
                "            run_id=run_id,\n",
                "            is_backfill=inputs.is_backfill,\n",
                "            batch_export_model=inputs.batch_export_model,\n",
                "            batch_export_schema=inputs.batch_export_schema,\n",
                "        )\n",
                "\n",
                "        await execute_batch_export_insert_activity(\n",
                "            insert_into_redshift_activity,\n",
                "            insert_inputs,\n",
                "            interval=inputs.interval,\n",
                "            non_retryable_error_types=[\n",
                "                # Raised on errors that are related to database operation.\n",
                "                # For example: unexpected disconnect, database or other object not found.\n",
                "                \"OperationalError\",\n",
                "                # The schema name provided is invalid (usually because it doesn't exist).\n",
                "                \"InvalidSchemaName\",\n",
                "                # Missing permissions to, e.g., insert into table.\n",
                "                \"InsufficientPrivilege\",\n",
                "            ],\n",
                "            finish_inputs=finish_inputs,\n",
                "        )"
            ]
        ],
        "posthog/temporal/batch_exports/snowflake_batch_export.py": [
            [
                "import asyncio\n",
                "import collections.abc\n",
                "import contextlib\n",
                "import dataclasses\n",
                "import datetime as dt\n",
                "import functools\n",
                "import io\n",
                "import json\n",
                "import typing\n",
                "\n",
                "import pyarrow as pa\n",
                "import snowflake.connector\n",
                "from django.conf import settings\n",
                "from snowflake.connector.connection import SnowflakeConnection\n",
                "from temporalio import activity, workflow\n",
                "from temporalio.common import RetryPolicy\n",
                "\n",
                "from posthog.batch_exports.models import BatchExportRun\n",
                "from posthog.batch_exports.service import (\n",
                "    BatchExportField,\n",
                "    BatchExportModel,\n",
                "    BatchExportSchema,\n",
                "    SnowflakeBatchExportInputs,\n",
                ")\n",
                "from posthog.temporal.batch_exports.base import PostHogWorkflow\n",
                "from posthog.temporal.batch_exports.batch_exports import (\n",
                "    FinishBatchExportRunInputs,\n",
                "    RecordsCompleted,\n",
                "    StartBatchExportRunInputs,\n",
                "    default_fields,\n",
                "    execute_batch_export_insert_activity,\n",
                "    get_data_interval,\n",
                "    iter_model_records,\n",
                "    start_batch_export_run,\n",
                ")\n",
                "from posthog.temporal.batch_exports.metrics import (\n",
                "    get_bytes_exported_metric,\n",
                "    get_rows_exported_metric,\n",
                ")\n",
                "from posthog.temporal.batch_exports.temporary_file import (\n",
                "    BatchExportTemporaryFile,\n",
                "    JSONLBatchExportWriter,\n",
                ")\n",
                "from posthog.temporal.batch_exports.utils import (\n",
                "    JsonType,\n",
                "    apeek_first_and_rewind,\n",
                "    cast_record_batch_json_columns,\n",
                "    set_status_to_running_task,\n",
                ")\n",
                "from posthog.temporal.common.clickhouse import get_client\n",
                "from posthog.temporal.common.heartbeat import Heartbeater\n",
                "from posthog.temporal.common.logger import bind_temporal_worker_logger\n",
                "from posthog.temporal.common.utils import (\n",
                "    BatchExportHeartbeatDetails,\n",
                "    HeartbeatParseError,\n",
                "    NotEnoughHeartbeatValuesError,\n",
                "    should_resume_from_activity_heartbeat,\n",
                ")\n",
                "\n",
                "\n",
                "class SnowflakeFileNotUploadedError(Exception):\n",
                "    \"\"\"Raised when a PUT Snowflake query fails to upload a file.\"\"\"\n",
                "\n",
                "    def __init__(self, table_name: str, status: str, message: str):\n",
                "        super().__init__(\n",
                "            f\"Snowflake upload for table '{table_name}' expected status 'UPLOADED' but got '{status}': {message}\"\n",
                "        )\n",
                "\n",
                "\n",
                "class SnowflakeFileNotLoadedError(Exception):\n",
                "    \"\"\"Raised when a COPY INTO Snowflake query fails to copy a file to a table.\"\"\"\n",
                "\n",
                "    def __init__(self, table_name: str, status: str, errors_seen: int, first_error: str):\n",
                "        super().__init__(\n",
                "            f\"Snowflake load for table '{table_name}' expected status 'LOADED' but got '{status}' with {errors_seen} errors: {first_error}\"\n",
                "        )\n",
                "\n",
                "\n",
                "class SnowflakeConnectionError(Exception):\n",
                "    \"\"\"Raised when a connection to Snowflake is not established.\"\"\"\n",
                "\n",
                "    pass\n",
                "\n",
                "\n",
                "@dataclasses.dataclass\n",
                "class SnowflakeHeartbeatDetails(BatchExportHeartbeatDetails):\n",
                "    \"\"\"The Snowflake batch export details included in every heartbeat.\n",
                "\n",
                "    Attributes:\n",
                "        file_no: The file number of the last file we managed to upload.\n",
                "    \"\"\"\n",
                "\n",
                "    file_no: int\n",
                "\n",
                "    @classmethod\n",
                "    def from_activity(cls, activity):\n",
                "        details = BatchExportHeartbeatDetails.from_activity(activity)\n",
                "\n",
                "        if details.total_details < 2:\n",
                "            raise NotEnoughHeartbeatValuesError(details.total_details, 2)\n",
                "\n",
                "        try:\n",
                "            file_no = int(details._remaining[0])\n",
                "        except (TypeError, ValueError) as e:\n",
                "            raise HeartbeatParseError(\"file_no\") from e\n",
                "\n",
                "        return cls(last_inserted_at=details.last_inserted_at, file_no=file_no, _remaining=details._remaining[2:])\n",
                "\n",
                "\n",
                "@dataclasses.dataclass\n",
                "class SnowflakeInsertInputs:\n",
                "    \"\"\"Inputs for Snowflake.\"\"\"\n",
                "\n",
                "    # TODO: do _not_ store credentials in temporal inputs. It makes it very hard\n",
                "    # to keep track of where credentials are being stored and increases the\n",
                "    # attach surface for credential leaks.\n",
                "\n",
                "    team_id: int\n",
                "    user: str\n",
                "    password: str\n",
                "    account: str\n",
                "    database: str\n",
                "    warehouse: str\n",
                "    schema: str\n",
                "    table_name: str\n",
                "    data_interval_start: str\n",
                "    data_interval_end: str\n",
                "    role: str | None = None\n",
                "    exclude_events: list[str] | None = None\n",
                "    include_events: list[str] | None = None\n",
                "    run_id: str | None = None\n",
                "    is_backfill: bool = False\n",
                "    batch_export_model: BatchExportModel | None = None\n",
                "    batch_export_schema: BatchExportSchema | None = None\n",
                "\n",
                "\n",
                "SnowflakeField = tuple[str, str]\n",
                "\n",
                "\n",
                "class SnowflakeClient:\n",
                "    \"\"\"Snowflake connection client used in batch exports.\"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self, user: str, password: str, account: str, warehouse: str, database: str, schema: str, role: str | None\n",
                "    ):\n",
                "        self.user = user\n",
                "        self.password = password\n",
                "        self.account = account\n",
                "        self.warehouse = warehouse\n",
                "        self.database = database\n",
                "        self.schema = schema\n",
                "        self.role = role\n",
                "        self._connection: SnowflakeConnection | None = None\n",
                "\n",
                "    @classmethod\n",
                "    def from_inputs(cls, inputs: SnowflakeInsertInputs) -> typing.Self:\n",
                "        \"\"\"Initialize `SnowflakeClient` from `SnowflakeInsertInputs`.\"\"\"\n",
                "        return cls(\n",
                "            user=inputs.user,\n",
                "            password=inputs.password,\n",
                "            account=inputs.account,\n",
                "            warehouse=inputs.warehouse,\n",
                "            database=inputs.database,\n",
                "            schema=inputs.schema,\n",
                "            role=inputs.role,\n",
                "        )\n",
                "\n",
                "    @property\n",
                "    def connection(self) -> SnowflakeConnection:\n",
                "        \"\"\"Raise if a `SnowflakeConnection` hasn't been established, else return it.\"\"\"\n",
                "        if self._connection is None:\n",
                "            raise SnowflakeConnectionError(\"Not connected, open a connection by calling connect\")\n",
                "        return self._connection\n",
                "\n",
                "    @contextlib.asynccontextmanager\n",
                "    async def connect(self):\n",
                "        \"\"\"Manage a `SnowflakeConnection`.\n",
                "\n",
                "        Methods that require a connection should be ran within this block.\n",
                "        \"\"\"\n",
                "        connection = await asyncio.to_thread(\n",
                "            snowflake.connector.connect,\n",
                "            user=self.user,\n",
                "            password=self.password,\n",
                "            account=self.account,\n",
                "            warehouse=self.warehouse,\n",
                "            database=self.database,\n",
                "            schema=self.schema,\n",
                "            role=self.role,\n",
                "        )\n",
                "        self._connection = connection\n",
                "\n",
                "        await self.use_namespace()\n",
                "        await self.execute_async_query(\"SET ABORT_DETACHED_QUERY = FALSE\")\n",
                "\n",
                "        try:\n",
                "            yield self\n",
                "\n",
                "        finally:\n",
                "            self._connection = None\n",
                "            await asyncio.to_thread(connection.close)\n",
                "\n",
                "    async def use_namespace(self) -> None:\n",
                "        \"\"\"Switch to a namespace given by database and schema.\n",
                "\n",
                "        This allows all queries that follow to ignore database and schema.\n",
                "        \"\"\"\n",
                "        await self.execute_async_query(f'USE DATABASE \"{self.database}\"')\n",
                "        await self.execute_async_query(f'USE SCHEMA \"{self.schema}\"')\n",
                "\n",
                "    async def execute_async_query(\n",
                "        self,\n",
                "        query: str,\n",
                "        parameters: dict | None = None,\n",
                "        file_stream=None,\n",
                "        poll_interval: float = 1.0,\n",
                "    ) -> list[tuple] | list[dict]:\n",
                "        \"\"\"Wrap Snowflake connector's polling API in a coroutine.\n",
                "\n",
                "        This enables asynchronous execution of queries to release the event loop to execute other tasks\n",
                "        while we poll for a query to be done. For example, the event loop may use this time for heartbeating.\n",
                "\n",
                "        Args:\n",
                "            connection: A SnowflakeConnection object as produced by snowflake.connector.connect.\n",
                "            query: A query string to run asynchronously.\n",
                "            parameters: An optional dictionary of parameters to bind to the query.\n",
                "            poll_interval: Specify how long to wait in between polls.\n",
                "        \"\"\"\n",
                "        with self.connection.cursor() as cursor:\n",
                "            # Snowflake docs incorrectly state that the 'params' argument is named 'parameters'.\n",
                "            result = cursor.execute_async(query, params=parameters, file_stream=file_stream)\n",
                "            query_id = cursor.sfqid or result[\"queryId\"]\n",
                "\n",
                "            # Snowflake does a blocking HTTP request, so we send it to a thread.\n",
                "            query_status = await asyncio.to_thread(self.connection.get_query_status_throw_if_error, query_id)\n",
                "\n",
                "        while self.connection.is_still_running(query_status):\n",
                "            query_status = await asyncio.to_thread(self.connection.get_query_status_throw_if_error, query_id)\n",
                "            await asyncio.sleep(poll_interval)\n",
                "\n",
                "        with self.connection.cursor() as cursor:\n",
                "            cursor.get_results_from_sfqid(query_id)\n",
                "            results = cursor.fetchall()\n",
                "        return results\n",
                "\n",
                "    async def acreate_table(self, table_name: str, fields: list[SnowflakeField]) -> None:\n",
                "        \"\"\"Asynchronously create the table if it doesn't exist.\n",
                "\n",
                "        Arguments:\n",
                "            table_name: The name of the table to create.\n",
                "            fields: An iterable of (name, type) tuples representing the fields of the table.\n",
                "        \"\"\"\n",
                "        field_ddl = \", \".join(f'\"{field[0]}\" {field[1]}' for field in fields)\n",
                "\n",
                "        await self.execute_async_query(\n",
                "            f\"\"\"\n",
                "            CREATE TABLE IF NOT EXISTS \"{table_name}\" (\n",
                "                {field_ddl}\n",
                "            )\n",
                "            COMMENT = 'PostHog generated events table'\n",
                "            \"\"\",\n",
                "        )\n",
                "\n",
                "    async def adelete_table(\n",
                "        self,\n",
                "        table_name: str,\n",
                "        not_found_ok: bool = False,\n",
                "    ) -> None:\n",
                "        \"\"\"Delete a table in BigQuery.\"\"\"\n",
                "        if not_found_ok is True:\n",
                "            query = f'DROP TABLE IF EXISTS \"{table_name}\"'\n",
                "        else:\n",
                "            query = f'DROP TABLE \"{table_name}\"'\n",
                "\n",
                "        await self.execute_async_query(query)\n",
                "        return None\n",
                "\n",
                "    @contextlib.asynccontextmanager\n",
                "    async def managed_table(\n",
                "        self,\n",
                "        table_name: str,\n",
                "        fields: list[SnowflakeField],\n",
                "        not_found_ok: bool = True,\n",
                "        delete: bool = True,\n",
                "        create: bool = True,\n",
                "    ) -> collections.abc.AsyncGenerator[str, None]:\n",
                "        \"\"\"Manage a table in Snowflake by ensure it exists while in context.\"\"\"\n",
                "        if create:\n",
                "            await self.acreate_table(table_name, fields)\n",
                "\n",
                "        try:\n",
                "            yield table_name\n",
                "        finally:\n",
                "            if delete is True:\n",
                "                await self.adelete_table(table_name, not_found_ok)\n",
                "\n",
                "    async def put_file_to_snowflake_table(\n",
                "        self,\n",
                "        file: BatchExportTemporaryFile,\n",
                "        table_name: str,\n",
                "        file_no: int,\n",
                "    ):\n",
                "        \"\"\"Executes a PUT query using the provided cursor to the provided table_name.\n",
                "\n",
                "        Sadly, Snowflake's execute_async does not work with PUT statements. So, we pass the execute\n",
                "        call to run_in_executor: Since execute ends up boiling down to blocking IO (HTTP request),\n",
                "        the event loop should not be locked up.\n",
                "\n",
                "        We add a file_no to the file_name when executing PUT as Snowflake will reject any files with the same\n",
                "        name. Since batch exports re-use the same file, our name does not change, but we don't want Snowflake\n",
                "        to reject or overwrite our new data.\n",
                "\n",
                "        Args:\n",
                "            file: The name of the local file to PUT.\n",
                "            table_name: The name of the Snowflake table where to PUT the file.\n",
                "            file_no: An int to identify which file number this is.\n",
                "\n",
                "        Raises:\n",
                "            TypeError: If we don't get a tuple back from Snowflake (should never happen).\n",
                "            SnowflakeFileNotUploadedError: If the upload status is not 'UPLOADED'.\n",
                "        \"\"\"\n",
                "        file.rewind()\n",
                "\n",
                "        # We comply with the file-like interface of io.IOBase.\n",
                "        # So we ask mypy to be nice with us.\n",
                "        reader = io.BufferedReader(file)  # type: ignore\n",
                "        query = f'PUT file://{file.name}_{file_no}.jsonl @%\"{table_name}\"'\n",
                "\n",
                "        with self.connection.cursor() as cursor:\n",
                "            cursor = self.connection.cursor()\n",
                "\n",
                "            execute_put = functools.partial(cursor.execute, query, file_stream=reader)\n",
                "\n",
                "            loop = asyncio.get_running_loop()\n",
                "            await loop.run_in_executor(None, func=execute_put)\n",
                "            reader.detach()  # BufferedReader closes the file otherwise.\n",
                "\n",
                "            result = cursor.fetchone()\n",
                "\n",
                "            if not isinstance(result, tuple):\n",
                "                # Mostly to appease mypy, as this query should always return a tuple.\n",
                "                raise TypeError(f\"Expected tuple from Snowflake PUT query but got: '{result.__class__.__name__}'\")\n",
                "\n",
                "        status, message = result[6:8]\n",
                "        if status != \"UPLOADED\":\n",
                "            raise SnowflakeFileNotUploadedError(table_name, status, message)\n",
                "\n",
                "    async def copy_loaded_files_to_snowflake_table(\n",
                "        self,\n",
                "        table_name: str,\n",
                "    ) -> None:\n",
                "        \"\"\"Execute a COPY query in Snowflake to load any files PUT into the table.\n",
                "\n",
                "        The query is executed asynchronously using Snowflake's polling API.\n",
                "\n",
                "        Args:\n",
                "            connection: A SnowflakeConnection as returned by snowflake.connector.connect.\n",
                "            table_name: The table we are COPY-ing files into.\n",
                "        \"\"\"\n",
                "        query = f\"\"\"\n",
                "        COPY INTO \"{table_name}\"\n",
                "        FILE_FORMAT = (TYPE = 'JSON')\n",
                "        MATCH_BY_COLUMN_NAME = CASE_SENSITIVE\n",
                "        PURGE = TRUE\n",
                "        \"\"\"\n",
                "        results = await self.execute_async_query(query)\n",
                "\n",
                "        for query_result in results:\n",
                "            if not isinstance(query_result, tuple):\n",
                "                # Mostly to appease mypy, as this query should always return a tuple.\n",
                "                raise TypeError(f\"Expected tuple from Snowflake COPY INTO query but got: '{type(query_result)}'\")\n",
                "\n",
                "            if len(query_result) < 2:\n",
                "                raise SnowflakeFileNotLoadedError(\n",
                "                    table_name,\n",
                "                    \"NO STATUS\",\n",
                "                    0,\n",
                "                    query_result[0] if len(query_result) == 1 else \"NO ERROR MESSAGE\",\n",
                "                )\n",
                "\n",
                "            _, status = query_result[0:2]\n",
                "\n",
                "            if status != \"LOADED\":\n",
                "                errors_seen, first_error = query_result[5:7]\n",
                "                raise SnowflakeFileNotLoadedError(\n",
                "                    table_name,\n",
                "                    status or \"NO STATUS\",\n",
                "                    errors_seen or 0,\n",
                "                    first_error or \"NO ERROR MESSAGE\",\n",
                "                )\n",
                "\n",
                "    async def amerge_person_tables(\n",
                "        self,\n",
                "        final_table: str,\n",
                "        stage_table: str,\n",
                "        merge_key: collections.abc.Iterable[SnowflakeField],\n",
                "        update_when_matched: collections.abc.Iterable[SnowflakeField],\n",
                "        person_version_key: str = \"person_version\",\n",
                "        person_distinct_id_version_key: str = \"person_distinct_id_version\",\n",
                "    ):\n",
                "        \"\"\"Merge two identical person model tables in Snowflake.\"\"\"\n",
                "        merge_condition = \"ON \"\n",
                "\n",
                "        for n, field in enumerate(merge_key):\n",
                "            if n > 0:\n",
                "                merge_condition += \" AND \"\n",
                "            merge_condition += f'final.\"{field[0]}\" = stage.\"{field[0]}\"'\n",
                "\n",
                "        update_clause = \"\"\n",
                "        values = \"\"\n",
                "        field_names = \"\"\n",
                "        for n, field in enumerate(update_when_matched):\n",
                "            if n > 0:\n",
                "                update_clause += \", \"\n",
                "                values += \", \"\n",
                "                field_names += \", \"\n",
                "\n",
                "            update_clause += f'final.\"{field[0]}\" = stage.\"{field[0]}\"'\n",
                "            field_names += f'\"{field[0]}\"'\n",
                "            values += f'stage.\"{field[0]}\"'\n",
                "\n",
                "        merge_query = f\"\"\"\n",
                "        MERGE INTO \"{final_table}\" AS final\n",
                "        USING \"{stage_table}\" AS stage\n",
                "        {merge_condition}\n",
                "\n",
                "        WHEN MATCHED AND (stage.\"{person_version_key}\" > final.\"{person_version_key}\" OR stage.\"{person_distinct_id_version_key}\" > final.\"{person_distinct_id_version_key}\") THEN\n",
                "            UPDATE SET\n",
                "                {update_clause}\n",
                "        WHEN NOT MATCHED THEN\n",
                "            INSERT ({field_names})\n",
                "            VALUES ({values});\n",
                "        \"\"\"\n",
                "\n",
                "        await self.execute_async_query(merge_query)\n",
                "\n",
                "\n",
                "def snowflake_default_fields() -> list[BatchExportField]:\n",
                "    \"\"\"Default fields for a Snowflake batch export.\n",
                "\n",
                "    Starting from the common default fields, we add and tweak some fields for\n",
                "    backwards compatibility.\n",
                "    \"\"\"\n",
                "    batch_export_fields = default_fields()\n",
                "    batch_export_fields.append(\n",
                "        {\n",
                "            \"expression\": \"nullIf(JSONExtractString(properties, '$ip'), '')\",\n",
                "            \"alias\": \"ip\",\n",
                "        }\n",
                "    )\n",
                "    # Fields kept for backwards compatibility with legacy apps schema.\n",
                "    batch_export_fields.append({\"expression\": \"elements_chain\", \"alias\": \"elements\"})\n",
                "    batch_export_fields.append({\"expression\": \"''\", \"alias\": \"site_url\"})\n",
                "    batch_export_fields.pop(batch_export_fields.index({\"expression\": \"created_at\", \"alias\": \"created_at\"}))\n",
                "\n",
                "    # For historical reasons, 'set' and 'set_once' are prefixed with 'people_'.\n",
                "    set_field = batch_export_fields.pop(batch_export_fields.index(BatchExportField(expression=\"set\", alias=\"set\")))\n",
                "    set_field[\"alias\"] = \"people_set\"\n",
                "\n",
                "    set_once_field = batch_export_fields.pop(\n",
                "        batch_export_fields.index(BatchExportField(expression=\"set_once\", alias=\"set_once\"))\n",
                "    )\n",
                "    set_once_field[\"alias\"] = \"people_set_once\"\n",
                "\n",
                "    batch_export_fields.append(set_field)\n",
                "    batch_export_fields.append(set_once_field)\n",
                "\n",
                "    return batch_export_fields\n",
                "\n",
                "\n",
                "def get_snowflake_fields_from_record_schema(\n",
                "    record_schema: pa.Schema, known_variant_columns: list[str]\n",
                ") -> list[SnowflakeField]:\n",
                "    \"\"\"Generate a list of supported Snowflake fields from PyArrow schema.\n",
                "    This function is used to map custom schemas to Snowflake-supported types. Some loss\n",
                "    of precision is expected.\n",
                "\n",
                "    Arguments:\n",
                "        record_schema: The schema of a PyArrow RecordBatch from which we'll attempt to\n",
                "            derive Snowflake-supported types.\n",
                "        known_variant_columns: If a string type field is a known VARIANT column then use VARIANT\n",
                "            as its Snowflake type.\n",
                "    \"\"\"\n",
                "    snowflake_schema: list[SnowflakeField] = []\n",
                "\n",
                "    for name in record_schema.names:\n",
                "        pa_field = record_schema.field(name)\n",
                "\n",
                "        if pa.types.is_string(pa_field.type) or isinstance(pa_field.type, JsonType):\n",
                "            if pa_field.name in known_variant_columns:\n",
                "                snowflake_type = \"VARIANT\"\n",
                "            else:\n",
                "                snowflake_type = \"STRING\"\n",
                "\n",
                "        elif pa.types.is_binary(pa_field.type):\n",
                "            snowflake_type = \"BYNARY\"\n",
                "\n",
                "        elif pa.types.is_signed_integer(pa_field.type) or pa.types.is_unsigned_integer(pa_field.type):\n",
                "            snowflake_type = \"INTEGER\"\n",
                "\n",
                "        elif pa.types.is_floating(pa_field.type):\n",
                "            snowflake_type = \"FLOAT\"\n",
                "\n",
                "        elif pa.types.is_boolean(pa_field.type):\n",
                "            snowflake_type = \"BOOL\"\n",
                "\n",
                "        elif pa.types.is_timestamp(pa_field.type):\n",
                "            snowflake_type = \"TIMESTAMP\"\n",
                "\n",
                "        else:\n",
                "            raise TypeError(f\"Unsupported type: {pa_field.type}\")\n",
                "\n",
                "        snowflake_schema.append((name, snowflake_type))\n",
                "\n",
                "    return snowflake_schema\n",
                "\n",
                "\n",
                "@activity.defn\n",
                "async def insert_into_snowflake_activity(inputs: SnowflakeInsertInputs) -> RecordsCompleted:\n",
                "    \"\"\"Activity streams data from ClickHouse to Snowflake.\n",
                "\n",
                "    TODO: We're using JSON here, it's not the most efficient way to do this.\n",
                "    \"\"\"\n",
                "    logger = await bind_temporal_worker_logger(team_id=inputs.team_id, destination=\"Snowflake\")\n",
                "    logger.info(\n",
                "        \"Batch exporting range %s - %s to Snowflake: %s.%s.%s\",\n",
                "        inputs.data_interval_start,\n",
                "        inputs.data_interval_end,\n",
                "        inputs.database,\n",
                "        inputs.schema,\n",
                "        inputs.table_name,\n",
                "    )\n",
                "\n",
                "    async with (\n",
                "        Heartbeater() as heartbeater,\n",
                "        set_status_to_running_task(run_id=inputs.run_id, logger=logger),\n",
                "        get_client(team_id=inputs.team_id) as client,\n",
                "    ):\n",
                "        if not await client.is_alive():\n",
                "            raise ConnectionError(\"Cannot establish connection to ClickHouse\")\n",
                "\n",
                "        should_resume, details = await should_resume_from_activity_heartbeat(\n",
                "            activity, SnowflakeHeartbeatDetails, logger\n",
                "        )\n",
                "\n",
                "        if should_resume is True and details is not None:\n",
                "            data_interval_start = details.last_inserted_at.isoformat()\n",
                "            current_flush_counter = details.file_no\n",
                "        else:\n",
                "            data_interval_start = inputs.data_interval_start\n",
                "            current_flush_counter = 0\n",
                "\n",
                "        rows_exported = get_rows_exported_metric()\n",
                "        bytes_exported = get_bytes_exported_metric()\n",
                "\n",
                "        model: BatchExportModel | BatchExportSchema | None = None\n",
                "        if inputs.batch_export_schema is None and \"batch_export_model\" in {\n",
                "            field.name for field in dataclasses.fields(inputs)\n",
                "        }:\n",
                "            model = inputs.batch_export_model\n",
                "        else:\n",
                "            model = inputs.batch_export_schema\n",
                "\n",
                "        records_iterator = iter_model_records(\n",
                "            client=client,\n",
                "            model=model,\n",
                "            team_id=inputs.team_id,\n",
                "            interval_start=data_interval_start,\n",
                "            interval_end=inputs.data_interval_end,\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            destination_default_fields=snowflake_default_fields(),\n",
                "            is_backfill=inputs.is_backfill,\n",
                "        )\n",
                "        first_record_batch, records_iterator = await apeek_first_and_rewind(records_iterator)\n",
                "\n",
                "        if first_record_batch is None:\n",
                "            return 0\n",
                "\n",
                "        known_variant_columns = [\"properties\", \"people_set\", \"people_set_once\", \"person_properties\"]\n",
                "        first_record_batch = cast_record_batch_json_columns(first_record_batch, json_columns=known_variant_columns)\n",
                "\n",
                "        if model is None or (isinstance(model, BatchExportModel) and model.name == \"events\"):\n",
                "            table_fields = [\n",
                "                (\"uuid\", \"STRING\"),\n",
                "                (\"event\", \"STRING\"),\n",
                "                (\"properties\", \"VARIANT\"),\n",
                "                (\"elements\", \"VARIANT\"),\n",
                "                (\"people_set\", \"VARIANT\"),\n",
                "                (\"people_set_once\", \"VARIANT\"),\n",
                "                (\"distinct_id\", \"STRING\"),\n",
                "                (\"team_id\", \"INTEGER\"),\n",
                "                (\"ip\", \"STRING\"),\n",
                "                (\"site_url\", \"STRING\"),\n",
                "                (\"timestamp\", \"TIMESTAMP\"),\n",
                "            ]\n",
                "\n",
                "        else:\n",
                "            column_names = [column for column in first_record_batch.schema.names if column != \"_inserted_at\"]\n",
                "            record_schema = first_record_batch.select(column_names).schema\n",
                "            table_fields = get_snowflake_fields_from_record_schema(\n",
                "                record_schema,\n",
                "                known_variant_columns=known_variant_columns,\n",
                "            )\n",
                "\n",
                "        requires_merge = (\n",
                "            isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n",
                "        )\n"
            ],
            {
                "type": "insert",
                "before": [],
                "after": [
                    "        data_interval_end_str = dt.datetime.fromisoformat(inputs.data_interval_end).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                    "        stagle_table_name = (\n",
                    "            f\"stage_{inputs.table_name}_{data_interval_end_str}\" if requires_merge else inputs.table_name\n",
                    "        )\n"
                ],
                "parent_version_range": {
                    "start": 608,
                    "end": 608
                },
                "child_version_range": {
                    "start": 608,
                    "end": 612
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "async with (\n        Heartbeater() as heartbeater,\n        set_status_to_running_task(run_id=inputs.run_id, logger=logger),\n        get_client(team_id=inputs.team_id) as client,\n    ):",
                        "start_line": 533,
                        "end_line": 673
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "insert_into_snowflake_activity",
                        "signature": "def insert_into_snowflake_activity(inputs: SnowflakeInsertInputs)->RecordsCompleted:",
                        "at_line": 518
                    }
                ],
                "idx": 3,
                "hunk_diff": "File: posthog/temporal/batch_exports/snowflake_batch_export.py\nCode:\n           def insert_into_snowflake_activity(inputs: SnowflakeInsertInputs)->RecordsCompleted:\n               ...\n605 605            requires_merge = (\n606 606                isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n607 607            )\n    608  +         data_interval_end_str = dt.datetime.fromisoformat(inputs.data_interval_end).strftime(\"%Y-%m-%d_%H-%M-%S\")\n    609  +         stagle_table_name = (\n    610  +             f\"stage_{inputs.table_name}_{data_interval_end_str}\" if requires_merge else inputs.table_name\n    611  +         )\n608 612    \n         ...\n",
                "file_path": "posthog/temporal/batch_exports/snowflake_batch_export.py",
                "identifiers_before": [],
                "identifiers_after": [
                    "data_interval_end",
                    "data_interval_end_str",
                    "datetime",
                    "dt",
                    "fromisoformat",
                    "inputs",
                    "requires_merge",
                    "stagle_table_name",
                    "strftime",
                    "table_name"
                ],
                "prefix": [
                    "        requires_merge = (\n",
                    "            isinstance(inputs.batch_export_model, BatchExportModel) and inputs.batch_export_model.name == \"persons\"\n",
                    "        )\n"
                ],
                "suffix": [
                    "\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "\n"
            ],
            {
                "type": "delete",
                "before": [
                    "        stagle_table_name = f\"stage_{inputs.table_name}\" if requires_merge else inputs.table_name\n"
                ],
                "after": [],
                "parent_version_range": {
                    "start": 609,
                    "end": 610
                },
                "child_version_range": {
                    "start": 613,
                    "end": 613
                },
                "control_flow": [
                    {
                        "type": "with_statement",
                        "statement": "async with (\n        Heartbeater() as heartbeater,\n        set_status_to_running_task(run_id=inputs.run_id, logger=logger),\n        get_client(team_id=inputs.team_id) as client,\n    ):",
                        "start_line": 533,
                        "end_line": 673
                    }
                ],
                "structural_path": [
                    {
                        "type": "function",
                        "name": "insert_into_snowflake_activity",
                        "signature": "def insert_into_snowflake_activity(inputs: SnowflakeInsertInputs)->RecordsCompleted:",
                        "at_line": 518
                    }
                ],
                "idx": 4,
                "hunk_diff": "File: posthog/temporal/batch_exports/snowflake_batch_export.py\nCode:\n           def insert_into_snowflake_activity(inputs: SnowflakeInsertInputs)->RecordsCompleted:\n               ...\n608 612    \n609      -         stagle_table_name = f\"stage_{inputs.table_name}\" if requires_merge else inputs.table_name\n610 613            async with SnowflakeClient.from_inputs(inputs).connect() as snow_client:\n611 614                async with (\n612 615                    snow_client.managed_table(inputs.table_name, table_fields, delete=False) as snow_table,\n         ...\n",
                "file_path": "posthog/temporal/batch_exports/snowflake_batch_export.py",
                "identifiers_before": [
                    "inputs",
                    "requires_merge",
                    "stagle_table_name",
                    "table_name"
                ],
                "identifiers_after": [],
                "prefix": [
                    "\n"
                ],
                "suffix": [
                    "        async with SnowflakeClient.from_inputs(inputs).connect() as snow_client:\n",
                    "            async with (\n",
                    "                snow_client.managed_table(inputs.table_name, table_fields, delete=False) as snow_table,\n"
                ],
                "base_dependency_callee": [],
                "base_dependency_caller": [],
                "head_dependency_callee": [],
                "head_dependency_caller": [],
                "other_clones": []
            },
            [
                "        async with SnowflakeClient.from_inputs(inputs).connect() as snow_client:\n",
                "            async with (\n",
                "                snow_client.managed_table(inputs.table_name, table_fields, delete=False) as snow_table,\n",
                "                snow_client.managed_table(\n",
                "                    stagle_table_name, table_fields, create=requires_merge, delete=requires_merge\n",
                "                ) as snow_stage_table,\n",
                "            ):\n",
                "                record_columns = [field[0] for field in table_fields]\n",
                "                record_schema = pa.schema(\n",
                "                    [field.with_nullable(True) for field in first_record_batch.select(record_columns).schema]\n",
                "                )\n",
                "\n",
                "                async def flush_to_snowflake(\n",
                "                    local_results_file,\n",
                "                    records_since_last_flush,\n",
                "                    bytes_since_last_flush,\n",
                "                    flush_counter: int,\n",
                "                    last_inserted_at,\n",
                "                    last: bool,\n",
                "                    error: Exception | None,\n",
                "                ):\n",
                "                    logger.info(\n",
                "                        \"Putting %sfile %s containing %s records with size %s bytes\",\n",
                "                        \"last \" if last else \"\",\n",
                "                        flush_counter,\n",
                "                        records_since_last_flush,\n",
                "                        bytes_since_last_flush,\n",
                "                    )\n",
                "\n",
                "                    table = snow_stage_table if requires_merge else snow_table\n",
                "\n",
                "                    await snow_client.put_file_to_snowflake_table(local_results_file, table, flush_counter)\n",
                "                    rows_exported.add(records_since_last_flush)\n",
                "                    bytes_exported.add(bytes_since_last_flush)\n",
                "\n",
                "                    heartbeater.details = (str(last_inserted_at), flush_counter)\n",
                "\n",
                "                writer = JSONLBatchExportWriter(\n",
                "                    max_bytes=settings.BATCH_EXPORT_SNOWFLAKE_UPLOAD_CHUNK_SIZE_BYTES,\n",
                "                    flush_callable=flush_to_snowflake,\n",
                "                )\n",
                "\n",
                "                async with writer.open_temporary_file(current_flush_counter):\n",
                "                    async for record_batch in records_iterator:\n",
                "                        record_batch = cast_record_batch_json_columns(record_batch, json_columns=known_variant_columns)\n",
                "\n",
                "                        await writer.write_record_batch(record_batch)\n",
                "\n",
                "                await snow_client.copy_loaded_files_to_snowflake_table(\n",
                "                    snow_stage_table if requires_merge else snow_table\n",
                "                )\n",
                "                if requires_merge:\n",
                "                    merge_key = (\n",
                "                        (\"team_id\", \"INT64\"),\n",
                "                        (\"distinct_id\", \"STRING\"),\n",
                "                    )\n",
                "                    await snow_client.amerge_person_tables(\n",
                "                        final_table=snow_table,\n",
                "                        stage_table=snow_stage_table,\n",
                "                        update_when_matched=table_fields,\n",
                "                        merge_key=merge_key,\n",
                "                    )\n",
                "\n",
                "                return writer.records_total\n",
                "\n",
                "\n",
                "@workflow.defn(name=\"snowflake-export\", failure_exception_types=[workflow.NondeterminismError])\n",
                "class SnowflakeBatchExportWorkflow(PostHogWorkflow):\n",
                "    \"\"\"A Temporal Workflow to export ClickHouse data into Snowflake.\n",
                "\n",
                "    This Workflow is intended to be executed both manually and by a Temporal\n",
                "    Schedule. When ran by a schedule, `data_interval_end` should be set to\n",
                "    `None` so that we will fetch the end of the interval from the Temporal\n",
                "    search attribute `TemporalScheduledStartTime`.\n",
                "    \"\"\"\n",
                "\n",
                "    @staticmethod\n",
                "    def parse_inputs(inputs: list[str]) -> SnowflakeBatchExportInputs:\n",
                "        \"\"\"Parse inputs from the management command CLI.\"\"\"\n",
                "        loaded = json.loads(inputs[0])\n",
                "        return SnowflakeBatchExportInputs(**loaded)\n",
                "\n",
                "    @workflow.run\n",
                "    async def run(self, inputs: SnowflakeBatchExportInputs):\n",
                "        \"\"\"Workflow implementation to export data to Snowflake table.\"\"\"\n",
                "        data_interval_start, data_interval_end = get_data_interval(inputs.interval, inputs.data_interval_end)\n",
                "\n",
                "        start_batch_export_run_inputs = StartBatchExportRunInputs(\n",
                "            team_id=inputs.team_id,\n",
                "            batch_export_id=inputs.batch_export_id,\n",
                "            data_interval_start=data_interval_start.isoformat(),\n",
                "            data_interval_end=data_interval_end.isoformat(),\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            is_backfill=inputs.is_backfill,\n",
                "        )\n",
                "        run_id = await workflow.execute_activity(\n",
                "            start_batch_export_run,\n",
                "            start_batch_export_run_inputs,\n",
                "            start_to_close_timeout=dt.timedelta(minutes=5),\n",
                "            retry_policy=RetryPolicy(\n",
                "                initial_interval=dt.timedelta(seconds=10),\n",
                "                maximum_interval=dt.timedelta(seconds=60),\n",
                "                maximum_attempts=0,\n",
                "                non_retryable_error_types=[\"NotNullViolation\", \"IntegrityError\"],\n",
                "            ),\n",
                "        )\n",
                "\n",
                "        finish_inputs = FinishBatchExportRunInputs(\n",
                "            id=run_id,\n",
                "            batch_export_id=inputs.batch_export_id,\n",
                "            status=BatchExportRun.Status.COMPLETED,\n",
                "            team_id=inputs.team_id,\n",
                "        )\n",
                "\n",
                "        insert_inputs = SnowflakeInsertInputs(\n",
                "            team_id=inputs.team_id,\n",
                "            user=inputs.user,\n",
                "            password=inputs.password,\n",
                "            account=inputs.account,\n",
                "            warehouse=inputs.warehouse,\n",
                "            database=inputs.database,\n",
                "            schema=inputs.schema,\n",
                "            table_name=inputs.table_name,\n",
                "            data_interval_start=data_interval_start.isoformat(),\n",
                "            data_interval_end=data_interval_end.isoformat(),\n",
                "            role=inputs.role,\n",
                "            exclude_events=inputs.exclude_events,\n",
                "            include_events=inputs.include_events,\n",
                "            run_id=run_id,\n",
                "            is_backfill=inputs.is_backfill,\n",
                "            batch_export_model=inputs.batch_export_model,\n",
                "            batch_export_schema=inputs.batch_export_schema,\n",
                "        )\n",
                "\n",
                "        await execute_batch_export_insert_activity(\n",
                "            insert_into_snowflake_activity,\n",
                "            insert_inputs,\n",
                "            interval=inputs.interval,\n",
                "            non_retryable_error_types=[\n",
                "                # Raised when we cannot connect to Snowflake.\n",
                "                \"DatabaseError\",\n",
                "                # Raised by Snowflake when a query cannot be compiled.\n",
                "                # Usually this means we don't have table permissions or something doesn't exist (db, schema).\n",
                "                \"ProgrammingError\",\n",
                "                # Raised by Snowflake with an incorrect account name.\n",
                "                \"ForbiddenError\",\n",
                "            ],\n",
                "            finish_inputs=finish_inputs,\n",
                "        )"
            ]
        ]
    },
    "edit_order": [
        [
            0,
            1,
            2,
            3,
            4
        ]
    ],
    "partial_orders": [
        {
            "edit_hunk_pair": [
                0,
                1
            ],
            "edit_order": "no relation",
            "reason": "because not identical"
        },
        {
            "edit_hunk_pair": [
                0,
                2
            ],
            "edit_order": "no relation",
            "reason": "because not identical"
        },
        {
            "edit_hunk_pair": [
                0,
                3
            ],
            "edit_order": "no relation",
            "reason": "because not identical"
        },
        {
            "edit_hunk_pair": [
                0,
                4
            ],
            "edit_order": "no relation",
            "reason": "because not identical"
        },
        {
            "edit_hunk_pair": [
                1,
                2
            ],
            "edit_order": "bi-directional",
            "reason": "code clones",
            "scenario of 0 -> 1": "user are editing code clones in batch, encounter edit 0 first, then found edit 1",
            "scenario of 1 -> 0": "user are editing code clones in batch, encounter edit 1 first, then found edit 0"
        },
        {
            "edit_hunk_pair": [
                1,
                3
            ],
            "edit_order": "no relation",
            "reason": "because not identical"
        },
        {
            "edit_hunk_pair": [
                1,
                4
            ],
            "edit_order": "no relation",
            "reason": "because not identical"
        },
        {
            "edit_hunk_pair": [
                2,
                3
            ],
            "edit_order": "no relation",
            "reason": "because not identical"
        },
        {
            "edit_hunk_pair": [
                2,
                4
            ],
            "edit_order": "no relation",
            "reason": "because not identical"
        },
        {
            "edit_hunk_pair": [
                3,
                4
            ],
            "edit_order": "no relation",
            "reason": "because not identical"
        }
    ]
}